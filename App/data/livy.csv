Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Î£ Original Estimate,Î£ Remaining Estimate,Î£ Time Spent,Security Level,Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Child-Issue),Inward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Outward issue link (Child-Issue),Inward issue link (Completes),Outward issue link (Completes),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Supercedes),Outward issue link (Supercedes),Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Fix Content-Types for nosniff header,LIVY-1026,13636159,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,gyogal,gyogal,gyogal,06/Dec/25 18:05,07/Dec/25 04:04,19/Dec/25 04:15,07/Dec/25 03:53,,,,0.9.0,,,,,,,,,,,,0,,,,,,"In LIVY-785 the `X-Content-Type-Options: nosniff` HTTP header was added, however this breaks transferring some files where the Content-Type is not correctly specified.",,"gyogal opened a new pull request, #502:
URL: https://github.com/apache/incubator-livy/pull/502

   ## What changes were proposed in this pull request?
   
   In LIVY-785 the `X-Content-Type-Options: nosniff` HTTP header was added, however this breaks transferring some files where the Content-Type is not correctly specified.
   
   ## How was this patch tested?
   
   Tested by running the server locally.


;06/Dec/25 18:11;githubbot;600","gyogal merged PR #502:
URL: https://github.com/apache/incubator-livy/pull/502


;07/Dec/25 03:53;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Dec 07 04:04:01 UTC 2025,,,,,,,,,,"0|z1yxi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Dec/25 04:04;gyogal;PR: https://github.com/apache/incubator-livy/pull/502;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup Python2InterpreterSpec post Python 2 deprecation,LIVY-1025,13635962,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,arnavb123,arnavbalyan,arnavbalyan,04/Dec/25 16:03,04/Dec/25 20:24,19/Dec/25 04:15,04/Dec/25 20:24,,,,0.10.0,,,,,,,,,,,,0,,,,,,Removes stray test left over from python2 deprecationÂ ,,"ArnavBalyan opened a new pull request, #501:
URL: https://github.com/apache/incubator-livy/pull/501

   ## What changes were proposed in this pull request?
   - Cleans up Python2InterpreterSpec since python 2 is deprecated.
   - Also removed the Python2SessionSpec counterpart
   
   ## How was this patch tested?
    - CI
    
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   


;04/Dec/25 16:11;githubbot;600","gyogal merged PR #501:
URL: https://github.com/apache/incubator-livy/pull/501


;04/Dec/25 20:23;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 04 20:24:21 UTC 2025,,,,,,,,,,"0|z1ywa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Dec/25 20:24;gyogal;PR: https://github.com/apache/incubator-livy/pull/501;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Livy to Python 3,LIVY-1024,13635910,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,arnavb123,arnavbalyan,arnavbalyan,04/Dec/25 04:50,04/Dec/25 15:05,19/Dec/25 04:15,04/Dec/25 15:05,,,,0.10.0,,,,,,,,,,,,0,,,,,,"Since Spark 2 is deprecated, upgrading Livy to Python3Â ",,"gyogal merged PR #485:
URL: https://github.com/apache/incubator-livy/pull/485


;04/Dec/25 15:04;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 04 15:05:36 UTC 2025,,,,,,,,,,"0|z1yvyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Dec/25 15:05;gyogal;PR: https://github.com/apache/incubator-livy/pull/485;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
init script missing quote,LIVY-1023,13635875,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,appodictic,appodictic,appodictic,03/Dec/25 17:13,16/Dec/25 19:31,19/Dec/25 04:15,03/Dec/25 20:34,,,,0.9.0,,,,,,,,,,,,0,,,,,,https://github.com/apache/incubator-livy/pull/500,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2025-12-03 17:13:28.0,,,,,,,,,,"0|z1yvqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix failing UT due to disk usage issues,LIVY-1022,13635637,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,arnavb123,arnavbalyan,arnavbalyan,01/Dec/25 16:37,01/Dec/25 19:58,19/Dec/25 04:15,01/Dec/25 19:58,,,,0.10.0,,,,,,,,,,,,0,,,,,,"JobApiIT.scala fails due to ~92% disk usage causing the node to be marked unhealthy, ensure that UT can go through with this.Â ",,"gyogal merged PR #498:
URL: https://github.com/apache/incubator-livy/pull/498


;01/Dec/25 19:57;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 01 17:05:15 UTC 2025,,,,,,,,,,"0|z1yua0:",9223372036854775807,,,,,,,,,,,,,,,,,,,"01/Dec/25 17:05;gyogal;A pull request is available at https://github.com/apache/incubator-livy/pull/498;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing Apache Links on Website,LIVY-1020,13632422,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,lmccay,lmccay,lmccay,24/Oct/25 17:48,06/Dec/25 20:35,19/Dec/25 04:15,06/Dec/25 20:18,,,,,,Docs,,,,,,,,,,0,graduation,,,,,"""https://whimsy.apache.org/pods/ shows you missing a spot for the ASF events and the privacy link""

We can just use the ASF security link for QU30 in the maturity model.
",,"lmccay opened a new pull request, #30:
URL: https://github.com/apache/incubator-livy-website/pull/30

   This change adds missing links for Privacy and Events to the Apache navigation menu as required for graduation.
   


;06/Dec/25 19:49;githubbot;600","lmccay merged PR #30:
URL: https://github.com/apache/incubator-livy-website/pull/30


;06/Dec/25 19:49;githubbot;600","lmccay opened a new pull request, #31:
URL: https://github.com/apache/incubator-livy-website/pull/31

   The release process doc was not being referenced in the site links so unless you knew it was there and manually navigated to it, it was hidden.


;06/Dec/25 20:35;githubbot;600","lmccay merged PR #31:
URL: https://github.com/apache/incubator-livy-website/pull/31


;06/Dec/25 20:35;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Dec 06 20:17:59 UTC 2025,,,,,,,,,,"0|z1yam8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Dec/25 19:25;lmccay;Okay, double checked that the site includes a Security link to the Apache Security Page.
That is done already.;;;","06/Dec/25 20:17;lmccay;I have updated the live site with links to all the missing required pages.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate Spark 2 Support ,LIVY-1019,13632380,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,arnavb123,arnavb123,arnavb123,24/Oct/25 09:15,14/Nov/25 01:47,19/Dec/25 04:15,14/Nov/25 01:47,,,,0.10.0,,,,,,,,,,,,0,,,,,,Tracking Spark 2 deprecation,,"ArnavBalyan opened a new pull request, #489:
URL: https://github.com/apache/incubator-livy/pull/489

   ## What changes were proposed in this pull request?
    - Spark 2.x is at end of life and no longer supported, newer Spark versions (Spark 4) are now available.
    - Deprecate Spark 2.x from Livy
    
   ## How was this patch tested?
    - CI and local builds are successful
   


;24/Oct/25 09:20;githubbot;600","ArnavBalyan commented on PR #489:
URL: https://github.com/apache/incubator-livy/pull/489#issuecomment-3442209917

   cc @gyogal @lmccay could you ptal thanks!


;24/Oct/25 09:48;githubbot;600","gyogal commented on PR #489:
URL: https://github.com/apache/incubator-livy/pull/489#issuecomment-3444566740

   Thank you for your PR @ArnavBalyan! This could be an important step of cleaning up unneeded features in an upcoming release. The change is quite big, so it will take some time to review. In the meantime, can I ask why you chose Spark version 3.3.4 in the main `pom.xml` and in `README.md`? The latest version should work with 3.5.6, but Spark 3.5.7 is already available as well.


;24/Oct/25 19:17;githubbot;600","ArnavBalyan commented on PR #489:
URL: https://github.com/apache/incubator-livy/pull/489#issuecomment-3444656331

   > Thank you for your PR @ArnavBalyan! This could be an important step of cleaning up unneeded features in an upcoming release. The change is quite big, so it will take some time to review. In the meantime, can I ask why you chose Spark version 3.3.4 in the main `pom.xml` and in `README.md`? The latest version should work with 3.5.6, but Spark 3.5.7 is already available as well.
   
   Hi @gyogal thanks so much, really appreciate it. For Spark, looks like `3.3.4` was the latest version that shipped hadoop 2 binaries. Moving to 3.5.7 may need us to bump to hadoop 3, was a bit unsure if this would have any side effects since this was getting a considerably large change. Let me know if we are comfortable directly moving to hadoop 3 happy to update. Thanks for taking a look


;24/Oct/25 19:38;githubbot;600","gyogal commented on PR #489:
URL: https://github.com/apache/incubator-livy/pull/489#issuecomment-3444753257

   > For Spark, looks like `3.3.4` was the latest version that shipped hadoop 2 binaries. Moving to 3.5.7 may need us to bump to hadoop 3, was a bit unsure if this would have any side effects since this was getting a considerably large change. Let me know if we are comfortable directly moving to hadoop 3 happy to update. Thanks for taking a look
   
   Oh right, it makes sense now, thanks! In #478 it has already been changed to version 3 in the `spark3` profile, so I think that should be OK. Alternatively, it could be done in a follow-up change later on.


;24/Oct/25 20:11;githubbot;600","ArnavBalyan commented on PR #489:
URL: https://github.com/apache/incubator-livy/pull/489#issuecomment-3448438244

   > > For Spark, looks like `3.3.4` was the latest version that shipped hadoop 2 binaries. Moving to 3.5.7 may need us to bump to hadoop 3, was a bit unsure if this would have any side effects since this was getting a considerably large change. Let me know if we are comfortable directly moving to hadoop 3 happy to update. Thanks for taking a look
   > 
   > Oh right, it makes sense now, thanks! In #478 it has already been changed to version 3 in the `spark3` profile, so I think that should be OK. Alternatively, it could be done in a follow-up change later on.
   
   Thanks @gyogal I can bump it up as a follow up change if it looks good to you?


;26/Oct/25 11:27;githubbot;600","ArnavBalyan commented on PR #489:
URL: https://github.com/apache/incubator-livy/pull/489#issuecomment-3454106861

   cc @gyogal gentle reminder thanks! ðŸ˜€


;28/Oct/25 01:50;githubbot;600","gyogal commented on PR #489:
URL: https://github.com/apache/incubator-livy/pull/489#issuecomment-3481723877

   @ArnavBalyan , I think both this and the Python 3 commit could be merged after `branch-0.9` is created. That would mean that the upcoming 0.9 release would still retain support, but the cleanup work can be started in preparation for the release afterwards. Please let me know what you think!


;03/Nov/25 17:35;githubbot;600","ArnavBalyan commented on PR #489:
URL: https://github.com/apache/incubator-livy/pull/489#issuecomment-3486239496

   Hi @gyogal thanks for taking a look! Makes perfect sense, will wait until we do the release. Let me know if you need support with release


;04/Nov/25 14:18;githubbot;600","gyogal commented on PR #489:
URL: https://github.com/apache/incubator-livy/pull/489#issuecomment-3508957784

   Hi @ArnavBalyan , the main Livy version has been updated to 0.10-incubating-SNAPSHOT, however this caused some conflicts in your PR. Could you please resolve them? Once that is done, I think this could be merged into version 0.10-incubating.


;09/Nov/25 23:47;githubbot;600","ArnavBalyan commented on PR #489:
URL: https://github.com/apache/incubator-livy/pull/489#issuecomment-3527359027

   Thanks @gyogal! Will rebase it shortly


;13/Nov/25 11:24;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,6600,,,0,6600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 14 01:47:06 UTC 2025,,,,,,,,,,"0|z1yacw:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Nov/25 01:47;gyogal;Issue resolved by the PR at https://github.com/apache/incubator-livy/pull/489;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extract field comments from Spark schema metadata and set in Datatypeutils,LIVY-1018,13630284,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,arnavb123,arnavb123,arnavb123,30/Sep/25 04:04,07/Oct/25 14:32,19/Dec/25 04:15,07/Oct/25 14:30,,,,0.9.0,,,,,,,,,,,,0,,,,,,"* Remove the todo comment and pass the actual comment metadata for complexToDataType and schemaFromSparkJson
Â ",,"ArnavBalyan commented on PR #482:
URL: https://github.com/apache/incubator-livy/pull/482#issuecomment-3349865985

   > @ArnavBalyan Thanks for adding the JIRA ticket to the PR description, however is LIVY-483 related to this issue? (It seems to be about sparkR memory configuration.) If a new issue is needed and you don't have a JIRA user yet, could you please request one?
   
   @gyogal updated could you PTAL thanks


;30/Sep/25 04:05;githubbot;600","ArnavBalyan commented on PR #482:
URL: https://github.com/apache/incubator-livy/pull/482#issuecomment-3361991283

   cc @gyogal gentle reminder thanks!


;02/Oct/25 16:01;githubbot;600","gyogal commented on PR #482:
URL: https://github.com/apache/incubator-livy/pull/482#issuecomment-3368241783

   Hi @ArnavBalyan , I was able to confirm that with your change the comments are populated for struct member fields on the server side, however I was not able to find a way to read this information with a JDBC client. Could you please let me know which client or API would allow me to check this on the client side?


;04/Oct/25 13:37;githubbot;600","ArnavBalyan commented on PR #482:
URL: https://github.com/apache/incubator-livy/pull/482#issuecomment-3370403058

   Hi @gyogal I've added a UT which should be able to e2e test the change thanks


;06/Oct/25 08:08;githubbot;600","ArnavBalyan commented on PR #482:
URL: https://github.com/apache/incubator-livy/pull/482#issuecomment-3375067306

   > Thank you for adding tests! Please let me know if this is the final version of your change.
   > 
   > Just a note, it would be even better if there was a way to extract and validate the ""Person ID"", ""Person name"", etc. comments from the struct members.
   
   Thanks, this is the final version of the change, I'll check if we can add recursive extraction in a future change


;07/Oct/25 03:34;githubbot;600","gyogal commented on PR #482:
URL: https://github.com/apache/incubator-livy/pull/482#issuecomment-3377139674

   Sounds good! Thank you @ArnavBalyan once again for your contribution!


;07/Oct/25 14:23;githubbot;600","gyogal merged PR #482:
URL: https://github.com/apache/incubator-livy/pull/482


;07/Oct/25 14:30;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4200,,,0,4200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 07 14:32:15 UTC 2025,,,,,,,,,,"0|z1xxf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Oct/25 14:32;gyogal;PR at https://github.com/apache/incubator-livy/issues/483;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Support Java 17, set up JDK 17 based tests",LIVY-1017,13622773,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,gyogal,gyogal,gyogal,08/Jul/25 07:25,22/Oct/25 15:17,19/Dec/25 04:15,22/Oct/25 15:17,,,,0.9.0,,,,,,,,,,,,0,,,,,,"Enable Livy to run with Java 17, set up unit test and integration test runs with JDK 17",,"gyogal opened a new pull request, #481:
URL: https://github.com/apache/incubator-livy/pull/481

   ## What changes were proposed in this pull request?
   
   Enable Livy to run with Java 17, set up unit test and integration test runs with JDK 17
   
   ## How was this patch tested?
   
   Unit and integration tests.


;12/Aug/25 19:25;githubbot;600","ArnavBalyan commented on PR #481:
URL: https://github.com/apache/incubator-livy/pull/481#issuecomment-3425919011

   Hi @gyogal, thanks for working on this, the change looks good to me. I was planning to take up the Java 8 deprecation next, so I just wanted to check if youâ€™re planning to land this soon? looks like a test is still failing. Thanks!


;21/Oct/25 10:46;githubbot;600","gyogal commented on PR #481:
URL: https://github.com/apache/incubator-livy/pull/481#issuecomment-3426885060

   Thank you very much for the review, @ArnavBalyan ! The checks are currently failing because the test Docker image does not contain JDK 17 yet, it is this change that adds it. After the merge the tests should look fine (I will confirm this and fix it if that is not the case).
   
   Java 8 deprecation would be a great addition, thanks for looking into it! I think anything that impacts compatibility (Python 2, Spark 2, JDK 8 and general cleanup removing old features) could be done in the release following the next one.


;21/Oct/25 14:10;githubbot;600","ArnavBalyan commented on PR #481:
URL: https://github.com/apache/incubator-livy/pull/481#issuecomment-3431049012

   Thanks so much! I'll wait for this PR to land JDK 17, and proceed with the Java 8 deprecation cleanup, let me check the Spark 2 and Python2, there is a PR for Py2 deprecation which should be feasible once Spark 2 is deprecated. 


;22/Oct/25 08:24;githubbot;600","gyogal merged PR #481:
URL: https://github.com/apache/incubator-livy/pull/481


;22/Oct/25 14:29;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 22 15:16:54 UTC 2025,,,,,,,,,,"0|z1wnf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Oct/25 15:16;gyogal;PR is available at https://github.com/apache/incubator-livy/pull/481;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use SslContextFactory.Server() instead of SslContextFactory,LIVY-1012,13612811,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,surendralilhore,surendralilhore,surendralilhore,24/Mar/25 07:12,24/Apr/25 07:55,19/Dec/25 04:15,23/Apr/25 13:49,0.5.0,,,0.9.0,,Server,,,,,,,,,,0,,,,,,"With Jetty 9.4.50, we should call SslContextFactory.Server(), instead of SslContextFactory(), to create SslContextFactory. Otherwise we get the following error when using a KeyStore with multiple certificates in it.
{code:java}
Exception in thread ""main"" java.lang.IllegalStateException: KeyStores with multiple certificates are not supported on the base class org.eclipse.jetty.util.ssl.SslContextFactory. (Use org.eclipse.jetty.util.ssl.
SslContextFactory$Server or org.eclipse.jetty.util.ssl.SslContextFactory$Client instead)
at org.eclipse.jetty.util.ssl.SslContextFactory.newSniX509ExtendedKeyManager(SslContextFactory.java:1289){code}",,"surendralilhore opened a new pull request, #471:
URL: https://github.com/apache/incubator-livy/pull/471

   ## What changes were proposed in this pull request?
   
   Use SslContextFactory.Server() instead of SslContextFactory() when constructing the factory.
   
   ## How was this patch tested?
   
   With Jetty 9.4.50, we should call SslContextFactory.Server(), instead of SslContextFactory(), to create SslContextFactory. Otherwise we get the following error when using a KeyStore with multiple certificates in it.
   
   ```
   Exception in thread ""main"" java.lang.IllegalStateException: KeyStores with multiple certificates are not supported on the base class org.eclipse.jetty.util.ssl.SslContextFactory. (Use org.eclipse.jetty.util.ssl.
   SslContextFactory$Server or org.eclipse.jetty.util.ssl.SslContextFactory$Client instead)
   at org.eclipse.jetty.util.ssl.SslContextFactory.newSniX509ExtendedKeyManager(SslContextFactory.java:1289)
   ```
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   


;24/Mar/25 07:14;githubbot;600","surendralilhore commented on PR #471:
URL: https://github.com/apache/incubator-livy/pull/471#issuecomment-2824140921

   Thank you, @gyogal , for the review. I have addressed your comment.
   
   Similar fix is present in [hive](https://github.com/apache/hive/blob/28abf17424e97112d75947f1d7a2f043b3961fa4/service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpCLIService.java#L163), and `ThriftHttpCLIService.scala` copy of hive code.


;23/Apr/25 12:34;githubbot;600","gyogal merged PR #471:
URL: https://github.com/apache/incubator-livy/pull/471


;23/Apr/25 13:46;githubbot;600","gyogal commented on PR #471:
URL: https://github.com/apache/incubator-livy/pull/471#issuecomment-2824379257

   Thank you for your contribution @surendralilhore !


;23/Apr/25 13:51;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 24 07:55:52 UTC 2025,,,,,,,,,,"0|z1uxz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Apr/25 04:07;surendralilhore;[~gyogal] , pls can you assign this Jira to me.;;;","24/Apr/25 07:55;gyogal;Done, thanks [~surendralilhore] !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Upgrade CI environment, fix failing PythonInterpreterSpec and integration tests with Python 3.9+",LIVY-1011,13606462,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,gyogal,gyogal,gyogal,28/Jan/25 09:53,02/Apr/25 16:05,19/Dec/25 04:15,02/Apr/25 16:05,0.9.0,,,0.9.0,,,,,,,,,,,,0,,,,,,"The CI environment broke due to library version conflicts between the image and the checkout action.

The livy-ci Docker image needs to be upgraded to a more recent version of Ubuntu, ensuring that the unit tests all pass. This includes fixing the issues with PythonInterpreterSpec with Python 3.9+ and other tests that fail to run with Spark 3.",,"gyogal opened a new pull request, #467:
URL: https://github.com/apache/incubator-livy/pull/467

   ## What changes were proposed in this pull request?
   
   The CI environment is broken due to library version conflicts between the image and the checkout action.
   
   The livy-ci Docker image needs to be upgraded to a more recent version of Ubuntu, ensuring that the unit tests and integration tests all pass.
   
   Due to a bug in Livy's GitHub workflows, tests had only been executed with Spark 2. Now that this is fixed, we also need to make updates to some of the Python-based tests to work with Python 3 because Spark 3.2 does not support Python 2.
   
   Two R-based integration tests are ignored in the Spark 2 integration test runs because SparkR 2 does not work with R 4, and the last version of Ubuntu with R 3.6 is Ubuntu 20. If we want to re-enable these tests, we may need to build R 3.6 for Ubuntu 24.
   
   ## How was this patch tested?
   
   CI and unit test runs in a private fork of the repo.


;10/Feb/25 15:05;githubbot;600","gyogal merged PR #467:
URL: https://github.com/apache/incubator-livy/pull/467


;02/Apr/25 11:23;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 10 15:07:56 UTC 2025,,,,,,,,,,"0|z1tv9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/Feb/25 15:07;gyogal;A PR is available at https://github.com/apache/incubator-livy/pull/467;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for Spark 3.5.6,LIVY-1010,13606170,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,gyogal,mnr,mnr,24/Jan/25 14:31,04/Nov/25 20:46,19/Dec/25 04:15,12/Aug/25 18:06,0.8.0,,,0.9.0,,,,,,,,,,,,0,,,,,,"It will be good to keep the Apache Livy project up to date with the latest Spark version (3.5.6).

Tried Apache Livy 0.8 for Spark 3.5.4 (with Java 11 and 17), and did not work as expected like Apache Livy 0.8 and Spark version 3.0.0.

Java 17, I was not able to start an Apache Livy session
{code:java}
Exception in thread ""main"" java.util.concurrent.ExecutionException: javax.security.sasl.SaslException: Client closed before SASL negotiation finished {code}
Java 11, I was able to start an Apache Livy session but running any sample code was throwing this exception
{code:java}
'JavaPackage' object is not callable
Traceback (most recent call last):
  File ""/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py"", line 1443, in createDataFrame
    return self._create_dataframe(
  File ""/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py"", line 1485, in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
  File ""/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py"", line 1093, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File ""/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py"", line 954, in _inferSchemaFromList
    prefer_timestamp_ntz = is_timestamp_ntz_preferred()
  File ""/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 153, in is_timestamp_ntz_preferred
    return jvm is not None and jvm.PythonSQLUtils.isTimestampNTZPreferred()
TypeError: 'JavaPackage' object is not callable {code}
The only way to make Spark 3.5.4 working with Java 11 is to issue some java_imports when a session starts
{code:java}
from py4j.java_gateway import java_import
java_import(spark._sc._jvm, ""org.apache.spark.SparkConf"")
java_import(spark._sc._jvm, ""org.apache.spark.api.java.*"")
java_import(spark._sc._jvm, ""org.apache.spark.api.python.*"")
java_import(spark._sc._jvm, ""org.apache.spark.ml.python.*"")
java_import(spark._sc._jvm, ""org.apache.spark.mllib.api.python.*"")
java_import(spark._sc._jvm, ""org.apache.spark.resource.*"")

java_import(spark._sc._jvm, ""org.apache.spark.sql.*"")
java_import(spark._sc._jvm, ""org.apache.spark.sql.api.python.*"")
java_import(spark._sc._jvm, ""org.apache.spark.sql.hive.*"") {code}",,"codecov-commenter commented on PR #478:
URL: https://github.com/apache/incubator-livy/pull/478#issuecomment-3121568457

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/478?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   :white_check_mark: All modified and coverable lines are covered by tests.
   :white_check_mark: Project coverage is 51.50%. Comparing base ([`86fc823`](https://app.codecov.io/gh/apache/incubator-livy/commit/86fc823893ee96d4effaa4f2b8ef6603cea9d77a?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)) to head ([`4793472`](https://app.codecov.io/gh/apache/incubator-livy/commit/47934721048c7fe689ad4201fc34bf1c9a224a7e?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)).
   :warning: Report is 15 commits behind head on master.
   > :exclamation:  There is a different number of reports uploaded between BASE (86fc823) and HEAD (4793472). Click for more details.
   > 
   > <details><summary>HEAD has 1 upload less than BASE</summary>
   >
   >| Flag | BASE (86fc823) | HEAD (4793472) |
   >|------|------|------|
   >||2|1|
   ></details>
   
   <details><summary>Additional details and impacted files</summary>
   
   
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #478       +/-   ##
   =============================================
   - Coverage     65.45%   51.50%   -13.95%     
   + Complexity      954      705      -249     
   =============================================
     Files           103      105        +2     
     Lines          6084     6751      +667     
     Branches        922     1017       +95     
   =============================================
   - Hits           3982     3477      -505     
   - Misses         1546     2736     +1190     
   + Partials        556      538       -18     
   ```
   </details>
   
   [:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/apache/incubator-livy/pull/478?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).   
   :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   <details><summary> :rocket: New features to boost your workflow: </summary>
   
   - :snowflake: [Test Analytics](https://docs.codecov.com/docs/test-analytics): Detect flaky tests, report on failures, and find test suite problems.
   - :package: [JS Bundle Analysis](https://docs.codecov.com/docs/javascript-bundle-analysis): Save yourself from yourself by tracking and limiting bundle sizes in JS merges.
   </details>


;26/Jul/25 09:39;githubbot;600","gyogal merged PR #478:
URL: https://github.com/apache/incubator-livy/pull/478


;12/Aug/25 17:38;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,LIVY-877,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 28 12:30:34 UTC 2025,,,,,,,,,,"0|z1ttgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Jul/25 12:30;gyogal;The missing imports were addressed in LIVY-863

A PR for the Spark 3 dependency upgrade is available at https://github.com/apache/incubator-livy/pull/478;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for global TTL for sessions,LIVY-1009,13605328,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,nileshrathi345,nileshrathi345,nileshrathi345,17/Jan/25 04:47,11/Aug/25 12:23,19/Dec/25 04:15,11/Aug/25 12:23,,,,0.9.0,,Core,,,,,,,,,17/Feb/25 00:00,0,,,,,,"There is a config called livy.session.timeout that sets a default for idleTimeout
Similarly we need to add support for a global default for the TTL field
Session level TTL should override the global TTL",,"nileshrathi345 opened a new pull request, #464:
URL: https://github.com/apache/incubator-livy/pull/464

   ## What changes were proposed in this pull request?
   
   Adde two new Livy configuration parameters as below:
   
   ```
   Enabled to check whether TTL Livy sessions should be stopped.
   livy.server.session.ttl-check = false
   
   Time in milliseconds on how long Livy will wait before TTL is an inactive session.
   Note that the inactive session could be busy running jobs.
   livy.server.session.ttl = 8h
   ```
   
   
   ## How was this patch tested?
   
   Read https://docs.google.com/document/d/1F2hU1RfShmNCKEWWkz_LpjEOev2BwbtZZBEueHpvgHM/edit for details.
   


;17/Jan/25 05:10;githubbot;600","gyogal merged PR #464:
URL: https://github.com/apache/incubator-livy/pull/464


;11/Aug/25 12:22;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 03 08:19:33 UTC 2025,,,,,,,,,,"0|z1toa8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Apr/25 08:19;gyogal;A PR is available at https://github.com/apache/incubator-livy/pull/464;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy should not spawn one thread per job to track the job on Kubernetes,LIVY-1007,13586488,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,asifkhatri,asifkhatri,asifkhatri,22/Jul/24 05:52,12/Aug/24 12:25,19/Dec/25 04:15,12/Aug/24 12:25,0.9.0,,,0.9.0,,Server,,,,,,,,,,0,,,,,,"SparkKubernetesApp spawns a new thread for each application.Â 

The similar behaviour is observed for Yarn in LIVY-336. This process can be improved in a few ways to use less resources, particularly threads, and to shorten the latency. Some of the improvements are straightforward.

Â ",,"askhatri opened a new pull request, #453:
URL: https://github.com/apache/incubator-livy/pull/453

   ## What changes were proposed in this pull request?
   
   Instead of spawning a monitor thread for every session, create a centralised thread to monitor all Kubernetes apps.
   
   JIRA link:https://issues.apache.org/jira/browse/LIVY-1007
   
   ## How was this patch tested?
   
   **Unit Tests:** The patch has been verified through comprehensive unit tests.
   **Manual Testing:** Conducted manual testing using Kubernetes on Docker Desktop.
   
   Environment: Helm charts. For detailed instructions on testing using Helm charts, please refer to the documentation available at [livycluster](https://github.com/askhatri/livycluster).
   


;25/Jul/24 06:15;githubbot;600","gyogal commented on PR #453:
URL: https://github.com/apache/incubator-livy/pull/453#issuecomment-2262894071

   The change looks good and it is great that we can ship this with the initial release of Livy with Kubernetes support. If anybody has any objections or code review comments, please let us know before this PR gets merged. Thank you Asif for your contribution!


;01/Aug/24 12:16;githubbot;600","gyogal merged PR #453:
URL: https://github.com/apache/incubator-livy/pull/453


;12/Aug/24 11:26;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 25 06:17:05 UTC 2024,,,,,,,,,,"0|z1qggo:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Jul/24 06:17;asifkhatri;A PR [https://github.com/apache/incubator-livy/pull/453|https://github.com/apache/incubator-livy/pull/453]Â has been submitted.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interactive session - Setting large value of rsc.server.connect.timeout blocks other tasks ,LIVY-1003,13581802,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,wangdengshan,tomcham,tomcham,06/Jun/24 14:00,13/Sep/24 08:10,19/Dec/25 04:15,13/Sep/24 08:10,0.8.0,,,0.9.0,,RSC,,,,,,,,,,0,,,,,,"Problem:

Livy is configured to deploy interactive sessions on YARN with `livy.rsc.server.connect.timeout` configured to a high value. Timeout is increased to allow more time for Livy session to be in YARN `ACCEPTED` state to prevent Livy server from killing the YARN app within the default timeout of 90 seconds.


Until the app is in YARN `RUNNING` state, it takes up a thread in Scala's global execution context - I think due to https://github.com/apache/incubator-livy/blob/v0.8.0-incubating/server/src/main/scala/org/apache/livy/server/interactive/InteractiveSession.scala#L474. Creating too many of these sessions that are stuck in `ACCEPTED` state causes other tasks that use that global execution context to be queued up.

How to reproduce:

1. Set `livy.rsc.server.connect.timeout` to something high like 24h.
2. Create enough interactive livy sessions in YARN so that they are queued in ACCEPTED state. The number of sessions that are stuck in ACCEPTED state should be equal to global execution context [thread pool size|https://docs.scala-lang.org/overviews/core/futures.html#the-global-execution-context] (Runtime.availableProcessors)
3. Try to delete a session using DELETE /sessions/{sessionId} and it should hang until one of the sessions is no longer stuck in ACCEPTED state.
Â ",,"gyogal commented on PR #449:
URL: https://github.com/apache/incubator-livy/pull/449#issuecomment-2283677113

   Thank you for your contribution @wangdengshan (and thanks also for reviewing @askhatri @nileshrathi345)!


;12/Aug/24 11:07;githubbot;600","gyogal merged PR #449:
URL: https://github.com/apache/incubator-livy/pull/449


;13/Sep/24 08:10;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 21 03:43:41 UTC 2024,,,,,,,,,,"0|z1pnl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"21/Jun/24 02:50;wangdengshan;As you know, it will mainly get stuck at the startup of RemoteDriver, which is the pulling up of yarn. On the one hand, the waiting time can be reduced through configuration(reduce the wait time), and at the same time, the use of the thread pool is also adjusted in our case.;;;","21/Jun/24 03:43;wangdengshan;[https://github.com/apache/incubator-livy/pull/449];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LivyThriftSessionManager killSparkApp may have NPE,LIVY-1002,13580984,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Invalid,,wangdengshan,wangdengshan,30/May/24 08:42,21/Jun/24 08:07,19/Dec/25 04:15,21/Jun/24 08:07,0.8.0,,,,,Thriftserver,,,,,,,,,,0,,,,,,"There may be a null pointer exception in killSparkApp. sparkApps will only put after the session is created. When the creation process fails, the kill application will not be successful.

Below is my runtime stackï¼š

{color:#ff8b00}2024-05-30 03:39:12 [ERROR] [LivyServer2-Background-Pool: Thread-139977] org.apache.livy.thriftserver.LivyThriftSessionManager:error:58 - Kill application failed{color}
{color:#ff8b00}java.lang.NullPointerException{color}
{color:#ff8b00}Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyThriftSessionManager.killSparkApp$1(LivyThriftSessionManager.scala:148){color}
{color:#ff8b00}Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyThriftSessionManager.getLivySession(LivyThriftSessionManager.scala:180){color}
{color:#ff8b00}Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyExecuteStatementOperation.execute(LivyExecuteStatementOperation.scala:157){color}
{color:#ff8b00}Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyExecuteStatementOperation$$anon$1$$anon$2.run(LivyExecuteStatementOperation.scala:102){color}
{color:#ff8b00}Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyExecuteStatementOperation$$anon$1$$anon$2.run(LivyExecuteStatementOperation.scala:99){color}
{color:#ff8b00}Â  Â  Â  Â  at java.security.AccessController.doPrivileged(Native Method){color}
{color:#ff8b00}Â  Â  Â  Â  at javax.security.auth.Subject.doAs(Subject.java:422){color}
{color:#ff8b00}Â  Â  Â  Â  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:2248){color}
{color:#ff8b00}Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyExecuteStatementOperation$$anon$1.run(LivyExecuteStatementOperation.scala:112){color}
{color:#ff8b00}Â  Â  Â  Â  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511){color}
{color:#ff8b00}Â  Â  Â  Â  at java.util.concurrent.FutureTask.run(FutureTask.java:266){color}
{color:#ff8b00}Â  Â  Â  Â  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149){color}
{color:#ff8b00}Â  Â  Â  Â  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624){color}
{color:#ff8b00}Â  Â  Â  Â  at java.lang.Thread.run(Thread.java:748){color}

Â 

{color:#172b4d}The key code is here{color}
{code:java}
Â  Â  def killSparkApp = {
Â  Â  Â  try {
Â  Â  Â  Â  sparkApps.get(sessionHandle.toString).foreach(sparkApp => {
Â  Â  Â  Â  Â  info(""Start kill spark job"")
Â  Â  Â  Â  Â  sparkApp.kill()
Â  Â  Â  Â  Â  info(""End kill spark job"")
Â  Â  Â  Â  })
Â  Â  Â  } catch {
Â  Â  Â  Â  case ex: Throwable => error(""Kill application failed"", ex)
Â  Â  Â  } finally {
Â  Â  Â  Â  sparkApps.remove(sessionHandle.toString)
Â  Â  Â  }
Â  Â  }
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,scala,,2024-05-30 08:42:56.0,,,,,,,,,,"0|z1pijs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support connecting to existing sessions using session name via Thrift Server,LIVY-998,13574404,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,asifkhatri,asifkhatri,asifkhatri,03/Apr/24 08:33,10/Jun/24 08:18,19/Dec/25 04:15,10/Jun/24 08:18,0.7.0,,,0.9.0,,Thriftserver,,,,,,,,,,0,,,,,,"Livy supports named sessions (LIVY-41), however these cannot be used when connecting to existing sessions via JDBC.

The Thrift Server can set the session name (see LIVY-652), but it seems like it cannot use an existing session name and connect to it. The task would be to implement this feature and add unit tests for it.",,"askhatri opened a new pull request, #445:
URL: https://github.com/apache/incubator-livy/pull/445

   Support connecting to an existing sessions using session name via Thrift Server
   
   ## What changes were proposed in this pull request?
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-998
   
   The Thrift Server can set the session name, but it cannot use an existing session name and connect to it. Added the fix to use an existing session name and connect to it via Thrift Server.
   
   ## How was this patch tested?
   
   Created a session using session name:
   
   ```
   curl --no-progress-meter -X POST -H ""Content-Type: application/json"" --data '{""kind"": ""spark"", ""name"": ""sesssionName""}' http://HOST:PORT/sessions
   ```
   
   Connected to the existing session via Thrift Server using:
   
   ```
   $ beeline -u ""jdbc:hive2://HOST:PORT/?livy.session.name=sesssionName""
   ```


;22/Apr/24 04:37;githubbot;600","askhatri commented on PR #445:
URL: https://github.com/apache/incubator-livy/pull/445#issuecomment-2147151805

   Thank you @gyogal , I have updated the comment now.


;04/Jun/24 10:12;githubbot;600","gyogal merged PR #445:
URL: https://github.com/apache/incubator-livy/pull/445


;10/Jun/24 08:16;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 23 06:02:46 UTC 2024,,,,,,,,,,"0|z1oegg:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/Apr/24 06:02;asifkhatri;I have created PR https://github.com/apache/incubator-livy/pull/445.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add file .sdkmanrc to .gitignore,LIVY-997,13574029,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,javiroman,javiroman,javiroman,31/Mar/24 15:40,26/Jun/24 08:21,19/Dec/25 04:15,26/Jun/24 08:21,0.9.0,,,0.9.0,,Docs,,,,,,,,,,0,,,,,,Add .sdkmanrc file pattern to gitignore file. Because Livy uses an older version of JDK it is more like switching versions using SDKMAN.,,"javiroman opened a new pull request, #444:
URL: https://github.com/apache/incubator-livy/pull/444

   ## What changes were proposed in this pull request?
   
   Add new ignore pattern to .gitignore file for .sdkmanrc file.
   https://issues.apache.org/jira/browse/LIVY-997
   
   ## How was this patch tested?
   
   N/A
   


;31/Mar/24 15:52;githubbot;600","codecov-commenter commented on PR #444:
URL: https://github.com/apache/incubator-livy/pull/444#issuecomment-2028811993

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/444?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   All modified and coverable lines are covered by tests :white_check_mark:
   > Project coverage is 28.66%. Comparing base [(`f615f27`)](https://app.codecov.io/gh/apache/incubator-livy/commit/f615f272e9130d02170024832ea308516b907195?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) to head [(`3f5c004`)](https://app.codecov.io/gh/apache/incubator-livy/pull/444?dropdown=coverage&src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   
   
   <details><summary>Additional details and impacted files</summary>
   
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #444       +/-   ##
   =============================================
   - Coverage     65.42%   28.66%   -36.77%     
   + Complexity      958      384      -574     
   =============================================
     Files           103      103               
     Lines          6109     6109               
     Branches        926      926               
   =============================================
   - Hits           3997     1751     -2246     
   - Misses         1553     3998     +2445     
   + Partials        559      360      -199     
   ```
   
   
   
   </details>
   
   [:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/apache/incubator-livy/pull/444?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).   
   :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   


;31/Mar/24 16:16;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Mar 31 15:53:42 UTC 2024,,,,,,,,,,"0|z1oc5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,"31/Mar/24 15:53;javiroman;Added PR: https://github.com/apache/incubator-livy/pull/444;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JsonParseException is thrown when closing Livy session when using python profile,LIVY-995,13565221,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,jianzhenwu,jianzhenwu,jianzhenwu,18/Jan/24 09:43,26/Jan/24 02:11,19/Dec/25 04:15,26/Jan/24 01:34,,,,0.9.0,,REPL,,,,,,,,,,0,,,,,,"StartupÂ  and enable spark.python.profile.
{code:java}
./bin/pyspark --master local --conf spark.python.profile=true
{code}
Â 
Execute code related to Spark RDD. When pyspark is closed, Pyspark will output profile information.
{code:java}
>>> rdd = sc.parallelize(range(100)).map(str)
>>> rdd.count()
[Stage 0:>Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (0 + 1) / 1]
100
>>>
============================================================
Profile of RDD<id=1>
============================================================
Â  Â  Â  Â  Â 244 function calls (241 primitive calls) in 0.001 seconds
Â 
Â  Â Ordered by: internal time, cumulative time
Â 
Â  Â ncallsÂ  tottimeÂ  percallÂ  cumtimeÂ  percall filename:lineno(function)
Â  Â  Â  101Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 rdd.py:1237(<genexpr>)
Â  Â  Â  101Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 util.py:72(wrapper)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 serializers.py:255(dump_stream)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 serializers.py:213(load_stream)
Â  Â  Â  Â  2Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{built-in method builtins.sum}
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.001Â  Â  0.001 worker.py:607(process)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 context.py:549(f)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{built-in method _pickle.dumps}
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 serializers.py:561(read_int)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 serializers.py:568(write_int)
Â  Â  Â  4/1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 rdd.py:2917(pipeline_func)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 serializers.py:426(dumps)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 rdd.py:1237(<lambda>)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 serializers.py:135(load_stream)
Â  Â  Â  Â  2Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 rdd.py:1072(func)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 rdd.py:384(func)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 util.py:67(fail_on_stopiteration)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 serializers.py:151(_read_with_length)
Â  Â  Â  Â  2Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 context.py:546(getStart)
Â  Â  Â  Â  3Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 rdd.py:416(func)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 serializers.py:216(_load_stream_without_unbatching)
Â  Â  Â  Â  2Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{method 'write' of '_io.BufferedWriter' objects}
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{method 'read' of '_io.BufferedReader' objects}
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{built-in method _operator.add}
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{built-in method builtins.hasattr}
Â  Â  Â  Â  3Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{built-in method builtins.len}
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{built-in method _struct.unpack}
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 rdd.py:1226(<lambda>)
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{method 'close' of 'generator' objects}
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{built-in method from_iterable}
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{built-in method _struct.pack}
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{method 'disable' of '_lsprof.Profiler' objects}
Â  Â  Â  Â  1Â  Â  0.000Â  Â  0.000Â  Â  0.000Â  Â  0.000 \{built-in method builtins.iter}
{code}
Â 
This is because Spark register show_profiles when Spark exit in profile.pyÂ 
{code:java}
Â  Â  def add_profiler(self, id, profiler):
Â  Â  Â  Â  """"""Add a profiler for RDD/UDF `id`""""""
Â  Â  Â  Â  if not self.profilers:
Â  Â  Â  Â  Â  Â  if self.profile_dump_path:
Â  Â  Â  Â  Â  Â  Â  Â  atexit.register(self.dump_profiles, self.profile_dump_path)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  atexit.register(self.show_profiles)
Â 
Â  Â  Â  Â  self.profilers.append([id, profiler, False])
{code}
Â 
Â 
For Livy session, Livy does not convert the output to JSON format. And throw below exception:
Â 
{code:java}
com.fasterxml.jackson.core.JsonParseException: Unexpected character ('=' (code 61)): expected a valid value (JSON String, Number, Array, Object or token 'null', 'true' or 'false')
 at [Source: (String)""============================================================""; line: 1, column: 2]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:2337)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:710)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:635)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleOddValue(ReaderBasedJsonParser.java:1952)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:781)
	at com.fasterxml.jackson.databind.ObjectReader._initForReading(ObjectReader.java:355)
	at com.fasterxml.jackson.databind.ObjectReader._bindAndClose(ObjectReader.java:2023)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1491)
	at org.livy.toolkit.shaded.org.json4s.jackson.JsonMethods.parse(JsonMethods.scala:33)
	at org.livy.toolkit.shaded.org.json4s.jackson.JsonMethods.parse$(JsonMethods.scala:20)
	at org.livy.toolkit.shaded.org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:71)
	at org.apache.livy.repl.PythonInterpreter.$anonfun$sendRequest$1(PythonInterpreter.scala:288)
	at scala.Option.map(Option.scala:230)
	at org.apache.livy.repl.PythonInterpreter.sendRequest(PythonInterpreter.scala:287)
	at org.apache.livy.repl.PythonInterpreter.sendShutdownRequest(PythonInterpreter.scala:277)
	at org.apache.livy.repl.ProcessInterpreter.close(ProcessInterpreter.scala:62)
	at org.apache.livy.repl.PythonInterpreter.close(PythonInterpreter.scala:234)
	at org.apache.livy.repl.Session.$anonfun$close$1(Session.scala:232)
	at org.apache.livy.repl.Session.$anonfun$close$1$adapted(Session.scala:232)
	at scala.collection.mutable.HashMap$$anon$2.$anonfun$foreach$3(HashMap.scala:158)
	at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)
	at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)
	at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:158)
	at org.apache.livy.repl.Session.close(Session.scala:232)
	at org.apache.livy.toolkit.IpynbBootstrap.close(IpynbBootstrap.scala:246)
	at org.apache.livy.toolkit.IpynbBootstrap$.main(IpynbBootstrap.scala:72)
	at org.apache.livy.toolkit.IpynbBootstrap.main(IpynbBootstrap.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:764) {code}
Â Livy sendShutdownRequest in PythonInterpreter.scala
{code:scala}
Â  override protected def sendShutdownRequest(): Unit = {
Â  Â  sendRequest(Map(
Â  Â  Â  ""msg_type"" -> ""shutdown_request"",
Â  Â  Â  ""content"" -> ()
Â  Â  )).foreach { case rep =>
Â  Â  Â  warn(f""process failed to shut down while returning $rep"")
Â  Â  }
Â  }

Â  private def sendRequest(request: Map[String, Any]): Option[JValue] = {
Â  Â  stdin.println(write(request))
Â  Â  stdin.flush()

Â  Â  Option(stdout.readLine()).map { case line =>
Â  Â  Â  parse(line)
Â  Â  }
Â  }
{code}
Livy does not convert stdout to json when exit in fake_shell.pyÂ 
{code:python}
def shutdown_request(_content):
Â  Â  sys.exit()

msg_type_router = {
Â  Â  'execute_request': execute_request,
Â  Â  'shutdown_request': shutdown_request,
}

Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  handler = msg_type_router[msg_type]
Â  Â  Â  Â  Â  Â  except KeyError:
Â  Â  Â  Â  Â  Â  Â  Â  LOG.error('unknown message type: %s', msg_type)
Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  response = handler(content)
{code}",,"jianzhenwu opened a new pull request, #439:
URL: https://github.com/apache/incubator-livy/pull/439

   ## What changes were proposed in this pull request?
   
   When Livy closes PythonInterpreter, the output may not be in JSON format, so there is no need to deserialize json.
   https://issues.apache.org/jira/browse/LIVY-995
   
   
   ## How was this patch tested?
   manual tests
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   


;18/Jan/24 10:05;githubbot;600","codecov-commenter commented on PR #439:
URL: https://github.com/apache/incubator-livy/pull/439#issuecomment-1898210127

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/439?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   Attention: `9 lines` in your changes are missing coverage. Please review.
   > Comparison is base [(`86fc823`)](https://app.codecov.io/gh/apache/incubator-livy/commit/86fc823893ee96d4effaa4f2b8ef6603cea9d77a?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) 65.45% compared to head [(`2e0e7a5`)](https://app.codecov.io/gh/apache/incubator-livy/pull/439?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) 28.56%.
   
   | [Files](https://app.codecov.io/gh/apache/incubator-livy/pull/439?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) | Patch % | Lines |
   |---|---|---|
   | [...scala/org/apache/livy/repl/PythonInterpreter.scala](https://app.codecov.io/gh/apache/incubator-livy/pull/439?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9QeXRob25JbnRlcnByZXRlci5zY2FsYQ==) | 0.00% | [9 Missing :warning: ](https://app.codecov.io/gh/apache/incubator-livy/pull/439?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) |
   
   <details><summary>Additional details and impacted files</summary>
   
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #439       +/-   ##
   =============================================
   - Coverage     65.45%   28.56%   -36.89%     
   + Complexity      954      380      -574     
   =============================================
     Files           103      103               
     Lines          6084     6091        +7     
     Branches        922      923        +1     
   =============================================
   - Hits           3982     1740     -2242     
   - Misses         1546     3994     +2448     
   + Partials        556      357      -199     
   ```
   
   
   
   </details>
   
   [:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/apache/incubator-livy/pull/439?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).   
   :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   


;18/Jan/24 10:30;githubbot;600","jianzhenwu commented on PR #439:
URL: https://github.com/apache/incubator-livy/pull/439#issuecomment-1899533633

   hi @jerryshao I noticed you contributed to Livy's REPL module. Can you help review this PR?


;19/Jan/24 02:16;githubbot;600","lmccay commented on PR #439:
URL: https://github.com/apache/incubator-livy/pull/439#issuecomment-1900736040

   @jianzhenwu - can you add a unit test to show that the issue is resolved and ensure that it doesn't regress in the future?
   It would also be good to point to coverage or add new tests to ensure that previous functional expectations remain intact.
   


;19/Jan/24 16:37;githubbot;600","jianzhenwu commented on PR #439:
URL: https://github.com/apache/incubator-livy/pull/439#issuecomment-1903369616

   @lmccay thank you for your reply. I added unit tests to ensure no impact on original functionality and to verify expected functionality.


;22/Jan/24 06:55;githubbot;600","lmccay merged PR #439:
URL: https://github.com/apache/incubator-livy/pull/439


;25/Jan/24 23:56;githubbot;600","lmccay commented on PR #439:
URL: https://github.com/apache/incubator-livy/pull/439#issuecomment-1911188351

   @jianzhenwu - thanks for your contribution! I have merged this and it will be available in the next release.


;25/Jan/24 23:57;githubbot;600","jianzhenwu commented on PR #439:
URL: https://github.com/apache/incubator-livy/pull/439#issuecomment-1911298578

   thank you


;26/Jan/24 02:11;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4800,,,0,4800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2024-01-18 09:43:52.0,,,,,,,,,,"0|z1muag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In livy logs, user is not able to differentiate between interactive session and batch session",LIVY-992,13558014,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,rajshekhar,rajshekhar,rajshekhar,15/Nov/23 08:10,25/Jun/24 14:11,19/Dec/25 04:15,25/Jun/24 14:11,0.8.0,,,0.9.0,,,,,,,,,,,,0,,,,,,"When session is created in livy, it is difficult to differentiate between interactive session and batch session.Â 

Â 

Logs can be improved",,"RajshekharMuchandi opened a new pull request, #435:
URL: https://github.com/apache/incubator-livy/pull/435

   ## What changes were proposed in this pull request?
   
   Changes done in log info to differentiate session type (interactive or batch) so that it will be easier to identify while deleting sessions
   https://issues.apache.org/jira/browse/LIVY-992 
   
   ## How was this patch tested?
   
   Created both interactive and batch sessions. Then deleted sessions manually with DELETE API. Then checked logs to verify session type
   


;20/Nov/23 06:00;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 20 06:00:47 UTC 2023,,,,,,,,,,"0|z1lm60:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Nov/23 06:00;rajshekhar;[https://github.com/apache/incubator-livy/pull/435]Â ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Facing issues with the Livy UI Driver link.,LIVY-991,13558013,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,rajshekhar,rajshekhar,rajshekhar,15/Nov/23 08:07,25/Jun/24 14:25,19/Dec/25 04:15,25/Jun/24 14:25,,,,0.9.0,,,,,,,,,,,,0,,,,,,"Customer is running hundreds of Spark applications from Livy. Once the applications are FINISHED, the Driver log link should disappear from the Livy UI. But for some random applications we see invalid driver links.",,"RajshekharMuchandi opened a new pull request, #437:
URL: https://github.com/apache/incubator-livy/pull/437

   ## What changes were proposed in this pull request?
   
   Added conditional check on finished state to set driverlogUrl to null
   https://issues.apache.org/jira/browse/LIVY-991 
   
   ## How was this patch tested?
   
   This patch is tested by adding 2 test cases. One for KILLED application state where both Spark UI URL and driver log url are displayed. Another for FINISHED application state where both Spark UI URL and driver log url are not displayed
   


;22/Nov/23 10:56;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 22 10:56:28 UTC 2023,,,,,,,,,,"0|z1lm5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Nov/23 10:56;rajshekhar;[https://github.com/apache/incubator-livy/pull/437]Â ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy core support for interactive session idleTimeout,LIVY-989,13551833,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,asifkhatri,asifkhatri,asifkhatri,25/Sep/23 05:34,31/Oct/23 09:25,19/Dec/25 04:15,31/Oct/23 05:04,0.8.0,,,0.9.0,,Server,,,,,,,,,,0,,,,,,"Currently, a Livy interactive session has a field called ttl, which kills the session if it has been idle for a given amount of time. However, here is the expected behaviour:
 # ttl: kills the session if it has been active for a certain duration, irrespective of idleness.
 # idleTimeout: kills the session if it has been idle for the given duration. (The current TTL behaves in this manner.)",,"askhatri opened a new pull request, #426:
URL: https://github.com/apache/incubator-livy/pull/426

   ## What changes were proposed in this pull request?
   
   Currently, a Livy interactive session has a field called ttl, which kills the session if it has been idle for a given amount of time. However, here is the expected behaviour:
   
   **ttl:** kills the session if it has been active for a certain duration, irrespective of idleness.
   **idleTimeout:** kills the session if it has been idle for the given duration. (The current TTL behaves in this manner.)
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-989
   
   ## How was this patch tested?
   
   Tested manually by creating interactive session with idle time and TTL.
   


;06/Oct/23 08:13;githubbot;600","codecov-commenter commented on PR #426:
URL: https://github.com/apache/incubator-livy/pull/426#issuecomment-1759522625

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/426?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   > Merging [#426](https://app.codecov.io/gh/apache/incubator-livy/pull/426?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) (9a1f31b) into [master](https://app.codecov.io/gh/apache/incubator-livy/commit/8b2e29fe9fd42c20395c63e1571f2492f005162b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) (8b2e29f) will **decrease** coverage by `36.86%`.
   > The diff coverage is `48.57%`.
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #426       +/-   ##
   =============================================
   - Coverage     65.48%   28.63%   -36.86%     
   + Complexity      950      382      -568     
   =============================================
     Files           103      103               
     Lines          6062     6084       +22     
     Branches        916      922        +6     
   =============================================
   - Hits           3970     1742     -2228     
   - Misses         1541     3984     +2443     
   + Partials        551      358      -193     
   ```
   
   
   | [Files](https://app.codecov.io/gh/apache/incubator-livy/pull/426?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) | Coverage Î” | |
   |---|---|---|
   | [...va/org/apache/livy/client/common/HttpMessages.java](https://app.codecov.io/gh/apache/incubator-livy/pull/426?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-Y2xpZW50LWNvbW1vbi9zcmMvbWFpbi9qYXZhL29yZy9hcGFjaGUvbGl2eS9jbGllbnQvY29tbW9uL0h0dHBNZXNzYWdlcy5qYXZh) | `85.71% <100.00%> (-10.65%)` | :arrow_down: |
   | [.../main/scala/org/apache/livy/sessions/Session.scala](https://app.codecov.io/gh/apache/incubator-livy/pull/426?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXNzaW9ucy9TZXNzaW9uLnNjYWxh) | `63.63% <100.00%> (-11.16%)` | :arrow_down: |
   | [...e/livy/server/interactive/InteractiveSession.scala](https://app.codecov.io/gh/apache/incubator-livy/pull/426?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `70.32% <85.71%> (+8.10%)` | :arrow_up: |
   | [.../server/interactive/CreateInteractiveRequest.scala](https://app.codecov.io/gh/apache/incubator-livy/pull/426?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvQ3JlYXRlSW50ZXJhY3RpdmVSZXF1ZXN0LnNjYWxh) | `61.76% <33.33%> (-13.24%)` | :arrow_down: |
   | [...server/interactive/InteractiveSessionServlet.scala](https://app.codecov.io/gh/apache/incubator-livy/pull/426?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `55.69% <66.66%> (-3.66%)` | :arrow_down: |
   | [...cala/org/apache/livy/sessions/SessionManager.scala](https://app.codecov.io/gh/apache/incubator-livy/pull/426?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXNzaW9ucy9TZXNzaW9uTWFuYWdlci5zY2FsYQ==) | `58.92% <7.14%> (-23.43%)` | :arrow_down: |
   
   ... and [80 files with indirect coverage changes](https://app.codecov.io/gh/apache/incubator-livy/pull/426/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)
   


;12/Oct/23 12:32;githubbot;600","gyogal commented on PR #426:
URL: https://github.com/apache/incubator-livy/pull/426#issuecomment-1766328010

   LGTM, thanks for your contribution!


;17/Oct/23 12:34;githubbot;600","gyogal merged PR #426:
URL: https://github.com/apache/incubator-livy/pull/426


;30/Oct/23 13:58;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 31 05:04:47 UTC 2023,,,,,,,,,,"0|z1kk2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Oct/23 08:13;asifkhatri;Created the pull request https://github.com/apache/incubator-livy/pull/426.;;;","31/Oct/23 05:04;asifkhatri;Pull #426 is merged. So marking this ticket as resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when waiting for thrift session to start timeout.,LIVY-987,13548631,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jianzhenwu,jianzhenwu,jianzhenwu,25/Aug/23 09:16,05/Sep/23 10:01,19/Dec/25 04:15,05/Sep/23 10:01,,,,0.9.0,,Thriftserver,,,,,,,,,,0,,,,,,"Â 

Livy spends 10 min waiting for the session to start. If it takes more than 10 minutes to start, it will throw a Timeout exception. There is no cause for the timeout exception. When Livy throws e.getCause, NPE occurs.

*Livy Code*
{code:java}
      Try(Await.result(future, maxSessionWait)) match {
        case Success(session) => session
        case Failure(e) => throw e.getCause
      } {code}
*Error Log*
{code:java}
23/08/25 16:01:41 INFO Â LivyExecuteStatementOperation: (Error executing query, currentState RUNNING, ,java.lang.NullPointerException)
23/08/25 16:01:41 ERROR Â LivyExecuteStatementOperation: Error running hive query:
org.apache.hive.service.cli.HiveSQLException: java.lang.NullPointerException
Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyExecuteStatementOperation.execute(LivyExecuteStatementOperation.scala:186)
Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyExecuteStatementOperation$$anon$2$$anon$3.run(LivyExecuteStatementOperation.scala:105)
Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyExecuteStatementOperation$$anon$2$$anon$3.run(LivyExecuteStatementOperation.scala:102)
Â  Â  Â  Â  at java.security.AccessController.doPrivileged(Native Method)
Â  Â  Â  Â  at javax.security.auth.Subject.doAs(Subject.java:422)
Â  Â  Â  Â  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:2038)
Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyExecuteStatementOperation$$anon$2.run(LivyExecuteStatementOperation.scala:115)
Â  Â  Â  Â  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Â  Â  Â  Â  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Â  Â  Â  Â  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Â  Â  Â  Â  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Â  Â  Â  Â  at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyThriftSessionManager.getLivySession(LivyThriftSessionManager.scala:99)
Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyExecuteStatementOperation.rpcClient$lzycompute(LivyExecuteStatementOperation.scala:65)
Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyExecuteStatementOperation.rpcClient(LivyExecuteStatementOperation.scala:58)
Â  Â  Â  Â  at org.apache.livy.thriftserver.LivyExecuteStatementOperation.execute(LivyExecuteStatementOperation.scala:173) {code}
Â 

Â ",,"jianzhenwu opened a new pull request, #416:
URL: https://github.com/apache/incubator-livy/pull/416

   ## What changes were proposed in this pull request?
   Fix NPE when waiting for thrift session to start timeout.
   https://issues.apache.org/jira/browse/LIVY-987
   
   ## How was this patch tested?
   manual tests
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   


;25/Aug/23 09:28;githubbot;600","mgaido91 commented on PR #416:
URL: https://github.com/apache/incubator-livy/pull/416#issuecomment-1693155712

   Can we add a UT to ensure that such problem will not appear again in the future?


;25/Aug/23 10:38;githubbot;600","jianzhenwu commented on PR #416:
URL: https://github.com/apache/incubator-livy/pull/416#issuecomment-1695019113

   > Can we add a UT to ensure that such problem will not appear again in the future?
   
   thank you for your reply. Of course, please review again.


;28/Aug/23 05:10;githubbot;600","mgaido91 commented on code in PR #416:
URL: https://github.com/apache/incubator-livy/pull/416#discussion_r1310372309


##########
thriftserver/server/src/test/scala/org/apache/livy/thriftserver/TestLivyThriftSessionManager.scala:
##########
@@ -90,7 +100,7 @@ class TestLivyThriftSessionManager {
 
   @Test
   def testLimitConnectionsByIpAddress(): Unit = {
-    val thriftSessionMgr = createThriftSessionManager(IpAddress)
+    val thriftSessionMgr = createThriftSessionManager(None, IpAddress)

Review Comment:
   if we create an overloaded method, we can avoid these changes



##########
thriftserver/server/src/test/scala/org/apache/livy/thriftserver/TestLivyThriftSessionManager.scala:
##########
@@ -36,11 +45,12 @@ class TestLivyThriftSessionManager {
 
   import ConnectionLimitType._
 
-  private def createThriftSessionManager(
+  private def createThriftSessionManager(maxSessionWait: Option[String],
       limitTypes: ConnectionLimitType*): LivyThriftSessionManager = {

Review Comment:
   I think it would be great if we could create an overloaded method where we can pass the `LivyConf`. Might be useful in the future as well.
   
   
   Btw, regarding style, it should be 
   
   ```
     private def createThriftSessionManager(
         maxSessionWait: Option[String], limitTypes: ConnectionLimitType*): LivyThriftSessionManager = {
   ```
   
   or 
   
   ```
     private def createThriftSessionManager(
         maxSessionWait: Option[String],
         limitTypes: ConnectionLimitType*): LivyThriftSessionManager = {
   ```
   
   if it is too long



##########
thriftserver/server/src/test/scala/org/apache/livy/thriftserver/TestLivyThriftSessionManager.scala:
##########
@@ -142,4 +152,28 @@ class TestLivyThriftSessionManager {
     val msg = s""Connection limit per user reached (user: $user limit: 3)""
     testLimit(thriftSessionMgr, user, ipAddress, forwardedAddresses, msg)
   }
+
+  @Test(expected = classOf[TimeoutException])
+  def testGetLivySessionWaitForTimeout(): Unit = {
+    val thriftSessionMgr = createThriftSessionManager(Some(""10ms""))
+    val sessionHandle = mock(classOf[SessionHandle])
+    val future = Future[InteractiveSession] {
+      sleep(100)
+      mock(classOf[InteractiveSession])
+    }
+    thriftSessionMgr._mockLivySession(sessionHandle, future)
+    thriftSessionMgr.getLivySession(sessionHandle)
+  }
+
+  @Test(expected = classOf[TimeoutException])
+  def testGetLivySessionWithTimeoutException(): Unit = {

Review Comment:
   what does this test check? What does it add to the test above?



##########
thriftserver/server/src/main/scala/org/apache/livy/thriftserver/LivyThriftSessionManager.scala:
##########
@@ -549,6 +549,11 @@ class LivyThriftSessionManager(val server: LivyThriftServer, val livyConf: LivyC
   def getSessionInfo(sessionHandle: SessionHandle): SessionInfo = {
     sessionInfo.get(sessionHandle)
   }
+
+  private[thriftserver] def _mockLivySession(

Review Comment:
   shall we rather make `sessionHandleToLivySession` visible by `private[thriftserver]` so we can avoid adding this method only for testing? Why do you think this option is better?



;30/Aug/23 14:33;githubbot;600","codecov-commenter commented on PR #416:
URL: https://github.com/apache/incubator-livy/pull/416#issuecomment-1700306940

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/416?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   > Merging [#416](https://app.codecov.io/gh/apache/incubator-livy/pull/416?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) (c2ad37a) into [master](https://app.codecov.io/gh/apache/incubator-livy/commit/5dccc479c6087112f048a7e5cff0723855ef14e9?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) (5dccc47) will **decrease** coverage by `36.92%`.
   > The diff coverage is `n/a`.
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #416       +/-   ##
   =============================================
   - Coverage     65.50%   28.58%   -36.92%     
   + Complexity      952      378      -574     
   =============================================
     Files           103      103               
     Lines          6062     6062               
     Branches        916      916               
   =============================================
   - Hits           3971     1733     -2238     
   - Misses         1542     3976     +2434     
   + Partials        549      353      -196     
   ```
   
   
   [see 86 files with indirect coverage changes](https://app.codecov.io/gh/apache/incubator-livy/pull/416/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)
   


;31/Aug/23 03:11;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 05 10:01:19 UTC 2023,,,,,,,,,,"0|z1k0b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Sep/23 10:01;mgaido;Issue resolved by https://github.com/apache/incubator-livy/pull/416.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to add null check for SessionInfo,LIVY-986,13547576,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,asifkhatri,asifkhatri,asifkhatri,16/Aug/23 14:15,18/Aug/23 11:54,19/Dec/25 04:15,18/Aug/23 11:54,0.8.0,,,0.9.0,,Server,,,,,,,,,,0,,,,,,"We need to add a null check for SessionInfo in InteractiveSessionServlet.scala to avoid that might occur during a Livy upgrade.
{code:java}
 new SessionInfo(session.id, session.name.orNull, session.appId.orNull,
      session.owner, session.state.toString, session.kind.toString,
      session.appInfo.asJavaMap, logs.asJava, session.ttl.orNull,
      session.driverMemory.orNull,
      session.driverCores.getOrElse(0), session.executorMemory.orNull,
      session.executorCores.getOrElse(0), session.conf.asJava, session.archives.asJava,
      session.files.asJava, session.heartbeatTimeoutS, session.jars.asJava,
      session.numExecutors.getOrElse(0), session.proxyUser.orNull, session.pyFiles.asJava,
      session.queue.orNull)
{code}
This code can throw exception when {{session.conf}} is not null.",,"askhatri opened a new pull request, #412:
URL: https://github.com/apache/incubator-livy/pull/412

   ## What changes were proposed in this pull request?
   
   [LIVY-986][SERVER] Adding null pointer check for SessionInfo
   
   ## What changes were proposed in this pull request?
   
   Adding null check for SessionInfo parameters ""conf"", ""archives"", ""jars"", ""pyFiles"" and ""files""
   JIRA: https://issues.apache.org/jira/browse/LIVY-986
   
   ## How was this patch tested?
   
   Tested manually by running session recovery.
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.


;17/Aug/23 07:48;githubbot;600","codecov-commenter commented on PR #412:
URL: https://github.com/apache/incubator-livy/pull/412#issuecomment-1681878284

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/412?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   > Merging [#412](https://app.codecov.io/gh/apache/incubator-livy/pull/412?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) (b2d0dc7) into [master](https://app.codecov.io/gh/apache/incubator-livy/commit/b175ba1bd1210e2d1db61bdf19a3dbfc1374dd8c?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) (b175ba1) will **decrease** coverage by `37.04%`.
   > Report is 1 commits behind head on master.
   > The diff coverage is `44.44%`.
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #412       +/-   ##
   =============================================
   - Coverage     65.61%   28.58%   -37.04%     
   + Complexity      952      379      -573     
   =============================================
     Files           103      103               
     Lines          6044     6059       +15     
     Branches        911      916        +5     
   =============================================
   - Hits           3966     1732     -2234     
   - Misses         1534     3973     +2439     
   + Partials        544      354      -190     
   ```
   
   
   | [Files Changed](https://app.codecov.io/gh/apache/incubator-livy/pull/412?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) | Coverage Î” | |
   |---|---|---|
   | [...server/interactive/InteractiveSessionServlet.scala](https://app.codecov.io/gh/apache/incubator-livy/pull/412?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `56.12% <44.44%> (-6.02%)` | :arrow_down: |
   
   ... and [85 files with indirect coverage changes](https://app.codecov.io/gh/apache/incubator-livy/pull/412/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)
   


;17/Aug/23 08:43;githubbot;600","gyogal merged PR #412:
URL: https://github.com/apache/incubator-livy/pull/412


;18/Aug/23 11:53;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 17 07:49:39 UTC 2023,,,,,,,,,,"0|z1jtsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/Aug/23 07:49;asifkhatri;Created PR https://github.com/apache/incubator-livy/pull/412.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Category X licenses in binary distribution,LIVY-985,13546626,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,dacort,dacort,dacort,09/Aug/23 05:00,25/Sep/23 17:19,19/Dec/25 04:15,25/Sep/23 17:19,0.8.0,,,0.8.0,,Build,,,,,,,,,,0,,,,,,"From the [VOTE thread|https://lists.apache.org/thread/k01z56h2fjr8k80c9z0nz6nqx5l97bh8] there are several GPL and LGPL licensed jars in the zip.

[https://www.apache.org/legal/resolved.html#category-x]

GNU General Public Library:
 * Streaming API for XML (javax.xml.stream:stax-api:1.0-2 - no url defined)

GNU Lesser General Public License (LGPL), Version 2.1:
 * JAX-RS provider for JSON content type (org.codehaus.jackson:jackson-jaxrs:1.8.3 -Â [http://jackson.codehaus.org|http://jackson.codehaus.org/])
 * Xml Compatibility extensions for Jackson (org.codehaus.jackson:jackson-xc:1.8.3 - [http://jackson.codehaus.org|http://jackson.codehaus.org/])

GPL:
 * JTransforms (com.github.rwl:jtransforms:2.4.0 -Â [http://sourceforge.net/projects/jtransforms/])

GPL2 w/ CPE:
 * JAXB API bundle for GlassFish V3 (javax.xml.bind:jaxb-api:2.2.2 [https://jaxb.dev.java.net/])

 * JAXB RI (com.sun.xml.bind:jaxb-impl:2.2.3-1 - [http://jaxb.java.net/])
 * javax.ws.rs-api (javax.ws.rs:javax.ws.rs-api:2.0.1 -[http://jax-rs-spec.java.net|http://jax-rs-spec.java.net/])
 * jersey-client (com.sun.jersey:jersey-client:1.9 -[https://jersey.java.net/jersey-client/])
 * jersey-core (com.sun.jersey:jersey-core:1.9 -[https://jersey.java.net/jersey-core/])
 * jersey-guice (com.sun.jersey.contribs:jersey-guice:1.9 -Â [https://jersey.java.net/jersey-contribs/jersey-guice/])
 * jersey-json (com.sun.jersey:jersey-json:1.9 -Â [https://jersey.java.net/jersey-json/])
 * jersey-server (com.sun.jersey:jersey-server:1.9 -Â [https://jersey.java.net/jersey-server/])

LGPL:
 * JTransforms (com.github.rwl:jtransforms:2.4.0 -Â [http://sourceforge.net/projects/jtransforms/])

LGPL 2.1:
 * Javassist (org.javassist:javassist:3.18.1-GA - [http://www.javassist.org/])

Â 

Â ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-981,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 25 17:19:10 UTC 2023,,,,,,,,,,"0|z1joaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Aug/23 19:24;lmccay;Pulling this back into 0.8.0 as it seems that [~jmclean] feels it is a blocker for the release.

Â ;;;","15/Aug/23 05:50;dacort;Spent some time going through mvn dependency:tree -Dincludes=package to figure out where they're coming from.
 * javax.xml.stream:stax-api:1.0-2 - Dependency of various hadoop imports, like hadoop-yarn-common and hadoop-common, via jaxb-api. [Hudi specifically excluded jaxb|https://github.com/apache/hudi/commit/ec965892b042b123e985db5676fdf599d934404c#diff-9c5fb3d1b7e3b0f54bc5c4182965c4fe1f9023d449017cece3005d3f90e8e4d8] per LEGAL-461.
 * org.codehaus.jackson:jackson-jaxrs:1.8.3 - Also pulled in via hadoop.
 * org.codehaus.jackson:jackson-xc:1.8.3 - Pulled in via hadoop.
 * com.github.rwl:jtransforms:2.4.0 - Pulled in via spark-repl_2.11
 * javax.xml.bind:jaxb-api:2.2.2 - Hadoop
 * com.sun.xml.bind:jaxb-impl:2.2.3-1 - via hadoop-common
 * javax.ws.rs:javax.ws.rs-api:2.0.1 - via spark-core
 * com.sun.jersey:jersey-* - via various hadoop-common/hadoop-yarn-common
 * com.sun.jersey.contribs:jersey-guice - hadoop-yarn-common
 * org.javassist:javassist:3.18.1-GA - via spark-core

Most of these are either test or provided scope, so I think we can safely add excludes in the relevant poms.;;;","22/Sep/23 16:35;dacort;As part of [https://github.com/apache/incubator-livy/pull/421] was able to remove some of these, but we still have a few left.Â 

  GNU Lesser General Public License (LGPL), Version 2.1:

    * JAX-RS provider for JSON content type (org.codehaus.jackson:jackson-jaxrs:1.8.3 - http://jackson.codehaus.org)
    * Xml Compatibility extensions for Jackson (org.codehaus.jackson:jackson-xc:1.8.3 - http://jackson.codehaus.org)

  GPL2 w/ CPE:

    * JAXB RI (com.sun.xml.bind:jaxb-impl:2.2.3-1 - http://jaxb.java.net/)
    * javax.ws.rs-api (javax.ws.rs:javax.ws.rs-api:2.0.1 - http://jax-rs-spec.java.net)
    * jersey-core (com.sun.jersey:jersey-core:1.9 - https://jersey.java.net/jersey-core/)
    * jersey-guice (com.sun.jersey.contribs:jersey-guice:1.9 - https://jersey.java.net/jersey-contribs/jersey-guice/)
    * jersey-json (com.sun.jersey:jersey-json:1.9 - https://jersey.java.net/jersey-json/)
    * jersey-server (com.sun.jersey:jersey-server:1.9 - https://jersey.java.net/jersey-server/)

That said, these are all dual-licensed so I will add appropriate document/process to the release process to indicate as such. In the future, we could potentially upgrade to the latest version (2.2.0) of license-maven-plugin as it supports an `overrideFile` feature that allows us to specify the desired license. Unfortunately it also requires an update to maven.

For the time being, documenting here the preferred license for the above.

- Jersey dependences (jersey-core, jersey-guice, jersey-json, jersey-server): Eclipse Public License - v 2.0
- javax.ws.rs-api: CDDL 1.1
- JAXB RI: CDDL 1.1
- Jackson dependencies (jackson-jaxrs, jackson-xc): Apache 2.0;;;","25/Sep/23 17:19;dacort;https://github.com/apache/incubator-livy/pull/421;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Copyrights in NOTICE and footer,LIVY-984,13546237,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,lmccay,jmclean,lmccay,05/Aug/23 15:41,02/Aug/25 15:40,19/Dec/25 04:15,17/Aug/23 02:57,0.8.0,,,0.8.0,,,,,,,,,,,,0,,,,,,"- NOTICE is OK, but please do not use ""Copyright 2018 and onwardsâ€.
- you may wan to update your copyright in [4]

4. ./docs/_includes/themes/apache/footer.html",,"lmccay opened a new pull request, #411:
URL: https://github.com/apache/incubator-livy/pull/411

   LIVY-984 - Update Copyrights in NOTICE and footer
   
   ## What changes were proposed in this pull request?
   
   Update copyrights in NOTICE and footer.html files as per release VOTE comments
   
   ## How was this patch tested?
   
   N/A
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   


;16/Aug/23 23:59;githubbot;600","lmccay merged PR #411:
URL: https://github.com/apache/incubator-livy/pull/411


;17/Aug/23 00:39;githubbot;600","codecov-commenter commented on PR #411:
URL: https://github.com/apache/incubator-livy/pull/411#issuecomment-3146574252

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/411?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   :white_check_mark: All modified and coverable lines are covered by tests.
   :white_check_mark: Project coverage is 28.65%. Comparing base ([`deffeeb`](https://app.codecov.io/gh/apache/incubator-livy/commit/deffeeb68b1adca0c9b50fadec1e0e5fc0aec0a6?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)) to head ([`8366c32`](https://app.codecov.io/gh/apache/incubator-livy/commit/8366c32baad72417236b0cc2289568f2101425c3?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)).
   :warning: Report is 3 commits behind head on branch-0.8.
   > :exclamation:  There is a different number of reports uploaded between BASE (deffeeb) and HEAD (8366c32). Click for more details.
   > 
   > <details><summary>HEAD has 3 uploads less than BASE</summary>
   >
   >| Flag | BASE (deffeeb) | HEAD (8366c32) |
   >|------|------|------|
   >||6|3|
   ></details>
   
   <details><summary>Additional details and impacted files</summary>
   
   
   
   ```diff
   @@                Coverage Diff                @@
   ##             branch-0.8     #411       +/-   ##
   =================================================
   - Coverage         65.56%   28.65%   -36.92%     
   + Complexity          952      379      -573     
   =================================================
     Files               103      103               
     Lines              6044     6044               
     Branches            911      911               
   =================================================
   - Hits               3963     1732     -2231     
   - Misses             1537     3963     +2426     
   + Partials            544      349      -195     
   ```
   </details>
   
   [:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/apache/incubator-livy/pull/411?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).   
   :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   <details><summary> :rocket: New features to boost your workflow: </summary>
   
   - :snowflake: [Test Analytics](https://docs.codecov.com/docs/test-analytics): Detect flaky tests, report on failures, and find test suite problems.
   - :package: [JS Bundle Analysis](https://docs.codecov.com/docs/javascript-bundle-analysis): Save yourself from yourself by tracking and limiting bundle sizes in JS merges.
   </details>


;02/Aug/25 15:40;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 17 02:56:55 UTC 2023,,,,,,,,,,"0|z1jlwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Aug/23 19:23;lmccay;Pulling this back into 0.8.0 as it seems that [~jmclean] feels it is a blocker for the release.

Â ;;;","17/Aug/23 02:56;lmccay;This has been committed to both master and branch-0.8 for the 0.8.0 release.

Â ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
licensing of ldapsdk,LIVY-981,13545570,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,dacort,pj.fanning,pj.fanning,31/Jul/23 17:57,12/Dec/25 21:59,19/Dec/25 04:15,12/Dec/25 21:59,,,,0.9.0,,Build,,,,,,,,,,0,,,,,,"The 0.8.0-incubating binaries at [https://dist.apache.org/repos/dist/dev/incubator/livy/0.8.0-incubating-rc1/] include a THIRD-PARTY file that lists ldapsdk as having an unknown license.

[https://dist.apache.org/repos/dist/dev/incubator/livy/0.8.0-incubating-rc1/]

We should find out what the license is.

https://issues.apache.org/jira/browse/LEGAL-160 seems to relate to this dependency.

This appears to be its GItHub page - [https://github.com/pingidentity/ldapsdk]

This appears to be the license for 4.x release.

[https://github.com/pingidentity/ldapsdk/blob/4.0.0/LICENSE-UnboundID-LDAPSDK.txt]

Since release 5.0, ldapsdk can be licensed as ASL. If you upgrade to v5.0, then we should explicitly state that we use ldapsdk under ASL license.",,"dacort opened a new pull request, #425:
URL: https://github.com/apache/incubator-livy/pull/425

   ## What changes were proposed in this pull request?
   
   - Added `test,provided` to `excludedScopes` property for license maven plugin.
   - https://issues.apache.org/jira/browse/LIVY-981
   
   ## How was this patch tested?
   
   - Generated licenses manually, identified test-scope license removed from generated file
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   


;05/Oct/23 18:46;githubbot;600","codecov-commenter commented on PR #425:
URL: https://github.com/apache/incubator-livy/pull/425#issuecomment-1749503516

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/425?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   > Merging [#425](https://app.codecov.io/gh/apache/incubator-livy/pull/425?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) (76a1f45) into [master](https://app.codecov.io/gh/apache/incubator-livy/commit/8b2e29fe9fd42c20395c63e1571f2492f005162b?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) (8b2e29f) will **increase** coverage by `2.58%`.
   > The diff coverage is `n/a`.
   
   ```diff
   @@             Coverage Diff              @@
   ##             master     #425      +/-   ##
   ============================================
   + Coverage     65.48%   68.07%   +2.58%     
   - Complexity      950      979      +29     
   ============================================
     Files           103      103              
     Lines          6062     6062              
     Branches        916      916              
   ============================================
   + Hits           3970     4127     +157     
   + Misses         1541     1350     -191     
   - Partials        551      585      +34     
   ```
   
   
   [see 24 files with indirect coverage changes](https://app.codecov.io/gh/apache/incubator-livy/pull/425/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)
   


;05/Oct/23 19:17;githubbot;600","gyogal merged PR #425:
URL: https://github.com/apache/incubator-livy/pull/425


;12/Dec/25 16:49;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,LIVY-982,LIVY-985,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 05 18:46:57 UTC 2023,,,,,,,,,,"0|z1jhs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"31/Jul/23 19:04;pj.fanning;Looks like this jar does not even get added to the bin zip. So this entry should be removed from the THIRD-PARTY file.;;;","01/Aug/23 05:48;dacort;Thanks [~pj.fanning] - Duly noted.

The jar gets pulled in in the test scope via {{apacheds-server-integ}}.;;;","05/Oct/23 17:16;fanningpj;[~dacort] I won't vote against the RC2 release but I would appreciate if the THIRD-PARTY file is fixed so that it does not include a reference to the ldapsdk jar.

;;;","05/Oct/23 18:46;dacort;[~pj.fanning] Thank you. PR to remove test/provided scopes opened here against the main branch. 

https://github.com/apache/incubator-livy/pull/425;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Livy release script,LIVY-980,13544493,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,dacort,dacort,dacort,21/Jul/23 21:49,27/Oct/25 17:28,19/Dec/25 04:15,27/Oct/25 17:24,0.8.0,,,0.9.0,,Build,,,,,,,,,,0,,,,,,I had to make several changes to the release-build.sh script to modernize it and account for new maven profiles. This is a tracking ticket for that work and some additional enhancements.,,"dacort opened a new pull request, #490:
URL: https://github.com/apache/incubator-livy/pull/490

   ## What changes were proposed in this pull request?
   
   https://issues.apache.org/jira/browse/LIVY-980
   
   During the previous release, several changes had to be made to the release script to get it to work. This PR includes those changes.
   
   ## How was this patch tested?
   
   Manually during the release process. 
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   


;24/Oct/25 18:28;githubbot;600","gyogal merged PR #490:
URL: https://github.com/apache/incubator-livy/pull/490


;27/Oct/25 17:23;githubbot;600","gyogal commented on PR #490:
URL: https://github.com/apache/incubator-livy/pull/490#issuecomment-3452491944

   Thank you @dacort !


;27/Oct/25 17:26;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 27 17:28:11 UTC 2025,,,,,,,,,,"0|z1jb5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Oct/25 17:28;gyogal;The PR can be found at https://github.com/apache/incubator-livy/pull/490;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the application tag generation logic in Livy,LIVY-978,13540984,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,asifkhatri,asifkhatri,asifkhatri,22/Jun/23 06:34,02/Aug/25 15:44,19/Dec/25 04:15,22/Jul/23 12:56,0.7.0,,,0.9.0,,Server,,,,,,,,,,0,,,,,,"The following issue occurs:
{code:java}
ERROR org.apache.livy.utils.SparkYarnApp: Error whiling refreshing YARN state
java.lang.IllegalStateException: No YARN application is found with tag livy-batch-2-per2hlwa in 120 seconds. This may be because 1) spark-submit fail to submit application to YARN; or 2) YARN cluster doesn't have enough resources to start the application in time. Please check Livy log and YARN log to know the details.
{code}
if the below configuration parameter is set in YARN:
{code:java}
yarn.resourcemanager.application-tag-based-placement.force-lowercase=false
{code}
We can fix this in InteractiveSession by updating app tag generation logic to generate appTag always in lowercase characters. InteractiveSession code can be changed

from
{code:java}
val appTag = s""livy-session-$id-${Random.alphanumeric.take(8).mkString}""
{code}
to
{code:java}
val appTag = s""livy-session-$id-${Random.alphanumeric.take(8).mkString}"".toLowerCase()
{code}
at [https://github.com/apache/incubator-livy/blob/master/server/src/main/scala/org/apache/livy/server/interactive/InteractiveSession.scala#L92]",,"askhatri opened a new pull request, #409:
URL: https://github.com/apache/incubator-livy/pull/409

   ## What changes were proposed in this pull request?
   
   The following issue occurs:
   
   ```
   ERROR org.apache.livy.utils.SparkYarnApp: Error whiling refreshing YARN state
   java.lang.IllegalStateException: No YARN application is found with tag livy-batch-2-per2hlwa in 120 seconds. This may be because 1) spark-submit fail to submit application to YARN; or 2) YARN cluster doesn't have enough resources to start the application in time. Please check Livy log and YARN log to know the details.
   ```
   
   if the below configuration parameter is set in YARN:
   ```
   yarn.resourcemanager.application-tag-based-placement.force-lowercase=false
   ```
   We can fix this in InteractiveSession / BatchSession by updating app tag generation logic to generate appTag always in lowercase characters.
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-978
   
   ## How was this patch tested?
   
   Verified manually by creating interactive / batch sessions via REST API call in a local Yarn cluster.
   


;28/Jun/23 06:31;githubbot;600","leesf merged PR #409:
URL: https://github.com/apache/incubator-livy/pull/409


;22/Jul/23 03:40;githubbot;600","codecov-commenter commented on PR #409:
URL: https://github.com/apache/incubator-livy/pull/409#issuecomment-3146575880

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/409?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   :white_check_mark: All modified and coverable lines are covered by tests.
   :white_check_mark: Project coverage is 28.57%. Comparing base ([`314f2de`](https://app.codecov.io/gh/apache/incubator-livy/commit/314f2def423954a77eb75f72e9bf7f4e51a6f7f4?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)) to head ([`0e40ed8`](https://app.codecov.io/gh/apache/incubator-livy/commit/0e40ed86f002e5ecaba32d8803681ddea7e6af78?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)).
   :warning: Report is 24 commits behind head on master.
   
   <details><summary>Additional details and impacted files</summary>
   
   
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #409       +/-   ##
   =============================================
   - Coverage     65.58%   28.57%   -37.02%     
   + Complexity      952      379      -573     
   =============================================
     Files           103      103               
     Lines          6044     6044               
     Branches        911      911               
   =============================================
   - Hits           3964     1727     -2237     
   - Misses         1536     3968     +2432     
   + Partials        544      349      -195     
   ```
   </details>
   
   [:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/apache/incubator-livy/pull/409?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).   
   :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   <details><summary> :rocket: New features to boost your workflow: </summary>
   
   - :snowflake: [Test Analytics](https://docs.codecov.com/docs/test-analytics): Detect flaky tests, report on failures, and find test suite problems.
   - :package: [JS Bundle Analysis](https://docs.codecov.com/docs/javascript-bundle-analysis): Save yourself from yourself by tracking and limiting bundle sizes in JS merges.
   </details>


;02/Aug/25 15:44;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 28 06:33:53 UTC 2023,,,,,,,,,,"0|z1iplc:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Jun/23 06:33;asifkhatri;I have created PR https://github.com/apache/incubator-livy/pull/409 for this JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy can not be started if HDFS is still in safe mode,LIVY-977,13539853,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,rajshekhar,asifkhatri,asifkhatri,13/Jun/23 12:57,12/Mar/24 15:28,19/Dec/25 04:15,12/Mar/24 15:28,0.7.0,,,0.9.0,,Server,,,,,,,,,,0,,,,,,"When services are started by {{cdp environments start-environment}}, sometimes HDFS is started too close to Livy and this may lead to

{noformat}
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /user/livy/recovery. Name node is in safe mode.
The reported blocks 407318 has reached the threshold 0.9990 of total blocks 407318. The number of live datanodes 4 has reached the minimum number 1. In safe mode extension. Safe mode will be turned off automatically in 18 seconds. NamenodeHostName:bpicdpdev-datahub-de-master0.bpicdpde.ggj4-2xqj.cloudera.site
...
        at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:812)
        at org.apache.livy.server.recovery.FileSystemStateStore.<init>(FileSystemStateStore.scala:62)
        at org.apache.livy.server.recovery.FileSystemStateStore.<init>(FileSystemStateStore.scala:42)
{noformat}",,"RajshekharMuchandi opened a new pull request, #440:
URL: https://github.com/apache/incubator-livy/pull/440

   Added safe mode check to implement safe mode
   
   ## What changes were proposed in this pull request?
   
   HDFS safe mode is checked when livy session is created. If safe mode is ON, then IllegalStateException is thrown after max retry attempts (configurable) with safe mode interval (configurable) checks are done. If safe mode is OFF, then livy will be able to create session.
    
   https://issues.apache.org/jira/browse/LIVY-977
   
   ## How was this patch tested?
   
   Added unit test cases to validate code changes. Also, done manual testing in CDP cluster by creating livy sessions with HDFS safe mode check ON/OFF.


;24/Jan/24 09:01;githubbot;600","codecov-commenter commented on PR #440:
URL: https://github.com/apache/incubator-livy/pull/440#issuecomment-1908175441

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/440?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   Attention: `7 lines` in your changes are missing coverage. Please review.
   > Comparison is base [(`86fc823`)](https://app.codecov.io/gh/apache/incubator-livy/commit/86fc823893ee96d4effaa4f2b8ef6603cea9d77a?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) 65.45% compared to head [(`10ee39f`)](https://app.codecov.io/gh/apache/incubator-livy/pull/440?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) 28.77%.
   
   | [Files](https://app.codecov.io/gh/apache/incubator-livy/pull/440?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) | Patch % | Lines |
   |---|---|---|
   | [...he/livy/server/recovery/FileSystemStateStore.scala](https://app.codecov.io/gh/apache/incubator-livy/pull/440?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvcmVjb3ZlcnkvRmlsZVN5c3RlbVN0YXRlU3RvcmUuc2NhbGE=) | 53.33% | [4 Missing and 3 partials :warning: ](https://app.codecov.io/gh/apache/incubator-livy/pull/440?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) |
   
   <details><summary>Additional details and impacted files</summary>
   
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #440       +/-   ##
   =============================================
   - Coverage     65.45%   28.77%   -36.68%     
   + Complexity      954      384      -570     
   =============================================
     Files           103      103               
     Lines          6084     6102       +18     
     Branches        922      925        +3     
   =============================================
   - Hits           3982     1756     -2226     
   - Misses         1546     3987     +2441     
   + Partials        556      359      -197     
   ```
   
   
   
   </details>
   
   [:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/apache/incubator-livy/pull/440?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).   
   :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   


;24/Jan/24 13:57;githubbot;600","gyogal commented on PR #440:
URL: https://github.com/apache/incubator-livy/pull/440#issuecomment-1918747185

   LGTM, thank you for your contribution @RajshekharMuchandi !


;31/Jan/24 09:46;githubbot;600","gyogal merged PR #440:
URL: https://github.com/apache/incubator-livy/pull/440


;12/Mar/24 15:27;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 24 09:01:50 UTC 2024,,,,,,,,,,"0|z1iin4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Jan/24 09:01;rajshekhar;https://github.com/apache/incubator-livy/pull/440;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix compatibility issue related to a change on the Spark side,LIVY-975,13534681,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,asifkhatri,asifkhatri,asifkhatri,02/May/23 12:14,02/Aug/25 15:03,19/Dec/25 04:15,09/May/23 07:41,,,,0.8.0,,Server,,,,,,,,,,0,,,,,,Recent change in spark required us to fix compatibility issue related to a change on the Spark side.,,"askhatri opened a new pull request, #403:
URL: https://github.com/apache/incubator-livy/pull/403

   ## What changes were proposed in this pull request?
   
   Recent change in spark required us to fix compatibility issue related to a change on the Spark side.
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-975
   
   ## How was this patch tested?
   
   Verified manually by creating interactive / batch sessions via REST API call in a local Yarn cluster. Also, we have updated the unit tests.
   


;03/May/23 04:10;githubbot;600","askhatri commented on PR #403:
URL: https://github.com/apache/incubator-livy/pull/403#issuecomment-1534121599

   > LGTM
   
   Thank you @gyogal !


;04/May/23 05:40;githubbot;600","gyogal merged PR #403:
URL: https://github.com/apache/incubator-livy/pull/403


;09/May/23 07:41;githubbot;600","codecov-commenter commented on PR #403:
URL: https://github.com/apache/incubator-livy/pull/403#issuecomment-3146554646

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/403?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   :x: Patch coverage is `66.66667%` with `2 lines` in your changes missing coverage. Please review.
   :white_check_mark: Project coverage is 29.13%. Comparing base ([`12505dd`](https://app.codecov.io/gh/apache/incubator-livy/commit/12505dd88fac4c2b9da92a020205ffb72524a9c8?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)) to head ([`c464b65`](https://app.codecov.io/gh/apache/incubator-livy/commit/c464b6547edb70b25c9599b436542f0d8219eae7?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)).
   :warning: Report is 28 commits behind head on master.
   
   | [Files with missing lines](https://app.codecov.io/gh/apache/incubator-livy/pull/403?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) | Patch % | Lines |
   |---|---|---|
   | [.../main/scala/org/apache/livy/sessions/Session.scala](https://app.codecov.io/gh/apache/incubator-livy/pull/403?src=pr&el=tree&filepath=server%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Flivy%2Fsessions%2FSession.scala&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXNzaW9ucy9TZXNzaW9uLnNjYWxh) | 60.00% | [1 Missing and 1 partial :warning: ](https://app.codecov.io/gh/apache/incubator-livy/pull/403?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) |
   
   <details><summary>Additional details and impacted files</summary>
   
   
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #403       +/-   ##
   =============================================
   - Coverage     65.97%   29.13%   -36.84%     
   + Complexity      835      375      -460     
   =============================================
     Files           103      103               
     Lines          6063     6068        +5     
     Branches        913      914        +1     
   =============================================
   - Hits           4000     1768     -2232     
   - Misses         1536     3948     +2412     
   + Partials        527      352      -175     
   ```
   </details>
   
   [:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/apache/incubator-livy/pull/403?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).   
   :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   <details><summary> :rocket: New features to boost your workflow: </summary>
   
   - :snowflake: [Test Analytics](https://docs.codecov.com/docs/test-analytics): Detect flaky tests, report on failures, and find test suite problems.
   - :package: [JS Bundle Analysis](https://docs.codecov.com/docs/javascript-bundle-analysis): Save yourself from yourself by tracking and limiting bundle sizes in JS merges.
   </details>


;02/Aug/25 15:03;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 03 04:11:21 UTC 2023,,,,,,,,,,"0|z1hmy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/May/23 04:11;asifkhatri;I have created PR [https://github.com/apache/incubator-livy/pull/403] for this JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove verbose output on Livy UI error pages,LIVY-974,13534666,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,asifkhatri,asifkhatri,asifkhatri,02/May/23 09:28,02/Aug/25 15:16,19/Dec/25 04:15,09/May/23 07:39,,,,0.8.0,,Server,,,,,,,,,,0,,,,,,"On error, the Livy UI shows verbose output on error pages including the Jetty version number. This could be considered as a security vulnerability. We can make it configurable and avoid sending server version details.

The Jetty version is there in every response header as well:
{noformat}
$ curl -v $LIVY_URL/sessions
...
< Server: Jetty(9.4.43.v20210629){noformat}",,"askhatri opened a new pull request, #404:
URL: https://github.com/apache/incubator-livy/pull/404

   ## What changes were proposed in this pull request?
   
   On error, the Livy UI shows verbose output on error pages including the Jetty version number. This could be considered as a security vulnerability. We can make it configurable and avoid sending server version details. The Jetty version is there in every response header as well.
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-974
   
   ## How was this patch tested?
   
   Verified manually by calling get sessions via REST API call in a local Yarn cluster.
   


;03/May/23 04:19;githubbot;600","askhatri commented on PR #404:
URL: https://github.com/apache/incubator-livy/pull/404#issuecomment-1534121722

   > Contributor
   
   Thank you @gyogal !


;04/May/23 05:40;githubbot;600","gyogal merged PR #404:
URL: https://github.com/apache/incubator-livy/pull/404


;09/May/23 07:13;githubbot;600","codecov-commenter commented on PR #404:
URL: https://github.com/apache/incubator-livy/pull/404#issuecomment-3146560720

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/404?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   :x: Patch coverage is `66.66667%` with `1 line` in your changes missing coverage. Please review.
   :white_check_mark: Project coverage is 29.08%. Comparing base ([`12505dd`](https://app.codecov.io/gh/apache/incubator-livy/commit/12505dd88fac4c2b9da92a020205ffb72524a9c8?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)) to head ([`88d1db9`](https://app.codecov.io/gh/apache/incubator-livy/commit/88d1db9d1303a51b97165bbc6f38d0a5db2c11b1?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)).
   :warning: Report is 28 commits behind head on master.
   
   | [Files with missing lines](https://app.codecov.io/gh/apache/incubator-livy/pull/404?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) | Patch % | Lines |
   |---|---|---|
   | [.../main/scala/org/apache/livy/server/WebServer.scala](https://app.codecov.io/gh/apache/incubator-livy/pull/404?src=pr&el=tree&filepath=server%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Flivy%2Fserver%2FWebServer.scala&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvV2ViU2VydmVyLnNjYWxh) | 50.00% | [1 Missing :warning: ](https://app.codecov.io/gh/apache/incubator-livy/pull/404?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) |
   
   <details><summary>Additional details and impacted files</summary>
   
   
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #404       +/-   ##
   =============================================
   - Coverage     65.97%   29.08%   -36.90%     
   + Complexity      835      375      -460     
   =============================================
     Files           103      103               
     Lines          6063     6066        +3     
     Branches        913      913               
   =============================================
   - Hits           4000     1764     -2236     
   - Misses         1536     3949     +2413     
   + Partials        527      353      -174     
   ```
   </details>
   
   [:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/apache/incubator-livy/pull/404?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).   
   :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   <details><summary> :rocket: New features to boost your workflow: </summary>
   
   - :snowflake: [Test Analytics](https://docs.codecov.com/docs/test-analytics): Detect flaky tests, report on failures, and find test suite problems.
   - :package: [JS Bundle Analysis](https://docs.codecov.com/docs/javascript-bundle-analysis): Save yourself from yourself by tracking and limiting bundle sizes in JS merges.
   </details>


;02/Aug/25 15:16;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/May/23 09:27;asifkhatri;image.png;https://issues.apache.org/jira/secure/attachment/13057771/image.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 03 04:21:03 UTC 2023,,,,,,,,,,"0|z1hmuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/May/23 04:21;asifkhatri;I have created PRÂ https://github.com/apache/incubator-livy/pull/404Â for this JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Always close RSCClient's EventLoopGroup,LIVY-973,13530184,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,rajshekhar,asifkhatri,asifkhatri,27/Mar/23 06:35,19/Apr/23 15:52,19/Dec/25 04:15,12/Apr/23 07:56,0.6.0,0.7.0,0.8.0,0.8.0,,Interpreter,Server,,,,,,,,,0,,,,,,"Livy server is going unresponsive after consuming/hitting the max configured open file descriptor limit. The issue observed with RSCClient's EventLoopGroup.

{{The `setFailure()` of a job promise before the `shutdownGracefully()` can end up in an `IllegalStateException` preventing the graceful shutdown:}}

{{java.lang.IllegalStateException: complete already: DefaultPromise@185d2680(failure: [java.io|http://java.io/].IOException: Child process exited with code 1.) at io.netty.util.concurrent.DefaultPromise.setFailure(DefaultPromise.java:112) at org.apache.livy.rsc.JobHandleImpl.setFailure(JobHandleImpl.java:101) at org.apache.livy.rsc.RSCClient.stop(RSCClient.java:264) at org.apache.livy.rsc.RSCClient.connectionError(RSCClient.java:171) at org.apache.livy.rsc.RSCClient.access$200(RSCClient.java:52) at org.apache.livy.rsc.RSCClient$1.onFailure(RSCClient.java:108) at org.apache.livy.rsc.Utils$2.operationComplete(Utils.java:108) at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578) at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571) at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550) at io.netty.util.concurrent.DefaultPromise.access$200(DefaultPromise.java:35) at io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:502) at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500) at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)}}

Â ",,"RajshekharMuchandi opened a new pull request, #398:
URL: https://github.com/apache/incubator-livy/pull/398

   ## What changes were proposed in this pull request?
   
   Added try-catch block to jobs' `setFailure()`and  log entries if there is any exception. `eventLoopGroup.shutdownGracefully();` statement will be executed after exception is caught.
   
   https://issues.apache.org/jira/browse/LIVY-973
   
   ## How was this patch tested?
   
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   
   


;03/Apr/23 12:28;githubbot;600","codecov-commenter commented on PR #398:
URL: https://github.com/apache/incubator-livy/pull/398#issuecomment-1502931018

   ## [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/398?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report
   > Merging [#398](https://codecov.io/gh/apache/incubator-livy/pull/398?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (5108bd6) into [master](https://codecov.io/gh/apache/incubator-livy/commit/45e07fec68f2b9ad1dc7ebce8db08ad8a778dddc?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (45e07fe) will **decrease** coverage by `40.73%`.
   > The diff coverage is `0.00%`.
   
   > :exclamation: Current head 5108bd6 differs from pull request most recent head fc08516. Consider uploading reports for the commit fc08516 to get more accurate results
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #398       +/-   ##
   =============================================
   - Coverage     68.37%   27.65%   -40.73%     
   + Complexity      849      363      -486     
   =============================================
     Files           103      103               
     Lines          5989     5992        +3     
     Branches        911      911               
   =============================================
   - Hits           4095     1657     -2438     
   - Misses         1330     3989     +2659     
   + Partials        564      346      -218     
   ```
   
   
   | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/398?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage Î” | |
   |---|---|---|
   | [...c/src/main/java/org/apache/livy/rsc/RSCClient.java](https://codecov.io/gh/apache/incubator-livy/pull/398?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9SU0NDbGllbnQuamF2YQ==) | `0.00% <0.00%> (-73.50%)` | :arrow_down: |
   
   ... and [81 files with indirect coverage changes](https://codecov.io/gh/apache/incubator-livy/pull/398/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   


;11/Apr/23 08:49;githubbot;600","gyogal merged PR #398:
URL: https://github.com/apache/incubator-livy/pull/398


;12/Apr/23 07:56;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 06 08:17:13 UTC 2023,,,,,,,,,,"0|z1gvbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Apr/23 08:17;gyogal;A PR is available at https://github.com/apache/incubator-livy/pull/398;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement CI for Livy project,LIVY-972,13528339,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,dacort,dacort,dacort,14/Mar/23 05:14,11/Apr/23 14:25,19/Dec/25 04:15,08/Apr/23 18:45,0.8.0,,,0.8.0,,,,,,,,,,,,0,,,,,,"As of February 2023, Travis CI has been deprecated across all Apache projects. 

We need to implement a replacement in order to ensure passing tests.

I propose using GitHub Actions - Other large projects like Spark and Airflow make use of them for CI and the infra page has some [more detail on it here|https://cwiki.apache.org/confluence/display/BUILDS/GitHub+Actions+status], but it's a bit dated.

I imagine three different actions being created:
* One build-ci-image workflow that creates an image that can be used for CI without having to rebuild every run
* One unit-test workflow that runs the unit tests for appropriate versions on each push
* One integration-test workflow that runs the longer integration tests on pull requests",,"codecov-commenter commented on PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#issuecomment-1470970692

   ## [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/393?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report
   > Merging [#393](https://codecov.io/gh/apache/incubator-livy/pull/393?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (01b12d4) into [master](https://codecov.io/gh/apache/incubator-livy/commit/45e07fec68f2b9ad1dc7ebce8db08ad8a778dddc?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (45e07fe) will **decrease** coverage by `2.78%`.
   > The diff coverage is `n/a`.
   
   ```diff
   @@             Coverage Diff              @@
   ##             master     #393      +/-   ##
   ============================================
   - Coverage     68.37%   65.60%   -2.78%     
   + Complexity      849      822      -27     
   ============================================
     Files           103      103              
     Lines          5989     5989              
     Branches        911      911              
   ============================================
   - Hits           4095     3929     -166     
   - Misses         1330     1534     +204     
   + Partials        564      526      -38     
   ```
   
   
   [see 24 files with indirect coverage changes](https://codecov.io/gh/apache/incubator-livy/pull/393/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   


;15/Mar/23 23:20;githubbot;600","ksumit commented on code in PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#discussion_r1142783327


##########
.github/workflows/build-ci-image.yaml:
##########
@@ -0,0 +1,48 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+name: 'Build CI images'
+on: 
+  push:
+    branches: [""main""]
+    paths:
+    - 'dev/docker/Dockerfile'
+jobs:
+  docker-build:
+    runs-on: ubuntu-latest
+    steps:
+      - 
+        name: Checkout
+        uses: actions/checkout@v3
+      -
+        name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v2
+      -
+        name: Login to the GitHub Container Registry
+        uses: docker/login-action@v2
+        with:
+          registry: ghcr.io
+          username: ${{ github.repository_owner }}
+          password: ${{ secrets.GITHUB_TOKEN }}
+      - 
+        name: Build and push image
+        id: docker_build
+        uses: docker/build-push-action@v4
+        with:
+          push: true

Review Comment:
   this is where we could change the logic to false if it's a PR build?



##########
.github/workflows/build-ci-image.yaml:
##########
@@ -0,0 +1,48 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+name: 'Build CI images'
+on: 
+  push:
+    branches: [""main""]
+    paths:
+    - 'dev/docker/Dockerfile'

Review Comment:
   1. should we add this file in this list too? we would like to trigger a build for changes in this file as well right?
   2. do we need a PR pipeline for validating PRs? I think all the logic will remain the same, we will build the image but won't push it unless it's a CI build? Should we use branches to determine a CI vs PR build, for example if the branch matches a pattern including ""release-XXX""?
   



##########
.github/workflows/unit-tests.yaml:
##########
@@ -0,0 +1,52 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+name: Unit Tests
+on: [push]
+env:
+  MAVEN_OPTS: -Dhttp.keepAlive=false -Dmaven.wagon.http.pool=false -Dmaven.wagon.http.retryHandler.class=standard -Dmaven.wagon.http.retryHandler.count=3

Review Comment:
   same as in `integration-tests.yaml`?



##########
.github/workflows/integration-tests.yaml:
##########
@@ -0,0 +1,54 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+name: Integration Tests
+on:
+  pull_request:
+    types: [opened, reopened, synchronize]
+env:
+  MAVEN_OPTS: -Dhttp.keepAlive=false -Dmaven.wagon.http.pool=false -Dmaven.wagon.http.retryHandler.class=standard -Dmaven.wagon.http.retryHandler.count=3
+jobs:
+  build:
+    runs-on: ubuntu-20.04
+    # TODO: Possibly point to the ./build-ci-image.yaml with the ""uses"" key
+    container: ghcr.io/${{ github.repository_owner }}/livy-ci:latest
+    strategy:
+      matrix:
+        spark_version: [""2.4"", ""3.0""]
+    steps:
+    - 
+      name: Checkout
+      uses: actions/checkout@v3
+    - 
+      name: Cache local Maven repository
+      uses: actions/cache@v3
+      with:
+        path: |
+          /root/.m2/repository
+          !/root/.m2/repository/org/apache/livy
+        key: ${{ runner.os }}-maven-${{ hashFiles('pom.xml', '*/pom.xml', 'thriftserver/*/pom.xml', 'core/*/pom.xml', 'repl/*/pom.xml', 'scala-api/*/pom.xml') }}

Review Comment:
   does `hashFiles` support `**/pom.xml`?



##########
.github/workflows/integration-tests.yaml:
##########
@@ -0,0 +1,54 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+name: Integration Tests
+on:
+  pull_request:
+    types: [opened, reopened, synchronize]
+env:
+  MAVEN_OPTS: -Dhttp.keepAlive=false -Dmaven.wagon.http.pool=false -Dmaven.wagon.http.retryHandler.class=standard -Dmaven.wagon.http.retryHandler.count=3

Review Comment:
   do we need the parameters other than the retry count logic?



;21/Mar/23 00:13;githubbot;600","ksumit commented on code in PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#discussion_r1142789396


##########
.github/workflows/integration-tests.yaml:
##########
@@ -0,0 +1,54 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+name: Integration Tests
+on:
+  pull_request:
+    types: [opened, reopened, synchronize]
+env:
+  MAVEN_OPTS: -Dhttp.keepAlive=false -Dmaven.wagon.http.pool=false -Dmaven.wagon.http.retryHandler.class=standard -Dmaven.wagon.http.retryHandler.count=3
+jobs:
+  build:
+    runs-on: ubuntu-20.04
+    # TODO: Possibly point to the ./build-ci-image.yaml with the ""uses"" key
+    container: ghcr.io/${{ github.repository_owner }}/livy-ci:latest
+    strategy:
+      matrix:
+        spark_version: [""2.4"", ""3.0""]

Review Comment:
   I think we can add the unit-test vs integration-test logic here.



;21/Mar/23 00:15;githubbot;600","dacort commented on code in PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#discussion_r1143885848


##########
.github/workflows/integration-tests.yaml:
##########
@@ -0,0 +1,54 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+name: Integration Tests
+on:
+  pull_request:
+    types: [opened, reopened, synchronize]
+env:
+  MAVEN_OPTS: -Dhttp.keepAlive=false -Dmaven.wagon.http.pool=false -Dmaven.wagon.http.retryHandler.class=standard -Dmaven.wagon.http.retryHandler.count=3

Review Comment:
   Unfortunately, yes. If the Docker build is busy for a while and doesn't download anything, the http pool times out apparently. There have been other projects that resolved this in a similar way like [pulsar](https://github.com/apache/pulsar/pull/8386). 



;21/Mar/23 19:15;githubbot;600","dacort commented on code in PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#discussion_r1143898274


##########
.github/workflows/integration-tests.yaml:
##########
@@ -0,0 +1,54 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+name: Integration Tests
+on:
+  pull_request:
+    types: [opened, reopened, synchronize]
+env:
+  MAVEN_OPTS: -Dhttp.keepAlive=false -Dmaven.wagon.http.pool=false -Dmaven.wagon.http.retryHandler.class=standard -Dmaven.wagon.http.retryHandler.count=3
+jobs:
+  build:
+    runs-on: ubuntu-20.04
+    # TODO: Possibly point to the ./build-ci-image.yaml with the ""uses"" key
+    container: ghcr.io/${{ github.repository_owner }}/livy-ci:latest
+    strategy:
+      matrix:
+        spark_version: [""2.4"", ""3.0""]
+    steps:
+    - 
+      name: Checkout
+      uses: actions/checkout@v3
+    - 
+      name: Cache local Maven repository
+      uses: actions/cache@v3
+      with:
+        path: |
+          /root/.m2/repository
+          !/root/.m2/repository/org/apache/livy
+        key: ${{ runner.os }}-maven-${{ hashFiles('pom.xml', '*/pom.xml', 'thriftserver/*/pom.xml', 'core/*/pom.xml', 'repl/*/pom.xml', 'scala-api/*/pom.xml') }}

Review Comment:
   It does, but due to the use of the custom `container` there are directories created during the build process that subsequent GitHub actions don't have the ability to access, receiving a ""permission denied"" error while attempting to process. There's some more info about it here: https://github.com/actions/runner/issues/449
   
   There's a couple options:
   - Change the userid that the container is running as to specifically be `1001`.
   - Be more specific with the `pom.xml` paths
   
   Option 2 seemed a little safer.



##########
.github/workflows/unit-tests.yaml:
##########
@@ -0,0 +1,52 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+name: Unit Tests
+on: [push]
+env:
+  MAVEN_OPTS: -Dhttp.keepAlive=false -Dmaven.wagon.http.pool=false -Dmaven.wagon.http.retryHandler.class=standard -Dmaven.wagon.http.retryHandler.count=3

Review Comment:
   Yep. 



;21/Mar/23 19:29;githubbot;600","dacort commented on code in PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#discussion_r1143902325


##########
.github/workflows/build-ci-image.yaml:
##########
@@ -0,0 +1,48 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+name: 'Build CI images'
+on: 
+  push:
+    branches: [""main""]
+    paths:
+    - 'dev/docker/Dockerfile'

Review Comment:
   1. Yes, good idea, will add that.
   2. Hm, good point. I had the `main` branch here so we wouldn't push the image unless it got merged into main, but would be good to be able to validate the image as well before doing so. 



;21/Mar/23 19:33;githubbot;600","dacort commented on PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#issuecomment-1478473651

   > Would it make sense to merge `integration-tests.yaml` and `unit-tests.yaml`
   
   I wasn't quite sure if there was an easy way to do that. Can definitely look more into it, but ideas welcome.


;21/Mar/23 19:35;githubbot;600","ksumit commented on PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#issuecomment-1480416082

   > > Would it make sense to merge `integration-tests.yaml` and `unit-tests.yaml`
   > 
   > I wasn't quite sure if there was an easy way to do that. Can definitely look more into it, but ideas welcome.
   
   Here is an example that i found on https://docs.github.com/en/actions/learn-github-actions/contexts
   ```
   name: CI
   on: push
   jobs:
     prod-check:
       if: ${{ github.ref == 'refs/heads/main' }}
       runs-on: ubuntu-latest
       steps:
         - run: echo ""Deploying to production server on branch $GITHUB_REF""
   ```


;23/Mar/23 00:18;githubbot;600","ksumit commented on code in PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#discussion_r1149796150


##########
.github/workflows/integration-tests.yaml:
##########
@@ -0,0 +1,54 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+name: Integration Tests
+on:
+  pull_request:
+    types: [opened, reopened, synchronize]
+env:
+  MAVEN_OPTS: -Dhttp.keepAlive=false -Dmaven.wagon.http.pool=false -Dmaven.wagon.http.retryHandler.class=standard -Dmaven.wagon.http.retryHandler.count=3
+jobs:
+  build:
+    runs-on: ubuntu-20.04
+    # TODO: Possibly point to the ./build-ci-image.yaml with the ""uses"" key
+    container: ghcr.io/${{ github.repository_owner }}/livy-ci:latest
+    strategy:
+      matrix:
+        spark_version: [""2.4"", ""3.0""]

Review Comment:
   @dacort I wouldn't want to hold this PR for minor things. If you feel comfortable with the current state, I would push it as it is (something better than nothing) and then iterate on the improvements later on. Let me know what you think.



;27/Mar/23 21:12;githubbot;600","gyogal commented on PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#issuecomment-1498993782

   Thanks a lot for working on this @dacort! I agree with @ksumit's points and if you feel like this is good to go as is, we could merge it to restore CI functionality for the project.
   Post-merge would it be possible to run this for PRs received recently where the builds were not kicked off (such as #396)?


;06/Apr/23 12:32;githubbot;600","dacort commented on PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#issuecomment-1500720773

   Thanks @gyogal - Yea, @ksumit I think for now we should merge this. I imagine the PR workflow and unit-test workflow will diverge in the future so would rather get this merged and iterate.
   
   @gyogal re: run the checks retroactively, I don't think it'll happen automatically but if folks push another commit it should.


;07/Apr/23 23:27;githubbot;600","gyogal merged PR #393:
URL: https://github.com/apache/incubator-livy/pull/393


;08/Apr/23 18:42;githubbot;600","gyogal commented on PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#issuecomment-1503237784

   Thank you for your work on this! I have merged your commit and pre-commit checks are looking good now. There are two minor issues I noticed (maybe these are just temporary and will go away after a few builds):
   
   1. codecov seems to report an unrealistic drop in coverage on PRs (Merging ... will decrease coverage by 40.09%.)
   2. post-commit unit tests are reporting failures in org.apache.livy.rsc.TestSparkClient
   
   Again, many thanks for adding these workflows because it unblocks PRs and is of great help for the project!


;11/Apr/23 12:27;githubbot;600","dacort commented on PR #393:
URL: https://github.com/apache/incubator-livy/pull/393#issuecomment-1503466473

   @gyogal Yep I noticed that too as I work on #392. I initially thought it was related to my changes, but then noticed the failures on the main branch after this PR got merged in. I'm not quite sure what's going on as the unit tests ran fine in the PR. 
   
   I'm also guessing the codecov might be related to the unit tests not running but that's just a theory at this point. 


;11/Apr/23 14:25;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,8400,,,0,8400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Apr 08 18:44:59 UTC 2023,,,,,,,,,,"0|z1gjy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Mar/23 17:27;ksumit;We would also need one ci workflow so we can generate and publish the built artifacts?;;;","08/Apr/23 18:44;gyogal;PR at https://github.com/apache/incubator-livy/pull/393;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ordering and pagination support in GET /statement request,LIVY-970,13525077,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,asifkhatri,gyogal,gyogal,16/Feb/23 18:42,11/Apr/23 12:29,19/Dec/25 04:15,11/Apr/23 12:14,,,,0.8.0,,API,,,,,,,,,,0,,,,,,"GET /sessions/id/statements returns a list of statements run in a session. However the ordering of the statements are older ones first. Livy could expose a parameter (something like orderBy=latest) that will order the list of statements as latest first.

Also there seems to be pagination support for statements but it is undocumented, this could be addressed as well.",,"askhatri opened a new pull request, #389:
URL: https://github.com/apache/incubator-livy/pull/389

   ## What changes were proposed in this pull request?
   
   GET /sessions/id/statements returns a list of statements run in a session. However the ordering of the statements are older ones first. Livy could expose a parameter (something like orderBy=latest) that will order the list of statements as latest first.
   JIRA: https://issues.apache.org/jira/browse/LIVY-970
   
   ## How was this patch tested?
   
   Verified manually by creating interactive session and statements via REST API call in a local Yarn cluster.
   


;17/Feb/23 05:07;githubbot;600","ksumit commented on code in PR #389:
URL: https://github.com/apache/incubator-livy/pull/389#discussion_r1132689120


##########
docs/rest-api.md:
##########
@@ -251,6 +251,27 @@ Gets the log lines from this session.
 
 Returns all the statements in a session.
 
+#### Request Parameters
+
+<table class=""table"">
+  <tr><th>Name</th><th>Description</th><th>Type</th></tr>
+  <tr>
+    <td>from</td>
+    <td>The start index to fetch sessions</td>
+    <td>int</td>
+  </tr>
+  <tr>
+    <td>size</td>
+    <td>Number of sessions to fetch</td>
+    <td>int</td>
+  </tr>
+  <tr>
+    <td>order</td>
+    <td>Provide value as ""latest"" to get latest statement first </td>

Review Comment:
   in sql world, ordering has two choices - `asc` or `desc`
   
   should we do something similar here by providing explicit choices with the default being the ordering behavior as it is right now?



##########
server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala:
##########
@@ -113,7 +113,12 @@ class InteractiveSessionServlet(
 
   get(""/:id/statements"") {
     withViewAccessSession { session =>
-      val statements = session.statements
+      val order = params.get(""order"")
+      val statements = if (order.map(_.trim).exists(_.equalsIgnoreCase(""latest""))) {
+        session.statements.reverse

Review Comment:
   should these parameters be passed to `ReplDriver` instead and the processing be done there instead? that will be consistent to how we are handling other parameters `from` and `size`, thoughts?



;10/Mar/23 18:03;githubbot;600","askhatri commented on code in PR #389:
URL: https://github.com/apache/incubator-livy/pull/389#discussion_r1135627204


##########
server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala:
##########
@@ -113,7 +113,12 @@ class InteractiveSessionServlet(
 
   get(""/:id/statements"") {
     withViewAccessSession { session =>
-      val statements = session.statements
+      val order = params.get(""order"")
+      val statements = if (order.map(_.trim).exists(_.equalsIgnoreCase(""latest""))) {
+        session.statements.reverse

Review Comment:
   I discussed with others in my team and found that we are good to use short it in Interactive Session Servlet for now.



;14/Mar/23 14:21;githubbot;600","askhatri commented on code in PR #389:
URL: https://github.com/apache/incubator-livy/pull/389#discussion_r1135630334


##########
docs/rest-api.md:
##########
@@ -251,6 +251,27 @@ Gets the log lines from this session.
 
 Returns all the statements in a session.
 
+#### Request Parameters
+
+<table class=""table"">
+  <tr><th>Name</th><th>Description</th><th>Type</th></tr>
+  <tr>
+    <td>from</td>
+    <td>The start index to fetch sessions</td>
+    <td>int</td>
+  </tr>
+  <tr>
+    <td>size</td>
+    <td>Number of sessions to fetch</td>
+    <td>int</td>
+  </tr>
+  <tr>
+    <td>order</td>
+    <td>Provide value as ""latest"" to get latest statement first </td>

Review Comment:
   We can try changing order=latest parameter to 1) id=asc and 2) id=desc. It is find with you or any other thoughts from your side?



;14/Mar/23 14:23;githubbot;600","ksumit commented on code in PR #389:
URL: https://github.com/apache/incubator-livy/pull/389#discussion_r1135966932


##########
server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala:
##########
@@ -113,7 +113,12 @@ class InteractiveSessionServlet(
 
   get(""/:id/statements"") {
     withViewAccessSession { session =>
-      val statements = session.statements
+      val order = params.get(""order"")
+      val statements = if (order.map(_.trim).exists(_.equalsIgnoreCase(""latest""))) {
+        session.statements.reverse

Review Comment:
   @lmccay thoughts on this shortcut logic? I find shortcuts bad for long term because they add burdens:
   1. future devs would have to worry about where such shortcuts exist.
   2. it's non-uniform so testing logic gets distributed to multiple layers.



;14/Mar/23 17:40;githubbot;600","ksumit commented on code in PR #389:
URL: https://github.com/apache/incubator-livy/pull/389#discussion_r1135967856


##########
docs/rest-api.md:
##########
@@ -251,6 +251,27 @@ Gets the log lines from this session.
 
 Returns all the statements in a session.
 
+#### Request Parameters
+
+<table class=""table"">
+  <tr><th>Name</th><th>Description</th><th>Type</th></tr>
+  <tr>
+    <td>from</td>
+    <td>The start index to fetch sessions</td>
+    <td>int</td>
+  </tr>
+  <tr>
+    <td>size</td>
+    <td>Number of sessions to fetch</td>
+    <td>int</td>
+  </tr>
+  <tr>
+    <td>order</td>
+    <td>Provide value as ""latest"" to get latest statement first </td>

Review Comment:
   did you mean `order=asc` and `order=desc`?



;14/Mar/23 17:40;githubbot;600","askhatri commented on code in PR #389:
URL: https://github.com/apache/incubator-livy/pull/389#discussion_r1136510116


##########
docs/rest-api.md:
##########
@@ -251,6 +251,27 @@ Gets the log lines from this session.
 
 Returns all the statements in a session.
 
+#### Request Parameters
+
+<table class=""table"">
+  <tr><th>Name</th><th>Description</th><th>Type</th></tr>
+  <tr>
+    <td>from</td>
+    <td>The start index to fetch sessions</td>
+    <td>int</td>
+  </tr>
+  <tr>
+    <td>size</td>
+    <td>Number of sessions to fetch</td>
+    <td>int</td>
+  </tr>
+  <tr>
+    <td>order</td>
+    <td>Provide value as ""latest"" to get latest statement first </td>

Review Comment:
   ok, initially I was thinking to use id but as you are suggesting let's use `order=asc` and `order=desc`.



;15/Mar/23 04:04;githubbot;600","askhatri commented on code in PR #389:
URL: https://github.com/apache/incubator-livy/pull/389#discussion_r1137056129


##########
docs/rest-api.md:
##########
@@ -251,6 +251,27 @@ Gets the log lines from this session.
 
 Returns all the statements in a session.
 
+#### Request Parameters
+
+<table class=""table"">
+  <tr><th>Name</th><th>Description</th><th>Type</th></tr>
+  <tr>
+    <td>from</td>
+    <td>The start index to fetch sessions</td>
+    <td>int</td>
+  </tr>
+  <tr>
+    <td>size</td>
+    <td>Number of sessions to fetch</td>
+    <td>int</td>
+  </tr>
+  <tr>
+    <td>order</td>
+    <td>Provide value as ""latest"" to get latest statement first </td>

Review Comment:
   I have updated the code @ksumit .. could you please review it again?



;15/Mar/23 13:20;githubbot;600","askhatri commented on code in PR #389:
URL: https://github.com/apache/incubator-livy/pull/389#discussion_r1152895583


##########
docs/rest-api.md:
##########
@@ -251,6 +251,27 @@ Gets the log lines from this session.
 
 Returns all the statements in a session.
 
+#### Request Parameters
+
+<table class=""table"">
+  <tr><th>Name</th><th>Description</th><th>Type</th></tr>
+  <tr>
+    <td>from</td>
+    <td>The start index to fetch sessions</td>
+    <td>int</td>
+  </tr>
+  <tr>
+    <td>size</td>
+    <td>Number of sessions to fetch</td>
+    <td>int</td>
+  </tr>
+  <tr>
+    <td>order</td>
+    <td>Provide value as ""latest"" to get latest statement first </td>

Review Comment:
   I hope we are good with those changes. Please let me know incase if otherwise.



;30/Mar/23 08:12;githubbot;600","gyogal commented on PR #389:
URL: https://github.com/apache/incubator-livy/pull/389#issuecomment-1498704908

   @ksumit Could you please let us know if you are OK with the changes made?


;06/Apr/23 08:47;githubbot;600","codecov-commenter commented on PR #389:
URL: https://github.com/apache/incubator-livy/pull/389#issuecomment-1503052422

   ## [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/389?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report
   > Merging [#389](https://codecov.io/gh/apache/incubator-livy/pull/389?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (c2a7aa7) into [master](https://codecov.io/gh/apache/incubator-livy/commit/45e07fec68f2b9ad1dc7ebce8db08ad8a778dddc?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (45e07fe) will **decrease** coverage by `40.11%`.
   > The diff coverage is `0.00%`.
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #389       +/-   ##
   =============================================
   - Coverage     68.37%   28.27%   -40.11%     
   + Complexity      849      363      -486     
   =============================================
     Files           103      103               
     Lines          5989     5992        +3     
     Branches        911      912        +1     
   =============================================
   - Hits           4095     1694     -2401     
   - Misses         1330     3945     +2615     
   + Partials        564      353      -211     
   ```
   
   
   | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/389?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage Î” | |
   |---|---|---|
   | [...server/interactive/InteractiveSessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/389?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `56.71% <0.00%> (-10.46%)` | :arrow_down: |
   
   ... and [79 files with indirect coverage changes](https://codecov.io/gh/apache/incubator-livy/pull/389/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   


;11/Apr/23 10:15;githubbot;600","gyogal merged PR #389:
URL: https://github.com/apache/incubator-livy/pull/389


;11/Apr/23 12:14;githubbot;600","gyogal commented on PR #389:
URL: https://github.com/apache/incubator-livy/pull/389#issuecomment-1503225277

   Thank you for your contribution, @askhatri!


;11/Apr/23 12:17;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,7800,,,0,7800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 17 05:08:50 UTC 2023,,,,,,,,,,"0|z1fzug:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/Feb/23 05:08;asifkhatri;I have createdÂ https://github.com/apache/incubator-livy/pull/389Â PR with the fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create docker based integration environment for local debugging,LIVY-969,13521954,13501215,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ksumit,ksumit,ksumit,30/Jan/23 01:19,17/Jun/23 06:27,19/Dec/25 04:15,17/Jun/23 06:27,,,,0.8.0,,,,,,,,,,,,0,,,,,,"The idea is to be able to come up with a local integrated environment that is easier for local debugging. Few ideas:
 # Set log levels dynamically
 # Ability to do debugging using IDE
 # Ability to quickly deploy private code onto the integrated environment",,"ksumit opened a new pull request, #387:
URL: https://github.com/apache/incubator-livy/pull/387

   ## What changes were proposed in this pull request?
   This PR introduces a docker and docker-compose based integrated environment for local debugging. The environment consists of:
   1. Standalone spark cluster consisting of one spark master and one spark worker
   2. Livy configured to connect to this standalone cluster
   
   Attached README explains how this integrated environment can be customized to test with private changes.
   
   ## How was this patch tested?
   1. Used `docker-compose up` to bring up the integrated environment
   2. Logged into livy container and submitted sample spark and sql statements.
   3. Verified spark master, spark worker and livy UIs to be working, verified statements to have succeeded.


;30/Jan/23 02:53;githubbot;600","lmccay commented on code in PR #387:
URL: https://github.com/apache/incubator-livy/pull/387#discussion_r1090853830


##########
dev/docker/livy-dev-base/Dockerfile:
##########
@@ -70,4 +73,3 @@ RUN python -m pip install -U ""pip < 21.0"" && \
 RUN python3 -m pip install -U pip
 
 WORKDIR /workspace

Review Comment:
   can you check for newline here?



##########
dev/docker/livy-dev-cluster/conf/worker/spark-env.sh:
##########
@@ -0,0 +1,18 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Nothing here yet

Review Comment:
   check for newline



;30/Jan/23 16:24;githubbot;600","askhatri commented on code in PR #387:
URL: https://github.com/apache/incubator-livy/pull/387#discussion_r1127750312


##########
dev/docker/build-images.sh:
##########
@@ -0,0 +1,49 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Fail if there is an error
+set -e
+APACHE_ARCHIVE_ROOT=http://archive.apache.org/dist
+HADOOP_VERSION=3.3.1
+HADOOP_PACKAGE=""hadoop-${HADOOP_VERSION}.tar.gz""
+SPARK_VERSION=3.2.3
+SPARK_PACKAGE=""spark-${SPARK_VERSION}-bin-without-hadoop.tgz""
+LIVY_VERSION=0.8.0-incubating-SNAPSHOT
+LIVY_PACKAGE=""apache-livy-${LIVY_VERSION}-bin.zip""
+
+# Download hadoop if needed
+if [ ! -f ""livy-dev-spark/${HADOOP_PACKAGE}"" ]; then
+    curl -sL --retry 3 -o ""livy-dev-spark/${HADOOP_PACKAGE}"" \

Review Comment:
   We can use `curl -L  --retry 3 -o ""livy-dev-spark/${HADOOP_PACKAGE}` to show download progress.



##########
dev/docker/build-images.sh:
##########
@@ -0,0 +1,49 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Fail if there is an error
+set -e
+APACHE_ARCHIVE_ROOT=http://archive.apache.org/dist
+HADOOP_VERSION=3.3.1
+HADOOP_PACKAGE=""hadoop-${HADOOP_VERSION}.tar.gz""
+SPARK_VERSION=3.2.3
+SPARK_PACKAGE=""spark-${SPARK_VERSION}-bin-without-hadoop.tgz""
+LIVY_VERSION=0.8.0-incubating-SNAPSHOT
+LIVY_PACKAGE=""apache-livy-${LIVY_VERSION}-bin.zip""
+
+# Download hadoop if needed
+if [ ! -f ""livy-dev-spark/${HADOOP_PACKAGE}"" ]; then
+    curl -sL --retry 3 -o ""livy-dev-spark/${HADOOP_PACKAGE}"" \
+      ""${APACHE_ARCHIVE_ROOT}/hadoop/common/hadoop-${HADOOP_VERSION}/${HADOOP_PACKAGE}"" 
+fi
+
+# Download spark if needed
+if [ ! -f ""livy-dev-spark/${SPARK_PACKAGE}"" ]; then
+    curl -sL --retry 3 -o ""livy-dev-spark/${SPARK_PACKAGE}"" \

Review Comment:
   We can use `curl -L  --retry 3 -o ""livy-dev-spark/${HADOOP_PACKAGE}` to show download progress.



##########
dev/docker/build-images.sh:
##########
@@ -0,0 +1,49 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Fail if there is an error
+set -e
+APACHE_ARCHIVE_ROOT=http://archive.apache.org/dist
+HADOOP_VERSION=3.3.1
+HADOOP_PACKAGE=""hadoop-${HADOOP_VERSION}.tar.gz""
+SPARK_VERSION=3.2.3
+SPARK_PACKAGE=""spark-${SPARK_VERSION}-bin-without-hadoop.tgz""
+LIVY_VERSION=0.8.0-incubating-SNAPSHOT
+LIVY_PACKAGE=""apache-livy-${LIVY_VERSION}-bin.zip""
+
+# Download hadoop if needed
+if [ ! -f ""livy-dev-spark/${HADOOP_PACKAGE}"" ]; then
+    curl -sL --retry 3 -o ""livy-dev-spark/${HADOOP_PACKAGE}"" \
+      ""${APACHE_ARCHIVE_ROOT}/hadoop/common/hadoop-${HADOOP_VERSION}/${HADOOP_PACKAGE}"" 
+fi
+
+# Download spark if needed
+if [ ! -f ""livy-dev-spark/${SPARK_PACKAGE}"" ]; then
+    curl -sL --retry 3 -o ""livy-dev-spark/${SPARK_PACKAGE}"" \
+      ""${APACHE_ARCHIVE_ROOT}/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}""
+fi
+
+# Download livy if needed
+if [ ! -f ""livy-dev-server/${LIVY_PACKAGE}"" ]; then
+    curl -sL --retry 3 -o ""livy-dev-server/${LIVY_PACKAGE}"" \

Review Comment:
   We can use `curl -L  --retry 3 -o ""livy-dev-spark/${HADOOP_PACKAGE}` to show download progress.



##########
dev/docker/build-images.sh:
##########
@@ -0,0 +1,49 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Fail if there is an error
+set -e
+APACHE_ARCHIVE_ROOT=http://archive.apache.org/dist
+HADOOP_VERSION=3.3.1
+HADOOP_PACKAGE=""hadoop-${HADOOP_VERSION}.tar.gz""
+SPARK_VERSION=3.2.3
+SPARK_PACKAGE=""spark-${SPARK_VERSION}-bin-without-hadoop.tgz""
+LIVY_VERSION=0.8.0-incubating-SNAPSHOT
+LIVY_PACKAGE=""apache-livy-${LIVY_VERSION}-bin.zip""
+
+# Download hadoop if needed
+if [ ! -f ""livy-dev-spark/${HADOOP_PACKAGE}"" ]; then
+    curl -sL --retry 3 -o ""livy-dev-spark/${HADOOP_PACKAGE}"" \
+      ""${APACHE_ARCHIVE_ROOT}/hadoop/common/hadoop-${HADOOP_VERSION}/${HADOOP_PACKAGE}"" 
+fi
+
+# Download spark if needed
+if [ ! -f ""livy-dev-spark/${SPARK_PACKAGE}"" ]; then
+    curl -sL --retry 3 -o ""livy-dev-spark/${SPARK_PACKAGE}"" \
+      ""${APACHE_ARCHIVE_ROOT}/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}""

Review Comment:
   If the URL does not exist, it creates an invalid zip file. We might to change the script to avoid creating zip file if the URL is invalid.



##########
dev/docker/build-images.sh:
##########
@@ -0,0 +1,49 @@
+#

Review Comment:
   We can change file permission by using `chmod +x build-images.sh`. The default checked out file does not have executable permission.



##########
dev/docker/build-images.sh:
##########
@@ -0,0 +1,49 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Fail if there is an error
+set -e
+APACHE_ARCHIVE_ROOT=http://archive.apache.org/dist
+HADOOP_VERSION=3.3.1
+HADOOP_PACKAGE=""hadoop-${HADOOP_VERSION}.tar.gz""
+SPARK_VERSION=3.2.3

Review Comment:
   `pom.xml` file has `https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz` URL. So the Spark and hadoop default version in not matching. `SPARK_VERSION =3.2.3` can be replace with `SPARK_VERSION =2.4.5`



##########
dev/docker/build-images.sh:
##########
@@ -0,0 +1,49 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Fail if there is an error
+set -e
+APACHE_ARCHIVE_ROOT=http://archive.apache.org/dist
+HADOOP_VERSION=3.3.1

Review Comment:
   `pom.xml` file has `https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz` URL. So the Spark and hadoop default version in not matching. `HADOOP_VERSION=3.3.1` can be replace with `HADOOP_VERSION=2.7.7`



;07/Mar/23 12:12;githubbot;600","ksumit commented on code in PR #387:
URL: https://github.com/apache/incubator-livy/pull/387#discussion_r1132712596


##########
dev/docker/build-images.sh:
##########
@@ -0,0 +1,49 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Fail if there is an error
+set -e
+APACHE_ARCHIVE_ROOT=http://archive.apache.org/dist
+HADOOP_VERSION=3.3.1
+HADOOP_PACKAGE=""hadoop-${HADOOP_VERSION}.tar.gz""
+SPARK_VERSION=3.2.3
+SPARK_PACKAGE=""spark-${SPARK_VERSION}-bin-without-hadoop.tgz""
+LIVY_VERSION=0.8.0-incubating-SNAPSHOT
+LIVY_PACKAGE=""apache-livy-${LIVY_VERSION}-bin.zip""
+
+# Download hadoop if needed
+if [ ! -f ""livy-dev-spark/${HADOOP_PACKAGE}"" ]; then
+    curl -sL --retry 3 -o ""livy-dev-spark/${HADOOP_PACKAGE}"" \

Review Comment:
   I wanted to keep it silent but from the other comment about invalid urls leading to corrupt zip files, it might be useful to show download progress. Will update it.



;10/Mar/23 18:10;githubbot;600","ksumit commented on code in PR #387:
URL: https://github.com/apache/incubator-livy/pull/387#discussion_r1132717838


##########
dev/docker/build-images.sh:
##########
@@ -0,0 +1,49 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Fail if there is an error
+set -e
+APACHE_ARCHIVE_ROOT=http://archive.apache.org/dist
+HADOOP_VERSION=3.3.1

Review Comment:
   That's a good point. It took me few iterations to get all the version combinations to work together. If we changed the versions, we will likely have to do more changes to accommodate runtime version conflicts. I will check how hard or easy that might be.



;10/Mar/23 18:14;githubbot;600","ayushtkn commented on code in PR #387:
URL: https://github.com/apache/incubator-livy/pull/387#discussion_r1147905218


##########
pom.xml:
##########
@@ -639,7 +639,7 @@
         <plugin>
           <groupId>net.alchim31.maven</groupId>
           <artifactId>scala-maven-plugin</artifactId>
-          <version>4.2.0</version>
+          <version>4.3.0</version>

Review Comment:
   this version upgrade is required in the scope of adding docker images? Could have been an independent change?



##########
dev/docker/build-images.sh:
##########
@@ -0,0 +1,49 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Fail if there is an error
+set -e
+APACHE_ARCHIVE_ROOT=http://archive.apache.org/dist
+HADOOP_VERSION=3.3.1

Review Comment:
   Well Hadoop 2.7.x line is EOL long back, if it works with 3.3.x it is quite better and we can give a try to update the main hadoop version in the pom itself



##########
dev/docker/README.md:
##########
@@ -0,0 +1,102 @@
+# Livy with standalone Spark Cluster
+## Pre-requisite
+Following steps use Ubuntu as development environment but most of the instructions can be modified to fit another OS as well.
+* Install wsl if on windows, instructions available [here](https://ubuntu.com/tutorials/install-ubuntu-on-wsl2-on-windows-11-with-gui-support)
+* Install docker engine, instructions available [here](https://docs.docker.com/engine/install/ubuntu/)
+* Install docker-compose, instructions available [here](https://docs.docker.com/compose/install/)
+
+## Standalone cluster using docker-compose
+### Build container images locally
+* Build livy-dev-base, livy-dev-spark, livy-dev-server container images using provided script `build-images.sh`
+```
+livy/dev/docker$ ls
+README.md  build-images.sh  livy-dev-base  livy-dev-cluster  livy-dev-server  livy-dev-spark
+livy/dev/docker$ ./build-images.sh
+```
+#### Customizing container images
+`build-images.sh` downloads built up artifacts from Apache's respository however, private builds can be copied to respective container directories to build a container image with private artifacts as well.
+```
+livy-dev-spark uses hadoop and spark tarballs
+livy-dev-server uses livy zip file
+```
+
+For quicker iteration, copy the modified jars to specific container directories and update corresponding `Dockerfile` to replace those jars as additional steps inside the image. Provided `Dockerfile`s have example lines that can be uncommented/modified to achieve this.
+
+`livy-dev-cluster` folder contains conf folder with customizable configurations (environment, .conf and log4j.properties files) that can be updated to suit specific needs. Restart the cluster after making changes (without rebuilding the images).
+### Launching the cluster
+```
+livy/dev/docker/livy-dev-cluster$ docker-compose up
+Starting spark-master   ... done
+Starting spark-worker-1 ... done
+Starting livy           ... done
+Attaching to spark-worker-1, spark-master, livy
+```
+### UIs
+* Livy UI at http://localhost:8998/
+* Spark Master at spark://master:7077 (http://localhost:8080/).
+* Spark Worker at spark://spark-worker-1:8881 (http://localhost:8081/)
+
+### Run spark shell
+* Login to spark-master or spark-worker or livy container using docker cli
+```
+$ docker exec -it spark-master /bin/bash
+root@master:/opt/spark-3.2.3-bin-without-hadoop# spark-shell
+Setting default log level to ""WARN"".
+To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
+2023-01-27 19:32:37,469 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
+Spark context Web UI available at http://localhost:4040
+Spark context available as 'sc' (master = spark://master:7077, app id = app-20230127193238-0002).
+Spark session available as 'spark'.
+Welcome to
+      ____              __
+     / __/__  ___ _____/ /__
+    _\ \/ _ \/ _ `/ __/  '_/
+   /___/ .__/\_,_/_/ /_/\_\   version 3.2.3
+      /_/
+
+Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_292)
+Type in expressions to have them evaluated.
+Type :help for more information.
+
+scala> println(""Hello world!"")
+Hello world!
+``` 
+### Submit requests to livy using REST apis
+Login to livy container directly and submit requests using REST endpoint
+```
+# Create a new session
+curl -X POST -d '{""kind"": ""spark"",""driverMemory"":""512M"",""executorMemory"":""512M""}' -H ""Content-Type: application/json"" http://localhost:8998/sessions/
+
+# Submit the simplest `1+1` statement
+curl -X POST -d '{""code"": ""1 + 1""}' -H ""Content-Type: application/json"" http://localhost:8998/sessions/0/statements
+
+# Submit simple spark code
+curl -X POST -d '{""code"": ""val data = Array(1,2,3); sc.parallelize(data).count""}' -H ""Content-Type: application/json"" http://localhost:8998/sessions/0/statements
+
+# Submit simple sql code (this setup still doesn't have hive metastore configured)
+curl -X POST -d '{""kind"": ""sql"", ""code"": ""show databases""}, ' -H ""Content-Type: application/json"" http://localhost:8998/sessions/0/statements
+```
+### Debugging Livy/Spark/Hadoop
+`livy-dev-cluster` has conf directory for spark-master, spark-worker and livy. Configuration files in those directories can be modified before launching the cluster, for example:
+1. `Setting log level` - log4j.properties file can be modified in `livy-dev-cluster` folder to change log level for root logger as well as for specific packages
+2. `Testing private changes` - copy private jars into respective container folder, update corresponding Dockerfile to copy/replace those jars into respective paths on the container image and rebuild all the images (Note: livy-dev-server builds on top of livy-dev-spark which builds on top of livy-dev-base)
+3. `Remote debugging` - livy-env.sh already has customization to start with remote debugging on 9010. Please follow IDE specific guidance on how to debug remotely connecting to specific JDWP port for the daemon. Instructions for IntelliJ are available [here](https://www.jetbrains.com/help/idea/tutorial-remote-debug.html) and for Eclipse, [here](https://help.eclipse.org/latest/index.jsp?topic=%2Forg.eclipse.jdt.doc.user%2Ftasks%2Ftask-remotejava_launch_config.htm)
+### Terminate the cluster
+Press `CTRL-C` to terminate
+```
+spark-worker-1    | 2023-01-27 19:16:47,921 INFO shuffle.ExternalShuffleBlockResolver: Application app-20230127191546-0000 removed, cleanupLocalDirs = true
+^CGracefully stopping... (press Ctrl+C again to force)
+Stopping spark-worker-1 ... done
+Stopping spark-master   ... done
+```
+
+## Common Gotchas
+1. Use `docker-compose down` to clean up all the resources created for the cluster
+2. Login to created images to check the state
+```
+docker run -it [imageId | imageName] /bin/bash
+```
+3. Create a Livy build for different scala version
+```
+mvn clean install -Pspark-3.0 -DskipITs -DskipTest -DskipTests -Drat.skip -Dscala.binary.version=2.12 -Dscala.version=2.12.15

Review Comment:
   just curious:
   we have both -DskipTest & -DskipTests, what difference both have here?



;24/Mar/23 18:01;githubbot;600","lmccay commented on PR #387:
URL: https://github.com/apache/incubator-livy/pull/387#issuecomment-1542129493

   @ayushtkn - have you tried this out yet? This has gone quiet and just trying to determine whether it can land in 0.8.0 or should move out to 0.9.0. @ksumit - any feel for whether this is ready for 0.8.0?
   


;10/May/23 12:32;githubbot;600","ksumit commented on code in PR #387:
URL: https://github.com/apache/incubator-livy/pull/387#discussion_r1200858567


##########
dev/docker/build-images.sh:
##########
@@ -0,0 +1,49 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Fail if there is an error
+set -e
+APACHE_ARCHIVE_ROOT=http://archive.apache.org/dist
+HADOOP_VERSION=3.3.1
+HADOOP_PACKAGE=""hadoop-${HADOOP_VERSION}.tar.gz""
+SPARK_VERSION=3.2.3
+SPARK_PACKAGE=""spark-${SPARK_VERSION}-bin-without-hadoop.tgz""
+LIVY_VERSION=0.8.0-incubating-SNAPSHOT
+LIVY_PACKAGE=""apache-livy-${LIVY_VERSION}-bin.zip""
+
+# Download hadoop if needed
+if [ ! -f ""livy-dev-spark/${HADOOP_PACKAGE}"" ]; then
+    curl -sL --retry 3 -o ""livy-dev-spark/${HADOOP_PACKAGE}"" \
+      ""${APACHE_ARCHIVE_ROOT}/hadoop/common/hadoop-${HADOOP_VERSION}/${HADOOP_PACKAGE}"" 
+fi
+
+# Download spark if needed
+if [ ! -f ""livy-dev-spark/${SPARK_PACKAGE}"" ]; then
+    curl -sL --retry 3 -o ""livy-dev-spark/${SPARK_PACKAGE}"" \
+      ""${APACHE_ARCHIVE_ROOT}/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}""

Review Comment:
   Updated to use `-L` instead so it shows the error messages



;22/May/23 18:03;githubbot;600","ksumit closed pull request #387: [LIVY-969] Create docker based integration environment for local debugging
URL: https://github.com/apache/incubator-livy/pull/387


;11/Jun/23 05:02;githubbot;600","ksumit opened a new pull request, #407:
URL: https://github.com/apache/incubator-livy/pull/407

   ## What changes were proposed in this pull request?
   This PR introduces a docker and docker-compose based integrated environment for local debugging. The environment consists of:
   1. Standalone spark cluster consisting of one spark master and one spark worker
   2. Livy configured to connect to this standalone cluster
   
   Attached README explains how this integrated environment can be customized to test with private changes.
   
   ## How was this patch tested?
   1. Used `docker-compose up` to bring up the integrated environment
   2. Logged into livy container and submitted sample spark and sql statements.
   3. Verified spark master, spark worker and livy UIs to be working, verified statements to have succeeded.
   4. Verified remote debugging to work by attaching IDE to Livy server
   
   Earlier PR #387 got closed accidentally when I force merged to the source branch. I've addressed the concerns in that PR as well.


;11/Jun/23 05:09;githubbot;600","ksumit commented on PR #407:
URL: https://github.com/apache/incubator-livy/pull/407#issuecomment-1586020206

   @lmccay @dacort @ayushtkn @askhatri  Please review. Sorry earlier PR #387 got closed accidentally when I force merged to the source branch and I'm not able to reopen it. I've addressed the concerns from that PR as well.


;11/Jun/23 05:11;githubbot;600","codecov-commenter commented on PR #407:
URL: https://github.com/apache/incubator-livy/pull/407#issuecomment-1586026690

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/407?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   > Merging [#407](https://app.codecov.io/gh/apache/incubator-livy/pull/407?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) (3240e68) into [master](https://app.codecov.io/gh/apache/incubator-livy/commit/c3dd6457c45662de13be72919532ed0ef1b57ab3?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) (c3dd645) will **decrease** coverage by `37.07%`.
   > The diff coverage is `n/a`.
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #407       +/-   ##
   =============================================
   - Coverage     65.51%   28.45%   -37.07%     
   + Complexity      950      375      -575     
   =============================================
     Files           103      103               
     Lines          6044     6044               
     Branches        911      911               
   =============================================
   - Hits           3960     1720     -2240     
   - Misses         1537     3977     +2440     
   + Partials        547      347      -200     
   ```
   
   
   [see 86 files with indirect coverage changes](https://app.codecov.io/gh/apache/incubator-livy/pull/407/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache)
   


;11/Jun/23 05:40;githubbot;600","ksumit commented on code in PR #387:
URL: https://github.com/apache/incubator-livy/pull/387#discussion_r1225837283


##########
pom.xml:
##########
@@ -639,7 +639,7 @@
         <plugin>
           <groupId>net.alchim31.maven</groupId>
           <artifactId>scala-maven-plugin</artifactId>
-          <version>4.2.0</version>
+          <version>4.3.0</version>

Review Comment:
   This was required for building scala and java together. I forgot the error I was getting but shouldn't hurt right? 



;11/Jun/23 15:04;githubbot;600","ksumit commented on code in PR #387:
URL: https://github.com/apache/incubator-livy/pull/387#discussion_r1225837319


##########
dev/docker/README.md:
##########
@@ -0,0 +1,102 @@
+# Livy with standalone Spark Cluster
+## Pre-requisite
+Following steps use Ubuntu as development environment but most of the instructions can be modified to fit another OS as well.
+* Install wsl if on windows, instructions available [here](https://ubuntu.com/tutorials/install-ubuntu-on-wsl2-on-windows-11-with-gui-support)
+* Install docker engine, instructions available [here](https://docs.docker.com/engine/install/ubuntu/)
+* Install docker-compose, instructions available [here](https://docs.docker.com/compose/install/)
+
+## Standalone cluster using docker-compose
+### Build container images locally
+* Build livy-dev-base, livy-dev-spark, livy-dev-server container images using provided script `build-images.sh`
+```
+livy/dev/docker$ ls
+README.md  build-images.sh  livy-dev-base  livy-dev-cluster  livy-dev-server  livy-dev-spark
+livy/dev/docker$ ./build-images.sh
+```
+#### Customizing container images
+`build-images.sh` downloads built up artifacts from Apache's respository however, private builds can be copied to respective container directories to build a container image with private artifacts as well.
+```
+livy-dev-spark uses hadoop and spark tarballs
+livy-dev-server uses livy zip file
+```
+
+For quicker iteration, copy the modified jars to specific container directories and update corresponding `Dockerfile` to replace those jars as additional steps inside the image. Provided `Dockerfile`s have example lines that can be uncommented/modified to achieve this.
+
+`livy-dev-cluster` folder contains conf folder with customizable configurations (environment, .conf and log4j.properties files) that can be updated to suit specific needs. Restart the cluster after making changes (without rebuilding the images).
+### Launching the cluster
+```
+livy/dev/docker/livy-dev-cluster$ docker-compose up
+Starting spark-master   ... done
+Starting spark-worker-1 ... done
+Starting livy           ... done
+Attaching to spark-worker-1, spark-master, livy
+```
+### UIs
+* Livy UI at http://localhost:8998/
+* Spark Master at spark://master:7077 (http://localhost:8080/).
+* Spark Worker at spark://spark-worker-1:8881 (http://localhost:8081/)
+
+### Run spark shell
+* Login to spark-master or spark-worker or livy container using docker cli
+```
+$ docker exec -it spark-master /bin/bash
+root@master:/opt/spark-3.2.3-bin-without-hadoop# spark-shell
+Setting default log level to ""WARN"".
+To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
+2023-01-27 19:32:37,469 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
+Spark context Web UI available at http://localhost:4040
+Spark context available as 'sc' (master = spark://master:7077, app id = app-20230127193238-0002).
+Spark session available as 'spark'.
+Welcome to
+      ____              __
+     / __/__  ___ _____/ /__
+    _\ \/ _ \/ _ `/ __/  '_/
+   /___/ .__/\_,_/_/ /_/\_\   version 3.2.3
+      /_/
+
+Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_292)
+Type in expressions to have them evaluated.
+Type :help for more information.
+
+scala> println(""Hello world!"")
+Hello world!
+``` 
+### Submit requests to livy using REST apis
+Login to livy container directly and submit requests using REST endpoint
+```
+# Create a new session
+curl -X POST -d '{""kind"": ""spark"",""driverMemory"":""512M"",""executorMemory"":""512M""}' -H ""Content-Type: application/json"" http://localhost:8998/sessions/
+
+# Submit the simplest `1+1` statement
+curl -X POST -d '{""code"": ""1 + 1""}' -H ""Content-Type: application/json"" http://localhost:8998/sessions/0/statements
+
+# Submit simple spark code
+curl -X POST -d '{""code"": ""val data = Array(1,2,3); sc.parallelize(data).count""}' -H ""Content-Type: application/json"" http://localhost:8998/sessions/0/statements
+
+# Submit simple sql code (this setup still doesn't have hive metastore configured)
+curl -X POST -d '{""kind"": ""sql"", ""code"": ""show databases""}, ' -H ""Content-Type: application/json"" http://localhost:8998/sessions/0/statements
+```
+### Debugging Livy/Spark/Hadoop
+`livy-dev-cluster` has conf directory for spark-master, spark-worker and livy. Configuration files in those directories can be modified before launching the cluster, for example:
+1. `Setting log level` - log4j.properties file can be modified in `livy-dev-cluster` folder to change log level for root logger as well as for specific packages
+2. `Testing private changes` - copy private jars into respective container folder, update corresponding Dockerfile to copy/replace those jars into respective paths on the container image and rebuild all the images (Note: livy-dev-server builds on top of livy-dev-spark which builds on top of livy-dev-base)
+3. `Remote debugging` - livy-env.sh already has customization to start with remote debugging on 9010. Please follow IDE specific guidance on how to debug remotely connecting to specific JDWP port for the daemon. Instructions for IntelliJ are available [here](https://www.jetbrains.com/help/idea/tutorial-remote-debug.html) and for Eclipse, [here](https://help.eclipse.org/latest/index.jsp?topic=%2Forg.eclipse.jdt.doc.user%2Ftasks%2Ftask-remotejava_launch_config.htm)
+### Terminate the cluster
+Press `CTRL-C` to terminate
+```
+spark-worker-1    | 2023-01-27 19:16:47,921 INFO shuffle.ExternalShuffleBlockResolver: Application app-20230127191546-0000 removed, cleanupLocalDirs = true
+^CGracefully stopping... (press Ctrl+C again to force)
+Stopping spark-worker-1 ... done
+Stopping spark-master   ... done
+```
+
+## Common Gotchas
+1. Use `docker-compose down` to clean up all the resources created for the cluster
+2. Login to created images to check the state
+```
+docker run -it [imageId | imageName] /bin/bash
+```
+3. Create a Livy build for different scala version
+```
+mvn clean install -Pspark-3.0 -DskipITs -DskipTest -DskipTests -Drat.skip -Dscala.binary.version=2.12 -Dscala.version=2.12.15

Review Comment:
   I cleaned it up now in the new PR, thanks for pointing out.



;11/Jun/23 15:05;githubbot;600","dacort commented on code in PR #407:
URL: https://github.com/apache/incubator-livy/pull/407#discussion_r1227180190


##########
dev/docker/livy-dev-base/Dockerfile:
##########


Review Comment:
   We need to update the main project README with this new location.
   
   Either update the `cd incubator-livy/dev/docker` and `docker run` commands or just the `docker build` command with the extra directory.



##########
dev/docker/build-images.sh:
##########


Review Comment:
   `chmod 755` this file.



##########
dev/docker/README.md:
##########
@@ -0,0 +1,116 @@
+# Livy with standalone Spark Cluster
+## Pre-requisite
+Following steps use Ubuntu as development environment but most of the instructions can be modified to fit another OS as well.
+* Install wsl if on windows, instructions available [here](https://ubuntu.com/tutorials/install-ubuntu-on-wsl2-on-windows-11-with-gui-support)
+* Install docker engine, instructions available [here](https://docs.docker.com/engine/install/ubuntu/)
+* Install docker-compose, instructions available [here](https://docs.docker.com/compose/install/)
+
+## Standalone cluster using docker-compose
+### Build the current livy branch and copy it to appropriate folder
+```
+$ mvn clean package -Pscala-2.12 -Pspark3 -DskipITs -DskipTests

Review Comment:
   We may want to call out the important of `clean` here. I had previously packaged with scala-2.11 and my `livy-dev-server` wouldn't start because of a version conflict with the below error message:
   
   ```
   Exception in thread ""main"" java.lang.NoSuchMethodError: scala.Product.$init$(Lscala/Product;)V
   ```



##########
pom.xml:
##########
@@ -1180,10 +1179,11 @@
     <profile>
       <id>spark3</id>
       <properties>
-        <spark.version>3.0.0</spark.version>
+        <spark.version>3.2.3</spark.version>

Review Comment:
   Are we OK bumping the version of Spark here? 
   
   I like it, just want to make sure it doesn't break anything...



;12/Jun/23 21:23;githubbot;600","ksumit commented on code in PR #407:
URL: https://github.com/apache/incubator-livy/pull/407#discussion_r1227284123


##########
pom.xml:
##########
@@ -1180,10 +1179,11 @@
     <profile>
       <id>spark3</id>
       <properties>
-        <spark.version>3.0.0</spark.version>
+        <spark.version>3.2.3</spark.version>

Review Comment:
   Builds and pipelines passed with the updated version, so seems ok. BTW version of spark doesn't matter for us right, we use spark-submit to do everything? :-) At work, we build with spark-3.0.0 and hadoop 2 but run it in a spark 3.1/3.2/3.3 with hadoop 3 and it seems to work just fine. Happy to learn if you see an issue.



;12/Jun/23 21:59;githubbot;600","dacort commented on code in PR #407:
URL: https://github.com/apache/incubator-livy/pull/407#discussion_r1227285681


##########
pom.xml:
##########
@@ -1180,10 +1179,11 @@
     <profile>
       <id>spark3</id>
       <properties>
-        <spark.version>3.0.0</spark.version>
+        <spark.version>3.2.3</spark.version>

Review Comment:
   Works for me, just wanted to double-check. :) 



;12/Jun/23 22:01;githubbot;600","ksumit commented on PR #407:
URL: https://github.com/apache/incubator-livy/pull/407#issuecomment-1588177576

   > Worked great for me!
   > 
   > Requesting a couple minor tweaks.
   
   @dacort, I've updated as per your review comments. I did change the file permissions for `build-images.sh` but don't see that reflected in the updated diff.


;12/Jun/23 22:11;githubbot;600","ksumit merged PR #407:
URL: https://github.com/apache/incubator-livy/pull/407


;17/Jun/23 00:04;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,11400,,,0,11400,,LIVY-593,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 30 23:28:03 UTC 2023,,,,,,,,,,"0|z1fgnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/Jan/23 02:57;ksumit;[~lmccay] , [~dacort] please have a look at https://github.com/apache/incubator-livy/pull/387;;;","30/Jan/23 05:02;lmccay;Will do, [~ksumit] - thank you!;;;","10/May/23 12:35;lmccay;[~ksumit]Â  - it seems the PR has gone quiet for a couple months. I never got back to trying to fix my environment. Do we think this should block 0.8.0 and is ready for use now or should we move to 0.9.0?

Â ;;;","15/May/23 03:22;ksumit;I will make progress on this, I do want this to go in so we can make development and debugging easier. There is dependency on LIVY-593, Iam hoping to make progress this week.;;;","30/May/23 23:28;dacort;Will also try to take a look at this in the next couple days if need be.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide ttl field for a livy session,LIVY-968,13520883,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,asifkhatri,vedantlodha,vedantlodha,21/Jan/23 05:30,10/Feb/23 16:15,19/Dec/25 04:15,10/Feb/23 16:14,,,,0.8.0,,Core,,,,,,,,,,0,,,,,,"A session should have a field 'ttl' which once exceeded should get the session killed.This field can be a parameter in the POST /sessions request and then be a part of session object. 

Livy has a config `livy.server.session.timeout` but this is a global config and cannot be configured for a session. A 'ttl' field can help set this for a session level. 

The precedence could be 'ttl' > 'livy.server.session.timeout'",,"askhatri opened a new pull request, #384:
URL: https://github.com/apache/incubator-livy/pull/384

   ## What changes were proposed in this pull request?
   
   A session should have a field 'ttl' (Time to live) which once exceeded should get the session killed.This field can be a parameter in the POST /sessions request and then be a part of session object. Livy has a config `livy.server.session.timeout` but this is a global config and cannot be configured for a session. A 'ttl' field can help set this for a session level.
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-968
   
   ## How was this patch tested?
   
   Verified manually by creating interactive session via REST API call in a local Yarn cluster. Also, we have add the new unit tests.
   


;23/Jan/23 11:45;githubbot;600","codecov-commenter commented on PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#issuecomment-1400242778

   # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report
   > Merging [#384](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (791f34f) into [master](https://codecov.io/gh/apache/incubator-livy/commit/8d07eee831221093f55203190d38d793234c5db7?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (8d07eee) will **decrease** coverage by `2.51%`.
   > The diff coverage is `84.84%`.
   
   ```diff
   @@             Coverage Diff              @@
   ##             master     #384      +/-   ##
   ============================================
   - Coverage     68.26%   65.76%   -2.51%     
   + Complexity      844      824      -20     
   ============================================
     Files           103      103              
     Lines          5965     5988      +23     
     Branches        907      911       +4     
   ============================================
   - Hits           4072     3938     -134     
   - Misses         1333     1524     +191     
   + Partials        560      526      -34     
   ```
   
   
   | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage Î” | |
   |---|---|---|
   | [.../server/interactive/CreateInteractiveRequest.scala](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvQ3JlYXRlSW50ZXJhY3RpdmVSZXF1ZXN0LnNjYWxh) | `75.00% <66.66%> (-5.00%)` | :arrow_down: |
   | [...java/org/apache/livy/client/common/ClientConf.java](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y2xpZW50LWNvbW1vbi9zcmMvbWFpbi9qYXZhL29yZy9hcGFjaGUvbGl2eS9jbGllbnQvY29tbW9uL0NsaWVudENvbmYuamF2YQ==) | `95.83% <71.42%> (-3.24%)` | :arrow_down: |
   | [...va/org/apache/livy/client/common/HttpMessages.java](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y2xpZW50LWNvbW1vbi9zcmMvbWFpbi9qYXZhL29yZy9hcGFjaGUvbGl2eS9jbGllbnQvY29tbW9uL0h0dHBNZXNzYWdlcy5qYXZh) | `95.34% <100.00%> (+0.11%)` | :arrow_up: |
   | [...la/org/apache/livy/server/batch/BatchSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvYmF0Y2gvQmF0Y2hTZXNzaW9uLnNjYWxh) | `84.69% <100.00%> (-2.05%)` | :arrow_down: |
   | [...e/livy/server/interactive/InteractiveSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `57.34% <100.00%> (-12.42%)` | :arrow_down: |
   | [...server/interactive/InteractiveSessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `61.71% <100.00%> (-6.00%)` | :arrow_down: |
   | [.../main/scala/org/apache/livy/sessions/Session.scala](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXNzaW9ucy9TZXNzaW9uLnNjYWxh) | `73.68% <100.00%> (+0.23%)` | :arrow_up: |
   | [...cala/org/apache/livy/sessions/SessionManager.scala](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXNzaW9ucy9TZXNzaW9uTWFuYWdlci5zY2FsYQ==) | `82.17% <100.00%> (+0.36%)` | :arrow_up: |
   | [...a/org/apache/livy/server/ThriftServerFactory.scala](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvVGhyaWZ0U2VydmVyRmFjdG9yeS5zY2FsYQ==) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |
   | [...main/scala/org/apache/livy/server/LivyServer.scala](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvTGl2eVNlcnZlci5zY2FsYQ==) | `2.23% <0.00%> (-29.47%)` | :arrow_down: |
   | ... and [22 more](https://codecov.io/gh/apache/incubator-livy/pull/384?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | |
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   


;23/Jan/23 12:11;githubbot;600","gyogal commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1086915353


##########
server/src/main/scala/org/apache/livy/sessions/SessionManager.scala:
##########
@@ -168,7 +169,9 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
             false
           } else {
             val currentTime = System.nanoTime()
-            currentTime - session.lastActivity > sessionTimeout
+            val calculatedTimeout =
+              ClientConf.getTimeAsNanos(session.ttl.orNull, session.id, sessionTimeout)

Review Comment:
   The TTL string is currently only validated here. If the formatting is invalid it seems like this will keep logging error messages every time `expired()` is called. To prevent this, the string could be validated as the session in created.



;25/Jan/23 17:04;githubbot;600","ksumit commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1089591529


##########
client-common/src/main/java/org/apache/livy/client/common/ClientConf.java:
##########
@@ -36,7 +36,7 @@
 public abstract class ClientConf<T extends ClientConf>
   implements Iterable<Map.Entry<String, String>> {
 
-  protected Logger LOG = LoggerFactory.getLogger(getClass());
+  protected static Logger LOG = LoggerFactory.getLogger(ClientConf.class);

Review Comment:
   should this be final too?



##########
server/src/main/scala/org/apache/livy/sessions/Session.scala:
##########
@@ -138,6 +138,7 @@ abstract class Session(
     val id: Int,
     val name: Option[String],
     val owner: String,
+    val ttl: Option[String],

Review Comment:
   should we add an auxiliary constructor as well that matches the existing constructor (4 parameters only) and takes the global session timeout as ttl by default? this will help reduce the amount of change in this PR.



##########
server/src/main/scala/org/apache/livy/sessions/SessionManager.scala:
##########
@@ -168,7 +169,9 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
             false
           } else {
             val currentTime = System.nanoTime()
-            currentTime - session.lastActivity > sessionTimeout
+            val calculatedTimeout =

Review Comment:
   nanoseconds seems too much to me, from my exposure to customer usecases i think ttl in minutes should be enough. curious to know when nanoseconds level of granularity would be useful.
   
   please consider simplifying this further by making this an `Int` or `Long` and avoiding all the parsing and validation logic. for the variable naming, what do you think of `ttlInMinutes` or `ttlInSeconds`?



##########
docs/rest-api.md:
##########
@@ -151,6 +151,11 @@ Creates a new interactive Scala, Python, or R shell in the cluster.
     <td>Timeout in second to which session be orphaned</td>
     <td>int</td>
   </tr>
+  <tr>
+    <td>ttl</td>
+    <td>The timeout for this inactive session</td>

Review Comment:
   would be useful to provide some examples here.
   
   `timeout for this inactive session in minutes/seconds, example: 10`



;28/Jan/23 02:34;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1089633885


##########
client-common/src/main/java/org/apache/livy/client/common/ClientConf.java:
##########
@@ -36,7 +36,7 @@
 public abstract class ClientConf<T extends ClientConf>
   implements Iterable<Map.Entry<String, String>> {
 
-  protected Logger LOG = LoggerFactory.getLogger(getClass());
+  protected static Logger LOG = LoggerFactory.getLogger(ClientConf.class);

Review Comment:
   Sure @ksumit , I will try to make it final



;28/Jan/23 04:18;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1089635745


##########
server/src/main/scala/org/apache/livy/sessions/SessionManager.scala:
##########
@@ -168,7 +169,9 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
             false
           } else {
             val currentTime = System.nanoTime()
-            currentTime - session.lastActivity > sessionTimeout
+            val calculatedTimeout =

Review Comment:
   nanoseconds logic is only for internal calculation and `System.nanoTime()` is an existing code. End user has option to provide the value like ""30s"" for 30 seconds and ""2m"" for 2 minutes.



;28/Jan/23 04:21;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1089636026


##########
docs/rest-api.md:
##########
@@ -151,6 +151,11 @@ Creates a new interactive Scala, Python, or R shell in the cluster.
     <td>Timeout in second to which session be orphaned</td>
     <td>int</td>
   </tr>
+  <tr>
+    <td>ttl</td>
+    <td>The timeout for this inactive session</td>

Review Comment:
   Sure @ksumit , I will try to update this note.



;28/Jan/23 04:22;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1089637122


##########
server/src/main/scala/org/apache/livy/sessions/Session.scala:
##########
@@ -138,6 +138,7 @@ abstract class Session(
     val id: Int,
     val name: Option[String],
     val owner: String,
+    val ttl: Option[String],

Review Comment:
   Sure @ksumit , I will check the possibility to add a auxiliary constructor.



;28/Jan/23 04:24;githubbot;600","ksumit commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1089796453


##########
server/src/main/scala/org/apache/livy/sessions/SessionManager.scala:
##########
@@ -168,7 +169,9 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
             false
           } else {
             val currentTime = System.nanoTime()
-            currentTime - session.lastActivity > sessionTimeout
+            val calculatedTimeout =

Review Comment:
   hi @askhatri, i should have been more specific, i had following specific concerns:
   1. the logic on parsing the string and converting it to seconds, milliseconds etc seems more complex. instead i was proposing that we use `Int` or `Long` and use number to denote the ttl value
   2. fix the granularity for ttl either in minutes or seconds to avoid the complexity
   3. depending on whether we fix it in minutes or seconds to be very explicit, name the variable as `ttlInMinutes` or `ttlInSeconds`



;28/Jan/23 19:11;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1092752125


##########
server/src/main/scala/org/apache/livy/sessions/SessionManager.scala:
##########
@@ -168,7 +169,9 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
             false
           } else {
             val currentTime = System.nanoTime()
-            currentTime - session.lastActivity > sessionTimeout
+            val calculatedTimeout =

Review Comment:
   I got it @ksumit . So presently we have three options now:
   1) Use ""ttl"" with ""String"" datatype and keep parsing logic same as global config.
   2) Change filed name to ""ttlInMinutes"" with ""Int"" datatype and support only minutes in user inputs.
   3) Change filed name to ""ttlInSeconds"" with ""Int"" datatype and support only seconds in user inputs.
   
   Since the logic to parse string already and it is consistent with global configuration we are planing to go with option 1. i.e. using ""ttl"" field as ""String"".



;01/Feb/23 04:26;githubbot;600","ksumit commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1093693598


##########
client-common/src/main/java/org/apache/livy/client/common/ClientConf.java:
##########
@@ -265,6 +265,51 @@ private Map<String, ConfPair> allAlternativeKeys() {
     return altToNewKeyMap;
   }
 
+  public static boolean validateTtl(String ttl) {

Review Comment:
   Looks like `validateTtl()` and `getTimeAsNanos()` are using a lot of duplicate code, could we avoid this? 



##########
server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala:
##########
@@ -52,15 +53,20 @@ class InteractiveSessionServlet(
 
   override protected def createSession(req: HttpServletRequest): InteractiveSession = {
     val createRequest = bodyAs[CreateInteractiveRequest](req)
-    InteractiveSession.create(
-      sessionManager.nextId(),
-      createRequest.name,
-      remoteUser(req),
-      proxyUser(req, createRequest.proxyUser),
-      livyConf,
-      accessManager,
-      createRequest,
-      sessionStore)
+    if (ClientConf.validateTtl(createRequest.ttl.orNull)) {
+      InteractiveSession.create(
+        sessionManager.nextId(),
+        createRequest.name,
+        remoteUser(req),
+        proxyUser(req, createRequest.proxyUser),
+        livyConf,
+        accessManager,
+        createRequest,
+        sessionStore,
+        createRequest.ttl)
+    } else {
+      null

Review Comment:
   should this be considered an error case instead and we throw an appropriate error message here? Or could we log the failure and use the global ttl value in this case as the default value?



##########
server/src/main/scala/org/apache/livy/sessions/SessionManager.scala:
##########
@@ -168,7 +169,9 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
             false
           } else {
             val currentTime = System.nanoTime()
-            currentTime - session.lastActivity > sessionTimeout
+            val calculatedTimeout =

Review Comment:
   I see now that global configuration is also using this complicated logic. Thanks for pointing that out. I also noticed that there are bunch of other configurations that follow this same logic. So we can leave this as it is. I'm concerned about duplicate code logic in `getTimeAsMs()` and `getTimeAsNanos()`, please see if we can avoid the same or reuse what's already there?



##########
server/src/main/scala/org/apache/livy/server/interactive/CreateInteractiveRequest.scala:
##########
@@ -50,6 +51,7 @@ class CreateInteractiveRequest {
       (if (queue.isDefined) s""queue: ${queue.get}, "" else """") +
       (if (name.isDefined) s""name: ${name.get}, "" else """") +
       (if (conf.nonEmpty) s""conf: ${conf.mkString("","")}, "" else """") +
-      s""heartbeatTimeoutInSecond: $heartbeatTimeoutInSecond]""
+      s""heartbeatTimeoutInSecond: $heartbeatTimeoutInSecond, "" +
+      (if (ttl.isDefined) s""driverMemory: ${ttl.get}]"" else ""]"")

Review Comment:
   typo: we need `ttl` instead of `driverMemory`



##########
client-common/src/main/java/org/apache/livy/client/common/HttpMessages.java:
##########
@@ -61,9 +61,10 @@ public static class SessionInfo implements ClientMessage {
     public final String kind;
     public final Map<String, String> appInfo;
     public final List<String> log;
+    public final String ttl;
 
     public SessionInfo(int id, String name, String appId, String owner, String proxyUser,

Review Comment:
   given that ttl is optional field, should we create an override constructor without this parameter? that way you will be able to avoid other changes in test classes.



##########
server/src/main/scala/org/apache/livy/server/SessionServlet.scala:
##########
@@ -131,13 +131,18 @@ abstract class SessionServlet[S <: Session, R <: RecoveryMetadata](
       if (tooManySessions) {
         BadRequest(ResponseMessage(""Rejected, too many sessions are being created!""))
       } else {
-        val session = sessionManager.register(createSession(request))
-        // Because it may take some time to establish the session, update the last activity
-        // time before returning the session info to the client.
-        session.recordActivity()
-        Created(clientSessionView(session, request),
-          headers = Map(""Location"" ->
-            (getRequestPathInfo(request) + url(getSession, ""id"" -> session.id.toString))))
+        val input = createSession(request)
+        if(input == null) {
+          BadRequest(ResponseMessage(""Rejected, invalid value for ttl field!""))

Review Comment:
   should we throw an exception in `createSession()` and use appropriate message from that to return the `ResponseMessage` here? There could potentially be other reasons on why `createSession()` could return null, right?



##########
client-common/src/main/java/org/apache/livy/client/common/ClientConf.java:
##########
@@ -265,6 +265,51 @@ private Map<String, ConfPair> allAlternativeKeys() {
     return altToNewKeyMap;
   }
 
+  public static boolean validateTtl(String ttl) {
+    if (ttl == null || ttl.trim().isEmpty()) {
+      return true;
+    }
+    Matcher m = Pattern.compile(""(-?[0-9]+)([a-z]+)?"").matcher(ttl.trim().toLowerCase());
+    if (!m.matches()) {
+      LOG.error(""Invalid ttl string: "" + ttl);
+      return false;
+    }
+    long val = Long.parseLong(m.group(1));
+    if (val <= 0L) {
+      LOG.error(""Invalid ttl value: "" + val);
+      return false;
+    }
+    String suffix = m.group(2);
+    if (suffix != null && !TIME_SUFFIXES.containsKey(suffix)) {
+      LOG.error(""Invalid ttl suffix: "" + suffix);
+      return false;
+    }
+    return true;
+  }
+
+  public static long getTimeAsNanos(String time, int sessionId, long defaultVale) {

Review Comment:
   What is your opinion in reusing `getTimeAsMs()` instead of creating a new one? Other than the granularity, do you see any other gap between `getTimeAsMs()` and `getTimeAsNanos()` implementations?



;01/Feb/23 20:33;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1094126344


##########
server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala:
##########
@@ -52,15 +53,20 @@ class InteractiveSessionServlet(
 
   override protected def createSession(req: HttpServletRequest): InteractiveSession = {
     val createRequest = bodyAs[CreateInteractiveRequest](req)
-    InteractiveSession.create(
-      sessionManager.nextId(),
-      createRequest.name,
-      remoteUser(req),
-      proxyUser(req, createRequest.proxyUser),
-      livyConf,
-      accessManager,
-      createRequest,
-      sessionStore)
+    if (ClientConf.validateTtl(createRequest.ttl.orNull)) {
+      InteractiveSession.create(
+        sessionManager.nextId(),
+        createRequest.name,
+        remoteUser(req),
+        proxyUser(req, createRequest.proxyUser),
+        livyConf,
+        accessManager,
+        createRequest,
+        sessionStore,
+        createRequest.ttl)
+    } else {
+      null

Review Comment:
   I have fixed this by adding logic to throw an error.



;02/Feb/23 07:17;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1094126791


##########
client-common/src/main/java/org/apache/livy/client/common/ClientConf.java:
##########
@@ -265,6 +265,51 @@ private Map<String, ConfPair> allAlternativeKeys() {
     return altToNewKeyMap;
   }
 
+  public static boolean validateTtl(String ttl) {

Review Comment:
   Added the logic to reuse the same method.



;02/Feb/23 07:18;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1094128364


##########
client-common/src/main/java/org/apache/livy/client/common/HttpMessages.java:
##########
@@ -61,9 +61,10 @@ public static class SessionInfo implements ClientMessage {
     public final String kind;
     public final Map<String, String> appInfo;
     public final List<String> log;
+    public final String ttl;
 
     public SessionInfo(int id, String name, String appId, String owner, String proxyUser,

Review Comment:
   This constructor is not used in test classes and it is only one place in InteractiveSessionServlet.scala so we can skip adding extra constructor I believe.



;02/Feb/23 07:20;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1094129015


##########
server/src/main/scala/org/apache/livy/server/SessionServlet.scala:
##########
@@ -131,13 +131,18 @@ abstract class SessionServlet[S <: Session, R <: RecoveryMetadata](
       if (tooManySessions) {
         BadRequest(ResponseMessage(""Rejected, too many sessions are being created!""))
       } else {
-        val session = sessionManager.register(createSession(request))
-        // Because it may take some time to establish the session, update the last activity
-        // time before returning the session info to the client.
-        session.recordActivity()
-        Created(clientSessionView(session, request),
-          headers = Map(""Location"" ->
-            (getRequestPathInfo(request) + url(getSession, ""id"" -> session.id.toString))))
+        val input = createSession(request)
+        if(input == null) {
+          BadRequest(ResponseMessage(""Rejected, invalid value for ttl field!""))

Review Comment:
   I have updated the code to avoid returning null.



;02/Feb/23 07:21;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1094129232


##########
server/src/main/scala/org/apache/livy/server/interactive/CreateInteractiveRequest.scala:
##########
@@ -50,6 +51,7 @@ class CreateInteractiveRequest {
       (if (queue.isDefined) s""queue: ${queue.get}, "" else """") +
       (if (name.isDefined) s""name: ${name.get}, "" else """") +
       (if (conf.nonEmpty) s""conf: ${conf.mkString("","")}, "" else """") +
-      s""heartbeatTimeoutInSecond: $heartbeatTimeoutInSecond]""
+      s""heartbeatTimeoutInSecond: $heartbeatTimeoutInSecond, "" +
+      (if (ttl.isDefined) s""driverMemory: ${ttl.get}]"" else ""]"")

Review Comment:
   Done



;02/Feb/23 07:21;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1094130842


##########
client-common/src/main/java/org/apache/livy/client/common/ClientConf.java:
##########
@@ -265,6 +265,51 @@ private Map<String, ConfPair> allAlternativeKeys() {
     return altToNewKeyMap;
   }
 
+  public static boolean validateTtl(String ttl) {
+    if (ttl == null || ttl.trim().isEmpty()) {
+      return true;
+    }
+    Matcher m = Pattern.compile(""(-?[0-9]+)([a-z]+)?"").matcher(ttl.trim().toLowerCase());
+    if (!m.matches()) {
+      LOG.error(""Invalid ttl string: "" + ttl);
+      return false;
+    }
+    long val = Long.parseLong(m.group(1));
+    if (val <= 0L) {
+      LOG.error(""Invalid ttl value: "" + val);
+      return false;
+    }
+    String suffix = m.group(2);
+    if (suffix != null && !TIME_SUFFIXES.containsKey(suffix)) {
+      LOG.error(""Invalid ttl suffix: "" + suffix);
+      return false;
+    }
+    return true;
+  }
+
+  public static long getTimeAsNanos(String time, int sessionId, long defaultVale) {

Review Comment:
   `getTimeAsMs()` is defined at class level as non-static method whereas `getTimeAsNanos()` is a static method. It not easy to reuse I believe.



;02/Feb/23 07:24;githubbot;600","askhatri commented on PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#issuecomment-1418960419

   Hi @ksumit , most of the changes are completed. Could you please check if we are good to merge it?
   CC: @gyogal 


;06/Feb/23 11:53;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1097283480


##########
server/src/main/scala/org/apache/livy/sessions/SessionManager.scala:
##########
@@ -168,7 +169,9 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
             false
           } else {
             val currentTime = System.nanoTime()
-            currentTime - session.lastActivity > sessionTimeout
+            val calculatedTimeout =
+              ClientConf.getTimeAsNanos(session.ttl.orNull, session.id, sessionTimeout)

Review Comment:
   I have completed this code changes.



;06/Feb/23 11:54;githubbot;600","ksumit commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1097865760


##########
server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala:
##########
@@ -52,15 +53,18 @@ class InteractiveSessionServlet(
 
   override protected def createSession(req: HttpServletRequest): InteractiveSession = {
     val createRequest = bodyAs[CreateInteractiveRequest](req)
+    val sessionId = sessionManager.nextId();
+    ClientConf.getTimeAsNanos(createRequest.ttl.orNull, sessionId, 0L);

Review Comment:
   this could be place for us to parse and set the ttl value correctly:
   1. if it's present and valid, parse it and set to the parsed value
   2. if it's invalid, set it to the global session timeout value or reject the request.
   
   This method call is doing the same thing but passing 0L as default value seems hacky/confusing to me. Also calling a get method in isolation without using the return value seems hacky/confusing to me.



##########
server/src/main/scala/org/apache/livy/sessions/SessionManager.scala:
##########
@@ -168,7 +169,9 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
             false
           } else {
             val currentTime = System.nanoTime()
-            currentTime - session.lastActivity > sessionTimeout
+            val calculatedTimeout =
+              ClientConf.getTimeAsNanos(session.ttl.orNull, session.id, sessionTimeout)

Review Comment:
   I agree with @gyogal, if the string was validated and converted to valid long value at session creation, we would not have to worry about these issues at later stages. If this value is not passed in the request, we can use the global timeout value from `livy.server.session.timeout` or `LivyConf.SESSION_TIMEOUT`?



##########
client-common/src/main/java/org/apache/livy/client/common/ClientConf.java:
##########
@@ -265,6 +265,51 @@ private Map<String, ConfPair> allAlternativeKeys() {
     return altToNewKeyMap;
   }
 
+  public static boolean validateTtl(String ttl) {
+    if (ttl == null || ttl.trim().isEmpty()) {
+      return true;
+    }
+    Matcher m = Pattern.compile(""(-?[0-9]+)([a-z]+)?"").matcher(ttl.trim().toLowerCase());
+    if (!m.matches()) {
+      LOG.error(""Invalid ttl string: "" + ttl);
+      return false;
+    }
+    long val = Long.parseLong(m.group(1));
+    if (val <= 0L) {
+      LOG.error(""Invalid ttl value: "" + val);
+      return false;
+    }
+    String suffix = m.group(2);
+    if (suffix != null && !TIME_SUFFIXES.containsKey(suffix)) {
+      LOG.error(""Invalid ttl suffix: "" + suffix);
+      return false;
+    }
+    return true;
+  }
+
+  public static long getTimeAsNanos(String time, int sessionId, long defaultVale) {

Review Comment:
   Hey @askhatri, there is a lot of duplicate code in `getTimeAsMs()` and `getTimeAsNanos()`, here is how i think this can be resolved (please see the changed methods below, i've merged some of your validation logic as well). My hope is that we will be able to use `getTimeAsMs()` in this form instead of `getTimeAsNanos()`.
   ```
     public long getTimeAsMs(ConfEntry e) {
       String time = get(e, String.class);
       if (time == null) {
         check(e.dflt() != null,
             ""ConfEntry %s doesn't have a default value, cannot convert to time value."", e.key());
         time = (String) e.dflt();
       }
       return getTimeAsMs(time);
     }
   
     public static long getTimeAsMs(String time) {
       check(time != null && !time.trim().isEmpty(), ""Invalid time string: %s"", time);
       Matcher m = Pattern.compile(""(-?[0-9]+)([a-z]+)?"").matcher(time.toLowerCase());
       if (!m.matches()) {
         throw new IllegalArgumentException(""Invalid time string: "" + time);
       }
   
       long val = Long.parseLong(m.group(1));
       String suffix = m.group(2);
   
       if (suffix != null && !TIME_SUFFIXES.containsKey(suffix)) {
         throw new IllegalArgumentException(""Invalid suffix: \"""" + suffix + ""\"""");
       }
   
       return TimeUnit.MILLISECONDS.convert(val,
         suffix != null ? TIME_SUFFIXES.get(suffix) : TimeUnit.MILLISECONDS);
     }
   
     private static void check(boolean test, String message, Object... args) {
       if (!test) {
         throw new IllegalArgumentException(String.format(message, args));
       }
     }
   ```



;06/Feb/23 20:11;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1098603641


##########
server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala:
##########
@@ -52,15 +53,18 @@ class InteractiveSessionServlet(
 
   override protected def createSession(req: HttpServletRequest): InteractiveSession = {
     val createRequest = bodyAs[CreateInteractiveRequest](req)
+    val sessionId = sessionManager.nextId();
+    ClientConf.getTimeAsNanos(createRequest.ttl.orNull, sessionId, 0L);

Review Comment:
   I have changed the code by removing 0L and also added comment to avoid the confusion. We can not parse and set the ttl value during session creation as ttl field is taking String where as calculated value is in long. If we are a need field as ""calculatedTtlValue"" then it will be more complexity and very default to support session recovery scenario.



;07/Feb/23 12:36;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1098605784


##########
server/src/main/scala/org/apache/livy/sessions/SessionManager.scala:
##########
@@ -168,7 +169,9 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
             false
           } else {
             val currentTime = System.nanoTime()
-            currentTime - session.lastActivity > sessionTimeout
+            val calculatedTimeout =
+              ClientConf.getTimeAsNanos(session.ttl.orNull, session.id, sessionTimeout)

Review Comment:
   Considering the support for session recovery, we have decided to avoid converting it into long value during session creation.



;07/Feb/23 12:39;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1098606596


##########
client-common/src/main/java/org/apache/livy/client/common/ClientConf.java:
##########
@@ -265,6 +265,51 @@ private Map<String, ConfPair> allAlternativeKeys() {
     return altToNewKeyMap;
   }
 
+  public static boolean validateTtl(String ttl) {
+    if (ttl == null || ttl.trim().isEmpty()) {
+      return true;
+    }
+    Matcher m = Pattern.compile(""(-?[0-9]+)([a-z]+)?"").matcher(ttl.trim().toLowerCase());
+    if (!m.matches()) {
+      LOG.error(""Invalid ttl string: "" + ttl);
+      return false;
+    }
+    long val = Long.parseLong(m.group(1));
+    if (val <= 0L) {
+      LOG.error(""Invalid ttl value: "" + val);
+      return false;
+    }
+    String suffix = m.group(2);
+    if (suffix != null && !TIME_SUFFIXES.containsKey(suffix)) {
+      LOG.error(""Invalid ttl suffix: "" + suffix);
+      return false;
+    }
+    return true;
+  }
+
+  public static long getTimeAsNanos(String time, int sessionId, long defaultVale) {

Review Comment:
   I have updated the code to avoid the code duplication.



;07/Feb/23 12:39;githubbot;600","askhatri commented on PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#issuecomment-1420928594

   Hi @ksumit , today I have updated the code for further review. ðŸ˜Š
   CC: @gyogal 


;07/Feb/23 15:07;githubbot;600","ksumit commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1099607027


##########
server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala:
##########
@@ -52,15 +53,23 @@ class InteractiveSessionServlet(
 
   override protected def createSession(req: HttpServletRequest): InteractiveSession = {
     val createRequest = bodyAs[CreateInteractiveRequest](req)
+    val sessionId = sessionManager.nextId();
+
+    // Calling getTimeAsMs just to validate the ttl value
+    if (createRequest.ttl.isDefined) {
+      ClientConf.getTimeAsMs(createRequest.ttl.orNull);

Review Comment:
   we don't need to do `orNull` here right?



##########
server/src/main/scala/org/apache/livy/server/SessionServlet.scala:
##########
@@ -131,13 +131,18 @@ abstract class SessionServlet[S <: Session, R <: RecoveryMetadata](
       if (tooManySessions) {
         BadRequest(ResponseMessage(""Rejected, too many sessions are being created!""))
       } else {
-        val session = sessionManager.register(createSession(request))
-        // Because it may take some time to establish the session, update the last activity
-        // time before returning the session info to the client.
-        session.recordActivity()
-        Created(clientSessionView(session, request),
-          headers = Map(""Location"" ->
-            (getRequestPathInfo(request) + url(getSession, ""id"" -> session.id.toString))))
+        try {
+          val session = sessionManager.register(createSession(request))
+          // Because it may take some time to establish the session, update the last activity
+          // time before returning the session info to the client.
+          session.recordActivity()
+          Created(clientSessionView(session, request),
+            headers = Map(""Location"" ->
+              (getRequestPathInfo(request) + url(getSession, ""id"" -> session.id.toString))))
+        } catch {
+          case e: IllegalArgumentException =>
+            BadRequest(ResponseMessage(""Rejected, Invalid ttl field: "" + e.getMessage))

Review Comment:
   Right now, the message may be related to ttl field validation but this doesn't have to be right? Can we do this instead please?
   
   ```
   BadRequest(ResponseMessage(""Rejected, Reason: "" + e.getMessage))
   ```



##########
server/src/main/scala/org/apache/livy/sessions/SessionManager.scala:
##########
@@ -168,7 +170,12 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
             false
           } else {
             val currentTime = System.nanoTime()
-            currentTime - session.lastActivity > sessionTimeout
+            var calculatedTimeout = sessionTimeout;
+            if (session.ttl.isDefined) {
+              calculatedTimeout = ClientConf.getTimeAsMs(session.ttl.orNull)

Review Comment:
   we don't need `orNull` here right?



;08/Feb/23 03:17;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1099667793


##########
server/src/main/scala/org/apache/livy/sessions/SessionManager.scala:
##########
@@ -168,7 +170,12 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
             false
           } else {
             val currentTime = System.nanoTime()
-            currentTime - session.lastActivity > sessionTimeout
+            var calculatedTimeout = sessionTimeout;
+            if (session.ttl.isDefined) {
+              calculatedTimeout = ClientConf.getTimeAsMs(session.ttl.orNull)

Review Comment:
   I have removed `orNull`.



;08/Feb/23 05:20;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1099667890


##########
server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala:
##########
@@ -52,15 +53,23 @@ class InteractiveSessionServlet(
 
   override protected def createSession(req: HttpServletRequest): InteractiveSession = {
     val createRequest = bodyAs[CreateInteractiveRequest](req)
+    val sessionId = sessionManager.nextId();
+
+    // Calling getTimeAsMs just to validate the ttl value
+    if (createRequest.ttl.isDefined) {
+      ClientConf.getTimeAsMs(createRequest.ttl.orNull);

Review Comment:
   I have removed `orNull`.



;08/Feb/23 05:20;githubbot;600","askhatri commented on code in PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#discussion_r1099668310


##########
server/src/main/scala/org/apache/livy/server/SessionServlet.scala:
##########
@@ -131,13 +131,18 @@ abstract class SessionServlet[S <: Session, R <: RecoveryMetadata](
       if (tooManySessions) {
         BadRequest(ResponseMessage(""Rejected, too many sessions are being created!""))
       } else {
-        val session = sessionManager.register(createSession(request))
-        // Because it may take some time to establish the session, update the last activity
-        // time before returning the session info to the client.
-        session.recordActivity()
-        Created(clientSessionView(session, request),
-          headers = Map(""Location"" ->
-            (getRequestPathInfo(request) + url(getSession, ""id"" -> session.id.toString))))
+        try {
+          val session = sessionManager.register(createSession(request))
+          // Because it may take some time to establish the session, update the last activity
+          // time before returning the session info to the client.
+          session.recordActivity()
+          Created(clientSessionView(session, request),
+            headers = Map(""Location"" ->
+              (getRequestPathInfo(request) + url(getSession, ""id"" -> session.id.toString))))
+        } catch {
+          case e: IllegalArgumentException =>
+            BadRequest(ResponseMessage(""Rejected, Invalid ttl field: "" + e.getMessage))

Review Comment:
   I have updated the code as requested.



;08/Feb/23 05:21;githubbot;600","askhatri commented on PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#issuecomment-1422675913

   > Minor comments, looks good otherwise. Please fix them and i will approve from my side. Thank you for your contribution @askhatri
   
   All comments fixed today as suggested by you.


;08/Feb/23 14:22;githubbot;600","gyogal commented on PR #384:
URL: https://github.com/apache/incubator-livy/pull/384#issuecomment-1423887628

   This looks good to me as well. Thinking about it, it may be good to introduce a maximum or a limit for this, so that it cannot be longer than a set amount. But this could be addressed in a separate ticket.


;09/Feb/23 09:32;githubbot;600","lmccay merged PR #384:
URL: https://github.com/apache/incubator-livy/pull/384


;10/Feb/23 16:12;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,18600,,,0,18600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 10 16:15:02 UTC 2023,,,,,,,,,,"0|z1fa40:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/Jan/23 11:46;asifkhatri;I have created [https://github.com/apache/incubator-livy/pull/384] PR with the fix.;;;","25/Jan/23 16:50;lmccay;Hi [~asifkhatri]Â  - thank you for the PR!

I've added you as as contributor to the Livy project and assigned this to you.

We can try to get this into 0.8.0 if you like and the reviews go well.

I'll add the Fix Version now - we can always move it out.;;;","27/Jan/23 08:42;asifkhatri;Thank you [~lmccay]Â ;;;","10/Feb/23 16:15;lmccay;This has been merged and will be available in the 0.8.0 release.

Thank you for your contribution to Livy, [~asifkhatri] !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Return session information with livy sessions APIs,LIVY-967,13520882,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,asifkhatri,vedantlodha,vedantlodha,21/Jan/23 05:27,18/Apr/23 12:17,19/Dec/25 04:15,18/Apr/23 12:17,,,,0.8.0,,API,,,,,,,,,,0,,,,,,"Currently livy GET /Sessions doesnt return fields like driver-executor memory, spark conf, etc. Ideally a session response should return all the values set in session request api call(POST /sessions).

Â 

Â 

Â 

The current response for GET /sessions or GET /sessions/\{id} is (ref: https://livy.incubator.apache.org/docs/latest/rest-api.html).
||Name||Description||Type||
|id|The session id|int|
|appId|The application id of this session|string|
|owner|Remote user who submitted this session|string|
|proxyUser|User to impersonate when running|string|
|kind|Session kind (spark, pyspark, sparkr, or sql)|[session kind|https://livy.incubator.apache.org/docs/latest/rest-api.html#session-kind]|
|log|The log lines|list of strings|
|state|The session state|string|
|appInfo|The detailed application info|Map of key=val|

However the session requests for additional spark related configurations like
 # jars
 # pyFiles
 # files
 # driverMemory
 # driverCores
 # executorMemory
 # executorCores
 # numExecutors
 # archives
 # queue
 # name
 # conf
 # heartbeatTimeoutInSecond

The API should return these fields with the response as well.",,"askhatri opened a new pull request, #383:
URL: https://github.com/apache/incubator-livy/pull/383

   ## What changes were proposed in this pull request?
   
   Currently livy GET /Sessions doesn't return fields like driver-executor memory, spark configuration, etc. Ideally a session response should return all the values set in session request api call(POST /sessions).
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-967
   
   ## How was this patch tested?
   
   Verified manually by creating interactive session via REST API call in a local Yarn cluster. Also, we have updated the unit tests.
   


;23/Jan/23 11:42;githubbot;600","codecov-commenter commented on PR #383:
URL: https://github.com/apache/incubator-livy/pull/383#issuecomment-1400243245

   # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report
   > Merging [#383](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (fe5a292) into [master](https://codecov.io/gh/apache/incubator-livy/commit/8d07eee831221093f55203190d38d793234c5db7?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (8d07eee) will **decrease** coverage by `2.22%`.
   > The diff coverage is `100.00%`.
   
   ```diff
   @@             Coverage Diff              @@
   ##             master     #383      +/-   ##
   ============================================
   - Coverage     68.26%   66.05%   -2.22%     
   + Complexity      844      830      -14     
   ============================================
     Files           103      103              
     Lines          5965     6030      +65     
     Branches        907      907              
   ============================================
   - Hits           4072     3983      -89     
   - Misses         1333     1523     +190     
   + Partials        560      524      -36     
   ```
   
   
   | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage Î” | |
   |---|---|---|
   | [...va/org/apache/livy/client/common/HttpMessages.java](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y2xpZW50LWNvbW1vbi9zcmMvbWFpbi9qYXZhL29yZy9hcGFjaGUvbGl2eS9jbGllbnQvY29tbW9uL0h0dHBNZXNzYWdlcy5qYXZh) | `96.29% <100.00%> (+1.05%)` | :arrow_up: |
   | [...e/livy/server/interactive/InteractiveSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `62.24% <100.00%> (-7.53%)` | :arrow_down: |
   | [...server/interactive/InteractiveSessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `62.87% <100.00%> (-4.84%)` | :arrow_down: |
   | [...a/org/apache/livy/server/ThriftServerFactory.scala](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvVGhyaWZ0U2VydmVyRmFjdG9yeS5zY2FsYQ==) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |
   | [...main/scala/org/apache/livy/server/LivyServer.scala](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvTGl2eVNlcnZlci5zY2FsYQ==) | `2.23% <0.00%> (-29.47%)` | :arrow_down: |
   | [...rc/main/scala/org/apache/livy/utils/SparkApp.scala](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya0FwcC5zY2FsYQ==) | `56.00% <0.00%> (-20.00%)` | :arrow_down: |
   | [core/src/main/scala/org/apache/livy/Logging.scala](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvTG9nZ2luZy5zY2FsYQ==) | `66.66% <0.00%> (-16.67%)` | :arrow_down: |
   | [...src/main/scala/org/apache/livy/sessions/Kind.scala](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvc2Vzc2lvbnMvS2luZC5zY2FsYQ==) | `66.66% <0.00%> (-9.53%)` | :arrow_down: |
   | [...la/org/apache/livy/utils/LineBufferedProcess.scala](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9MaW5lQnVmZmVyZWRQcm9jZXNzLnNjYWxh) | `78.57% <0.00%> (-7.15%)` | :arrow_down: |
   | ... and [19 more](https://codecov.io/gh/apache/incubator-livy/pull/383?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | |
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   


;23/Jan/23 12:12;githubbot;600","gyogal merged PR #383:
URL: https://github.com/apache/incubator-livy/pull/383


;18/Apr/23 12:15;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jan 23 11:46:56 UTC 2023,,,,,,,,,,"0|z1fa3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/Jan/23 11:46;asifkhatri;I have created [https://github.com/apache/incubator-livy/pull/383] PR with the fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Address Jetty Dependency Upgrades for 0.8.0,LIVY-904,13513477,13512393,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,lmccay,lmccay,lmccay,13/Dec/22 20:21,24/Dec/22 17:33,19/Dec/25 04:15,24/Dec/22 17:33,0.7.0,,,0.8.0,,Server,,,,,,,,,,0,,,,,,"Assess the dependency-check reported items for Jetty 9.3.24 and upgrade as appropriate.

See LIVY-900",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Dec 24 17:33:53 UTC 2022,,,,,,,,,,"0|z1e0rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Dec/22 23:27;lmccay;Consider bumping jetty.version to 9.4.50.v20221201

Â ;;;","24/Dec/22 17:33;lmccay;[https://github.com/apache/incubator-livy/pull/372] merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Address Netty Upgrades for 0.8.0,LIVY-903,13513236,13512393,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,lmccay,lmccay,lmccay,12/Dec/22 17:20,15/Dec/22 18:01,19/Dec/25 04:15,15/Dec/22 18:01,0.7.0,,,0.8.0,,Server,,,,,,,,,,0,,,,,,Assess all CRITICAL Netty related CVE items from Livy-900 attached report. Determine whether an upgrade is required.,,"lmccay opened a new pull request, #371:
URL: https://github.com/apache/incubator-livy/pull/371

   LIVY-903 - Address Netty Upgrades for 0.8.0
   
   ## What changes were proposed in this pull request?
   
   Upgrade of netty version from 4.1.17 to 4.1.71
   
   ## How was this patch tested?
   
   Build with unit and integration tests.
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   


;13/Dec/22 18:15;githubbot;600","dacort commented on code in PR #371:
URL: https://github.com/apache/incubator-livy/pull/371#discussion_r1047869152


##########
pom.xml:
##########
@@ -100,8 +100,8 @@
     <kryo.version>4.0.2</kryo.version>
     <metrics.version>3.1.0</metrics.version>
     <mockito.version>1.10.19</mockito.version>
-    <netty.spark-2.11.version>4.1.17.Final</netty.spark-2.11.version>
-    <netty.spark-2.12.version>4.1.17.Final</netty.spark-2.12.version>
+    <netty.spark-2.11.version>4.1.71.Final</netty.spark-2.11.version>

Review Comment:
   Any reason we didn't go higher? 4.1.86 is out, but it's tough to tell what the diff / impact is.



;13/Dec/22 23:56;githubbot;600","lmccay commented on code in PR #371:
URL: https://github.com/apache/incubator-livy/pull/371#discussion_r1047872458


##########
pom.xml:
##########
@@ -100,8 +100,8 @@
     <kryo.version>4.0.2</kryo.version>
     <metrics.version>3.1.0</metrics.version>
     <mockito.version>1.10.19</mockito.version>
-    <netty.spark-2.11.version>4.1.17.Final</netty.spark-2.11.version>
-    <netty.spark-2.12.version>4.1.17.Final</netty.spark-2.12.version>
+    <netty.spark-2.11.version>4.1.71.Final</netty.spark-2.11.version>

Review Comment:
   Good question, I just looked for the one above the last reported issue. Let me do some more digging.



;14/Dec/22 00:03;githubbot;600","codecov-commenter commented on PR #371:
URL: https://github.com/apache/incubator-livy/pull/371#issuecomment-1352270381

   # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/371?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report
   > Merging [#371](https://codecov.io/gh/apache/incubator-livy/pull/371?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (73f6c70) into [master](https://codecov.io/gh/apache/incubator-livy/commit/41304e8ef1d82127740820b4998fd54f8137ee60?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (41304e8) will **decrease** coverage by `2.65%`.
   > The diff coverage is `n/a`.
   
   ```diff
   @@             Coverage Diff              @@
   ##             master     #371      +/-   ##
   ============================================
   - Coverage     68.39%   65.73%   -2.66%     
   + Complexity      843      816      -27     
   ============================================
     Files           103      103              
     Lines          5948     5948              
     Branches        899      899              
   ============================================
   - Hits           4068     3910     -158     
   - Misses         1319     1512     +193     
   + Partials        561      526      -35     
   ```
   
   
   | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/371?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage Î” | |
   |---|---|---|
   | [...a/org/apache/livy/server/ThriftServerFactory.scala](https://codecov.io/gh/apache/incubator-livy/pull/371/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvVGhyaWZ0U2VydmVyRmFjdG9yeS5zY2FsYQ==) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |
   | [...main/scala/org/apache/livy/server/LivyServer.scala](https://codecov.io/gh/apache/incubator-livy/pull/371/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvTGl2eVNlcnZlci5zY2FsYQ==) | `2.23% <0.00%> (-29.92%)` | :arrow_down: |
   | [...rc/main/scala/org/apache/livy/utils/SparkApp.scala](https://codecov.io/gh/apache/incubator-livy/pull/371/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya0FwcC5zY2FsYQ==) | `56.00% <0.00%> (-20.00%)` | :arrow_down: |
   | [core/src/main/scala/org/apache/livy/Logging.scala](https://codecov.io/gh/apache/incubator-livy/pull/371/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvTG9nZ2luZy5zY2FsYQ==) | `66.66% <0.00%> (-16.67%)` | :arrow_down: |
   | [...e/livy/server/interactive/InteractiveSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/371/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `56.97% <0.00%> (-12.80%)` | :arrow_down: |
   | [...src/main/scala/org/apache/livy/sessions/Kind.scala](https://codecov.io/gh/apache/incubator-livy/pull/371/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvc2Vzc2lvbnMvS2luZC5zY2FsYQ==) | `66.66% <0.00%> (-9.53%)` | :arrow_down: |
   | [...la/org/apache/livy/utils/LineBufferedProcess.scala](https://codecov.io/gh/apache/incubator-livy/pull/371/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9MaW5lQnVmZmVyZWRQcm9jZXNzLnNjYWxh) | `78.57% <0.00%> (-7.15%)` | :arrow_down: |
   | [...server/interactive/InteractiveSessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/371/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `61.41% <0.00%> (-6.30%)` | :arrow_down: |
   | [.../scala/org/apache/livy/server/SessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/371/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvU2Vzc2lvblNlcnZsZXQuc2NhbGE=) | `66.31% <0.00%> (-5.27%)` | :arrow_down: |
   | [...org/apache/livy/server/recovery/SessionStore.scala](https://codecov.io/gh/apache/incubator-livy/pull/371/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvcmVjb3ZlcnkvU2Vzc2lvblN0b3JlLnNjYWxh) | `75.00% <0.00%> (-5.00%)` | :arrow_down: |
   | ... and [14 more](https://codecov.io/gh/apache/incubator-livy/pull/371/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | |
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   


;14/Dec/22 22:00;githubbot;600","lmccay commented on PR #371:
URL: https://github.com/apache/incubator-livy/pull/371#issuecomment-1352326082

   Triggered a new build as I think it is likely a flaky test.
   


;14/Dec/22 22:52;githubbot;600","lmccay merged PR #371:
URL: https://github.com/apache/incubator-livy/pull/371


;15/Dec/22 01:27;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2022-12-12 17:20:09.0,,,,,,,,,,"0|z1dza0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Address Spark Dependency Upgrades for 0.8.0,LIVY-902,13512397,13512393,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Problem,lmccay,lmccay,lmccay,11/Dec/22 18:28,12/Dec/22 17:20,19/Dec/25 04:15,12/Dec/22 16:56,0.7.0,,,0.8.0,,Core,,,,,,,,,,0,,,,,,Address the CRITICAL severities for Spark dependencies.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 12 16:57:50 UTC 2022,,,,,,,,,,"0|z1du3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Dec/22 18:32;lmccay;[CVE-2018-17190 |http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2018-17190]- appears to be mitigated by requiring authentication on the Spark standalone resource manager.

I imagine that this is not a responsibility of Livy and instead of the Spark server running wherever Livy is expecting it to be.

Therefore, Livy does not need any specific mitigation for this CVE but Spark in the environment where Livy would be used is responsible.

[~dacort] - any thoughts on this?;;;","11/Dec/22 18:35;lmccay;[CVE-2020-9480|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2020-9480]Â - is likewise the responsibility of the Spark server implementation and deployment and not of the client side within Livy.

Â ;;;","11/Dec/22 18:45;lmccay;[CVE-2022-33891|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2022-33891] - is also a Spark server/UI related issue and outside of the Livy scope for these dependencies.

It seems that this is also talking about trusted proxy or impersonation capabilities which are generally implemented only with kerberos enabled.

I'll look at that implementation in Spark separately if I have a chance.

[~irashid]Â  - FYI ^^^

Â ;;;","11/Dec/22 18:52;lmccay;*[CVE-2018-11804|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2018-11804],*Â *[CVE-2021-38296|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2021-38296],*Â *[CVE-2022-31777|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2022-31777],*Â {*}[CVE-2018-11770|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2018-11770]{*}{*}{*}{*}{*}{*}{*}

All of the above seem to be Spark server related issues and not related to Livy directly.

*[CVE-2018-11770|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2018-11770]* - is related to the REST server in the standalone master for submitting jobs - even if Livy is using this API, it is reasonable to expect this to be the responsibility of the Spark master rather than the Livy side.

Â ;;;","11/Dec/22 18:55;lmccay;[CVE-2021-4125|http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2021-4125]Â - OpenShift metering specific versions - Livy is not affected.;;;","11/Dec/22 19:08;lmccay;Okay - I believe that I covered all of the CRITICALs (and a few lower) for Spark and I don't think mitigation should be required for Livy. This is all assuming that Livy is not bundling and starting its own standalone Spark master, etc. This could use some verification from those more familiar.

cc. [~jbonofre] , [~irashid] , [~ajbozarth]Â ;;;","12/Dec/22 09:27;jbonofre;Yes, it looks good to me. Thanks.;;;","12/Dec/22 16:57;lmccay;Resolved this as Not a Problem as assessment has determined that we are not vulnerable to the spark server related issues in the latest scan report attached to Livy-900.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy could intermittently returns batch as SUCCEED even Spark on Yarn actually fails,LIVY-896,13486392,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,jeff.xu.z@gmail.com,jeff.xu.z@gmail.com,jeff.xu.z@gmail.com,14/Oct/22 17:52,26/Nov/22 04:54,19/Dec/25 04:15,26/Nov/22 04:54,,,,0.8.0,,Server,,,,,,,,,,0,,,,,,"*Summary:*
 * I ran into this issue using AWS EMR.
 * Frequency of the issue varies. On one EMR cluster, I typically see ~10-20% chance of hitting the issue. But on another EMR cluster, the chance is ~1%. I suspect the chance depends on how busy AWS hardware actually was (my EMR likely share hardware resources with other AWS tenants).
 * I believe that I have identify the root cause in Livy source code (refer to a later section).

Â 

*How to reproduce:*
 * An EMR with Spark, Yarn and Livy configured.
 * Use the attached livy_batch.py to trigger a Livy batch by using livy python client (0.8.0). See attached livy_client.py.
 * Repeat the testing and you should see when the issue happens, even though the spark program errors out, Livy still reports the batch as SUCCEED.

Â 

*Livy log for a good case when Livy returns batch as DEAD (expected behavior):*
22/10/14 02:46:22 INFO BatchSessionManager: Registered new session 1
22/10/14 02:46:42 DEBUG BatchSession: BatchSession 1 state changed from STARTING to RUNNING
22/10/14 02:46:43 WARN BatchSession$: spark-submit exited with code 1
22/10/14 02:46:47 DEBUG BatchSession: BatchSession 1 state changed from RUNNING to FINISHED
22/10/14 02:46:47 DEBUG BatchSession: BatchSession 1 state changed from FINISHED to FAILED
Â 

*Livy log for bad case when Livy returns batch as SUCCEED (bug):*
22/10/14 02:47:40 INFO BatchSessionManager: Registered new session 3
22/10/14 02:48:00 DEBUG BatchSession: BatchSession 3 state changed from STARTING to FINISHED
22/10/14 02:48:01 WARN BatchSession$: spark-submit exited with code 1
Â 

*Root cause analysis:*
 * I think the bug is in the YarnAppMonitorThread of Livy server.
 * The bug is in this section: [https://github.com/apache/incubator-livy/blob/v0.7.1-incubating-rc1/server/src/main/scala/org/apache/livy/utils/SparkYarnApp.scala#L283-L299]
 * When the right timing happens,
 ** Yarn sees the application completed
 ** But spark-submit process is still running
 * So line 286 returns app report saying the application completes successfully (Yarn has not context of the Spark application succeeds or not).
 * Line 288 set the Livy session as succeed.
 * Line 293 returns false as the spark-submit is still running.
 * So we hit the bug.

Even without hitting the timing condition, the code logic itself is {*}still incorrect{*}.

If you take a look at the log from a ""good"" case, the session state was updated twice: FINISHED, then FAILED. If a client query arrives on the perfect timing, the livy server could can still return a wrong state.
{noformat}
22/10/14 02:46:47 DEBUG BatchSession: BatchSession 1 state changed from RUNNING to FINISHED 
22/10/14 02:46:47 DEBUG BatchSession: BatchSession 1 state changed from FINISHED to FAILED{noformat}
Â 

I hope we can work together to have the issue addressed ASAP as the bug hit our production code pretty bad. I think the right code logic should be:
 # read the spark-submit process's state, if still running, do nothing
 # If the spark-submit process finishes, read Yarn report, and determines the actual application finish state in a single shot.
 # Update the session state in a single step.

Â 

At the same time, I will see if I can create a PR with suggested fix soon. The challenge on my side is that it's almost impossible for me to swap a few jars from open-source code base on AWS EMR (not compatible with EMR runtime).

Â 

Thank you, Livy team!

Regards,
Jeff Xu, a Workday engineer",,"jeff-xu-z opened a new pull request, #358:
URL: https://github.com/apache/incubator-livy/pull/358

   
   ## What changes were proposed in this pull request?
   
   Proposed code fix for [https://issues.apache.org/jira/browse/LIVY-896](https://issues.apache.org/jira/browse/LIVY-896).
   
   ## How was this patch tested?
   
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   
   ### Unit tests
   
   Two new unit tests are included in the PR. Outputs of the new unit tests are attached as [unit-tests.txt](https://github.com/apache/incubator-livy/files/9793910/unit-tests.txt).
   
   ### System tests
   
   I run system tests manually to verify the fix on an EMR in AWS.
   
   1. Install open source Livy 0.7.1 and configured it running on port 8999.
   2. Upload my PySpark program run_sql.py to the cluster's HDFS (see the artifact below)
   3. Loop my test_livy.py (see the artifact below) for 10 times. I hit the issue 5 out of 10 tries (Livy reported SessionState.SUCCESS even though spark-submit failed). See [reproduced.txt](https://github.com/apache/incubator-livy/files/9793894/reproduced.txt) for details.
   5. Replaced livy-server jar with the fixed version.
   6. Loop my test_livy.py for 100 times. I never hit the issue again. See [fixed.txt](https://github.com/apache/incubator-livy/files/9793943/fixed.txt) for details.
   7. I also quickly verified that Livy Session does not have the issue. See [livy_session.txt](https://github.com/apache/incubator-livy/files/9793963/livy_session.txt) for details.
   
   ### Artifact: test_livy.py (the verification program)
   ```
   #!/usr/bin/env python3
   from livy import LivyBatch
   import sys
   
   import logging
   logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s', stream=sys.stderr, level=logging.INFO)
   
   
   if __name__ == ""__main__"":
       batch = LivyBatch.create(
               url=""http://ip-100-64-129-199.us-west-2.compute.internal:8999"",
               file=""/tmp/run_sql.py"", args=[""-s"", ""select * from abc""],
               )
       logging.info(f""batch id={batch.batch_id} created ..."")
       batch.wait()
       logging.info(f""batch id={batch.batch_id}, state={batch.state}"")
   ```
   
   ### Artifact: run_sql.py (the Spark program to run a given SQL)
   
   ```
   from pyspark.sql import SparkSession
   import sys
   import argparse
   
   if __name__ == ""__main__"":
       parser = argparse.ArgumentParser(
           formatter_class = argparse.ArgumentDefaultsHelpFormatter,
       )
       parser.add_argument(""-s"", action=""store"", dest=""sql"")
       args = parser.parse_args()
   
       spark = SparkSession.builder.\
               appName(""PySpark SparkSQL"").\
               enableHiveSupport().\
               config(""spark.ui.enabled"", ""false"").\
               getOrCreate()
       try:
           spark.sql(args.sql).show()
       finally:
          spark.stop()
   ```
   
   ### Artifact: test_session.py (verify Livy session does not have the issue)
   ```
   #!/usr/bin/env python3
   from livy import LivySession, SessionKind
   import sys
   import logging
   logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s', stream=sys.stderr, level=logging.INFO)
   
   
   if __name__ == ""__main__"":
       sess = LivySession.create(
               url=""http://ip-100-64-129-199.us-west-2.compute.internal:8999"", 
               kind=SessionKind.SQL)
       logging.info(f""session id={sess.session_id} created ..."")
       sess.wait()
       logging.info(f""session id={sess.session_id} is ready"")
       try:
         sess.download_sql(""SELECT * from xxyyzz"")
       except Exception as e:
         logging.info(str(e))
   ```
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   


;16/Oct/22 08:48;githubbot;600","jeff-xu-z commented on PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#issuecomment-1279925971

   @jerryshao @alex-the-man @ajbozarth Would you please take a look at the build failures? The errors don't seem to be related to my changes. Also I can see the same error started occurring a while ago.
   
   Thanks,
   Jeff


;16/Oct/22 09:01;githubbot;600","rliuamzn commented on code in PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#discussion_r1000966624


##########
server/src/main/scala/org/apache/livy/utils/SparkYarnApp.scala:
##########
@@ -282,34 +286,45 @@ class SparkYarnApp private[utils] (
         try {
           Clock.sleep(pollInterval.toMillis)
 
-          // Refresh application state
-          val appReport = yarnClient.getApplicationReport(appId)
-          yarnDiagnostics = getYarnDiagnostics(appReport)
-          changeState(mapYarnState(
-            appReport.getApplicationId,
-            appReport.getYarnApplicationState,
-            appReport.getFinalApplicationStatus))
-
-          if (isProcessErrExit()) {
-            if (killed) {
-              changeState(SparkApp.State.KILLED)
-            } else {
-              changeState(SparkApp.State.FAILED)
+          if (!isProcessAlive()) {

Review Comment:
   If isProcessAlive == true, will it create an infinite loop doing nothing?



;20/Oct/22 18:09;githubbot;600","jeff-xu-z commented on code in PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#discussion_r1000987512


##########
server/src/main/scala/org/apache/livy/utils/SparkYarnApp.scala:
##########
@@ -282,34 +286,45 @@ class SparkYarnApp private[utils] (
         try {
           Clock.sleep(pollInterval.toMillis)
 
-          // Refresh application state
-          val appReport = yarnClient.getApplicationReport(appId)
-          yarnDiagnostics = getYarnDiagnostics(appReport)
-          changeState(mapYarnState(
-            appReport.getApplicationId,
-            appReport.getYarnApplicationState,
-            appReport.getFinalApplicationStatus))
-
-          if (isProcessErrExit()) {
-            if (killed) {
-              changeState(SparkApp.State.KILLED)
-            } else {
-              changeState(SparkApp.State.FAILED)
+          if (!isProcessAlive()) {

Review Comment:
   @rliuamzn Correct, a Livy batch is supposed to wait for the spark-submit process launched by the Livy batch to complete before the batch can change its state. If the spark-submit is alive, the monitoring thread should be in the loop of ""check if still alive, sleep if so"". This should be the right logic.



;20/Oct/22 18:32;githubbot;600","jeff-xu-z commented on code in PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#discussion_r1001033403


##########
server/src/main/scala/org/apache/livy/utils/SparkYarnApp.scala:
##########
@@ -282,34 +286,45 @@ class SparkYarnApp private[utils] (
         try {
           Clock.sleep(pollInterval.toMillis)
 
-          // Refresh application state
-          val appReport = yarnClient.getApplicationReport(appId)
-          yarnDiagnostics = getYarnDiagnostics(appReport)
-          changeState(mapYarnState(
-            appReport.getApplicationId,
-            appReport.getYarnApplicationState,
-            appReport.getFinalApplicationStatus))
-
-          if (isProcessErrExit()) {
-            if (killed) {
-              changeState(SparkApp.State.KILLED)
-            } else {
-              changeState(SparkApp.State.FAILED)
+          if (!isProcessAlive()) {

Review Comment:
   @rliuamzn The process relationship should be like this (based on my understanding),
   
   1. A client sends a rest API asking for a new Livy batch
   2. Livy process on EMR master creates the batch object and respond the batch id back to the ReST client.
   3. Livy process invokes spark-submit process on the EMR master and wait for spark-submit to finish.
   4. The spark-submit creates Yarn application, which will run the actual spark batch work on EMR worker nodes since Livy was configured with ""yarn"" as spark deployment mode.
   5. Before the spark application finishes, both the livy batch and the spark-submit process just wait.
   6. Whence the Yarn application finishes, the spark-submit should exit with appropriate exit code. Then Livy should set the batch to corresponding state.
   
   Livy should **only** rely on completion of the spark-submit process, not Yarn, as the condition for completion of the batch.
   
   I don't see why the spark-submit process won't finish whence Yarn completes the application. So the theoretical infinite loop should not happen (otherwise we should open a bug with Spark project).  
   
   Thanks,
   Jeff



;20/Oct/22 19:28;githubbot;600","rliuamzn commented on code in PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#discussion_r1001103665


##########
server/src/main/scala/org/apache/livy/utils/SparkYarnApp.scala:
##########
@@ -282,34 +286,45 @@ class SparkYarnApp private[utils] (
         try {
           Clock.sleep(pollInterval.toMillis)
 
-          // Refresh application state
-          val appReport = yarnClient.getApplicationReport(appId)
-          yarnDiagnostics = getYarnDiagnostics(appReport)
-          changeState(mapYarnState(
-            appReport.getApplicationId,
-            appReport.getYarnApplicationState,
-            appReport.getFinalApplicationStatus))
-
-          if (isProcessErrExit()) {
-            if (killed) {
-              changeState(SparkApp.State.KILLED)
-            } else {
-              changeState(SparkApp.State.FAILED)
+          if (!isProcessAlive()) {

Review Comment:
   I see your point. Thank you for the explanation.



;20/Oct/22 20:55;githubbot;600","rliuamzn commented on code in PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#discussion_r1014529813


##########
server/src/test/scala/org/apache/livy/utils/SparkYarnAppSpec.scala:
##########
@@ -269,6 +269,90 @@ class SparkYarnAppSpec extends FunSpec with LivyBaseUnitTestSuite {
       }
     }
 
+    it(""should end with state failed when spark submit failed but Yarn reports application SUCCESS (LIVY-896)"") {

Review Comment:
   Could you shorten this line to ```should end with state failed when spark submit failed but Yarn reports SUCCESS```? This line exceeds 100 chars and failed the build for me.



;05/Nov/22 00:02;githubbot;600","jeff-xu-z commented on code in PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#discussion_r1014538560


##########
server/src/test/scala/org/apache/livy/utils/SparkYarnAppSpec.scala:
##########
@@ -269,6 +269,90 @@ class SparkYarnAppSpec extends FunSpec with LivyBaseUnitTestSuite {
       }
     }
 
+    it(""should end with state failed when spark submit failed but Yarn reports application SUCCESS (LIVY-896)"") {

Review Comment:
   Done, can you retry?



;05/Nov/22 00:51;githubbot;600","leesf commented on PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#issuecomment-1305006237

   @jeff-xu-z would you please rebase to latest master to fix the CI?


;07/Nov/22 02:36;githubbot;600","jeff-xu-z commented on PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#issuecomment-1305311485

   Hi @leesf:
   
   This is what I see in my LIVY-896. I should already be on top of latest master commit (4d8a912699683b973eee76d4e91447d769a0cb0d). 
   
   I just tried ""git checkout master && git pull"". I didn't see any new changes from master branch.
   
   ```
   commit fb5f9d08d0a6dc84a62658269bcbb3522d37a0ac (HEAD -> LIVY-896, origin/LIVY-896)
   Author: Jeff Xu <jeff.xu.z@gmail.com>
   Date:   Fri Oct 14 12:53:10 2022 -0700
   
       [LIVY-896] Livy not capture spark-submit error exit if timing is right
   
   commit 4d8a912699683b973eee76d4e91447d769a0cb0d (origin/master, origin/HEAD, master)
   Author: Marco Gaido <mgaido@apache.org>
   Date:   Fri Aug 14 17:25:54 2020 -0700
   ```
   
   Thanks,
   Jeff


;07/Nov/22 09:15;githubbot;600","leesf commented on PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#issuecomment-1305459101

   > Hi @leesf:
   > 
   > This is what I see in my LIVY-896. I should already be on top of latest master commit ([4d8a912](https://github.com/apache/incubator-livy/commit/4d8a912699683b973eee76d4e91447d769a0cb0d)).
   > 
   > I just tried ""git checkout master && git pull"". I didn't see any new changes from master branch.
   > 
   > ```
   > commit fb5f9d08d0a6dc84a62658269bcbb3522d37a0ac (HEAD -> LIVY-896, origin/LIVY-896)
   > Author: Jeff Xu <jeff.xu.z@gmail.com>
   > Date:   Fri Oct 14 12:53:10 2022 -0700
   > 
   >     [LIVY-896] Livy not capture spark-submit error exit if timing is right
   > 
   > commit 4d8a912699683b973eee76d4e91447d769a0cb0d (origin/master, origin/HEAD, master)
   > Author: Marco Gaido <mgaido@apache.org>
   > Date:   Fri Aug 14 17:25:54 2020 -0700
   > ```
   > 
   > Thanks, Jeff
   
   @jeff-xu-z something must be wrong, you would see that I did push a fix on the master branch. maybe you need `git checkout master && git pull upstream master` the apache/livy repo should be upstream instead of origin in your config?


;07/Nov/22 11:17;githubbot;600","leesf commented on PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#issuecomment-1306375058

   @jeff-xu-z pls check the travis CI failure.


;07/Nov/22 23:35;githubbot;600","jeff-xu-z commented on PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#issuecomment-1306709656

   @leesf Pushed an updated commit. The build failure looks to be a scala style check failure.
   
   ```
   [INFO] -;08/Nov/22 06:41;githubbot;600","codecov-commenter commented on PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#issuecomment-1306728064

   # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/358?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report
   > Merging [#358](https://codecov.io/gh/apache/incubator-livy/pull/358?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (aeba182) into [master](https://codecov.io/gh/apache/incubator-livy/commit/1954868ccb185cbe9c87ec885edc208658a55869?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (1954868) will **decrease** coverage by `2.54%`.
   > The diff coverage is `64.28%`.
   
   ```diff
   @@             Coverage Diff              @@
   ##             master     #358      +/-   ##
   ============================================
   - Coverage     68.35%   65.80%   -2.55%     
   + Complexity      838      817      -21     
   ============================================
     Files           103      103              
     Lines          5940     5948       +8     
     Branches        898      899       +1     
   ============================================
   - Hits           4060     3914     -146     
   - Misses         1318     1509     +191     
   + Partials        562      525      -37     
   ```
   
   
   | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/358?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage Î” | |
   |---|---|---|
   | [...ain/scala/org/apache/livy/utils/SparkYarnApp.scala](https://codecov.io/gh/apache/incubator-livy/pull/358/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya1lhcm5BcHAuc2NhbGE=) | `71.42% <64.28%> (-2.33%)` | :arrow_down: |
   | [...a/org/apache/livy/server/ThriftServerFactory.scala](https://codecov.io/gh/apache/incubator-livy/pull/358/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvVGhyaWZ0U2VydmVyRmFjdG9yeS5zY2FsYQ==) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |
   | [...main/scala/org/apache/livy/server/LivyServer.scala](https://codecov.io/gh/apache/incubator-livy/pull/358/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvTGl2eVNlcnZlci5zY2FsYQ==) | `2.23% <0.00%> (-28.58%)` | :arrow_down: |
   | [...rc/main/scala/org/apache/livy/utils/SparkApp.scala](https://codecov.io/gh/apache/incubator-livy/pull/358/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya0FwcC5zY2FsYQ==) | `56.00% <0.00%> (-20.00%)` | :arrow_down: |
   | [core/src/main/scala/org/apache/livy/Logging.scala](https://codecov.io/gh/apache/incubator-livy/pull/358/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvTG9nZ2luZy5zY2FsYQ==) | `66.66% <0.00%> (-16.67%)` | :arrow_down: |
   | [...e/livy/server/interactive/InteractiveSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/358/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `56.97% <0.00%> (-12.80%)` | :arrow_down: |
   | [...src/main/scala/org/apache/livy/sessions/Kind.scala](https://codecov.io/gh/apache/incubator-livy/pull/358/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvc2Vzc2lvbnMvS2luZC5zY2FsYQ==) | `66.66% <0.00%> (-9.53%)` | :arrow_down: |
   | [...la/org/apache/livy/utils/LineBufferedProcess.scala](https://codecov.io/gh/apache/incubator-livy/pull/358/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9MaW5lQnVmZmVyZWRQcm9jZXNzLnNjYWxh) | `78.57% <0.00%> (-7.15%)` | :arrow_down: |
   | [...server/interactive/InteractiveSessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/358/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `61.41% <0.00%> (-6.30%)` | :arrow_down: |
   | [.../scala/org/apache/livy/server/SessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/358/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvU2Vzc2lvblNlcnZsZXQuc2NhbGE=) | `66.31% <0.00%> (-5.27%)` | :arrow_down: |
   | ... and [14 more](https://codecov.io/gh/apache/incubator-livy/pull/358/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | |
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   


;08/Nov/22 07:06;githubbot;600","leesf merged PR #358:
URL: https://github.com/apache/incubator-livy/pull/358


;08/Nov/22 09:39;githubbot;600","jeff-xu-z commented on PR #358:
URL: https://github.com/apache/incubator-livy/pull/358#issuecomment-1308089810

   @leesf Thank you for accepted & merged the PR! My 1st open source contribution. ðŸ‘¯ 


;09/Nov/22 01:55;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,9600,,,0,9600,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/22 02:31;jeff.xu.z@gmail.com;Screen Shot 2022-11-25 at 6.31.30 PM.png;https://issues.apache.org/jira/secure/attachment/13053158/Screen+Shot+2022-11-25+at+6.31.30+PM.png","14/Oct/22 17:22;jeff.xu.z@gmail.com;livy_batch.py;https://issues.apache.org/jira/secure/attachment/13050961/livy_batch.py","14/Oct/22 17:26;jeff.xu.z@gmail.com;livy_client.py;https://issues.apache.org/jira/secure/attachment/13050960/livy_client.py",,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Nov 26 04:54:03 UTC 2022,,,,,,,,,,"0|z19duw:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/Oct/22 18:19;jeff.xu.z@gmail.com;https://github.com/apache/incubator-livy/pull/358;;;","12/Nov/22 22:10;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;","15/Nov/22 04:33;jeff.xu.z@gmail.com;[~lmccay] This fix should be in release as early as possible because when it's hit, the result is really bad. On some AWS EMRs, our production code hit this issue very frequently (~50% chance in one particular EMR).;;;","15/Nov/22 04:33;jeff.xu.z@gmail.com;[~lmccay] I modified the release to 0.8.0. Is there anything else I need to do?;;;","20/Nov/22 19:05;lmccay;[~jeff.xu.z@gmail.com] - thanks for bringing this back in for consideration. Are you committing to rebasing this fix and providing a PR for it to get it in within the next few weeks?

If so, we can definitely pull it in. If not, I would say that it isn't a regression and that it can wait until the next release.

Again, happy to pull it in if you can take this up and do the appropriate testing and with a proper PR.;;;","26/Nov/22 00:51;jeff.xu.z@gmail.com;Hi [~lmccay]: Yes, I would like to create PR for 0.8.0. To which branch should I create the new PR? I didn't see 0.8.0 branch.;;;","26/Nov/22 01:58;lmccay;Hi [~jeff.xu.z@gmail.com] - that's great! I added you as a contributor to the projects and assigned the Jira to you.

I believe we will branch for 0.8.0 as we get closer to release, so we will just target master branch for now.

Thank you!;;;","26/Nov/22 02:32;jeff.xu.z@gmail.com;[~lmccay] : The PR was already merged into master. Please see the commit history.

!Screen Shot 2022-11-25 at 6.31.30 PM.png!;;;","26/Nov/22 04:54;lmccay;Oh, awesome, [~jeff.xu.z@gmail.com] - we just need to resolve it as fixed then!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy pyspark sessions dies after 1 hour,LIVY-893,13469630,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,,dishka_krauch,dishka_krauch,01/Jul/22 10:15,01/Jul/22 13:30,19/Dec/25 04:15,01/Jul/22 13:30,0.8.0,,,0.8.0,,API,,,,,,,,,,0,performance,,,,,"Hello there.

My livy pyspark sessions alwas die after 1 hour no matter following edited configuraton at livy.conf. Can anybody help me to understad why?

Â 
{code:java}
livy.spark.master = yarn
livy.spark.deploy-mode = client
livy.spark.session.timeout = 24h
livy.server.session.state-retain.sec = 86400s
livy.server.session.timeout-check = false {code}","livy 0.8.0
spark 3.1.3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 01 13:30:13 UTC 2022,,,,,,,,,,"0|z16j54:",9223372036854775807,,,,,,,,,,,,,,,,,,,"01/Jul/22 13:30;dishka_krauch;Fixed by editing livy.server.session.timeout;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding jars to Spark Interpreter classpath on an already running Livy Session,LIVY-889,13443165,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,praneetsharma,praneetsharma,praneetsharma,04/May/22 17:07,13/Mar/23 18:07,19/Dec/25 04:15,26/Dec/22 20:41,0.7.0,,,0.8.0,,API,Core,REPL,RSC,,,,,,,0,,,,,,"Currently, once the Livy session is started, addition of new jar resources does not result in them being added to Spark interpreter classpath. Whereas during spark interpreter initialization, additional jar resources are added to spark interpreter classpath. This Jira is requesting that the jars uploaded using LivyClient#addJar after session start should also be added to spark interpreter classpath.",,"praneetsharma opened a new pull request, #344:
URL: https://github.com/apache/incubator-livy/pull/344

   ## What changes were proposed in this pull request?
   
   * addJar invoked for a running Livy session will add that jar resource to SparkInterpreter classpath in addition to Spark driver classpath.
   * addJarOrPyFile signature changed to return the local path to which it was copied. This local path is required to add the same jar resource to SparkInterpreter classpath.
   * addFile now goes through appropriate Interpreter based on the interpreter instead of default PythonInterpreter.
   
   https://issues.apache.org/jira/browse/LIVY-889
   
   ## How was this patch tested?
   
   This patch was tested by adding a Jar on a running Livy session and then trying to load a class from that jar using spark interpreter.
   


;30/Jun/22 00:51;githubbot;600","jianzhenwu commented on PR #344:
URL: https://github.com/apache/incubator-livy/pull/344#issuecomment-1312486254

   Hi @praneetsharma , can you provide the facts on which this PR is based? If it's a problem, can you provide steps to reproduce it?


;12/Nov/22 14:03;githubbot;600","lmccay commented on PR #344:
URL: https://github.com/apache/incubator-livy/pull/344#issuecomment-1364563322

   @jianzhenwu - I believe the description in LIVY-889 is a decent summary of the facts, no?


;24/Dec/22 17:49;githubbot;600","codecov-commenter commented on PR #344:
URL: https://github.com/apache/incubator-livy/pull/344#issuecomment-1364566593

   # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/344?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report
   > Merging [#344](https://codecov.io/gh/apache/incubator-livy/pull/344?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (d23744c) into [master](https://codecov.io/gh/apache/incubator-livy/commit/4d8a912699683b973eee76d4e91447d769a0cb0d?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (4d8a912) will **decrease** coverage by `2.89%`.
   > The diff coverage is `9.09%`.
   
   ```diff
   @@             Coverage Diff              @@
   ##             master     #344      +/-   ##
   ============================================
   - Coverage     68.50%   65.61%   -2.90%     
   + Complexity      841      816      -25     
   ============================================
     Files           103      103              
     Lines          5940     5964      +24     
     Branches        898      907       +9     
   ============================================
   - Hits           4069     3913     -156     
   - Misses         1312     1525     +213     
   + Partials        559      526      -33     
   ```
   
   
   | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/344?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage Î” | |
   |---|---|---|
   | [...rg/apache/livy/repl/AbstractSparkInterpreter.scala](https://codecov.io/gh/apache/incubator-livy/pull/344/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9BYnN0cmFjdFNwYXJrSW50ZXJwcmV0ZXIuc2NhbGE=) | `57.14% <Ã¸> (Ã¸)` | |
   | [...scala/org/apache/livy/repl/PythonInterpreter.scala](https://codecov.io/gh/apache/incubator-livy/pull/344/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9QeXRob25JbnRlcnByZXRlci5zY2FsYQ==) | `44.17% <0.00%> (Ã¸)` | |
   | [...c/main/scala/org/apache/livy/repl/ReplDriver.scala](https://codecov.io/gh/apache/incubator-livy/pull/344/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9SZXBsRHJpdmVyLnNjYWxh) | `27.27% <5.00%> (-8.63%)` | :arrow_down: |
   | [...ain/java/org/apache/livy/rsc/driver/RSCDriver.java](https://codecov.io/gh/apache/incubator-livy/pull/344/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9kcml2ZXIvUlNDRHJpdmVyLmphdmE=) | `78.75% <100.00%> (-2.09%)` | :arrow_down: |
   | [...a/org/apache/livy/server/ThriftServerFactory.scala](https://codecov.io/gh/apache/incubator-livy/pull/344/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvVGhyaWZ0U2VydmVyRmFjdG9yeS5zY2FsYQ==) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |
   | [...main/scala/org/apache/livy/server/LivyServer.scala](https://codecov.io/gh/apache/incubator-livy/pull/344/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvTGl2eVNlcnZlci5zY2FsYQ==) | `2.23% <0.00%> (-29.47%)` | :arrow_down: |
   | [...rc/main/scala/org/apache/livy/utils/SparkApp.scala](https://codecov.io/gh/apache/incubator-livy/pull/344/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya0FwcC5zY2FsYQ==) | `56.00% <0.00%> (-20.00%)` | :arrow_down: |
   | [core/src/main/scala/org/apache/livy/Logging.scala](https://codecov.io/gh/apache/incubator-livy/pull/344/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvTG9nZ2luZy5zY2FsYQ==) | `66.66% <0.00%> (-16.67%)` | :arrow_down: |
   | [...e/livy/server/interactive/InteractiveSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/344/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `56.97% <0.00%> (-12.80%)` | :arrow_down: |
   | [...src/main/scala/org/apache/livy/sessions/Kind.scala](https://codecov.io/gh/apache/incubator-livy/pull/344/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvc2Vzc2lvbnMvS2luZC5zY2FsYQ==) | `66.66% <0.00%> (-9.53%)` | :arrow_down: |
   | ... and [17 more](https://codecov.io/gh/apache/incubator-livy/pull/344/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | |
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   


;24/Dec/22 18:12;githubbot;600","lmccay merged PR #344:
URL: https://github.com/apache/incubator-livy/pull/344


;26/Dec/22 20:38;githubbot;600","wjxiz1992 commented on PR #344:
URL: https://github.com/apache/incubator-livy/pull/344#issuecomment-1462007667

    Hi @praneetsharma very grateful that you contributed this feature! I met a problem which may be related to your contribution so I'd like to consult you here, hope you don't mind:
   
   Livy's REST API has a filed `jars` for both `session` and `batch` creation. but I found this `jars` doesn't work for `session` type, but only works for  `batch` type. this is an identical issue reported here: https://community.cloudera.com/t5/Support-Questions/How-to-post-a-Spark-Job-as-JAR-via-Livy-interactive-REST/m-p/299806/highlight/true#M219859 . Do you know what's the cause? 
   thanks in advance!


;09/Mar/23 12:50;githubbot;600","praneetsharma commented on PR #344:
URL: https://github.com/apache/incubator-livy/pull/344#issuecomment-1466654651

   Hi @wjxiz1992, could you please explain what's happening when you are attempting to use ""jars"" option with ""session"" type? Are you facing some sort of classloading issue? Also, do you know if this is a regression and was working in earlier versions?
   
   Regards


;13/Mar/23 18:07;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4200,,,0,4200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 30 00:59:14 UTC 2022,,,,,,,,,,"0|z12248:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/Jun/22 00:59;praneetsharma;PR Submitted - https://github.com/apache/incubator-livy/pull/344;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AddJar or AddFile call on a duplicate file should not result in failure for Livy session,LIVY-888,13442880,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,praneetsharma,praneetsharma,praneetsharma,02/May/22 22:14,27/Dec/22 05:35,19/Dec/25 04:15,27/Dec/22 05:35,0.7.0,,,0.8.0,,API,,,,,,,,,,0,,,,,,"Currently, when addFile or addJar is used to upload a file or a jar which was already uploaded, it results in IOException. On the other hand, the SparkContext's behavior is to ignore adding of duplicate jars/files and logging appropriate warning message. The same should be done for addFile and addJar APIs in Livy.",,"praneetsharma opened a new pull request, #341:
URL: https://github.com/apache/incubator-livy/pull/341

   ## What changes were proposed in this pull request?
   
   When a duplicate file is attempted to be added using LivyClient, duplicate file's addition is skipped and an appropriate warning is logged instead of the old behavior of throwing fatal IOException. This is now consistent with SparkContext's addJar and addFile APIs behavior. 
   
   https://issues.apache.org/jira/browse/LIVY-888
   
   ## How was this patch tested?
   
   Modified the unit test TestSparkClient#testAddJarsAndFiles to attempt adding a duplicate file, and the test did not fail.
   


;03/May/22 18:21;githubbot;600","jianzhenwu commented on PR #341:
URL: https://github.com/apache/incubator-livy/pull/341#issuecomment-1312489534

   Hi @praneetsharma , I think statement failure is acceptable and easy to be perceived by the user, not just a warning.


;12/Nov/22 14:13;githubbot;600","lmccay closed pull request #341: [LIVY-888] AddJar or AddFile call on a duplicate file should not result in failure for Livy session
URL: https://github.com/apache/incubator-livy/pull/341


;24/Dec/22 17:38;githubbot;600","lmccay commented on PR #341:
URL: https://github.com/apache/incubator-livy/pull/341#issuecomment-1364561643

   > 
   
   


;24/Dec/22 17:38;githubbot;600","praneetsharma opened a new pull request, #341:
URL: https://github.com/apache/incubator-livy/pull/341

   ## What changes were proposed in this pull request?
   
   When a duplicate file is attempted to be added using LivyClient, duplicate file's addition is skipped and an appropriate warning is logged instead of the old behavior of throwing fatal IOException. This is now consistent with SparkContext's addJar and addFile APIs behavior. 
   
   https://issues.apache.org/jira/browse/LIVY-888
   
   ## How was this patch tested?
   
   Modified the unit test TestSparkClient#testAddJarsAndFiles to attempt adding a duplicate file, and the test did not fail.
   


;24/Dec/22 17:38;githubbot;600","codecov-commenter commented on PR #341:
URL: https://github.com/apache/incubator-livy/pull/341#issuecomment-1364565640

   # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/341?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report
   > Merging [#341](https://codecov.io/gh/apache/incubator-livy/pull/341?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (ebb6d63) into [master](https://codecov.io/gh/apache/incubator-livy/commit/4d8a912699683b973eee76d4e91447d769a0cb0d?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (4d8a912) will **decrease** coverage by `2.77%`.
   > The diff coverage is `100.00%`.
   
   ```diff
   @@             Coverage Diff              @@
   ##             master     #341      +/-   ##
   ============================================
   - Coverage     68.50%   65.72%   -2.78%     
   + Complexity      841      816      -25     
   ============================================
     Files           103      103              
     Lines          5940     5949       +9     
     Branches        898      899       +1     
   ============================================
   - Hits           4069     3910     -159     
   - Misses         1312     1513     +201     
   + Partials        559      526      -33     
   ```
   
   
   | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/341?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage Î” | |
   |---|---|---|
   | [...ain/java/org/apache/livy/rsc/driver/RSCDriver.java](https://codecov.io/gh/apache/incubator-livy/pull/341/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9kcml2ZXIvUlNDRHJpdmVyLmphdmE=) | `79.66% <100.00%> (-1.17%)` | :arrow_down: |
   | [...a/org/apache/livy/server/ThriftServerFactory.scala](https://codecov.io/gh/apache/incubator-livy/pull/341/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvVGhyaWZ0U2VydmVyRmFjdG9yeS5zY2FsYQ==) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |
   | [...main/scala/org/apache/livy/server/LivyServer.scala](https://codecov.io/gh/apache/incubator-livy/pull/341/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvTGl2eVNlcnZlci5zY2FsYQ==) | `2.23% <0.00%> (-29.47%)` | :arrow_down: |
   | [...rc/main/scala/org/apache/livy/utils/SparkApp.scala](https://codecov.io/gh/apache/incubator-livy/pull/341/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya0FwcC5zY2FsYQ==) | `56.00% <0.00%> (-20.00%)` | :arrow_down: |
   | [core/src/main/scala/org/apache/livy/Logging.scala](https://codecov.io/gh/apache/incubator-livy/pull/341/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvTG9nZ2luZy5zY2FsYQ==) | `66.66% <0.00%> (-16.67%)` | :arrow_down: |
   | [...e/livy/server/interactive/InteractiveSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/341/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `56.97% <0.00%> (-12.80%)` | :arrow_down: |
   | [...src/main/scala/org/apache/livy/sessions/Kind.scala](https://codecov.io/gh/apache/incubator-livy/pull/341/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvc2Vzc2lvbnMvS2luZC5zY2FsYQ==) | `66.66% <0.00%> (-9.53%)` | :arrow_down: |
   | [...la/org/apache/livy/utils/LineBufferedProcess.scala](https://codecov.io/gh/apache/incubator-livy/pull/341/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9MaW5lQnVmZmVyZWRQcm9jZXNzLnNjYWxh) | `78.57% <0.00%> (-7.15%)` | :arrow_down: |
   | [...server/interactive/InteractiveSessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/341/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `61.41% <0.00%> (-6.30%)` | :arrow_down: |
   | [.../scala/org/apache/livy/server/SessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/341/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvU2Vzc2lvblNlcnZsZXQuc2NhbGE=) | `66.31% <0.00%> (-5.27%)` | :arrow_down: |
   | ... and [15 more](https://codecov.io/gh/apache/incubator-livy/pull/341/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | |
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   


;24/Dec/22 18:05;githubbot;600","lmccay merged PR #341:
URL: https://github.com/apache/incubator-livy/pull/341


;27/Dec/22 05:33;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4200,,,0,4200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,scala,Tue May 03 18:23:41 UTC 2022,,,,,,,,,,"0|z120cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/May/22 18:23;praneetsharma;PR submitted for this Jira: https://github.com/apache/incubator-livy/pull/341;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy is incompatible with python 3.8+,LIVY-886,13440301,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,prongs,prongs,19/Apr/22 06:28,04/Nov/25 20:58,19/Dec/25 04:15,04/Nov/25 20:58,0.5.0,0.6.0,0.7.0,,,Interpreter,REPL,,,,,,,,,0,,,,,,"Due to python behaviour change after 3.8+, livy has become incompatible with python versions 3.8+. As of now, python 3.6 is out of support, so the only version livy is compatible with is 3.7. 

This is going to make the project obsolete very fast. ",,"prongs opened a new pull request, #340:
URL: https://github.com/apache/incubator-livy/pull/340

   ## What changes were proposed in this pull request?
   
   (Please fill in changes proposed in this fix)
   (Include a link to the associated JIRA and make sure to add a link to this pr on the JIRA as well)
   
   ## How was this patch tested?
   
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   


;19/Apr/22 06:40;githubbot;600","gabrielmagno commented on PR #340:
URL: https://github.com/apache/incubator-livy/pull/340#issuecomment-1122420471

   This PR is duplicated with #314


;10/May/22 13:46;githubbot;600","prongs commented on PR #340:
URL: https://github.com/apache/incubator-livy/pull/340#issuecomment-1123239313

   Yes @gabrielmagno both solutions should work. Problem is, livy is an un-maintained project by now. 
   
   * Last commit in maste was more than a year ago. 
   * Livy hasn't been released after adding support for spark-3/scala-2.12
   
   


;11/May/22 06:31;githubbot;600","uberadam2392 commented on PR #340:
URL: https://github.com/apache/incubator-livy/pull/340#issuecomment-1263662348

   Hi @prongs and @gabrielmagno is there any solution to this that can be patched on the user end?


;30/Sep/22 14:39;githubbot;600","leesf commented on PR #340:
URL: https://github.com/apache/incubator-livy/pull/340#issuecomment-1308052657

   just merge another one. https://github.com/apache/incubator-livy/pull/314. Closing this one.


;09/Nov/22 01:06;githubbot;600","leesf closed pull request #340: LIVY-886: Make livy compatible with python 3.8+
URL: https://github.com/apache/incubator-livy/pull/340


;09/Nov/22 01:06;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,LIVY-795,,,,,,,,"19/Apr/22 06:34;prongs;livy-886.mov;https://issues.apache.org/jira/secure/attachment/13042607/livy-886.mov","19/Apr/22 06:31;prongs;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13042603/screenshot-1.png","19/Apr/22 06:32;prongs;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13042604/screenshot-2.png","19/Apr/22 06:32;prongs;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/13042605/screenshot-3.png",,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Nov 12 22:10:11 UTC 2022,,,,,,,,,,"0|z11kvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,"19/Apr/22 06:33;prongs;We see this error when trying to run any python code

 !screenshot-1.png! 

Looking at line 223: https://github.com/apache/incubator-livy/blob/master/repl/src/main/resources/fake_shell.py#L222-L223, we see this.

 !screenshot-2.png! 

After a bit of googling, we found this

https://bugs.python.org/issue35894#msg334808

 !screenshot-3.png! 



So, python 3.8 has changed the behaviour of `ast` module, which livy was relying upon. The recommendation from Guido is that the code should be passing an empty list as the second argument. 

I tried that out and it just makes it work. 

;;;","19/Apr/22 06:36;prongs;Please look at the [attached video recording |https://issues.apache.org/jira/secure/attachment/13042607/livy-886.mov]. Which contains: 

* python 3.7 running fake_shell.py just fine. 
* python 3.8 bombing on it. 
* python 3.8 is able to run it fine after incorporating Guido's recommendation. 

;;;","12/Nov/22 22:10;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Log4j upgrade for Livy 0.8.0 version,LIVY-878,13418236,13512393,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,dacort,tinujose,tinujose,17/Dec/21 12:59,26/Apr/23 21:46,19/Dec/25 04:15,26/Apr/23 21:46,,,,0.8.0,,,,,,,,,,,,0,,,,,,"We are looking for an advise from you in context of the below mentioned issue:

Â 

*A high severity vulnerability (CVE-2021-44228) impacting multiple versions of the Apache Log4j 2 utility was disclosed publicly via the projectâ€™s GitHub on December 9, 2021.* 

*The vulnerability impacts Apache Log4j 2 versions 2.0 to 2.14.1.*

Â 

Apache Livy version 0.7.0 version is being used by our team for processing the spark jobs . It uses the Log4j 1.x.x. which is not having any continued support.

We would like to upgrade the Log4j versions to the latest stable version Â 2.15 without having any impact on the installations .

Â 

Could you please recommend the possible ways to do the upgrade .Please note , we are not looking to upgrade the Livy version to 0.7.1 to resolve this issue .

Our requirement is to retain the current installed version and configurations with only changes in the Log4j versionsÂ Â ",,"ayushtkn commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1483189148

   If it isn't getting used, its better if we could exclude the log4j dependency completely even from test scope


;24/Mar/23 17:48;githubbot;600","dacort commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1500721635

   AhI thought I addressed the `apacheds-server-integ` - will double-check that.


;07/Apr/23 23:30;githubbot;600","dacort commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1500961788

   Will trigger CI with a push today. 


;08/Apr/23 19:30;githubbot;600","codecov-commenter commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1500991826

   ## [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/392?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report
   > Merging [#392](https://codecov.io/gh/apache/incubator-livy/pull/392?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (7ff3ec1) into [master](https://codecov.io/gh/apache/incubator-livy/commit/8d07eee831221093f55203190d38d793234c5db7?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (8d07eee) will **decrease** coverage by `40.64%`.
   > The diff coverage is `75.60%`.
   
   > :exclamation: Current head 7ff3ec1 differs from pull request most recent head a59259c. Consider uploading reports for the commit a59259c to get more accurate results
   
   ```diff
   @@              Coverage Diff              @@
   ##             master     #392       +/-   ##
   =============================================
   - Coverage     68.26%   27.63%   -40.64%     
   + Complexity      844      362      -482     
   =============================================
     Files           103      103               
     Lines          5965     5989       +24     
     Branches        907      911        +4     
   =============================================
   - Hits           4072     1655     -2417     
   - Misses         1333     3988     +2655     
   + Partials        560      346      -214     
   ```
   
   
   | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/392?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage Î” | |
   |---|---|---|
   | [...cala/org/apache/livy/sessions/SessionManager.scala](https://codecov.io/gh/apache/incubator-livy/pull/392?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXNzaW9ucy9TZXNzaW9uTWFuYWdlci5zY2FsYQ==) | `63.72% <50.00%> (-18.10%)` | :arrow_down: |
   | [...java/org/apache/livy/client/common/ClientConf.java](https://codecov.io/gh/apache/incubator-livy/pull/392?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y2xpZW50LWNvbW1vbi9zcmMvbWFpbi9qYXZhL29yZy9hcGFjaGUvbGl2eS9jbGllbnQvY29tbW9uL0NsaWVudENvbmYuamF2YQ==) | `69.36% <62.50%> (-29.70%)` | :arrow_down: |
   | [.../server/interactive/CreateInteractiveRequest.scala](https://codecov.io/gh/apache/incubator-livy/pull/392?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvQ3JlYXRlSW50ZXJhY3RpdmVSZXF1ZXN0LnNjYWxh) | `62.50% <66.66%> (-17.50%)` | :arrow_down: |
   | [...server/interactive/InteractiveSessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/392?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `58.01% <71.42%> (-9.71%)` | :arrow_down: |
   | [.../scala/org/apache/livy/server/SessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/392?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvU2Vzc2lvblNlcnZsZXQuc2NhbGE=) | `52.57% <85.71%> (-19.01%)` | :arrow_down: |
   | [...va/org/apache/livy/client/common/HttpMessages.java](https://codecov.io/gh/apache/incubator-livy/pull/392?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y2xpZW50LWNvbW1vbi9zcmMvbWFpbi9qYXZhL29yZy9hcGFjaGUvbGl2eS9jbGllbnQvY29tbW9uL0h0dHBNZXNzYWdlcy5qYXZh) | `81.39% <100.00%> (-13.85%)` | :arrow_down: |
   | [...e/livy/server/interactive/InteractiveSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/392?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `66.28% <100.00%> (-3.49%)` | :arrow_down: |
   | [.../main/scala/org/apache/livy/sessions/Session.scala](https://codecov.io/gh/apache/incubator-livy/pull/392?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXNzaW9ucy9TZXNzaW9uLnNjYWxh) | `63.47% <100.00%> (-9.98%)` | :arrow_down: |
   
   ... and [74 files with indirect coverage changes](https://codecov.io/gh/apache/incubator-livy/pull/392/indirect-changes?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   


;08/Apr/23 22:47;githubbot;600","dacort commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1500992072

   Bummer, CI failed. Will take a look either later today or Monday. 


;08/Apr/23 22:49;githubbot;600","dacort commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1501165029

   Integration tests are working, but unit tests still failing with, what I think is an error related to slf4j/log4j loading.
   
   ```
   [INFO] Running org.apache.livy.thriftserver.session.ColumnBufferTest
   [ERROR] Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.572 s <<< FAILURE! - in org.apache.livy.thriftserver.session.ColumnBufferTest
   [ERROR] testColumnBuffer(org.apache.livy.thriftserver.session.ColumnBufferTest)  Time elapsed: 2.147 s  <<< ERROR!
   java.lang.NoClassDefFoundError: org/apache/log4j/Level
           at org.apache.livy.thriftserver.session.ColumnBufferTest.testColumnBuffer(ColumnBufferTest.java:53)
   Caused by: java.lang.ClassNotFoundException: org.apache.log4j.Level
           at org.apache.livy.thriftserver.session.ColumnBufferTest.testColumnBuffer(ColumnBufferTest.java:53)
   
   [INFO] Running org.apache.livy.thriftserver.session.ThriftSessionTest
   [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.024 s <<< FAILURE! - in org.apache.livy.thriftserver.session.ThriftSessionTest
   [ERROR] org.apache.livy.thriftserver.session.ThriftSessionTest  Time elapsed: 0.024 s  <<< ERROR!
   java.lang.ExceptionInInitializerError
           at org.apache.livy.thriftserver.session.ThriftSessionTest.setUp(ThriftSessionTest.java:60)
   Caused by: java.lang.IllegalStateException: org.slf4j.LoggerFactory in failed state. Original exception was thrown EARLIER. See also http://www.slf4j.org/codes.html#unsuccessfulInit
           at org.apache.livy.thriftserver.session.ThriftSessionTest.setUp(ThriftSessionTest.java:60)
   ```


;09/Apr/23 16:27;githubbot;600","dacort commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1503829916

   Fixed the `java.lang.NoClassDefFoundError` error with additional exclusions. 
   
   Unit tests are failing, however I think it's due to a flaky test based on behavior observed in CI where unit tests pass on the PR but fail once merged into master and vice-cersa.
   
   There's an old [mailing list thread](https://www.mail-archive.com/dev@livy.incubator.apache.org/msg00794.html) that also discusses the issue, but with no resolution.
   
   ```
   The second error, TestSparkClient.testJobSubmission, is random. I see the
   failure ~25% of the times. As it turned out, the Echo job was returning,
   sometimes, even before we added the listener on the job handle. So the
   listener.onJobStarted method is not invoked. This can be fixed in multiple
   ways but I updated the Echo job, which used to simply return its attribute
   value, to do what ScalaEcho job is doing. So the Echo job now returns
   ""jc.sc().parallelize(list,
   1).collect().get(0);"". Even this could still be fast enough to cause the
   same error. However I couldn't reproduce it again. Also the updated code is
   taking ~1 sec or more on spark's side. So hopefully this should be ok.
   ```


;11/Apr/23 17:41;githubbot;600","gyogal commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1505025583

   Interesting, this test does seem to be flaky. The recent post-commit tests have succeeded, but that may be just pure luck. Could you please retrigger the tests on this one? If this test proves to be flaky, we should file a separate JIRA for it. Also, thanks for finding that old email thread about this test!


;12/Apr/23 10:17;githubbot;600","dacort commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1505278667

   I've been poking at the failure for this change more and turns out it's something different. When I add exclusions for `slf4j-log4j12` to `spark-core`, Netty fails to find slf4j logger, instead tries to use log4j and, I think, ends up failing. If I don't add that exclusion, the test runs fine. 
   
   I'm still trying to figure out what exactly is going on there but not having much luck. 


;12/Apr/23 13:28;githubbot;600","dacort commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1505601797

   Might have figured this out! Swapping `slf4j-api` with `slf4j-reload4j` seems to get me past `TestSparkClient`. Running the full suite now.


;12/Apr/23 16:48;githubbot;600","lmccay commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1513881909

   @dacort - looks like the same failures?


;18/Apr/23 22:51;githubbot;600","dacort commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1513888628

   @lmccay Yes but I've got another working branch over here https://github.com/apache/incubator-livy/tree/fix/log4j-reload4j-part2 that appears to be working. Just need to verify everything looks good. 


;18/Apr/23 22:59;githubbot;600","lmccay commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1520865529

   @dacort - curious where you are with the separate branch and plan here?


;24/Apr/23 21:40;githubbot;600","dacort commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1522662904

   @lmccay @ksumit OK! I've merged in changes from my working branch. Tests are green! (After some manual retries...)
   
   Could y'all take one more glance at this and the I think it's good to merge in. Apologies this has taken so long, appreciate your patience and follow ups. ðŸ™ 


;26/Apr/23 02:12;githubbot;600","dacort commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1523744674

   > slf4j-reload4j as provided dependency
   
   I'll check, but it is defined as a non-provided dependency in the main `pom.xml`.


;26/Apr/23 16:47;githubbot;600","ksumit commented on PR #392:
URL: https://github.com/apache/incubator-livy/pull/392#issuecomment-1523745343

   Given that this has taken us quiet some time, i do want to checkpoint this work and iterate over if there are issues. Approving for check-in.
   
   +1, LGTM
   
   


;26/Apr/23 16:47;githubbot;600","dacort merged PR #392:
URL: https://github.com/apache/incubator-livy/pull/392


;26/Apr/23 17:14;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,10200,,,0,10200,,,,,,,,,,,,,,,,,,,,,,LIVY-884,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 26 21:46:27 UTC 2023,,,,,,,,,,"0|z0xttc:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Feb/22 15:24;pj.fanning;[~tinujose] you could try replacing the log4j jars in your deploy with [https://reload4j.qos.ch/]

Or you can replace the log4j jars in your deploy with log4j-1.2-api (a bit more complicated to set up - see https://logging.apache.org/log4j/2.x/manual/migration.html);;;","12/Nov/22 22:10;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;","15/Dec/22 05:41;dacort;Pulling back in to 0.8.0 per CVE checks. See LIVY-900;;;","27/Dec/22 05:54;lmccay;[~dacort] - do you have a sense of level of effort for this?

Is it something that we should consider breaking into separate tasks that others can help with, by any chance?;;;","28/Dec/22 22:36;dacort;[~lmccay] Probably moderate level of effort. It's mostly just making the fixes, re-running dependency check, seeing if there are other compile-time inclusions of log4j. The time consuming part is figuring out how log4j is getting pulled in for some of the modules and if it's actually necessary or the can be excluded. I'm not familiar enough with the the project yet to make those calls quickly. :)

I still have the following modules to figure out based on a dependency check run on [this pr|https://github.com/apache/incubator-livy/pull/381]. 

*  livy-integration-test:compile
*  livy-coverage-report:compile
*  livy-assembly:compile
*  livy-server:compile;;;","28/Dec/22 23:29;dacort;Additional thoughts on the approach for the current PR. I'd prefer not to do a wholesale upgrade to log4jv2 for the next release, so that leaves a couple options that I've been able to find. 

* [log4j 1.2 bridge|https://logging.apache.org/log4j/2.x/log4j-1.2-api/index.html] - A temporary solution to use Log4j 2 with minor API differences
* [reload4j|https://reload4j.qos.ch/] - A third-party (but written by the author of log4j 1) dropin replacement for log4j version 1.2.17.

It appears reload4j was built for folks to quickly and easily replace log4j in their applications. While still maintained, I chose the bridge route for the PR as we have the time to implement it and can then do a larger v2 upgrade later.;;;","28/Dec/22 23:53;lmccay;[~dacort] - this makes sense to me as well. Thank you for the insights!

I'd like to make sure others in the community (especially those that have had to address this in internal forks) have a chance to consider it and weigh in on the approach. Keeping in mind that this is an 0.8.0 release specific decision and as you said a later upgrade to v2 should still be considered. I believe there are some compatibility issues with trying to do this in a minor release.

[~irashid] , Â [~ggal] - FYI ^^^;;;","29/Dec/22 01:00;ksumit;In internal projects, we have replaced log4j1.x references with reload4j in pom files however we ended up doing a lot of exclusions because log4j1.x was still coming as a transient dependency (it wasn't pretty to say the least). Most of the Apache projects had not moved to log4j2 (ex: hadoop, zookeeper, kafka etc) last i checked so this is going to be a pain to manage for some time to come. Few additional datapoints to consider from other projects:
 # zookeeper community decided to suppress the owasp checker because SocketServer is not used. 
 # There were efforts on moving multiple open source projects to log4j2.x but they have been going on for several months and in worst case multiple years. Few listed below:

* [ZOOKEEPER-3677] owasp checker failing for - CVE-2019-17571 Apache Log4j 1.2 deserialization of untrusted data in SocketServer
* [ZOOKEEPER-2342] Migrate to Log4J 2. - ASF JIRA (apache.org) â€“ open since Jan 2016, closed recently without resolution.
* [KAFKA-9366] Upgrade log4j to log4j2 - ASF JIRA (apache.org) â€“ open since Jan 2020
* [HADOOP-16206] Migrate from Log4j1 to Log4j2 - ASF JIRA (apache.org) â€“ open since Mar 2019
* [ZEPPELIN-3527] Upgrade log4j to log4j2 - ASF JIRA (apache.org) â€“ open since Jun 2018

One hacky approach could be to replace log4j1.x jars with reload4j jars in the final generated build artifact and wait until dependencies move to log4j2 as well, thoughts?;;;","29/Dec/22 15:18;lmccay;[~ksumit] - thanks for the details. The way that I read this, it seems that we have a couple possible directions:
 # [~dacort]'s current direction which would help move Livy along upstream from internal versions and be on an official API bridge for log4j 2. This will necessarily tease out all of the transient dependencies where they will need to be excluded.
 # Another alternative would be to take the short term approach that others have done internally for the 0.8.0 release as a stop gap for this initial release with the rebooted community. It would require teasing out all of the same transient dependencies of log4j to be excluded.

[~dacort] - would the API bridge require any actual code changes? If so, simply excluding transient versions that are pulled in by other dependencies may not be sufficient?

Personally, I would like to see an approach that moves the upstream project farther along rather than just catch up with forks. This would however require little to no heavy lifting in order for this to be picked up by downstream consumers. Otherwise, it would be introducing risk.

It seems like we can probably continue down the API bridge path since it will require the same exclusions. If it turns out that we need to change it to reload4j then it should only require changing the poms to a different package and the exclusions should remain the same. Right?;;;","29/Dec/22 16:03;ksumit;I was also hinting that we could ignore this work until dependencies like zookeeper and hadoop move to log4j2 but just noticed that Spark has moved on in their recent 3.3.0 release (please see SPARK-37814). With this new datapoint, I am now thinking that eliminating log4j1.x dependency is more appropriate and will be helpful for production environments that use Spark on kubernetes or mesos. For those depending on hadoop and zookeeper, they will have to do additional work that they are already doing for those components.

Please see similar discussions and challenges the community faced for Spark in [https://github.com/apache/spark/pull/34877#issuecomment-994155205] and [https://github.com/apache/spark/pull/34895];;;","29/Dec/22 18:43;lmccay;[~ksumit]Â  - okay, I see now. You were describing us ignoring it and then downstream consumers swapping out the log4j for reload4j jars?

I'd personally think that we shouldn't put out a new release that contains these vulnerabilities as the first one of the revived community. The most conservative route, IMO, would be to be on par with downstream projects now which seem to be on reload4j. Deployments with these changes are presumably in use and being tested already.

That said, if the difference between moving to reload4j and the official API bridge is trivial perhaps we go that far.

We could also target reload4j as the most conservative for 0.8.0 and follow up with an 0.8.1 that moves to the bridge.
{quote}I am now thinking that eliminating log4j1.x dependency is more appropriate and will be helpful for production environments that use Spark on kubernetes or mesos.
{quote}
Does this refer to moving to the API bridge or all the way to log4j v2?;;;","31/Dec/22 22:19;ksumit;Regarding downstreams consuming livy, are you aware of projects consuming livy as library dependency? That would be harder. I was suspecting most of the consumers would be on-prem or cloud deployments and in those cases, they would have their own custom deployment logic and might already be handling the vulnerability in some way.

That said I agree with your sentiments about not shipping with these vulnerabilities. May be we can do this in 2 phases:
1. With reload4j as drop in dependency replacement for log4j1.x
2. With log4j2 replacing reload4j.

We can avoid effort for api bridge in that case because there are few compatibility issues and it's anyways going to be temporary. Some of the work (like exclusions) will be same for the bridge as well as reload4j and will also be helpful when moving to log4j2. What do you think?

Yes for k8s and mesos deployments, I was suggesting that this vulnerability mitigation effort will be useful.;;;","31/Dec/22 23:35;lmccay;[~ksumit] - I can buy that.

[~dacort]Â  - what do you think about backing up to reload4j for this release then following up directly to log4j2 in a future release?;;;","16/Jan/23 18:31;lmccay;[~dacort]Â  - curious what your thoughts are on the above discussion.

Have you made any additional progress on PR [https://github.com/apache/incubator-livy/pull/381/files] ?

What do you think are the next steps here?;;;","08/Mar/23 20:51;lmccay;[~dacort] - have you had a chance to review the above discussion?

I'd like to nail down the approach for this release and unblock it by getting this issue resolved.

If we know which direction then we may have others that can help out.

cc. [~imranr] , [~ggal]Â ;;;","10/Mar/23 17:15;gyogal;I agree with [~ksumit] that in order to unblock the 0.8 release it may be better to go with reload4j for now and tackle the transition to Log4J2 in an upcoming release (when support for Spark 3.3 and above is added). My biggest concern would be compatibility with existing log4j properties files.

In our downstream Livy fork we use Spark 2 with reload4j and Spark 3.3 with log4j-1.2-api. Both are workable, but we had to replace the configuration files and fix some of the unit tests in the latter case.

The current upstream version of Livy only supports Spark 3.1 and below ([LivySparkUtils.scala|https://github.com/apache/incubator-livy/blob/45e07fec68f2b9ad1dc7ebce8db08ad8a778dddc/server/src/main/scala/org/apache/livy/utils/LivySparkUtils.scala#L44-L45]), so it seems like a safer option to stick with reload4j and the old log4j config file format.

Adding Spark 3.3+ support seems like a larger task and it may require us to move to Log4J2 anyway to prevent classpath and configuration conflicts. Please let me know your thoughts on this.;;;","10/Mar/23 18:26;dacort;Yea, I think reload4j is a good approach to go with as well. There's more prior art there, especially as it relates to the other Apache projects. I'm going to try to tackle this today.;;;","10/Mar/23 19:25;lmccay;Awesome - thanks, [~ksumit] , [~gyogal] and [~dacort] - sounds like we have the safest path forward!

Let me know if I can help in any way.

Â ;;;","13/Mar/23 06:08;dacort;Think I'm making some good progress on this. The biggest blocker for me has just been all the transitive dependencies. The versions spark-core, hadoop-common, and zookeper are all extremely out-of-date, but through an iterative process of {{mvn org.owasp:dependency-check-maven:aggregate -DskipSystemScope=true}} and {{mvn dependency:tree -Dincludes=log4j:log4j}} I think I'll have a PR open tomorrow.;;;","13/Mar/23 15:12;lmccay;Good to hear, [~dacort] !

Â ;;;","20/Mar/23 21:51;dacort;New pull request open here: https://github.com/apache/incubator-livy/pull/392

I think we should merge LIVY-972 first as that has automated unit/integration test updates. https://github.com/apache/incubator-livy/pull/393;;;","07/Apr/23 23:06;lmccay;Sorry for the delay in responding here. [~dacort] - I think that if these PRs are ready to go as is that we should get them in.

There seems to be an outstanding comment on 392 about test dependencies - we can follow up for those separately.

Thoughts?

Â ;;;","26/Apr/23 21:46;dacort;[https://github.com/apache/incubator-livy/pull/392] merged in! ðŸš€Â ;;;",,,,,,,,,,,,,,,,,,,,,
Support spark 3.2,LIVY-877,13414051,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,tprelle-ubi,tprelle-ubi,26/Nov/21 20:35,04/Nov/25 20:46,19/Dec/25 04:15,04/Nov/25 20:46,,,,,,,,,,,,,,,,0,,,,,,With the upgrade of spark 3.2 to scala 2.12.15 we need to upgradeÂ scala-maven-plugin to the latest version because the plugin it's not compatible,,"tprelle opened a new pull request #334:
URL: https://github.com/apache/incubator-livy/pull/334


   ## What changes were proposed in this pull request?
   
   Upgrade version on scala-maven-plugin in order to support spark 3.2
   
   ## How was this patch tested?
   
   Current tests and manuels test 
   
   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Nov/21 20:38;githubbot;600","ubisoft closed pull request #334: [LIVY-877] Fix dependency and add test for 3.2 support
URL: https://github.com/apache/incubator-livy/pull/334


;24/Mar/25 13:01;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,LIVY-1010,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Nov 12 22:09:52 UTC 2022,,,,,,,,,,"0|z0x4i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Nov/22 22:09;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing JVM class imports for Spark3,LIVY-863,13384813,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,nileshrathi345,Meister,Meister,21/Jun/21 08:50,12/Nov/24 14:51,19/Dec/25 04:15,12/Nov/24 13:34,,,,0.9.0,,REPL,,,,,,,,,,1,,,,,,"Livy creates a fake spark shell initiating its own java gateway. This gateway imports some java classes for the user which have diverged from the ones in Spark 3. Some imports are missing which lead to methods like `dataframe.explain()` to be broken:
{code:java}
result.explain(True)

An error was encountered:
'JavaPackage' object is not callable
Traceback (most recent call last):
  File ""/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py"", line 386, in explain
    print(self._sc._jvm.PythonSQLUtils.explainString(self._jdf.queryExecution(), explain_mode))
TypeError: 'JavaPackage' object is not callable
{code}
Spark is importing:
[https://github.com/apache/spark/blob/87bf6b0ea4ca0618c8604895d05037edce8b7cb0/python/pyspark/java_gateway.py#L153]

Livy is currently importing:
[https://github.com/apache/incubator-livy/blob/4d8a912699683b973eee76d4e91447d769a0cb0d/repl/src/main/resources/fake_shell.py#L581]",,"moritzmeister opened a new pull request #322:
URL: https://github.com/apache/incubator-livy/pull/322


   ## What changes were proposed in this pull request?
   
   Description of the problem: https://issues.apache.org/jira/browse/LIVY-863
   
   The proposed fix consists of adding the missing imports that upstream Spark has when it's initiating the Java Gateway.
   See the imports of Spark here: https://github.com/apache/spark/blob/87bf6b0ea4ca0618c8604895d05037edce8b7cb0/python/pyspark/java_gateway.py#L153
   
   ## How was this patch tested?
   
   If you could please provide some guidance where to add tests for this, I am happy to add them.
   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jun/21 08:56;githubbot;600","moritzmeister opened a new pull request #322:
URL: https://github.com/apache/incubator-livy/pull/322


   ## What changes were proposed in this pull request?
   
   Description of the problem: https://issues.apache.org/jira/browse/LIVY-863
   
   The proposed fix consists of adding the missing imports that upstream Spark has when it's initiating the Java Gateway.
   See the imports of Spark here: https://github.com/apache/spark/blob/87bf6b0ea4ca0618c8604895d05037edce8b7cb0/python/pyspark/java_gateway.py#L153
   
   ## How was this patch tested?
   
   If you could please provide some guidance where to add tests for this, I am happy to add them.
   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jun/21 07:41;githubbot;600","yantzu commented on PR #322:
URL: https://github.com/apache/incubator-livy/pull/322#issuecomment-1308449357

   Retrigger the test.


;09/Nov/22 09:16;githubbot;600","yantzu closed pull request #322: [LIVY-863] Missing JVM class imports for Spark3
URL: https://github.com/apache/incubator-livy/pull/322


;09/Nov/22 09:16;githubbot;600","moritzmeister opened a new pull request, #322:
URL: https://github.com/apache/incubator-livy/pull/322

   ## What changes were proposed in this pull request?
   
   Description of the problem: https://issues.apache.org/jira/browse/LIVY-863
   
   The proposed fix consists of adding the missing imports that upstream Spark has when it's initiating the Java Gateway.
   See the imports of Spark here: https://github.com/apache/spark/blob/87bf6b0ea4ca0618c8604895d05037edce8b7cb0/python/pyspark/java_gateway.py#L153
   
   As far as I am aware, the `java_import()` does not fail or fails silently if the imported class does not exist.
   But I might need to add some code to account for different Spark versions, looking for some guidance on this.
   
   ## How was this patch tested?
   
   If you could please provide some guidance where to add tests for this, I am happy to add them.
   


;09/Nov/22 09:16;githubbot;600","lmccay commented on PR #322:
URL: https://github.com/apache/incubator-livy/pull/322#issuecomment-1951396188

   @moritzmeister - did you try adding a bogus import to see whether your assumption about it failing silently is true?


;18/Feb/24 17:44;githubbot;600","moritzmeister commented on PR #322:
URL: https://github.com/apache/incubator-livy/pull/322#issuecomment-2146984409

   No I have never done that, I kind of moved on to new work. I see someone in the original Jira issue tested it though: https://issues.apache.org/jira/browse/LIVY-863


;04/Jun/24 08:59;githubbot;600","nileshrathi345 commented on PR #322:
URL: https://github.com/apache/incubator-livy/pull/322#issuecomment-2470518869

   looks good to me...
   Even tested for few fake imports which has been suggested in https://issues.apache.org/jira/browse/LIVY-863
   All test pass


;12/Nov/24 13:22;githubbot;600","gyogal commented on PR #322:
URL: https://github.com/apache/incubator-livy/pull/322#issuecomment-2470534284

   Thanks everyone for the review and comments, this looks like a useful fix to have in the next Livy release. Thank you @moritzmeister for your contribution!


;12/Nov/24 13:29;githubbot;600","gyogal merged PR #322:
URL: https://github.com/apache/incubator-livy/pull/322


;12/Nov/24 13:33;githubbot;600","moritzmeister commented on PR #322:
URL: https://github.com/apache/incubator-livy/pull/322#issuecomment-2470743436

   My pleasure @gyogal, only three years later :) 


;12/Nov/24 14:51;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,6600,,,0,6600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 26 14:02:32 UTC 2024,,,,,,,,,,"0|z0s4dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,"21/Jun/21 08:59;Meister;I proposed a fix here, it's missing a test and potentially needs some code to account for different spark versions.

If you can help me with this and give some guidance, that would be great and I can add the additional things :)

Â 

https://github.com/apache/incubator-livy/pull/322;;;","12/Nov/22 22:09;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;","13/Feb/24 09:02;eyal;Hi [~lmccay] Â [~Meister]Â  - is there any update with this? Any chance of pushing it to a ""small"" 0.8.1 release? Or is 0.9.0 expected to come out soon?

Â 

(we've also encountered the problem it solves);;;","17/Feb/24 18:43;lmccay;[~eyal] - I just spent a little time looking at this and I'm just not familiar enough to provide guidance to [~Meister] in terms of how to test this in a meaningful way that would ensure multiple spark versions are good with the change.

Do you have any suggestions?

Anyone else?;;;","26/Feb/24 14:02;eyal;I don't have any suggestions, but I saw your question in this issue's PR and that looks like a good idea.

I modified some [similar code|https://github.com/apache/datafu/blob/main/datafu-spark/src/main/resources/pyspark_utils/bridge_utils.py#L47] in [Apache DataFu|https://github.com/apache/datafu] to do a bogus import like you suggested ({_}org.fake.fake{_}). Our tests pass with this change, so it appears that non-existing imports are ignored.

Does that help?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
about Chinese display problem?,LIVY-858,13379462,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,,ighack,ighack,20/May/21 08:49,21/May/21 07:59,19/Dec/25 04:15,21/May/21 07:59,,,,,,,,,,,,,,,,0,,,,,,"headersÂ =Â \{'Content-Type':Â 'application/json;charset=UTF-8'}
statements_urlÂ =Â session_urlÂ +Â '/statements'
 dataÂ =Â 

{ 'code':Â textwrap.dedent("""""" Â Â Â Â importÂ org.apache.spark.sql.

{SaveMode,Â SparkSession}

Â Â Â Â importÂ org.apache.spark.sql.types._
 Â Â Â Â importÂ org.apache.spark.\{SparkConf,Â SparkContext}

Â Â Â Â valÂ sparkhiveÂ =Â SparkSession.builder().appName(""sparkÂ readÂ hive"").enableHiveSupport().getOrCreate()

Â Â Â Â valÂ dfÂ =Â sparkhive.sql(""selectÂ *Â fromÂ hivetest.chinese_par"")
 Â Â Â Â df.show()
 Â Â Â Â """""")
 }

rÂ =Â requests.post(statements_url,Â data=json.dumps(data),Â headers=headers)
 pprint.pprint(r.json())

statement_urlÂ =Â hostÂ +Â r.headers['location']
 rÂ =Â requests.get(statement_url,Â headers=headers)
 pprint.pprint(r.json())
 Â 
 Â 
 I getÂ 
 Â 
 Â '+-----+--+++--------\n'
 '| id|name|year|city|\n'
 '+-----+--+++--------\n'
 '| 4| wk|2019|??|\n'
 '| 5| wk|2019|??|\n'
 '| 6| wk|2019|??|\n'
 '| 7| wk|2019|??|\n'
 '| |null|null|??|\n'
 '| |null|null|??|\n'
 '| |null|null|??|\n'
 '| 8| wk|2020|??|\n'
 '| 9| wk|2020|??|\n'
 '| 10| wk|2020|??|\n'
 '| 11| wk|2020|??|\n'
 '| 1| wk|2018|??|\n'
 '| 2| wk|2018|??|\n'
 '| 3| wk|2018|??|\n'
 '| 3| wk|2018|??|\n'
 '+-----+--+++--------\n'
 Â 
 city is chinese, but display as ""?""
 Â 

Â 

in zeppelin is ok

Â 

+----+---++--------+
|id|name|year|city|

+----+---++--------+
|4|wk|2019|é‡åº†|
|5|wk|2019|é‡åº†|
|6|wk|2019|é‡åº†|
|7|wk|2019|é‡åº†|Â |
|Â |null|null|é‡åº†|
|Â |null|null|é‡åº†|
|Â |null|null|é‡åº†|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 20 09:30:34 UTC 2021,,,,,,,,,,"0|z0r7fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/May/21 09:30;ighack;dataÂ =Â \{'kind':Â 'spark','jars':['hdfs://bigdser1:8020/sparklib/tispark-assembly-2.3.14.jar'],'proxyUser':'hive','conf':{'spark.executor.extraJavaOptions':'-Dfile.encoding=UTF-8','spark.driver.extraJavaOptions':'-Dfile.encoding=UTF-8'}}
Â 
need conf params
Â 
{'conf':\{'spark.executor.extraJavaOptions':'-Dfile.encoding=UTF-8','spark.driver.extraJavaOptions':'-Dfile.encoding=UTF-8'}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails),LIVY-855,13374050,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,,ighack,ighack,21/Apr/21 07:46,21/May/21 07:59,19/Dec/25 04:15,17/May/21 07:56,0.7.0,,,,,,,,,,,,,,,0,,,,,,"In my logÂ 

Â 

21/04/21 15:37:45 INFO KerberosAuthenticationHandler: Login using keytab /hadoop/app/jzyc.keytab, for principal jzyc/hadoop@JOIN.COM21/04/21 15:37:45 INFO KerberosAuthenticationHandler: Login using keytab /hadoop/app/jzyc.keytab, for principal jzyc/hadoop@JOIN.COMDebug isÂ  true storeKey true useTicketCache true useKeyTab true doNotPrompt true ticketCache is null isInitiator false KeyTab is /hadoop/app/jzyc.keytab refreshKrb5Config is true principal is jzyc/hadoop@JOIN.COM tryFirstPass is false useFirstPass is false storePass is false clearPass is falseRefreshing Kerberos configurationAcquire TGT from CachePrincipal is jzyc/hadoop@JOIN.COMnull credentials from Ticket Cacheprincipal is jzyc/hadoop@JOIN.COMWill use keytabCommit SucceededÂ 
21/04/21 15:37:45 INFO WebServer: Starting server on http://bigdser4:899821/04/21 15:37:59 DEBUG ClientCnxn: Got ping response for sessionid: 0x178d9a24b6b24d9 after 0ms21/04/21 15:38:12 DEBUG ClientCnxn: Got ping response for sessionid: 0x178d9a24b6b24d9 after 0ms21/04/21 15:38:25 DEBUG ClientCnxn: Got ping response for sessionid: 0x178d9a24b6b24d9 after 0ms21/04/21 15:38:39 DEBUG ClientCnxn: Got ping response for sessionid: 0x178d9a24b6b24d9 after 0ms21/04/21 15:38:52 DEBUG ClientCnxn: Got ping response for sessionid: 0x178d9a24b6b24d9 after 0ms21/04/21 15:39:03 DEBUG AuthenticationFilter: Request [http://bigdser4:8998/] triggering authentication21/04/21 15:39:03 DEBUG AuthenticationFilter: Request [http://bigdser4:8998/] triggering authentication21/04/21 15:39:03 DEBUG AuthenticationFilter: Authentication exception: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails)org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails) at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:398) at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:518) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) at org.eclipse.jetty.server.Server.handle(Server.java:539) at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333) at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589) at java.lang.Thread.run(Thread.java:748)Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails) at sun.security.jgss.krb5.Krb5AcceptCredential.getInstance(Krb5AcceptCredential.java:87) at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:129) at sun.security.jgss.GSSManagerImpl.getCredentialElement(GSSManagerImpl.java:193) at sun.security.jgss.spnego.SpNegoMechFactory.getCredentialElement(SpNegoMechFactory.java:142) at sun.security.jgss.GSSManagerImpl.getCredentialElement(GSSManagerImpl.java:193) at sun.security.jgss.GSSCredentialImpl.add(GSSCredentialImpl.java:427) at sun.security.jgss.GSSCredentialImpl.<init>(GSSCredentialImpl.java:77) at sun.security.jgss.GSSManagerImpl.createCredential(GSSManagerImpl.java:160) at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:355) at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:347) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:347) ... 21 more21/04/21 15:39:05 DEBUG ClientCnxn: Got ping response for sessionid: 0x178d9a24b6b24d9 after 0ms21/04/21 15:39:06 DEBUG AuthenticationFilter: Request [http://bigdser4:8998/ui] triggering authentication21/04/21 15:39:06 DEBUG AuthenticationFilter: Request [http://bigdser4:8998/ui] triggering authentication21/04/21 15:39:06 DEBUG AuthenticationFilter: Authentication exception: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails)org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails) at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:398) at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:518) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) at org.eclipse.jetty.server.Server.handle(Server.java:539) at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333) at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589) at java.lang.Thread.run(Thread.java:748)Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails) at sun.security.jgss.krb5.Krb5AcceptCredential.getInstance(Krb5AcceptCredential.java:87) at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:129) at sun.security.jgss.GSSManagerImpl.getCredentialElement(GSSManagerImpl.java:193) at sun.security.jgss.spnego.SpNegoMechFactory.getCredentialElement(SpNegoMechFactory.java:142) at sun.security.jgss.GSSManagerImpl.getCredentialElement(GSSManagerImpl.java:193) at sun.security.jgss.GSSCredentialImpl.add(GSSCredentialImpl.java:427) at sun.security.jgss.GSSCredentialImpl.<init>(GSSCredentialImpl.java:77) at sun.security.jgss.GSSManagerImpl.createCredential(GSSManagerImpl.java:160) at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:355) at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:347) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:347) ... 21 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 17 07:56:09 UTC 2021,,,,,,,,,,"0|z0qa3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,"21/Apr/21 07:47;ighack;{{my kerberos step}}

{{1.admin.local -q ""addprinc jzyc/hadoop""}}

{{2.Â }}{{kadmin.local -q ""xst -k jzyc.keytab jzyc/hadoop@JOIN.COM""}};;;","21/Apr/21 09:20;ighack;in livy.conf

livy.server.auth.type = kerberos
 livy.server.auth.kerberos.principal = jzyc/bigdser4@JOIN.COM
 livy.server.auth.kerberos.keytab = /hadoop/app/jzyc_bigdser4.keytab
 livy.server.launch.kerberos.keytab = /hadoop/app/HTTP.keytab
 livy.server.launch.kerberos.principal = HTTP/bigdser4@JOIN.COM
 livy.impersonation.enabled = false

Â 

{{1.admin.local -q ""addprinc jzyc/bigdser4""}}

{{2.Â }}{{kadmin.local -q ""xst -k jzyc_bigdser4.keytab jzyc/bigdser4@JOIN.COM""}}

Â 

I get the same error.;;;","17/May/21 07:56;ighack;ä¿®æ”¹livy.conf

livy.server.auth.type = kerberos
 livy.server.auth.kerberos.principal = xxxxxx
 livy.server.auth.kerberos.keytab =xxxxxx
 ä¸Šé¢ä¸‰ä¸ªä¸è¦é…åˆ¶ï¼Œåªéœ€è¦ä¸‹é¢ä¸¤ä¸ª

livy.server.launch.kerberos.keytab = xxxxxx
 livy.server.launch.kerberos.principal = xxxxxx;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy unable to recover upon losing connection with Zookeeper,LIVY-852,13372620,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Cannot Reproduce,,jameschen1519,jameschen1519,15/Apr/21 12:08,19/Apr/21 12:02,19/Dec/25 04:15,19/Apr/21 12:02,0.6.0,,,,,Server,,,,,,,,,,0,,,,,,"We've noticed that LIVY-732 appears to change Livy's behavior upon loss of connection with Zookeeper. Originally, before this pull request, upon loss of connection with Zookeeper, Livy would exit with an exit code of 1, allowing it to be restarted. At the moment, however, Livy continues to run, but returns a 404 upon interaction with the REST API:

<html>
 <head>
 <meta http-equiv=""Content-Type"" content=""text/html;charset=ISO-8859-1""/>
 <title>Error 404 </title>
 </head>
 <body>
 <h2>HTTP ERROR: 404</h2>
 <p>Problem accessing /sessions. Reason:
 <pre> Not Found</pre></p>
 <hr /><a href=""http://eclipse.org/jetty"">Powered by Jetty:// 9.3.24.v20180605</a><hr/>
 </body>
 </html>

The direct cause of this change in behavior appears to be from the UnhandledErrorListener being converted from a System.exit(1) to throwing a LivyUncaughtException--see lines 74 from server/src/main/scala/org/apache/livy/server/recovery/ZooKeeperStateStore.scala and lines 72 from server/src/main/scala/org/apache/livy/server/recovery/ZooKeeperManager.scala, at [https://github.com/apache/incubator-livy/pull/267/files.]

Â 

As a whole, this change appears to be undesirable,Â as Livy becomes completely unresponsive after zookeeper reconnects (No logging/error messages are printed out after the uncaught exception is thrown) and needs to be manually checked and restarted.Â On the other hand, System.exit(1) seems to be a roundabout way of fixing the issue, and specifying a ConnectionStateListener instead of a UnhandledErrorListener might be better.

Â 

It would be good to figure out if this line should be reverted to a System.exit(1), or if there is a better way of handling this issue.

Â 

Â 

Â ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 19 12:02:24 UTC 2021,,,,,,,,,,"0|z0q19k:",9223372036854775807,,,,,,,,,,,,,,,,,,,"19/Apr/21 12:02;jameschen1519;This may have been a false alarm on second thought--There was a local change that was incompatible with this one. Closing for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to Jetty 9.4.39.v20210325,LIVY-851,13369002,,Dependency upgrade,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,jbonofre,jbonofre,01/Apr/21 03:24,04/Nov/25 20:40,19/Dec/25 04:15,04/Nov/25 20:40,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-599,LIVY-694,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 04 20:40:18 UTC 2025,,,,,,,,,,"0|z0pfmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,"01/Apr/21 03:24;jbonofre;I have a PR almost ready.;;;","12/Nov/22 22:09;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;","05/Oct/23 20:28;fanningpj;At this stage, you should consider at least 9.4.52.v20230823 - https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.52.v20230823 - which has 4 security fixes;;;","04/Nov/25 20:40;gyogal;Jetty version is already at 9.4.56.v20240826, resolving this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Livy allows users to see password in config files (spark.ssl.keyPassword,spark.ssl.keyStorePassword,spark.ssl.trustStorePassword, etc)",LIVY-833,13360156,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,,kzhao,kzhao,22/Feb/21 18:07,24/Feb/21 01:39,19/Dec/25 04:15,24/Feb/21 01:39,0.7.0,,,,,Server,,,,,,,,,,0,security,,,,,"It looks like a regular user (client) of Livy, can use commands like:Â 

spark.sparkContext.getConf().getAll()

The command will retry all spark configurations including those passwords (such asÂ spark.ssl.trustStorePassword, spark.ssl.keyPassword).Â 

I would suggest to block / mask these password.Â 

PS, Spark's UI fixed this issue in thisÂ https://issues.apache.org/jira/browse/SPARK-16796",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 24 01:39:42 UTC 2021,,,,,,,,,,"0|z0nxv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Feb/21 01:39;jerryshao;This is the problem of Spark, not Livy. Spark uses the configuration to store everything including passwords, and user could get configurations within application through many ways. Besides Livy, user still could get password by using spark-shell, spark-submit and others.

If user could submit code through Livy to spark when Livy security is enabled, it means user permission to execute code, it is acceptable to see the passwords.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Facing intermittent issue while submit job to Livy,LIVY-797,13336891,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,naksh9619,naksh9619,23/Oct/20 09:40,23/Oct/20 10:59,19/Dec/25 04:15,23/Oct/20 10:59,,,,,,,,,,,,,,,,0,,,,,,"While submitting spark jobs to Apache Livy via Airflow, we are intermittently getting the below Exception. Can someone please help in extracting the root cause for this:

2020-10-23 06:05:55,422 ERROR utils.SparkYarnApp: Error whiling refreshing YARN state
java.lang.IllegalStateException: No YARN application is found with tag livy-batch-82850-44ej2bgm in 300 seconds. This may be because 1) spark-submit fail to submit application to YARN; or 2) YARN cluster doesn't have enough resources to start the application in time. Please check Livy log and YARN log to know the details.
 at org.apache.livy.utils.SparkYarnApp.org$apache$livy$utils$SparkYarnApp$$getAppIdFromTag(SparkYarnApp.scala:206)
 at org.apache.livy.utils.SparkYarnApp$$anonfun$1$$anonfun$4.apply(SparkYarnApp.scala:267)
 at org.apache.livy.utils.SparkYarnApp$$anonfun$1$$anonfun$4.apply(SparkYarnApp.scala:264)
 at scala.Option.getOrElse(Option.scala:121)
 at org.apache.livy.utils.SparkYarnApp$$anonfun$1.apply$mcV$sp(SparkYarnApp.scala:264)
 at org.apache.livy.Utils$$anon$1.run(Utils.scala:97)
2020-10-23 06:05:55,528 INFO utils.LineBufferedStream: Warning: Skip remote jar <jar_path>
Exception in thread ""Thread-57893"" java.io.IOException: Stream closed
 at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)
 at java.io.BufferedInputStream.read1(BufferedInputStream.java:283)
 at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
 at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
 at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
 at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
 at java.io.InputStreamReader.read(InputStreamReader.java:184)
 at java.io.BufferedReader.fill(BufferedReader.java:161)
 at java.io.BufferedReader.readLine(BufferedReader.java:324)
 at java.io.BufferedReader.readLine(BufferedReader.java:389)
 at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:72)
 at scala.collection.Iterator$class.foreach(Iterator.scala:891)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
 at org.apache.livy.utils.LineBufferedStream$$anon$1.run(LineBufferedStream.scala:46)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2020-10-23 09:40:43.0,,,,,,,,,,"0|z0jy8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't work with PySpark with Python >= 3.8 due to https://bugs.python.org/issue35894,LIVY-795,13334479,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Fixed,,shiquanwang,shiquanwang,08/Oct/20 18:12,04/Nov/25 20:58,19/Dec/25 04:15,27/Dec/22 05:58,0.7.0,,,0.8.0,,REPL,,,,,,,,,,1,easyfix,,,,,"Â 

When working with PySpark with Python >= 3.8, below error is raised:
{code:java}
TypeError: required field ""type_ignores"" missing from Module{code}
The error points to code in [https://github.com/apache/incubator-livy/blob/4d8a912699683b973eee76d4e91447d769a0cb0d/repl/src/main/resources/fake_shell.py#L223]

Â 

The error is caused by an API change in Python 3.8: [https://github.com/python/cpython/pull/11645/files]Â and was reported hereÂ [https://bugs.python.org/issue35894]

IPython has faced similar issue: [https://github.com/ipython/ipython/issues/11590]

IPython solved the issue in this PR: [https://github.com/ipython/ipython/pull/11593]Â Â 

Â ",EMR 6.1.0,"gabrielmagno opened a new pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314


   ## What changes were proposed in this pull request?
   
   A mock for the `ast.Module` is created in case of running older Python versions (< 3.8). The mocked module will ignore the new `type_ignores` parameter, and pass only the first parameter to the old module.
   
   As already mentioned by Shiquan Wang (the issue reporter), Python 3.8 introduced a change in the API (https://github.com/python/cpython/pull/11645/files), and IPython had a similar problem. The strategy used for this fix is similar to the one utilized by IPython in this PR: https://github.com/ipython/ipython/pull/11593 
   
   JIRA: https://issues.apache.org/jira/projects/LIVY/issues/LIVY-795
   
   ## How was this patch tested?
   
   Tested locally, using Jupyter notebook to run pyspark commands.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/21 19:34;githubbot;600","gabrielmagno commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-756101719


   The build for `Spark 3.0 Unit Tests` failed, but so does the master branch. Probably not related to the Python fix.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/21 13:00;githubbot;600","siege089 commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-820862784


   It appears master tests are passing, any chance this build could be rechecked and merged? Would love to be able to run multiple lines of python at once in 3.8.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/21 02:24;githubbot;600","gabrielmagno commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-821155642


   @siege089 would be glad to do so, but [latest master commit](https://github.com/apache/incubator-livy/commit/4d8a912699683b973eee76d4e91447d769a0cb0d) is from Aug 2020, and it was already incorporated in my branch when I made the pull request. It has a [""failed"" build status](https://travis-ci.org/github/apache/incubator-livy/builds/718090124).


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Apr/21 12:58;githubbot;600","dacort commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-888688271


   *bump* Any chance we can revive this?


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Jul/21 23:34;githubbot;600","gabrielmagno commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-891095853


   > @siege089 would be glad to do so, but [latest master commit](https://github.com/apache/incubator-livy/commit/4d8a912699683b973eee76d4e91447d769a0cb0d) is from Aug 2020, and it was already incorporated in my branch when I made the pull request. It has a [""failed"" build status](https://travis-ci.org/github/apache/incubator-livy/builds/718090124).
   
   @dacort Unfortunately I'm still limited to the master's fail commit from August 2020. If there is anything I could do to circumvent it in the meantime, I would be glad to do it.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/21 14:59;githubbot;600","dacort commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-891156411


   @gabrielmagno Thanks for the reply! Let me see if I can clone the change and open a new PR. 


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/21 16:19;githubbot;600","dacort edited a comment on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-891156411


   @gabrielmagno Thanks for the reply! ~~Let me see if I can clone the change and open a new PR.~~ 
   
   edit: I see what you mean. I wonder if that was just a test blip? The change seems unrelated... :\


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/21 16:24;githubbot;600","windmark commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-942032154


   Hi, bumping this issue @dacort @gabrielmagno since we blocked by not being able to use 3.8 in our setup.
   
   Looking at the failed master build, https://travis-ci.org/github/apache/incubator-livy/builds/718090124, from PR #302, do you have any plans to revert that change since it seems to block any further development? Looping in owners of that PR @mgaido91 @ajbozarth 
   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Oct/21 07:59;githubbot;600","windmark edited a comment on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-942032154


   Hi, bumping this issue @dacort @gabrielmagno since we are blocked by not being able to use 3.8 in our setup.
   
   Looking at the failed master build, https://travis-ci.org/github/apache/incubator-livy/builds/718090124, from PR #302, do you have any plans to revert that change since it seems to block any further development? Looping in owners of that PR @mgaido91 @ajbozarth 
   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Oct/21 08:00;githubbot;600","mgaido91 commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-942067151


   I am more familiar with the thriftserver part, so I am not confident reviewing this PR, btw the CI is failing, so a first step would be having the CI working fine.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Oct/21 08:38;githubbot;600","mgaido91 commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-942067151


   I am more familiar with the thriftserver part, so I am not confident reviewing this PR, btw the CI is failing, so a first step would be having the CI working fine.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Oct/21 18:42;githubbot;600","windmark edited a comment on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-942032154


   Hi, bumping this issue @dacort @gabrielmagno since we are blocked by not being able to use 3.8 in our setup.
   
   Looking at the failed master build, https://travis-ci.org/github/apache/incubator-livy/builds/718090124, from PR #302, do you have any plans to revert that change since it seems to block any further development? Looping in owners of that PR @mgaido91 @ajbozarth 
   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Oct/21 18:43;githubbot;600","windmark commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-942032154


   Hi, bumping this issue @dacort @gabrielmagno since we blocked by not being able to use 3.8 in our setup.
   
   Looking at the failed master build, https://travis-ci.org/github/apache/incubator-livy/builds/718090124, from PR #302, do you have any plans to revert that change since it seems to block any further development? Looping in owners of that PR @mgaido91 @ajbozarth 
   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Oct/21 18:49;githubbot;600","dacort commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-942722327


   I [tried to get CI working](https://github.com/apache/incubator-livy/pull/326#issuecomment-891374655), but didn't have much luck. :(


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Oct/21 21:08;githubbot;600","gabrielmagno commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-942758569


   @mgaido91 just to point out that the [latest master commit](https://github.com/apache/incubator-livy/commit/4d8a912699683b973eee76d4e91447d769a0cb0d) has a [""failed"" build status](https://travis-ci.org/github/apache/incubator-livy/builds/718090124).
   
   This latest master commit is already incorporated in my branch, and I believe it is failing because of that, not exactly because of the Python 3.8 fix.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Oct/21 22:11;githubbot;600","mgaido91 commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-942762346


   Then I would recommend fixing the tests on the master branch and then go ahead. It is the safest option IMHO.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Oct/21 22:18;githubbot;600","gabrielmagno commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-942766366


   > Then I would recommend fixing the tests on the master branch and then go ahead. It is the safest option IMHO.
   
   I agree with you, thank you for the feedback!
   
   I would be glad to do it, but this is out of my expertise and knowledge of the project :-(


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Oct/21 22:26;githubbot;600","dacort commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-943411436


   @mgaido91 It looks like https://github.com/apache/incubator-livy/pull/318 did the work to fix the tests on the master branch. Is there somebody that could take a look at that / merge it in?


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Oct/21 14:26;githubbot;600","derache123 commented on pull request #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1068526101


   Any update on this PR? Is anyone still looking at this?


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Mar/22 22:25;githubbot;600","virginiayung commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1114132512

   Hi, can we get some help here on pushing this forward? We are also blocked by not being able to use Python 3.8 because of this issue. Thanks!


;01/May/22 05:00;githubbot;600","derache123 commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1121600145

   Bump. This seems like a crucial issue to fix and I'm honestly quite surprised that it's been sitting for so long.


;09/May/22 21:28;githubbot;600","gabrielmagno commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1122426274

   Yes, it is so sad that the build of the main branch is broken for almost 2 years ðŸ˜¢ 


;10/May/22 13:51;githubbot;600","timrepo opened a new pull request, #350:
URL: https://github.com/apache/incubator-livy/pull/350

   ## What changes were proposed in this pull request?
   Porting `ast.Module()` call to be compatible with Python 3.8 and later versions.
   
   ## How was this patch tested?
   in a Yarn application
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   


;14/Jul/22 06:09;githubbot;600","uberadam2392 commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1263651574

   Hi I ran into this issue recently, where if I run my code as follows in separate cells line-by-line, there is no error:
   
   ```
   import scipy
   ```
   
   ```
   print(scipy.__version__)
   ```
   
   ```
   import IPython
   ```
   
   But as soon as I put them into the same Jupyter cell:
   
   ```
   import scipy
   import IPython
   print(scipy.__version__)
   ```
   
   I get the following error:
   
   ```
   required field ""type_ignores"" missing from Module
   Traceback (most recent call last):
     File ""/tmp/362488836129912165"", line 215, in execute
       code = compile(mod, self.cell_name, 'exec')
   TypeError: required field ""type_ignores"" missing from Module
   ```
   
   See attached screenshot.
   
   <img width=""1104"" alt=""Screen Shot 2022-09-30 at 10 28 19 AM"" src=""https://user-images.githubusercontent.com/113077556/193292221-7b15c5e3-07c8-4fb7-a79f-0966b2c54430.png"">
   
   Is there anyway to fix this on the user end? I have a SparkMagic / PySpark cluster provisioned for me, and Idk how long a hack fix would take upstream...


;30/Sep/22 14:30;githubbot;600","uberadam2392 commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1263657868

   Does anyone know an easy workaround if I need to use Python3.8+, Jupyter notebook, PySpark (3.0+ to support Python3.8)?
   
   Thanks!


;30/Sep/22 14:35;githubbot;600","gabrielmagno commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1263676433

   Hi @uberadam2392. 
   
   You could try using my personal fork of livy: https://github.com/gabrielmagno/incubator-livy .
   In the README there is a section `Building Livy` which you can follow to build locally.
   
   Alternatevely, you could try applying the patch by yourself, you have to change a single file.
   Look at the commit with the change: https://github.com/gabrielmagno/incubator-livy/commit/81a4e92b5abbc65f27b35454a8f28a72b8f4179b


;30/Sep/22 14:50;githubbot;600","aabdullah-getguru commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1285986271

   > Bump. This seems like a crucial issue to fix and I'm honestly quite surprised that it's been sitting for so long.
   
   Well, it is what it is :), I'm surprised too, but the contributors are doing this on their own free time, so have to be patient :).


;20/Oct/22 18:42;githubbot;600","dacort commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1307354567

   It looks like the project is being reinstated!! @gabrielmagno looks like there are some new commits including a fix for CI. 
   
   While I don't think the team is pulling in old PRs quite yet (based on mailing list chatter), might be good to rebase this on master if you have the chance!


;08/Nov/22 15:01;githubbot;600","lmccay commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1307432548

   > It looks like the project is being reinstated!! @gabrielmagno looks like there are some new commits including a fix for CI.
   > 
   > While I don't think the team is pulling in old PRs quite yet (based on mailing list chatter), might be good to rebase this on master if you have the chance!
   
   We can consider PRs with active representation for the first release. Just don't want to chase folks down and hold things up when we can just turn the crank and get a clean release out as a first goal.


;08/Nov/22 15:50;githubbot;600","dacort commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1307434246

   Awesome thanks @lmccay !


;08/Nov/22 15:51;githubbot;600","gabrielmagno commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1307481751

   Thanks for letting me know @dacort !
   I've just merged the master branch to my fork.
   The build was created and is awaiting in the queue.
   Let's cross our fingers that it will pass the tests this time :)


;08/Nov/22 16:21;githubbot;600","codecov-commenter commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1307715257

   # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/314?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report
   > Merging [#314](https://codecov.io/gh/apache/incubator-livy/pull/314?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (2650d41) into [master](https://codecov.io/gh/apache/incubator-livy/commit/8c6f60b74633f25c211ca248ba290de65aee89f9?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (8c6f60b) will **decrease** coverage by `2.62%`.
   > The diff coverage is `n/a`.
   
   ```diff
   @@             Coverage Diff              @@
   ##             master     #314      +/-   ##
   ============================================
   - Coverage     68.40%   65.78%   -2.63%     
   + Complexity      842      816      -26     
   ============================================
     Files           103      103              
     Lines          5948     5948              
     Branches        899      899              
   ============================================
   - Hits           4069     3913     -156     
   - Misses         1317     1508     +191     
   + Partials        562      527      -35     
   ```
   
   
   | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/314?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage Î” | |
   |---|---|---|
   | [...a/org/apache/livy/server/ThriftServerFactory.scala](https://codecov.io/gh/apache/incubator-livy/pull/314/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvVGhyaWZ0U2VydmVyRmFjdG9yeS5zY2FsYQ==) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |
   | [...main/scala/org/apache/livy/server/LivyServer.scala](https://codecov.io/gh/apache/incubator-livy/pull/314/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvTGl2eVNlcnZlci5zY2FsYQ==) | `2.23% <0.00%> (-29.47%)` | :arrow_down: |
   | [...rc/main/scala/org/apache/livy/utils/SparkApp.scala](https://codecov.io/gh/apache/incubator-livy/pull/314/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya0FwcC5zY2FsYQ==) | `56.00% <0.00%> (-20.00%)` | :arrow_down: |
   | [core/src/main/scala/org/apache/livy/Logging.scala](https://codecov.io/gh/apache/incubator-livy/pull/314/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvTG9nZ2luZy5zY2FsYQ==) | `66.66% <0.00%> (-16.67%)` | :arrow_down: |
   | [...e/livy/server/interactive/InteractiveSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/314/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `56.97% <0.00%> (-12.80%)` | :arrow_down: |
   | [...src/main/scala/org/apache/livy/sessions/Kind.scala](https://codecov.io/gh/apache/incubator-livy/pull/314/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvc2Vzc2lvbnMvS2luZC5zY2FsYQ==) | `66.66% <0.00%> (-9.53%)` | :arrow_down: |
   | [...la/org/apache/livy/utils/LineBufferedProcess.scala](https://codecov.io/gh/apache/incubator-livy/pull/314/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9MaW5lQnVmZmVyZWRQcm9jZXNzLnNjYWxh) | `78.57% <0.00%> (-7.15%)` | :arrow_down: |
   | [...server/interactive/InteractiveSessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/314/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `61.41% <0.00%> (-6.30%)` | :arrow_down: |
   | [.../scala/org/apache/livy/server/SessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/314/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvU2Vzc2lvblNlcnZsZXQuc2NhbGE=) | `66.31% <0.00%> (-5.27%)` | :arrow_down: |
   | [...org/apache/livy/server/recovery/SessionStore.scala](https://codecov.io/gh/apache/incubator-livy/pull/314/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvcmVjb3ZlcnkvU2Vzc2lvblN0b3JlLnNjYWxh) | `75.00% <0.00%> (-5.00%)` | :arrow_down: |
   | ... and [13 more](https://codecov.io/gh/apache/incubator-livy/pull/314/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | |
   
   :mega: Weâ€™re building smart automated test selection to slash your CI/CD build times. [Learn more](https://about.codecov.io/iterative-testing/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)
   


;08/Nov/22 19:18;githubbot;600","gabrielmagno commented on PR #314:
URL: https://github.com/apache/incubator-livy/pull/314#issuecomment-1307751595

   @dacort (cc @lmccay)
   
   It is building properly now! ðŸ¥³ 
   
   I believe this PR has passed all the checks and is ready to be merged upstream, as soon as the core developers see as appropriate and have the time to look into it ðŸ™‚
   
   Just as a ""pitch"" for merging this PR (for any core developer looking into this): this is a relatively simple PR that modifies a single Python file, and does not even modify the main core Scala code of the project. On the other hand, I think it is very important because Python 3.7 will reach end-of-life in just a couple of months ([2023-06-27](https://devguide.python.org/versions/)), so making livy compatible with Python 3.8+ seems to be a nice goal to have.


;08/Nov/22 19:52;githubbot;600","leesf merged PR #314:
URL: https://github.com/apache/incubator-livy/pull/314


;09/Nov/22 01:03;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,21000,,,0,21000,,,,,,,,,,,,,,,,,,LIVY-886,,,,,,,,,,"13/Nov/22 08:41;xleesf;image-2022-11-13-16-41-13-052.png;https://issues.apache.org/jira/secure/attachment/13052146/image-2022-11-13-16-41-13-052.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 21 01:09:29 UTC 2022,,,,,,,,,,"0|z0jje0:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Jan/21 19:35;gmagno;Suggested a fix here: https://github.com/apache/incubator-livy/pull/314;;;","10/Nov/22 15:55;lmccay;I assume this JIRA can be resolved as fixed now since it was merged, [~xleesf]?
 If so, we can do so and set the Fix Version to 0.8.0 and maybe the Target Version as 0.8.0?;;;","10/Nov/22 15:56;lmccay;Okay - doesn't look like we have a Target Version field.
I'd like to look into adding that for tracking release candidate fixes.;;;","12/Nov/22 22:56;xleesf;[~lmccay] we would use Fix Version/s to track it. but I can not Update the Jira ticket, maybe with no permission, would you please add me as admin for livy Jira project?;;;","12/Nov/22 23:03;lmccay;Hi [~xleesf] - You should be admin already as it has been granted to all PPMC members.
Also, Fix Version is already set... :);;;","13/Nov/22 08:41;xleesf;hi [~lmccay] I found that i am still not the admin yet. you would see the pic below.
!image-2022-11-13-16-41-13-052.png!;;;","13/Nov/22 18:08;lmccay;Okay, [~xleesf] - I've added you explicitly.
I don't know why you aren't getting it by virtue of being livy-pmc but whatever.
Enjoy. :);;;","20/Nov/22 19:07;lmccay;[~xleesf] - have you checked your admin status recently?;;;","21/Nov/22 01:09;xleesf;[~lmccay] Thanks, I have checked the status, it is already admin status.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make keystore type configurable,LIVY-793,13330573,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,andrasbeni,andrasbeni,andrasbeni,02/Oct/20 14:50,18/Aug/23 11:58,19/Dec/25 04:15,18/Aug/23 11:58,0.8.0,,,0.9.0,,,,,,,,,,,,0,,,,,,Currently JKS keystores are hardwired in Livy.,,"andrasbeni opened a new pull request #311:
URL: https://github.com/apache/incubator-livy/pull/311


   ## What changes were proposed in this pull request?
   
   This change introduces configuration parameter `livy.keystore.type`.
   The default value is JKS which is equivalent to current functionality.
   
   ## How was this patch tested?
   
   This change was tested by running existing tests and manually verifying functionality using non-JKS keystore.
   
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Oct/20 14:52;githubbot;600","andrasbeni closed pull request #311: [LIVY-793] Make keystore type configurable
URL: https://github.com/apache/incubator-livy/pull/311


;17/Oct/22 08:02;githubbot;600","andrasbeni opened a new pull request, #395:
URL: https://github.com/apache/incubator-livy/pull/395

   ## What changes were proposed in this pull request?
   
   This change introduces configuration parameter `livy.keystore.type`.
   The default value is JKS which is equivalent to current functionality.
   
   ## How was this patch tested?
   
   This change was tested by running existing tests and manually verifying functionality using non-JKS keystore.
   
   


;27/Mar/23 11:06;githubbot;600","gyogal commented on PR #395:
URL: https://github.com/apache/incubator-livy/pull/395#issuecomment-1504983652

   @andrasbeni I noticed that I can't assign JIRA issues to you, do you have an ASF JIRA user?


;12/Apr/23 09:48;githubbot;600","andrasbeni commented on PR #395:
URL: https://github.com/apache/incubator-livy/pull/395#issuecomment-1505606101

   @gyogal , my ASF JIRA username is [`andrasbeni`](https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrasbeni).


;12/Apr/23 16:52;githubbot;600","gyogal merged PR #395:
URL: https://github.com/apache/incubator-livy/pull/395


;18/Aug/23 11:57;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 12 07:58:14 UTC 2023,,,,,,,,,,"0|z0j48g:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Nov/22 22:10;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;","12/Apr/23 07:58;gyogal;A pull request is available at https://github.com/apache/incubator-livy/pull/395;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable adding security related HTTP headers,LIVY-785,13323495,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,nileshrathi345,andrasbeni,andrasbeni,19/Aug/20 07:03,14/Apr/25 13:56,19/Dec/25 04:15,14/Apr/25 13:56,0.8.0,,,0.9.0,,,,,,,,,,,,0,,,,,,"The following HTTP headers help eliminate the risk of various client side vulnerabilities:
* X-XSS-Protection
* X-Frame_options
* X-Content-Type-Options
Thus we need to allow users to easily add them to HTTP responses of Livy server.",,"andrasbeni opened a new pull request #304:
URL: https://github.com/apache/incubator-livy/pull/304


   
   
   ## What changes were proposed in this pull request?
   
   This change introduces a new configuration option `livy.server.security-headers.enabled`.
   When this property is set to true, the following security headers are added to HTTP
   responses by default:
   * X-XSS-Protection
   * X-Frame_options
   * X-Content-Type-Options
   
   Also, adds content type information to all responses as required when using content type option nosniff
   
   ## How was this patch tested?
   
   Tested manually
   
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/20 07:11;githubbot;600","andrasbeni closed pull request #304: [LIVY-785] Enable adding security related HTTP headers to responses
URL: https://github.com/apache/incubator-livy/pull/304


;17/Oct/22 08:02;githubbot;600","andrasbeni opened a new pull request, #396:
URL: https://github.com/apache/incubator-livy/pull/396

   
   ## What changes were proposed in this pull request?
   
   This change introduces a new configuration option `livy.server.security-headers.enabled`.
   When this property is set to true, the following security headers are added to HTTP
   responses by default:
   * X-XSS-Protection
   * X-Frame_options
   * X-Content-Type-Options
   
   Also, adds content type information to all responses as required when using content type option nosniff
   
   ## How was this patch tested?
   
   Tested manually
   
   


;27/Mar/23 11:16;githubbot;600","nileshrathi345 opened a new pull request, #456:
URL: https://github.com/apache/incubator-livy/pull/456

   Added security related HTTP headers
   
   (Please fill in changes proposed in this fix)
   (Include a link to the associated JIRA and make sure to add a link to this pr on the JIRA as well)
   
   ## How was this patch tested?
   
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   


;12/Sep/24 13:54;githubbot;600","gyogal commented on PR #396:
URL: https://github.com/apache/incubator-livy/pull/396#issuecomment-2476404422

   FYI a new PR #456 has been submitted for this issue that includes the missing SecurityHeadersFilter class and adds two new headers ""Strict-Transport-Security"" and ""Content-Security-Policy"". Please feel free to leave a review comment there.


;14/Nov/24 13:49;githubbot;600","andrasbeni closed pull request #396: [LIVY-785] Enable adding security related HTTP headers to responses
URL: https://github.com/apache/incubator-livy/pull/396


;14/Nov/24 13:52;githubbot;600","gyogal merged PR #456:
URL: https://github.com/apache/incubator-livy/pull/456


;14/Apr/25 13:55;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4200,,,0,4200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 13 08:04:11 UTC 2024,,,,,,,,,,"0|z0hwmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Nov/22 22:10;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;","13/Sep/24 08:04;gyogal;PR is uploaded at https://github.com/apache/incubator-livy/pull/456;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not able build version 0.8.0,LIVY-784,13320338,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Cannot Reproduce,dacort,sukeshpabolu,sukeshpabolu,31/Jul/20 08:12,11/Apr/23 23:38,19/Dec/25 04:15,07/Dec/22 22:41,,,,0.8.0,,,,,,,,,,,,1,,,,,,"getting the following error

Â 

Failed to execute goal on project livy-repl-parent: Could not resolve dependencies for project org.apache.livy:livy-repl-parent:pom:0.8.0-incubating
-SNAPSHOT: The following artifacts could not be resolved: org.apache.livy:livy-rsc:jar:0.8.0-incubating-SNAPSHOT, org.apache.livy:livy-core_2.11:jar:0.8.0-i
ncubating-SNAPSHOT, org.apache.livy:livy-core_2.11:jar:tests:0.8.0-incubating-SNAPSHOT: Failure to find org.apache.livy:livy-rsc:jar:0.8.0-incubating-SNAPSH
OT in [https://repository.apache.org/snapshots] was cached in the local repository, resolution will not be reattempted until the update interval of apache.sna
pshots has elapsed or updates are forced",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 11 23:38:28 UTC 2023,,,,,,,,,,"0|z0hd94:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Jan/21 19:12;gmagno;Apparently, the build instruction on GitHub is wrong. I was also not able to build it using `mvn package`.

Then, I tried using `mvn install` and it worked.;;;","12/Nov/22 22:10;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;","02/Dec/22 23:20;dacort;FYI, bumping the {{maven-surefire-plugin}} version to {{2.22.1}} has fixed this for me.

Unsure of the underlying reason, but [this stackoverflow pos|https://stackoverflow.com/questions/55422765/maven-surefire-error-occured-in-starting-fork#comment97564718_55422765]t led me in the right direction.;;;","03/Dec/22 01:58;lmccay;Interesting, [~dacort] - let me give that a shot!

Â ;;;","03/Dec/22 21:36;lmccay;[~dacort] - are you going to provide a PR?

We could pull this back into the 0.8.0 release.;;;","04/Dec/22 18:27;dacort;[~lmccay] Yes, for sure. I'm just trying to get through a full build first. :)Â 

Just taking a little while, but will be able to dedicate more time early next week.;;;","07/Dec/22 22:19;dacort;For some reason, I've been unable to reproduce the original error. It might have been a specific environment or version of maven or java, but I'm not sure.

Nevertheless, I've opened [https://github.com/apache/incubator-livy/pull/367] to add a Dockerfile to the repo that can help with reproducible builds. I'm still working on it as there's one failing Python unicode test.;;;","07/Dec/22 22:23;lmccay;[~dacort] - okay, perhaps we should just close this as can't reproduce or whatever is appropriate.;;;","07/Dec/22 22:34;dacort;[~lmccay] Yea, I think let's close this one. There are other issues preventing the current main branch from building (LIVY-580 and LIVY-579), but I can't repro this specific one.;;;","11/Apr/23 23:38;dacort;Just as an FYI, I ended up running into this again while trying to unit test LIVY-878 locally.

I could not for the life of me get the tests past livy-rsc module, nor did I see any useful errors except for something about SASL DIGEST-MD5 auth failing. This was on my local intel mac with the latest Docker. But bumping surefire allows me to get past that now. Â¯\_(ãƒ„)_/Â¯Â ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"java.lang.IncompatibleClassChangeError: Found interface org.objectweb.asm.MethodVisitor, but class was expected",LIVY-783,13319839,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,Minutis,lucky@unifisoftware.com,lucky@unifisoftware.com,28/Jul/20 20:34,11/Nov/22 02:08,19/Dec/25 04:15,11/Nov/22 02:08,0.8.0,,,,,,,,,,,,,,,0,asm,,,,,"Â 

Steps to reproduce:
 # Pull master branch (0.8.0-SNAPSHOT)
 # changed <scala.binary.version> to 2.12 and <scala.version> toÂ ${scala-2.12.version} in pom.xml
 # package the jars through maven package
 # extract theÂ assembly/target/apache-livy-0.8.0-incubating-SNAPSHOT-bin.zipÂ 
 # Start livy server
 # Create an interactive session
 # Fails with java.lang.IncompatibleClassChangeError. Find attached stack trace.

Â 

Observations:

I see that Hadoop 2.7 uses asm-3.2.jar. And livy 0.8.0-SNAPSHOT usesÂ asm-5.0.4.jar","Hadoop 2.7, Spark 3.0, Scala 2.12","Minutis opened a new pull request #313:
URL: https://github.com/apache/incubator-livy/pull/313


   ## What changes were proposed in this pull request?
   
   Use shaded kryo library which causes inconsistencies in asm dependencies. https://issues.apache.org/jira/browse/LIVY-783
   
   ## How was this patch tested?
   
   Tested on live cluster.
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Dec/20 21:27;githubbot;600","Minutis closed pull request #313:
URL: https://github.com/apache/incubator-livy/pull/313


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Dec/20 07:51;githubbot;600","Minutis opened a new pull request #313:
URL: https://github.com/apache/incubator-livy/pull/313


   ## What changes were proposed in this pull request?
   
   Use shaded kryo library which causes inconsistencies in asm dependencies. https://issues.apache.org/jira/browse/LIVY-783
   
   ## How was this patch tested?
   
   Tested on live cluster.
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Dec/20 07:51;githubbot;600","geosmart commented on pull request #313:
URL: https://github.com/apache/incubator-livy/pull/313#issuecomment-823917485


   this pr working for me
   version:
   * livy 0.8.0
   * spark-2.4.7
   * hadoop 2.6.0
   @Minutis  thanks for your commit


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Apr/21 09:24;githubbot;600","yeskarthik commented on pull request #313:
URL: https://github.com/apache/incubator-livy/pull/313#issuecomment-1032058891


   I faced the same issue and this fix works for me. ðŸ‘ 


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Feb/22 23:52;githubbot;600","yeskarthik edited a comment on pull request #313:
URL: https://github.com/apache/incubator-livy/pull/313#issuecomment-1032058891


   I faced the same issue and this fix works for me. ðŸ‘ Will this be merged into master?


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Feb/22 23:54;githubbot;600","yeskarthik edited a comment on pull request #313:
URL: https://github.com/apache/incubator-livy/pull/313#issuecomment-1032058891


   I faced the same issue and this fix works for me. ðŸ‘ Will this be merged into master?
   version: 
   * livy 0.5
   * spark-3.1.2
   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Feb/22 23:58;githubbot;600","Minutis commented on pull request #313:
URL: https://github.com/apache/incubator-livy/pull/313#issuecomment-1039157931


   Just FYI since this project is no longer supported we took the main idea and wrote our own service practically identical to this one: https://github.com/exacaster/lighter. Feel free to try it out.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Feb/22 14:34;githubbot;600","agsachin commented on pull request #313:
URL: https://github.com/apache/incubator-livy/pull/313#issuecomment-1039491809


   +1


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Feb/22 19:54;githubbot;600","agsachin commented on pull request #313:
URL: https://github.com/apache/incubator-livy/pull/313#issuecomment-1039491809


   +1


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Feb/22 18:34;githubbot;600","yantzu merged PR #313:
URL: https://github.com/apache/incubator-livy/pull/313


;09/Nov/22 10:02;githubbot;600","yantzu commented on PR #313:
URL: https://github.com/apache/incubator-livy/pull/313#issuecomment-1308506404

   LGTM
   @Minutis may I know your apache id, then I will assign  https://issues.apache.org/jira/browse/LIVY-783 to you and close it


;09/Nov/22 10:03;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,7200,,,0,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/20 20:39;lucky@unifisoftware.com;livy-stack-trace.log;https://issues.apache.org/jira/secure/attachment/13008624/livy-stack-trace.log",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 09 10:58:25 UTC 2022,,,,,,,,,,"0|z0ha6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Dec/20 21:30;Minutis;Suggested a fix here: https://github.com/apache/incubator-livy/pull/313;;;","09/Nov/22 10:58;xilangyan;Fixed by [https://github.com/apache/incubator-livy/pull/313];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Maven Shade Plugin to 3.2.1,LIVY-776,13313080,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,coheigea,coheigea,23/Jun/20 16:10,02/Jul/20 09:28,19/Dec/25 04:15,02/Jul/20 09:28,,,,0.8.0,,,,,,,,,,,,0,,,,,,"In order to apply the PR to update Jackson to 2.10.1 ([https://github.com/apache/incubator-livy/pull/258/files),] it is also necessary to update the Maven Shade Plugin to avoid an ASM IllegalArgumentException error in the client-http module.",,"coheigea opened a new pull request #299:
URL: https://github.com/apache/incubator-livy/pull/299


   In order to apply the PR to update Jackson to 2.10.1 (https://github.com/apache/incubator-livy/pull/258/files), it is also necessary to update the Maven Shade Plugin to avoid an ASM IllegalArgumentException error in the client-http module.
   
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jun/20 16:16;githubbot;600","tprelle commented on pull request #299:
URL: https://github.com/apache/incubator-livy/pull/299#issuecomment-648874686


   Hi @coheigea  I faced the same issue (shaded plugin and jackson version) on https://github.com/apache/incubator-livy/pull/289 for scala 2.12 and spark 3 support. I need also to change this. I do not know how deal with multiple PR.
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 15:00;githubbot;600","coheigea commented on pull request #299:
URL: https://github.com/apache/incubator-livy/pull/299#issuecomment-648887533


   @tprelle It's up to the maintainers of the project, either it could all be done in one go (as in your PR), or maybe they might prefer to just update Jackson separately. If your PR is merged first, then I'll just close this PR. Either way it would be good from my POV to get us updated to Jackson 2.10.x ;-)


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 15:21;githubbot;600","jerryshao commented on pull request #299:
URL: https://github.com/apache/incubator-livy/pull/299#issuecomment-652848737


   Fixed in #300 


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jul/20 07:54;githubbot;600","jerryshao closed pull request #299:
URL: https://github.com/apache/incubator-livy/pull/299


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jul/20 07:54;githubbot;600","jerryshao edited a comment on pull request #299:
URL: https://github.com/apache/incubator-livy/pull/299#issuecomment-652848737


   Fixed in #300 , thanks a lot for the contribution.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jul/20 07:55;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-756,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2020-06-23 16:10:15.0,,,,,,,,,,"0|z0g4pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FD leak in InteractiveSession and PythonInterpreter,LIVY-763,13300896,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,rajshekhar,cooper6581,cooper6581,24/Apr/20 22:02,19/Apr/23 15:52,19/Dec/25 04:15,11/Apr/23 12:16,0.6.0,0.7.0,,0.8.0,,Interpreter,Server,,,,,,,,,0,,,,,,"We recently upgraded from Livy 0.4 to Livy 0.6 and started to run out of FDs (we fire up thousands of InteractiveSessions a day). Looking at the open FDs, they were all DIR handles for `/usr/lib/spark/python/lib`.

I believe this change was introduced in https://github.com/apache/incubator-livy/commit/1bbefe601641aa4db74dbed1f7faf458b9a70a63#diff-7649a51ad4bddc91b6f1038e06479d41R259-R264

The behavior without this change and the default config is Livy will open 2 FDs for each InteractiveSession, and they will never be closed (I think because `DirectoryStream` is 
`AutoCloseable` and try-with-resource doesn't work with Scala's `Try`.

As a workaround, in our production clusters, we have set `livy.rsc.pyspark.archives` to an empty string in `livy.conf`",,"cooper6581 opened a new pull request #291:
URL: https://github.com/apache/incubator-livy/pull/291


   ## What changes were proposed in this pull request?
   
   Wrap calls to `Files.newDirectoryStream` with `usingResource` from the Utils package in 
   `InteractiveSession` and `PythonInterpreter`
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-763
   
   We recently upgraded from Livy 0.4 to Livy 0.6 and started to run out of FDs (we fire up thousands of InteractiveSessions a day).  Looking at the open FDs, they were all DIR handles for `/usr/lib/spark/python/lib`.
   
   I believe this change was introduced in https://github.com/apache/incubator-livy/commit/1bbefe601641aa4db74dbed1f7faf458b9a70a63#diff-7649a51ad4bddc91b6f1038e06479d41R259-R264
   
   The behavior without this change and the default config is Livy will open 2 FDs for each InteractiveSession, and they will never be closed (I think because `DirectoryStream` is 
   `AutoCloseable` and try-with-resource doesn't work with Scala's `Try`.
   
   As a workaround, in our production clusters, we have set `livy.rsc.pyspark.archives` to an empty string in `livy.conf`
   
   ## How was this patch tested?
   
   Ran the functions in a scala repl and monitored the FDs using lsof.  Ran them again with `usingResource` and verified no FDs leak.  Ran unit tests.
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Apr/20 22:03;githubbot;600","RajshekharMuchandi opened a new pull request, #397:
URL: https://github.com/apache/incubator-livy/pull/397

   ## What changes were proposed in this pull request?
   
   Took separate handle for directory stream. Closed directory stream handle in finally block. Code changes done in `InteractiveSession.scala`.
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-763
   
   ## How was this patch tested?
   
   Ran `lsof | grep python` command to see the Directory FDs for `{SPARK_HOME}/python/lib`. Tested by creating interactive session for pyspark and verified FDs are not created.
   
   


;30/Mar/23 14:20;githubbot;600","gyogal commented on code in PR #397:
URL: https://github.com/apache/incubator-livy/pull/397#discussion_r1156047595


##########
server/src/main/scala/org/apache/livy/server/interactive/InteractiveSession.scala:
##########
@@ -259,21 +256,27 @@ object InteractiveSession extends Logging {
           sys.env.get(""SPARK_HOME"") .map { case sparkHome =>
             val pyLibPath = Seq(sparkHome, ""python"", ""lib"").mkString(File.separator)
             val pyArchivesFile = new File(pyLibPath, ""pyspark.zip"")
-            val py4jFile = Try {
-              Files.newDirectoryStream(Paths.get(pyLibPath), ""py4j-*-src.zip"")
-                .iterator()
+            var py4jZip : DirectoryStream[java.nio.file.Path] = null;
+            var py4jFile : File = null;

Review Comment:
   Could you please remove the extra spaces from the above two lines as well (after the variable name)?



;03/Apr/23 14:30;githubbot;600","gyogal merged PR #397:
URL: https://github.com/apache/incubator-livy/pull/397


;11/Apr/23 12:12;githubbot;600","gyogal commented on PR #397:
URL: https://github.com/apache/incubator-livy/pull/397#issuecomment-1503225859

   Thank you for your contribution, @RajshekharMuchandi!


;11/Apr/23 12:18;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 31 15:26:30 UTC 2023,,,,,,,,,,"0|z0e1zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Jun/20 14:41;lakunma;-It is actually 'livy.pyspark.archives' in livy.conf (so not in livy-client.conf, and not livy.rsc.pyspark.archives option)-

Now, i'm suddenly not sure about that. It seems that indeed what is suggested in summary works. But interesting question is why that option is not in the template of livy.conf (but inside livy-client.conf).;;;","19/Jun/20 16:45;cooper6581;Yeah, I ended up going through the same thing.Â  Fwiw, this is the setting that ended up doing the trick for the Livy that is shipped with EMR 5.29 (0.6.0);;;","25/Sep/20 06:27;amitkin;I am using EMR 5.30.0 which usesÂ Livy 0.7.0 and still see this issue. I used the workaround and looks like it is working.;;;","12/Nov/22 22:10;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;","31/Mar/23 15:26;gyogal;A PR is available at https://github.com/apache/incubator-livy/pull/397;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""mvn package"" build failure NullPointerException",LIVY-762,13300716,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,,mzhang,mzhang,24/Apr/20 05:55,24/Apr/20 19:18,19/Dec/25 04:15,24/Apr/20 19:18,0.7.0,,,,,Build,,,,,,,,,,0,,,,,,"Newbie problem.

Â 

I tried running `mvn package` but gotÂ NullPointerException

Â 

{{[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile (scala-compile-first) on project livy-api: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile failed.: NullPointerException -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile (scala-compile-first) on project livy-api: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile failed.
 at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)
 at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
 at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
 at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
 at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
 at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
 at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
 at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
 at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
 at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
 at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)
 at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)
 at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)
 at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
 at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
 at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke (Method.java:567)
 at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
 at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
 at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
 at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
Caused by: org.apache.maven.plugin.PluginExecutionException: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.2.2:compile failed.
 at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:148)
 at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
 at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
 at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
 at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
 at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
 at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
 at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
 at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
 at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
 at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
 at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)
 at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)
 at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)
 at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
 at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
 at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke (Method.java:567)
 at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
 at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
 at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
 at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
Caused by: java.lang.NullPointerException
 at java.util.regex.Matcher.getTextLength (Matcher.java:1770)
 at java.util.regex.Matcher.reset (Matcher.java:416)
 at java.util.regex.Matcher.<init> (Matcher.java:253)
 at java.util.regex.Pattern.matcher (Pattern.java:1134)
 at java.util.regex.Pattern.split (Pattern.java:1262)
 at java.util.regex.Pattern.split (Pattern.java:1335)
 at sbt.IO$.pathSplit (IO.scala:727)
 at sbt.IO$.parseClasspath (IO.scala:825)
 at sbt.compiler.CompilerArguments.extClasspath (CompilerArguments.scala:64)
 at sbt.compiler.AggressiveCompile.withBootclasspath (AggressiveCompile.scala:51)
 at sbt.compiler.AggressiveCompile.compile2 (AggressiveCompile.scala:84)
 at sbt.compiler.AggressiveCompile.compile1 (AggressiveCompile.scala:71)
 at com.typesafe.zinc.Compiler.compile (Compiler.scala:184)
 at com.typesafe.zinc.Compiler.compile (Compiler.scala:164)
 at sbt_inc.SbtIncrementalCompiler.compile (SbtIncrementalCompiler.java:92)
 at scala_maven.ScalaCompilerSupport.incrementalCompile (ScalaCompilerSupport.java:303)
 at scala_maven.ScalaCompilerSupport.compile (ScalaCompilerSupport.java:119)
 at scala_maven.ScalaCompilerSupport.doExecute (ScalaCompilerSupport.java:99)
 at scala_maven.ScalaMojoSupport.execute (ScalaMojoSupport.java:482)
 at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
 at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
 at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
 at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
 at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
 at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
 at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
 at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
 at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
 at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
 at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
 at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)
 at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)
 at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)
 at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
 at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
 at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke (Method.java:567)
 at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
 at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
 at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
 at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
[ERROR] }}

Â 

Â ","$ mvn -v
Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)
Maven home: /usr/local/Cellar/maven/3.6.3_1/libexec
Java version: 13.0.2, vendor: N/A, runtime: /usr/local/Cellar/openjdk/13.0.2+8_2/libexec/openjdk.jdk/Contents/Home
Default locale: en_US, platform encoding: UTF-8
OS name: ""mac os x"", version: ""10.15.3"", arch: ""x86_64"", family: ""mac""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 24 19:18:07 UTC 2020,,,,,,,,,,"0|z0e0vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Apr/20 19:17;mzhang;The error is gone when using jdk 8;;;","24/Apr/20 19:18;mzhang;Use JDK 8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add spark 3 support,LIVY-756,13296038,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tprelle-ubi,tprelle-ubi,tprelle-ubi,03/Apr/20 14:19,18/Jan/22 14:27,19/Dec/25 04:15,02/Jul/20 07:50,,,,0.8.0,,,,,,,,,,,,0,,,,,,"Spark 3 will be release soon.
A support of spark 3 will be nice.",,"tprelle-ubi commented on pull request #289: [WIP] [LIVY-756] Add Support Spark 3
URL: https://github.com/apache/incubator-livy/pull/289
 
 
   ## What changes were proposed in this pull request?
   
   Add spark 3.0 support to livy with scala 2.12 and Python 3
   Fix also https://issues.apache.org/jira/browse/LIVY-423 
   
   ## How was this patch tested?
   Scala and Python 3 test work
   I'm not able yet to make python 2 and sparkR work for spark 3 yet.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Apr/20 22:21;githubbot;600","andrasbeni commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r434684045



##########
File path: api/src/main/java/org/apache/livy/JobContext.java
##########
@@ -41,12 +40,12 @@
   /**
    * @return The shared SQLContext instance.
    */
-  SQLContext sqlctx();
+  SQLContext sqlctx() throws Exception ;
 
   /**
-   * @return The shared HiveContext instance.
+   * @return The shared hive SQLContext instance.
    */
-  HiveContext hivectx();

Review comment:
       This will break compatibility to those who keep using Livy with Spark 2.4.
   I'd recommend to maintain two versions of this file and adding the correct version to compile sources based on profiles. 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jun/20 16:44;githubbot;600","tprelle commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r434725675



##########
File path: api/src/main/java/org/apache/livy/JobContext.java
##########
@@ -41,12 +40,12 @@
   /**
    * @return The shared SQLContext instance.
    */
-  SQLContext sqlctx();
+  SQLContext sqlctx() throws Exception ;
 
   /**
-   * @return The shared HiveContext instance.
+   * @return The shared hive SQLContext instance.
    */
-  HiveContext hivectx();

Review comment:
       Hi, @andrasbeni thanks for the review, I do the code in december with spark 3.0.0-preview2, I just check on spark and they push back they class in https://issues.apache.org/jira/browse/SPARK-31088, I will make the change back when they will released spark 3.0.0




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jun/20 17:11;githubbot;600","Minutis commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-646481118


   Spark 3 was released. https://spark.apache.org/downloads.html, please, continue :+1: 


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jun/20 07:20;githubbot;600","zjffdu commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r442677351



##########
File path: assembly/pom.xml
##########
@@ -29,7 +29,7 @@
   <packaging>pom</packaging>
 
   <properties>
-    <assembly.name>apache-livy-${project.version}-bin</assembly.name>
+    <assembly.name>apache-livy-${project.version}-bin-${scala.binary.version}</assembly.name>

Review comment:
       to  `_${scala.binary.version}` ?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jun/20 07:27;githubbot;600","zjffdu commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r442677351



##########
File path: assembly/pom.xml
##########
@@ -29,7 +29,7 @@
   <packaging>pom</packaging>
 
   <properties>
-    <assembly.name>apache-livy-${project.version}-bin</assembly.name>
+    <assembly.name>apache-livy-${project.version}-bin-${scala.binary.version}</assembly.name>

Review comment:
       Use `_${scala.binary.version}`  to align with convention 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jun/20 07:27;githubbot;600","tprelle commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-646867581


   > Spark 3 was released. https://spark.apache.org/downloads.html, please, continue ðŸ‘
   
   Move to spark 3.0.0.
   Do not have time yet to test spark R


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jun/20 21:31;githubbot;600","jerryshao commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-647246039


   @tprelle is this PR ready for review?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jun/20 03:14;githubbot;600","jerryshao commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-648101189


   I'm going to try this PR locally to see if the current code is complete or not.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jun/20 12:02;githubbot;600","tprelle commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-648102517


   @jerryshao the code it's working for spark scala and python but I do not managed to make it work for sparkR.
   
   I can submit the PR and open a ticket for sparkR support for spark3 if it's ok for the community


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jun/20 12:05;githubbot;600","jerryshao commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-648103728


   Yes, I think it is fine to leave R support to another PR. Please make sure this PR is OK for Scala and Python part.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jun/20 12:08;githubbot;600","tprelle commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-648289289


   All unit and intregration scala and Python test are working.
   I'm currently redeploy it to make more integration test because of https://issues.apache.org/jira/browse/SPARK-29748 (the biggest change I spot for livy between 3.0.0-preview2 version en 3.0.0 version.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jun/20 16:57;githubbot;600","jerryshao commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-648547180


   I just did a quick review about the overall implementation. Seems like we can only support one Scala version for one build here  in this PR. But Livy could actually support different versions of Scala in one build (we used to support Scala 2.10 and 2.11 for one build). So I think we should have both Scala 2.12 and 2.11 support, and let Livy to choose jars automatically based on the Spark.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 02:40;githubbot;600","jerryshao commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r444613917



##########
File path: client-http/pom.xml
##########
@@ -109,58 +109,6 @@
 
   <build>
     <plugins>
-      <plugin>

Review comment:
       What's the reason to remove this?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 02:45;githubbot;600","jerryshao commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r444614548



##########
File path: .travis.yml
##########
@@ -34,6 +34,12 @@ matrix:
     env: MVN_FLAG='-Pspark-2.4 -Pthriftserver -DskipITs'
   - name: ""Spark 2.4 ITs""
     env: MVN_FLAG='-Pspark-2.4 -Pthriftserver -DskipTests'
+  - name: ""Spark 3.0 Unit Tests""
+    env: MVN_FLAG='-Pthriftserver -Pspark-3.0 -DskipITs'
+  - name: ""Spark 3.0 ITs""
+    env: 
+      - MVN_FLAG='-Pthriftserver -Pspark-3.0 -DskipTests'
+      - PYSPARK_ROW_FIELD_SORTING_ENABLED=true

Review comment:
       What is the purpose of setting this?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 02:48;githubbot;600","jerryshao commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r444618359



##########
File path: .travis.yml
##########
@@ -34,6 +34,12 @@ matrix:
     env: MVN_FLAG='-Pspark-2.4 -Pthriftserver -DskipITs'
   - name: ""Spark 2.4 ITs""
     env: MVN_FLAG='-Pspark-2.4 -Pthriftserver -DskipTests'
+  - name: ""Spark 3.0 Unit Tests""
+    env: MVN_FLAG='-Pthriftserver -Pspark-3.0 -DskipITs'
+  - name: ""Spark 3.0 ITs""
+    env: 
+      - MVN_FLAG='-Pthriftserver -Pspark-3.0 -DskipTests'
+      - PYSPARK_ROW_FIELD_SORTING_ENABLED=true

Review comment:
       Got it, seems an incompatible change in Spark 3.0




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 03:03;githubbot;600","jerryshao commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r444628517



##########
File path: pom.xml
##########
@@ -86,24 +87,25 @@
     <commons-codec.version>1.9</commons-codec.version>
     <httpclient.version>4.5.3</httpclient.version>
     <httpcore.version>4.4.4</httpcore.version>
-    <jackson.version>2.9.9</jackson.version>
+    <jackson.version>2.10.1</jackson.version>
     <javax.servlet-api.version>3.1.0</javax.servlet-api.version>
     <jetty.version>9.3.24.v20180605</jetty.version>
     <json4s.version>3.2.11</json4s.version>
     <junit.version>4.11</junit.version>
     <libthrift.version>0.9.3</libthrift.version>
-    <kryo.version>2.22</kryo.version>
+    <kryo.version>4.0.2</kryo.version>
     <metrics.version>3.1.0</metrics.version>
-    <mockito.version>1.9.5</mockito.version>
+    <mockito.version>1.10.19</mockito.version>
     <netty.spark-2.11.version>4.0.37.Final</netty.spark-2.11.version>
     <netty.version>${netty.spark-2.11.version}</netty.version>
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
     <py4j.version>0.10.7</py4j.version>
     <scala-2.11.version>2.11.12</scala-2.11.version>
+    <scala-2.12.version>2.12.10</scala-2.12.version>
     <scala.binary.version>2.11</scala.binary.version>
     <scala.version>${scala-2.11.version}</scala.version>
-    <scalatest.version>2.2.4</scalatest.version>
-    <scalatra.version>2.3.0</scalatra.version>
+    <scalatest.version>3.0.8</scalatest.version>
+    <scalatra.version>2.6.5</scalatra.version>

Review comment:
       Are this artifacts upgrading necessary for Spark 3.0 support?

##########
File path: pom.xml
##########
@@ -86,24 +87,25 @@
     <commons-codec.version>1.9</commons-codec.version>
     <httpclient.version>4.5.3</httpclient.version>
     <httpcore.version>4.4.4</httpcore.version>
-    <jackson.version>2.9.9</jackson.version>
+    <jackson.version>2.10.1</jackson.version>
     <javax.servlet-api.version>3.1.0</javax.servlet-api.version>
     <jetty.version>9.3.24.v20180605</jetty.version>
     <json4s.version>3.2.11</json4s.version>
     <junit.version>4.11</junit.version>
     <libthrift.version>0.9.3</libthrift.version>
-    <kryo.version>2.22</kryo.version>
+    <kryo.version>4.0.2</kryo.version>
     <metrics.version>3.1.0</metrics.version>
-    <mockito.version>1.9.5</mockito.version>
+    <mockito.version>1.10.19</mockito.version>
     <netty.spark-2.11.version>4.0.37.Final</netty.spark-2.11.version>
     <netty.version>${netty.spark-2.11.version}</netty.version>
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
     <py4j.version>0.10.7</py4j.version>
     <scala-2.11.version>2.11.12</scala-2.11.version>
+    <scala-2.12.version>2.12.10</scala-2.12.version>
     <scala.binary.version>2.11</scala.binary.version>
     <scala.version>${scala-2.11.version}</scala.version>
-    <scalatest.version>2.2.4</scalatest.version>
-    <scalatra.version>2.3.0</scalatra.version>
+    <scalatest.version>3.0.8</scalatest.version>
+    <scalatra.version>2.6.5</scalatra.version>

Review comment:
       Are these artifacts upgrading necessary for Spark 3.0 support?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 03:48;githubbot;600","andrasbeni commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r444719602



##########
File path: api/src/main/java/org/apache/livy/JobContext.java
##########
@@ -41,12 +41,12 @@
   /**
    * @return The shared SQLContext instance.
    */
-  SQLContext sqlctx();
+  SQLContext sqlctx() throws Exception ;

Review comment:
       I believe adding throws clauses to these methods could break Java client applications. I suggest to wrap exceptions that are not RuntimeException in a RuntimeException instead.

##########
File path: pom.xml
##########
@@ -86,24 +87,25 @@
     <commons-codec.version>1.9</commons-codec.version>
     <httpclient.version>4.5.3</httpclient.version>
     <httpcore.version>4.4.4</httpcore.version>
-    <jackson.version>2.9.9</jackson.version>
+    <jackson.version>2.10.1</jackson.version>
     <javax.servlet-api.version>3.1.0</javax.servlet-api.version>
     <jetty.version>9.3.24.v20180605</jetty.version>
     <json4s.version>3.2.11</json4s.version>
     <junit.version>4.11</junit.version>
     <libthrift.version>0.9.3</libthrift.version>
-    <kryo.version>2.22</kryo.version>
+    <kryo.version>4.0.2</kryo.version>
     <metrics.version>3.1.0</metrics.version>
-    <mockito.version>1.9.5</mockito.version>
+    <mockito.version>1.10.19</mockito.version>
     <netty.spark-2.11.version>4.0.37.Final</netty.spark-2.11.version>
     <netty.version>${netty.spark-2.11.version}</netty.version>
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
     <py4j.version>0.10.7</py4j.version>
     <scala-2.11.version>2.11.12</scala-2.11.version>
+    <scala-2.12.version>2.12.10</scala-2.12.version>
     <scala.binary.version>2.11</scala.binary.version>
     <scala.version>${scala-2.11.version}</scala.version>
-    <scalatest.version>2.2.4</scalatest.version>
-    <scalatra.version>2.3.0</scalatra.version>
+    <scalatest.version>3.0.8</scalatest.version>
+    <scalatra.version>2.6.5</scalatra.version>

Review comment:
       Scalatra added scala 2.12 support in version 2.5.0 and scalatest in 3.0.0. So the version currently in use are incompatibel with Spark 3.0's Scala 2.12.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 08:13;githubbot;600","andrasbeni commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r444717000



##########
File path: pom.xml
##########
@@ -86,24 +87,25 @@
     <commons-codec.version>1.9</commons-codec.version>
     <httpclient.version>4.5.3</httpclient.version>
     <httpcore.version>4.4.4</httpcore.version>
-    <jackson.version>2.9.9</jackson.version>
+    <jackson.version>2.10.1</jackson.version>
     <javax.servlet-api.version>3.1.0</javax.servlet-api.version>
     <jetty.version>9.3.24.v20180605</jetty.version>
     <json4s.version>3.2.11</json4s.version>
     <junit.version>4.11</junit.version>
     <libthrift.version>0.9.3</libthrift.version>
-    <kryo.version>2.22</kryo.version>
+    <kryo.version>4.0.2</kryo.version>
     <metrics.version>3.1.0</metrics.version>
-    <mockito.version>1.9.5</mockito.version>
+    <mockito.version>1.10.19</mockito.version>
     <netty.spark-2.11.version>4.0.37.Final</netty.spark-2.11.version>
     <netty.version>${netty.spark-2.11.version}</netty.version>
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
     <py4j.version>0.10.7</py4j.version>
     <scala-2.11.version>2.11.12</scala-2.11.version>
+    <scala-2.12.version>2.12.10</scala-2.12.version>
     <scala.binary.version>2.11</scala.binary.version>
     <scala.version>${scala-2.11.version}</scala.version>
-    <scalatest.version>2.2.4</scalatest.version>
-    <scalatra.version>2.3.0</scalatra.version>
+    <scalatest.version>3.0.8</scalatest.version>
+    <scalatra.version>2.6.5</scalatra.version>

Review comment:
       Scalatra added scala 2.12 support in version 2.5.0 and scalatest in 3.0.0. So the version currently in use are incompatible with Spark 3.0's Scala 2.12.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 08:13;githubbot;600","jerryshao commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r444770427



##########
File path: pom.xml
##########
@@ -86,24 +87,25 @@
     <commons-codec.version>1.9</commons-codec.version>
     <httpclient.version>4.5.3</httpclient.version>
     <httpcore.version>4.4.4</httpcore.version>
-    <jackson.version>2.9.9</jackson.version>
+    <jackson.version>2.10.1</jackson.version>
     <javax.servlet-api.version>3.1.0</javax.servlet-api.version>
     <jetty.version>9.3.24.v20180605</jetty.version>
     <json4s.version>3.2.11</json4s.version>
     <junit.version>4.11</junit.version>
     <libthrift.version>0.9.3</libthrift.version>
-    <kryo.version>2.22</kryo.version>
+    <kryo.version>4.0.2</kryo.version>
     <metrics.version>3.1.0</metrics.version>
-    <mockito.version>1.9.5</mockito.version>
+    <mockito.version>1.10.19</mockito.version>
     <netty.spark-2.11.version>4.0.37.Final</netty.spark-2.11.version>
     <netty.version>${netty.spark-2.11.version}</netty.version>
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
     <py4j.version>0.10.7</py4j.version>
     <scala-2.11.version>2.11.12</scala-2.11.version>
+    <scala-2.12.version>2.12.10</scala-2.12.version>
     <scala.binary.version>2.11</scala.binary.version>
     <scala.version>${scala-2.11.version}</scala.version>
-    <scalatest.version>2.2.4</scalatest.version>
-    <scalatra.version>2.3.0</scalatra.version>
+    <scalatest.version>3.0.8</scalatest.version>
+    <scalatra.version>2.6.5</scalatra.version>

Review comment:
       Scalatra is only used by Livy Server, we don't have to upgrade Livy Server's Scala version to 2.12, we could leave the Livy Server Scala support to another JIRA.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 09:37;githubbot;600","jerryshao commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r444770427



##########
File path: pom.xml
##########
@@ -86,24 +87,25 @@
     <commons-codec.version>1.9</commons-codec.version>
     <httpclient.version>4.5.3</httpclient.version>
     <httpcore.version>4.4.4</httpcore.version>
-    <jackson.version>2.9.9</jackson.version>
+    <jackson.version>2.10.1</jackson.version>
     <javax.servlet-api.version>3.1.0</javax.servlet-api.version>
     <jetty.version>9.3.24.v20180605</jetty.version>
     <json4s.version>3.2.11</json4s.version>
     <junit.version>4.11</junit.version>
     <libthrift.version>0.9.3</libthrift.version>
-    <kryo.version>2.22</kryo.version>
+    <kryo.version>4.0.2</kryo.version>
     <metrics.version>3.1.0</metrics.version>
-    <mockito.version>1.9.5</mockito.version>
+    <mockito.version>1.10.19</mockito.version>
     <netty.spark-2.11.version>4.0.37.Final</netty.spark-2.11.version>
     <netty.version>${netty.spark-2.11.version}</netty.version>
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
     <py4j.version>0.10.7</py4j.version>
     <scala-2.11.version>2.11.12</scala-2.11.version>
+    <scala-2.12.version>2.12.10</scala-2.12.version>
     <scala.binary.version>2.11</scala.binary.version>
     <scala.version>${scala-2.11.version}</scala.version>
-    <scalatest.version>2.2.4</scalatest.version>
-    <scalatra.version>2.3.0</scalatra.version>
+    <scalatest.version>3.0.8</scalatest.version>
+    <scalatra.version>2.6.5</scalatra.version>

Review comment:
       Scalatra is only used by Livy Server, we don't have to upgrade Livy Server's Scala version to 2.12, we could leave the Livy Server Scala upgrade to another JIRA.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 09:37;githubbot;600","jerryshao commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r444774680



##########
File path: api/src/main/java/org/apache/livy/JobContext.java
##########
@@ -41,12 +41,12 @@
   /**
    * @return The shared SQLContext instance.
    */
-  SQLContext sqlctx();
+  SQLContext sqlctx() throws Exception ;

Review comment:
       Yes, I have the same concern about changing the signature of this interface.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 09:44;githubbot;600","jerryshao commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-648718274


   I'm doing some local improvements based on @tprelle 's current PR, I can submit a PR when everything is ready.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 09:49;githubbot;600","tprelle commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r444876495



##########
File path: api/src/main/java/org/apache/livy/JobContext.java
##########
@@ -41,12 +41,12 @@
   /**
    * @return The shared SQLContext instance.
    */
-  SQLContext sqlctx();
+  SQLContext sqlctx() throws Exception ;

Review comment:
       It was before https://issues.apache.org/jira/browse/SPARK-31088, when they put back HiveContext class back to spark.
   When I revert the change I forgot to revert also the method signature.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 13:03;githubbot;600","tprelle commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r444880828



##########
File path: client-http/pom.xml
##########
@@ -109,58 +109,6 @@
 
   <build>
     <plugins>
-      <plugin>

Review comment:
       It was a mistake, i will put it back




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 13:10;githubbot;600","tprelle commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-648813898


   > I just did a quick review about the overall implementation. Seems like we can only support one Scala version for one build here in this PR. But Livy could actually support different versions of Scala in one build (we used to support Scala 2.10 and 2.11 for one build). So I think we should have both Scala 2.12 and 2.11 support, and let Livy to choose jars automatically based on the Spark.
   
   I like the idea, I try it, but I was not able to make it work.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 13:17;githubbot;600","tprelle commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r444891736



##########
File path: pom.xml
##########
@@ -86,24 +87,25 @@
     <commons-codec.version>1.9</commons-codec.version>
     <httpclient.version>4.5.3</httpclient.version>
     <httpcore.version>4.4.4</httpcore.version>
-    <jackson.version>2.9.9</jackson.version>
+    <jackson.version>2.10.1</jackson.version>
     <javax.servlet-api.version>3.1.0</javax.servlet-api.version>
     <jetty.version>9.3.24.v20180605</jetty.version>
     <json4s.version>3.2.11</json4s.version>
     <junit.version>4.11</junit.version>
     <libthrift.version>0.9.3</libthrift.version>
-    <kryo.version>2.22</kryo.version>
+    <kryo.version>4.0.2</kryo.version>
     <metrics.version>3.1.0</metrics.version>
-    <mockito.version>1.9.5</mockito.version>
+    <mockito.version>1.10.19</mockito.version>
     <netty.spark-2.11.version>4.0.37.Final</netty.spark-2.11.version>
     <netty.version>${netty.spark-2.11.version}</netty.version>
     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
     <py4j.version>0.10.7</py4j.version>
     <scala-2.11.version>2.11.12</scala-2.11.version>
+    <scala-2.12.version>2.12.10</scala-2.12.version>
     <scala.binary.version>2.11</scala.binary.version>
     <scala.version>${scala-2.11.version}</scala.version>
-    <scalatest.version>2.2.4</scalatest.version>
-    <scalatra.version>2.3.0</scalatra.version>
+    <scalatest.version>3.0.8</scalatest.version>
+    <scalatra.version>2.6.5</scalatra.version>

Review comment:
       I try to limit the number of version change, but because of scala 2.12 like @andrasbeni says I need to use scalatest 3.0.8 who are some breaking change like  org.scalatest.Suite =Â­>org.scalatest.TestSuite, so to do not copy the class I need to change also the global version of scalatest. So I need to change the version of scalatra because of the incompatibility between scalatest 3.0.8 and scalatra-scalatest 2.3.0. 
   As I was working and easier to move forward with these upgrade of version I choose to submit like this.
   




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jun/20 13:27;githubbot;600","mmigdiso commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-650323339


   Hey @tprelle , will @jerryshao comments be a blocker for that PR?  


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jun/20 18:14;githubbot;600","jerryshao commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r446593365



##########
File path: scala-api/src/main/scala/org/apache/livy/scalaapi/ScalaJobHandle.scala
##########
@@ -190,6 +190,19 @@ class ScalaJobHandle[T] private[livy] (jobHandle: JobHandle[T]) extends Future[T
     getJavaFutureResult(jobHandle, atMost)
     this
   }
+
+  // These two methods must be implemented in Scala 2.12. They're implemented as a no-op here

Review comment:
       Would you please elaborate why the below two methods are needed for Scala 2.12?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Jun/20 03:06;githubbot;600","andrasbeni commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r446670999



##########
File path: scala-api/src/main/scala/org/apache/livy/scalaapi/ScalaJobHandle.scala
##########
@@ -190,6 +190,19 @@ class ScalaJobHandle[T] private[livy] (jobHandle: JobHandle[T]) extends Future[T
     getJavaFutureResult(jobHandle, atMost)
     this
   }
+
+  // These two methods must be implemented in Scala 2.12. They're implemented as a no-op here

Review comment:
       In Scala 2.12 these two new methods ([see source](https://github.com/scala/scala/blob/v2.12.10/src/library/scala/concurrent/Future.scala#L248-L268 )) were added to the trait `Future`.  As a result, `ScalaJobHandle`, which extends Future needs to implement them.
   
   At the same time I am not sure what @tprelle means by ""the two subclasses below"". AFAICT `ScalaJobHandle` does not have any subclasses. 
   Also, I wouldn't say throwing an exception is a no-op. I suggest ""They are implemented as unsupported operations here"" if they are really not going to be [implemented](https://github.com/scala/scala/blob/v2.12.10/src/library/scala/concurrent/impl/Promise.scala#L31-L48).




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Jun/20 16:33;githubbot;600","jerryshao commented on a change in pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#discussion_r446820287



##########
File path: scala-api/src/main/scala/org/apache/livy/scalaapi/ScalaJobHandle.scala
##########
@@ -190,6 +190,19 @@ class ScalaJobHandle[T] private[livy] (jobHandle: JobHandle[T]) extends Future[T
     getJavaFutureResult(jobHandle, atMost)
     this
   }
+
+  // These two methods must be implemented in Scala 2.12. They're implemented as a no-op here

Review comment:
       Yes, the comments seems confusing.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jun/20 07:19;githubbot;600","jerryshao opened a new pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300


   ## What changes were proposed in this pull request?
   
   This PR is based @tprelle 's PR #289 , and address all the left issues in that PR:
   
   1. multi-scala version support in one build (Scala 2.11 and 2.12 support).
   2. make SparkR work.
   
   Also reverts most of the unnecessary changes. Besides this PR remove the build below 2.4 (2.2, 2.3), since Spark 2.2 and 2.3 only ships with Scala 2.11, hard to maintain multiple version. But user could still use 2.2 and 2.3 without changes.
   
   All credits to @tprelle.
   
   ## How was this patch tested?
   
   Run UT and IT with Spark 2.4.5 and 3.0.0 locally.
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jun/20 13:39;githubbot;600","tprelle commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-651137090


   @jerryshao How you manage to make sparkR work ?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jun/20 13:54;githubbot;600","jerryshao commented on a change in pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#discussion_r447039881



##########
File path: repl/src/main/scala/org/apache/livy/repl/Session.scala
##########
@@ -348,8 +348,10 @@ class Session(
           case ""1"" =>
             (s""""""setJobGroup(sc, ""$jobGroup"", ""Job group for statement $jobGroup"", FALSE)"""""",
              SparkR)
-          case ""2"" =>
+          case ""2"" | ""3"" =>

Review comment:
       @tprelle this should be changed to support SparkR, otherwise it will be hung when running against spark 3.0.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jun/20 15:02;githubbot;600","jerryshao commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-651474672


   Travis image seems too old to support Spark 3.0 R version, we should also upgrade Travis script.
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jun/20 02:05;githubbot;600","jerryshao commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-652203533


   reopen to trigger the test again.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Jul/20 05:41;githubbot;600","jerryshao opened a new pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300


   ## What changes were proposed in this pull request?
   
   This PR is based @tprelle 's PR #289 , and address all the left issues in that PR:
   
   1. multi-scala version support in one build (Scala 2.11 and 2.12 support).
   2. make SparkR work.
   
   Also reverts most of the unnecessary changes. Besides this PR remove the build below 2.4 (2.2, 2.3), since Spark 2.2 and 2.3 only ships with Scala 2.11, hard to maintain multiple version. But user could still use 2.2 and 2.3 without changes.
   
   All credits to @tprelle.
   
   ## How was this patch tested?
   
   Run UT and IT with Spark 2.4.5 and 3.0.0 locally.
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Jul/20 05:41;githubbot;600","jerryshao closed pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Jul/20 05:41;githubbot;600","jerryshao commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-652320407


   @tprelle @andrasbeni would you please help to review, thanks!


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Jul/20 09:57;githubbot;600","jerryshao commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-652839568


   I've triggered the travis test several times, seems everything is fine. I'm going to merge this, thanks @tprelle for the contribution.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jul/20 07:36;githubbot;600","jerryshao closed pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jul/20 07:44;githubbot;600","jerryshao closed pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jul/20 07:53;githubbot;600","jerryshao commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-652848316


   Thanks @tprelle for your work, I've already merged this PR, all credits to you.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Jul/20 07:53;githubbot;600","stczwd commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-657356354


   Glad to see this patch merged. I have test this path, it works well with spark3.0.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Jul/20 04:50;githubbot;600","pdambrauskas commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-663497351


   Hey, are there any plans to release this feature :) ?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jul/20 11:37;githubbot;600","jerryshao edited a comment on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-664907671


   There still have some minor issues in JDBC side, will prepare a release when all the issues are done.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/20 07:21;githubbot;600","giftkugel commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-664894916


   Is there any timeline to add this patch into a release? Maybe Livy 0.8?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/20 07:51;githubbot;600","jerryshao commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-664907671


   There still have some minor issues in JDBC side, we prepare a release when all the issues are done.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/20 07:51;githubbot;600","jerryshao commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-664909277


   There still have some minor issues in JDBC side related to Scala versions. Will prepare a new release when everything is done.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/20 08:00;githubbot;600","hongweijia commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-673302574


   Hi @tprelle
   Has livy already supported Spark 3 with Python 3.6?
   Thanks!


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/20 07:06;githubbot;600","tprelle commented on pull request #289:
URL: https://github.com/apache/incubator-livy/pull/289#issuecomment-673399619


   Hi @hongweijia, never test it with Python 3.6 but it's working well with 3.7,
   
   Regards


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/20 10:32;githubbot;600","tooptoop4 commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-706393343


   @jerryshao will this be released soon?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Oct/20 20:42;githubbot;600","Dom-nik commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-721148882


   @jerryshao Is it possible that you add the instruction to build the Spark 3.0 version of Livy to the README? I'd been struggling with Maven Profiles and didn't get anywhere.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Nov/20 14:28;githubbot;600","Dom-nik edited a comment on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-721148882


   @jerryshao Is it possible that you add the instruction to build the Spark 3.0 version of Livy to the README? I'd been struggling with Maven Profiles and didn't get anywhere (some tests are failing when I try to run `mvn package -P spark-3.0`)


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Nov/20 14:29;githubbot;600","fmarchand commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-731576185


   > @jerryshao Is it possible that you add the instruction to build the Spark 3.0 version of Livy to the README? I'd been struggling with Maven Profiles and didn't get anywhere (some tests are failing when I try to run `mvn package -P spark-3.0`)
   
   There's 3 tests failing in module livy-rsc yes @Dom-nik, I guess we missed something ;)
   ```
   Tests in error: 
     TestSparkClient.testConnectToRunningContext:338->runTest:575->runTest:588 Â» Execution
     TestSparkClient.testImpersonation:321->runTest:575->runTest:588 Â» Execution ja...
     TestSparkClient.testRemoteClient:213->runTest:575->runTest:588 Â» Execution jav...
   ```


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Nov/20 13:01;githubbot;600","redsanket commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-758727710


   @jerryshao Is there any progress with 0.8.0 release?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jan/21 15:21;githubbot;600","derSascha commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-771916664


   The changes introduced here upgrade Kryo from 2.22 to 4.0.2. This change makes the API incompatible. The latest AWS EMR release (6.2.0) includes Spark 3 and Livy 0.7.0 together with Kryo 2.22 and works fine. Can we revert the Kryo upgrade before the Livy 0.8.0 release to keep the API compatible?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Feb/21 19:31;githubbot;600","derSascha commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-771916664


   The changes introduced here upgrade Kryo from 2.22 to 4.0.2. This change makes the API incompatible. The latest AWS EMR release (6.2.0) includes Spark 3 and Livy 0.7.0 together with Kryo 2.22 and works fine. Can we revert the Kryo upgrade before the Livy 0.8.0 release to keep the API compatible?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Feb/21 01:24;githubbot;600","pnayek commented on pull request #300:
URL: https://github.com/apache/incubator-livy/pull/300#issuecomment-899837371


   Hi @jerryshao I have a Cloudera CDP Private Cloud 7.1.6 Hadoop system. As CDP Private Cloud 7.1.6 supports Spark 3.1 along with Spark 2.4, I have separately installed Spark version 3.1.1.3.1.7280.2-11 on the system (as per Cloudera document https://docs.cloudera.com/cdp-private-cloud-base/7.1.6/cds-3/topics/spark-spark-3-overview.html). I am trying to use Livy (version livy-0.7.1-incubating) to connect to Spark 3.1.1.3.1.7280.2-11. Does livy-0.7.1 support Spark version 3.1? If so, are there any requirements that need to be followed?


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/21 21:42;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,35400,,,0,35400,,,,,,,,,,,,,,,,,,,,,,,,BIGTOP-3625,,,LIVY-776,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 02 07:47:55 UTC 2020,,,,,,,,,,"0|z0d94g:",9223372036854775807,,,,,,,,,,,,,,,,,,,"02/Jul/20 07:47;jerryshao;Issue resolved by pull request 300
https://github.com/apache/incubator-livy/pull/300;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
precision and scale are not encoded in decimal type,LIVY-754,13295667,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,wypoon,wypoon,wypoon,02/Apr/20 01:27,20/May/20 17:39,19/Dec/25 04:15,20/May/20 17:39,0.7.0,,,0.8.0,,Thriftserver,,,,,,,,,,0,,,,,,"The Livy Thrift server support for decimal type in 0.7 is inadequate.
Before LIVY-699, decimal is mapped to the catch-all string type. With LIVY-699, decimal is mapped to a decimal type that is inadequate in that it does not encode the precision and scale. The type in Livy is represented by a BasicDataType case class which contains a String field, name; and a DataType (an enum) field, dataType. In the case of decimal, the dataType is DataType.DECIMAL. The precision and scale of the decimal is not encoded.
When the DataType is converted to a TTypeDesc for sending a Thrift response to a client request for result set metadata, the TTypeDesc contains a TPrimitiveTypeEntry(TTypeId.DECIMAL_TYPE) without TTypeQualifiers (which are needed to capture the precision and scale). This results in problems for clients. E.g., if we connect to the Thrift server in beeline and do a select from a table with column of decimal type, we get
{noformat}
java.lang.NullPointerException
	at org.apache.hive.jdbc.JdbcColumn.columnPrecision(JdbcColumn.java:310)
	at org.apache.hive.jdbc.JdbcColumn.columnDisplaySize(JdbcColumn.java:262)
	at org.apache.hive.jdbc.HiveResultSetMetaData.getColumnDisplaySize(HiveResultSetMetaData.java:63)
	at org.apache.hive.beeline.IncrementalRows.<init>(IncrementalRows.java:57)
	at org.apache.hive.beeline.IncrementalRowsWithNormalization.<init>(IncrementalRowsWithNormalization.java:47)
	at org.apache.hive.beeline.BeeLine.print(BeeLine.java:2322)
	at org.apache.hive.beeline.Commands.executeInternal(Commands.java:1026)
	at org.apache.hive.beeline.Commands.execute(Commands.java:1215)
	at org.apache.hive.beeline.Commands.sql(Commands.java:1144)
	at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:1497)
	at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:1355)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1134)
	at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:1082)
	at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:546)
	at org.apache.hive.beeline.BeeLine.main(BeeLine.java:528)
{noformat}
Note: You have to use ""--verbose"" with beeline to see the stack trace for the NPE.
",,"wypoon commented on pull request #288: [LIVY-754][THRIFT] Encode precision and decimal for decimal type.
URL: https://github.com/apache/incubator-livy/pull/288
 
 
   ## What changes were proposed in this pull request?
   
   When a `org.apache.livy.thriftserver.session.DataType` is converted to a `org.apache.hive.service.rpc.thrift.TTypeDesc` for sending a Thrift response to a client request for result set metadata, the `TTypeDesc` contains a `TPrimitiveTypeEntry(TTypeId.DECIMAL_TYPE)` without `TTypeQualifiers` (which are needed to capture the precision and scale). 
   With this change, we include the qualifiers in the `TPrimitiveTypeEntry`. We use both the name and the `DataType` of a field type to construct the `TTypeDesc`. We are able to do this without changing the existing internal representation for data types because we can obtain the precision and scale from the name of the decimal type.
   
   ## How was this patch tested?
   
   Use beeline to connect to the Thrift server. Do a select from a table with a column of decimal type.
   Also extended an existing integration test.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Apr/20 01:50;githubbot;600","wypoon commented on pull request #288:
URL: https://github.com/apache/incubator-livy/pull/288#issuecomment-631224025


   @mgaido91 can you please merge this (since you have already approved it)?
   I thought it was already merged, but it appears that it isn't.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/May/20 04:08;githubbot;600","mgaido91 closed pull request #288:
URL: https://github.com/apache/incubator-livy/pull/288


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/May/20 17:37;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 20 17:39:11 UTC 2020,,,,,,,,,,"0|z0d6u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,"02/Apr/20 01:27;wypoon;I'll create a PR with a fix.;;;","20/May/20 17:39;mgaido;Issue resolved by PR:Â https://github.com/apache/incubator-livy/pull/288.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy TS does not accept any connections when limits are set on connections,LIVY-752,13288470,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,wypoon,wypoon,wypoon,29/Feb/20 22:49,16/Mar/20 16:48,19/Dec/25 04:15,16/Mar/20 16:48,0.7.0,,,0.8.0,,Thriftserver,,,,,,,,,,0,,,,,,"I setÂ livy.server.thrift.limit.connections.per.user=20 on my Livy Server. When I try to connect to it, I get
{noformat}
2020-02-28 17:13:30,443 WARN org.apache.livy.thriftserver.cli.ThriftBinaryCLIService: Error opening session: 
java.lang.NullPointerException
	at org.apache.livy.thriftserver.LivyThriftSessionManager.incrementConnectionsCount(LivyThriftSessionManager.scala:438)
	at org.apache.livy.thriftserver.LivyThriftSessionManager.incrementConnections(LivyThriftSessionManager.scala:425)
	at org.apache.livy.thriftserver.LivyThriftSessionManager.openSession(LivyThriftSessionManager.scala:222)
	at org.apache.livy.thriftserver.LivyCLIService.openSessionWithImpersonation(LivyCLIService.scala:121)
	at org.apache.livy.thriftserver.cli.ThriftCLIService.getSessionHandle(ThriftCLIService.scala:324)
	at org.apache.livy.thriftserver.cli.ThriftCLIService.OpenSession(ThriftCLIService.scala:203)
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1497)
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1482)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{noformat}
",,"wypoon commented on pull request #284: [LIVY-752] Fix implementation of limits on connections.
URL: https://github.com/apache/incubator-livy/pull/284
 
 
   ## What changes were proposed in this pull request?
   
   `LivyThriftSessionManager` keeps a `ConcurrentHashMap[String, AtomicLong]` named `connectionsCount` to track the number of connections per user, etc. The `incrementConnectionsCount` and `decrementConnectionsCount` methods in `LivyThriftSessionManager` check that `connectionsCount` does not contain a key (instead of contains the key) before getting the value and incrementing or decrementing the count (leading to a `NullPointerException`). Even accounting for the incorrect condition, they do not use the `ConcurrentHashMap` correctly. There is a race -- a thread can get a count, find that it's within a limit, create a new session and then increment the count, while in the meantime, another thread could have incremented the count and so the limit is now actually exceeded.
   
   We increment all relevant counts optimistically before creating a new session, check if any limits are violated, and if so, decrement all incremented counts.
   
   ## How was this patch tested?
   
   Tested by deploying the change on a cluster and setting livy.server.thrift.limit.connections.per.user. Verified that the number of connections reaches but does not exceed the limit.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Mar/20 03:10;githubbot;600","mgaido91 commented on pull request #284: [LIVY-752][THRIFT] Fix implementation of limits on connections.
URL: https://github.com/apache/incubator-livy/pull/284
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/20 16:46;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 16 16:48:36 UTC 2020,,,,,,,,,,"0|z0c15k:",9223372036854775807,,,,,,,,,,,,,,,,,,,"29/Feb/20 22:50;wypoon;I'll create a PR with a fix.;;;","02/Mar/20 03:25;wypoon;The NPE is due to checking if a ConcurrentHashMap (connectionsCount) does not contains a key (instead of contains it) before getting the value and incrementing or decrementing the count:
{code:java}
  private def incrementConnectionsCount(key: String): Unit = {
    if (!connectionsCount.containsKey(key)) connectionsCount.get(key).incrementAndGet
    else connectionsCount.put(key, new AtomicLong)
  }
{code}
However, correcting the condition is not enough. There is a race in the code â€“ a thread can get a count, find that it's within a limit, create a new session and then increment the count, while in the meantime, another thread could have incremented the count and so the limit is now actually exceeded.
I have fixed this in my PR.
Â ;;;","16/Mar/20 16:48;mgaido;Issue resolved by PR:Â [https://github.com/apache/incubator-livy/pull/284].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy server should allow to customize LIVY_CLASSPATH,LIVY-751,13287921,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,marblejenka,marblejenka,marblejenka,27/Feb/20 04:59,26/Mar/20 06:10,19/Dec/25 04:15,26/Mar/20 06:08,0.7.0,,,0.8.0,,Server,,,,,,,,,,0,,,,,,"What we want to do is - specify LIVY_CLASSPATH at the time of booting livy server to use the other version of hadoop, which is not included in livy artifact.


The background is - we are trying to use livy 0.7.0-incubating and spark 2.4.5 with YARN HDP2.6.4, but we encountered an error due to the incompatibility of livy included hadoop and HDP2.6.4 hadoop. We came up with a workaround that 1. remove hadoop from livy artifact by building with `mvn -Dhadoop.scope=provided` and 2. add HDP2.6.4 hadoop jars to the classpath for livy server by `hadoop classpath` command at the time of booting livy server. However, bin/livy-server is not allowing change LIVY_CLASSPATH.",,"marblejenka commented on pull request #282: [LIVY-751]Livy server should allow to customize LIVY_CLASSPATH
URL: https://github.com/apache/incubator-livy/pull/282
 
 
   ## What changes were proposed in this pull request?
   
   The purpose and background is https://issues.apache.org/jira/browse/LIVY-751
   
   ## How was this patch tested?
   
   I tested the following two manually.
   
   1. To confirm there is no degradation, I run 0.7.0-incubating livy server with sources in this PR. I also run an example jobs, and it completed without error.
   2.  To confirm our workaround works, I build 0.7.0-incubating branch with specifying `-Dhadoop.scope=provided` and sources with this PR. After that, I added `export LIVY_CLASSPATH=""$LIVY_HOME/jars/*:$(hadoop classpath)""` in conf/livy-env.sh and boot livy server.  I also run an example jobs, and it completed without error.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Feb/20 06:06;githubbot;600","jerryshao commented on pull request #282: [LIVY-751]Livy server should allow to customize LIVY_CLASSPATH
URL: https://github.com/apache/incubator-livy/pull/282
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/20 06:07;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 26 06:08:59 UTC 2020,,,,,,,,,,"0|z0bxsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Feb/20 05:00;marblejenka;I will soon send a PR to github.;;;","26/Mar/20 06:08;jerryshao;Issue resolved by pull request 282
https://github.com/apache/incubator-livy/pull/282;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for running Livy Integration tests against secure external clusters,LIVY-748,13284034,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,roliu,roliu,roliu,07/Feb/20 22:57,02/Mar/20 02:06,19/Dec/25 04:15,02/Mar/20 02:06,,,,0.8.0,,,,,,,,,,,,0,,,,,,Add support so Livy integration tests can be run against secure external clusters. Currently Livy integration tests only test Livy functionality against a minicluster and does not support running them against an external livy endpoint,,"jerryshao commented on pull request #278: [LIVY-748] Add support for running Livy Integration tests against secure external clusters
URL: https://github.com/apache/incubator-livy/pull/278
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Feb/20 01:20;githubbot;600","RogPodge commented on pull request #278: [LIVY-748] Add support for running Livy Integration tests against secure external clusters
URL: https://github.com/apache/incubator-livy/pull/278
 
 
   ## What changes were proposed in this pull request?
   
   https://issues.apache.org/jira/projects/LIVY/issues/LIVY-748?filter=reportedbyme
   
   This PR configures the integration tests to work on Livy Servers instances that require basic or kerberos authentication. To do this we had to replace the async httpclient with apaches normal httpclient, as there were issues found getting kerberos authentication to work with the async client.
   
   This PR also contains a minor fix to the InteractiveIT test suite. One of the tests would occasionally fail as a statement submitted to the interactive session would not be finished processing when the result was checked. The correctness check was also changed to correctly check for code completition
   
   ## How was this patch tested?
   This patch was tested on local deployments of livy server, both on a instance that required kerberos authentication and on one that required basic authentication. 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Feb/20 01:20;githubbot;600","jerryshao commented on pull request #278: [LIVY-748] Add support for running Livy Integration tests against secure external clusters
URL: https://github.com/apache/incubator-livy/pull/278
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Mar/20 02:03;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 02 02:06:04 UTC 2020,,,,,,,,,,"0|z0ba1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Feb/20 23:00;roliu;Working PR

[https://github.com/apache/incubator-livy/pull/278];;;","02/Mar/20 02:06;jerryshao;Issue resolved by pull request 278
https://github.com/apache/incubator-livy/pull/278;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The class of  Utils in RSC has a hidden bug,LIVY-747,13282521,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Invalid,,zhiyuan,zhiyuan,31/Jan/20 03:09,31/Jan/20 03:40,19/Dec/25 04:15,31/Jan/20 03:40,0.6.0,,,0.6.0,,RSC,,,,,,,,,,0,,,,,,Utils#join may throw a StringIndexOutOfBoundsException whenÂ (sb.length() - sep.length()) < 0 in line 87,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2020-01-31 03:09:35.0,,,,,,,,,,"0|z0b0zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
more than 11 concurrent clients lead to java.io.IOException: Unable to connect to provided ports 10000~10010,LIVY-745,13280231,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,wypoon,wypoon,wypoon,17/Jan/20 21:44,21/Mar/23 09:48,19/Dec/25 04:15,15/Feb/20 21:44,0.6.0,,,0.8.0,,RSC,,,,,,,,,,0,,,,,,"In testing scalability of the Livy Thrift server, I am simultaneously starting multiple connections to it. When there are more than 11 connections started simultaneously, the 12th (and subsequent) connection will fail with:
{noformat}
2020-01-10 13:53:28,686 ERROR org.apache.livy.thriftserver.LivyExecuteStatementOperation: Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: java.lang.RuntimeException: java.io.IOException: Unable to connect to provided ports 10000~10010
{noformat}
Here is the excerpt from the Livy server log:
{noformat}
2020-01-10 13:53:28,138 INFO org.apache.livy.server.interactive.InteractiveSession$: Creating Interactive session 0: [owner: systest, request: [kind: spark, proxyUser: None, heartbeatTimeoutInSecond: 0]]
...
2020-01-10 13:53:28,147 INFO org.apache.livy.server.interactive.InteractiveSession$: Creating Interactive session 1: [owner: systest, request: [kind: spark, proxyUser: None, heartbeatTimeoutInSecond: 0]]
...
2020-01-10 13:53:28,196 INFO org.apache.livy.server.interactive.InteractiveSession$: Creating Interactive session 2: [owner: systest, request: [kind: spark, proxyUser: None, heartbeatTimeoutInSecond: 0]]
...
2020-01-10 13:53:28,247 INFO org.apache.livy.server.interactive.InteractiveSession$: Creating Interactive session 3: [owner: systest, request: [kind: spark, proxyUser: None, heartbeatTimeoutInSecond: 0]]
...
2020-01-10 13:53:28,304 INFO org.apache.livy.server.interactive.InteractiveSession$: Creating Interactive session 4: [owner: systest, request: [kind: spark, proxyUser: None, heartbeatTimeoutInSecond: 0]]
2020-01-10 13:53:28,329 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10000 Address already in use
2020-01-10 13:53:28,329 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10000 Address already in use
2020-01-10 13:53:28,329 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10000 Address already in use
2020-01-10 13:53:28,329 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10000 Address already in use
2020-01-10 13:53:28,329 INFO org.apache.livy.rsc.rpc.RpcServer: Connected to the port 10000
...
2020-01-10 13:53:28,331 INFO org.apache.livy.rsc.rpc.RpcServer: Connected to the port 10001
...
2020-01-10 13:53:28,335 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10001 Address already in use
2020-01-10 13:53:28,335 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10001 Address already in use
2020-01-10 13:53:28,335 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10001 Address already in use
2020-01-10 13:53:28,338 INFO org.apache.livy.rsc.rpc.RpcServer: Connected to the port 10002
2020-01-10 13:53:28,338 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10002 Address already in use
...
2020-01-10 13:53:28,339 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10002 Address already in use
2020-01-10 13:53:28,341 INFO org.apache.livy.rsc.rpc.RpcServer: Connected to the port 10003
...
2020-01-10 13:53:28,341 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10003 Address already in use
...
2020-01-10 13:53:28,343 INFO org.apache.livy.rsc.rpc.RpcServer: Connected to the port 10004
...
2020-01-10 13:53:28,362 INFO org.apache.livy.server.interactive.InteractiveSession$: Creating Interactive session 5: [owner: systest, request: [kind: spark, proxyUser: None, heartbeatTimeoutInSecond: 0]]
2020-01-10 13:53:28,367 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10000 Address already in use
2020-01-10 13:53:28,369 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10001 Address already in use
2020-01-10 13:53:28,371 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10002 Address already in use
2020-01-10 13:53:28,373 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10003 Address already in use
2020-01-10 13:53:28,376 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10004 Address already in use
2020-01-10 13:53:28,379 INFO org.apache.livy.rsc.rpc.RpcServer: Connected to the port 10005
...
2020-01-10 13:53:28,412 INFO org.apache.livy.server.interactive.InteractiveSession$: Creating Interactive session 6: [owner: systest, request: [kind: spark, proxyUser: None, heartbeatTimeoutInSecond: 0]]
...
2020-01-10 13:53:28,416 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10000 Address already in use
...
2020-01-10 13:53:28,418 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10001 Address already in use
2020-01-10 13:53:28,419 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10002 Address already in use
2020-01-10 13:53:28,420 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10003 Address already in use
2020-01-10 13:53:28,422 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10004 Address already in use
2020-01-10 13:53:28,423 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10005 Address already in use
2020-01-10 13:53:28,424 INFO org.apache.livy.rsc.rpc.RpcServer: Connected to the port 10006
...
2020-01-10 13:53:28,462 INFO org.apache.livy.server.interactive.InteractiveSession$: Creating Interactive session 7: [owner: systest, request: [kind: spark, proxyUser: None, heartbeatTimeoutInSecond: 0]]
2020-01-10 13:53:28,466 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10000 Address already in use
2020-01-10 13:53:28,468 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10001 Address already in use
2020-01-10 13:53:28,470 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10002 Address already in use
2020-01-10 13:53:28,471 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10003 Address already in use
2020-01-10 13:53:28,472 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10004 Address already in use
2020-01-10 13:53:28,473 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10005 Address already in use
2020-01-10 13:53:28,475 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10006 Address already in use
2020-01-10 13:53:28,476 INFO org.apache.livy.rsc.rpc.RpcServer: Connected to the port 10007
...
2020-01-10 13:53:28,510 INFO org.apache.livy.server.interactive.InteractiveSession$: Creating Interactive session 8: [owner: systest, request: [kind: spark, proxyUser: None, heartbeatTimeoutInSecond: 0]]
2020-01-10 13:53:28,513 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10000 Address already in use
2020-01-10 13:53:28,514 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10001 Address already in use
2020-01-10 13:53:28,515 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10002 Address already in use
2020-01-10 13:53:28,522 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10003 Address already in use
2020-01-10 13:53:28,524 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10004 Address already in use
2020-01-10 13:53:28,525 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10005 Address already in use
2020-01-10 13:53:28,526 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10006 Address already in use
2020-01-10 13:53:28,527 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10007 Address already in use
2020-01-10 13:53:28,527 INFO org.apache.livy.rsc.rpc.RpcServer: Connected to the port 10008
...
2020-01-10 13:53:28,561 INFO org.apache.livy.server.interactive.InteractiveSession$: Creating Interactive session 9: [owner: systest, request: [kind: spark, proxyUser: None, heartbeatTimeoutInSecond: 0]]
2020-01-10 13:53:28,564 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10000 Address already in use
2020-01-10 13:53:28,565 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10001 Address already in use
2020-01-10 13:53:28,565 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10002 Address already in use
2020-01-10 13:53:28,567 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10003 Address already in use
2020-01-10 13:53:28,568 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10004 Address already in use
2020-01-10 13:53:28,568 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10005 Address already in use
2020-01-10 13:53:28,569 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10006 Address already in use
2020-01-10 13:53:28,569 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10007 Address already in use
2020-01-10 13:53:28,570 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10008 Address already in use
2020-01-10 13:53:28,570 INFO org.apache.livy.rsc.rpc.RpcServer: Connected to the port 10009
...
2020-01-10 13:53:28,611 INFO org.apache.livy.server.interactive.InteractiveSession$: Creating Interactive session 10: [owner: systest, request: [kind: spark, proxyUser: None, heartbeatTimeoutInSecond: 0]]
2020-01-10 13:53:28,614 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10000 Address already in use
2020-01-10 13:53:28,615 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10001 Address already in use
2020-01-10 13:53:28,616 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10002 Address already in use
2020-01-10 13:53:28,617 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10003 Address already in use
2020-01-10 13:53:28,618 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10004 Address already in use
2020-01-10 13:53:28,618 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10005 Address already in use
2020-01-10 13:53:28,619 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10006 Address already in use
2020-01-10 13:53:28,620 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10007 Address already in use
2020-01-10 13:53:28,620 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10008 Address already in use
2020-01-10 13:53:28,621 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10009 Address already in use
2020-01-10 13:53:28,621 INFO org.apache.livy.rsc.rpc.RpcServer: Connected to the port 10010
...
2020-01-10 13:53:28,661 INFO org.apache.livy.server.interactive.InteractiveSession$: Creating Interactive session 11: [owner: systest, request: [kind: spark, proxyUser: None, heartbeatTimeoutInSecond: 0]]
2020-01-10 13:53:28,664 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10000 Address already in use
2020-01-10 13:53:28,664 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10001 Address already in use
2020-01-10 13:53:28,665 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10002 Address already in use
2020-01-10 13:53:28,666 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10003 Address already in use
2020-01-10 13:53:28,667 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10004 Address already in use
2020-01-10 13:53:28,677 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10005 Address already in use
2020-01-10 13:53:28,678 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10006 Address already in use
2020-01-10 13:53:28,679 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10007 Address already in use
2020-01-10 13:53:28,679 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10008 Address already in use
2020-01-10 13:53:28,679 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10009 Address already in use
2020-01-10 13:53:28,680 DEBUG org.apache.livy.rsc.rpc.RpcServer: RPC not able to connect port 10010 Address already in use
2020-01-10 13:53:28,683 INFO org.apache.livy.thriftserver.LivyExecuteStatementOperation: (Error executing query, currentState RUNNING, ,java.lang.RuntimeException: java.io.IOException: Unable to connect to provided ports 10000~10010)
2020-01-10 13:53:28,686 ERROR org.apache.livy.thriftserver.LivyExecuteStatementOperation: Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: java.lang.RuntimeException: java.io.IOException: Unable to connect to provided ports 10000~10010
	at org.apache.livy.thriftserver.LivyExecuteStatementOperation.execute(LivyExecuteStatementOperation.scala:147)
	at org.apache.livy.thriftserver.LivyExecuteStatementOperation$$anon$1$$anon$2.run(LivyExecuteStatementOperation.scala:97)
	at org.apache.livy.thriftserver.LivyExecuteStatementOperation$$anon$1$$anon$2.run(LivyExecuteStatementOperation.scala:94)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
	at org.apache.livy.thriftserver.LivyExecuteStatementOperation$$anon$1.run(LivyExecuteStatementOperation.scala:107)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.io.IOException: Unable to connect to provided ports 10000~10010
	at org.apache.livy.rsc.Utils.propagate(Utils.java:60)
	at org.apache.livy.rsc.RSCClientFactory.createClient(RSCClientFactory.java:81)
	at org.apache.livy.LivyClientBuilder.build(LivyClientBuilder.java:130)
	at org.apache.livy.server.interactive.InteractiveSession$$anonfun$2.apply(InteractiveSession.scala:114)
	at org.apache.livy.server.interactive.InteractiveSession$$anonfun$2.apply(InteractiveSession.scala:84)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.livy.server.interactive.InteractiveSession$.create(InteractiveSession.scala:84)
	at org.apache.livy.thriftserver.LivyThriftSessionManager$$anonfun$4.apply(LivyThriftSessionManager.scala:229)
	at org.apache.livy.thriftserver.LivyThriftSessionManager$$anonfun$4.apply(LivyThriftSessionManager.scala:227)
	at org.apache.livy.thriftserver.LivyThriftSessionManager.org$apache$livy$thriftserver$LivyThriftSessionManager$$getOrCreateLivySession(LivyThriftSessionManager.scala:185)
	at org.apache.livy.thriftserver.LivyThriftSessionManager$$anonfun$5$$anon$1.run(LivyThriftSessionManager.scala:248)
	at org.apache.livy.thriftserver.LivyThriftSessionManager$$anonfun$5$$anon$1.run(LivyThriftSessionManager.scala:245)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)
	at org.apache.livy.thriftserver.LivyThriftSessionManager$$anonfun$5.apply(LivyThriftSessionManager.scala:245)
	at org.apache.livy.thriftserver.LivyThriftSessionManager$$anonfun$5.apply(LivyThriftSessionManager.scala:241)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: java.io.IOException: Unable to connect to provided ports 10000~10010
	at org.apache.livy.rsc.rpc.RpcServer.<init>(RpcServer.java:101)
	at org.apache.livy.rsc.RSCClientFactory.ref(RSCClientFactory.java:98)
	at org.apache.livy.rsc.RSCClientFactory.createClient(RSCClientFactory.java:71)
	... 22 more
{noformat}
As can be seen, the first 5 connections (session 0 to session 4) are created almost at the same time, and after some jostling, claim ports 10000 to 10004. The log then shows subsequent connections trying the ports from 10000 on up until they find one that is not in use. The last successful one is session 10, which claims port 10010. After that, session 11 fails.
The port range (10000~10010) is configurable, but I'm using the default.",,"wypoon commented on pull request #275: [LIVY-745] Ensure that a single LivyClientFactory gets loaded.
URL: https://github.com/apache/incubator-livy/pull/275
 
 
   ## What changes were proposed in this pull request?
   
   In LivyClientBuilder, a ServiceLoader is used to load configured LivyClientFactory providers (there is only one, RSCClientFactory). Correct behavior of RSCClientFactory implicitly depends on there being a single instance of it in the Livy server.
   Instead of instantiating a new ServiceLoader every time the build method is called on a LivyClientBuilder, keep a single ServiceLoader in a static field. The ServiceLoader will lazily load the LivyClientFactory (RSCClientFactory) and cache it. ServiceLoader instances are not safe for use by multiple concurrent threads, so call the ServiceLoader's iterator in a synchronized block.
   
   ## How was this patch tested?
   
   Tested by having 20 clients connect to the Livy Thrift server simultaneously. Before the change, the behavior described in https://issues.apache.org/jira/browse/LIVY-745 was encountered. With the change, the problem is not encountered.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/20 22:22;githubbot;600","mgaido91 commented on pull request #275: [LIVY-745] Ensure that a single RSCClientFactory gets loaded.
URL: https://github.com/apache/incubator-livy/pull/275
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Feb/20 21:42;githubbot;600","artparks commented on pull request #275:
URL: https://github.com/apache/incubator-livy/pull/275#issuecomment-718706461


   I built Livy from master to try and include this fix, but I still get the same problem of exhausted ports.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Oct/20 11:59;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 03 05:04:26 UTC 2020,,,,,,,,,,"0|z0amv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/Jan/20 22:16;wypoon;Tracing from the last stack trace in the description:
`RSCClientFactory.createClient` is called, which calls `RSCClientFactory.ref`.
Now, `RSCClientFactory` keeps an `AtomicInteger` field `refCount` and a `RpcServer` field `server`. The stack trace indicates that for each of the connections, `refCount.get() == 0` and `server == null`  when `RSCClientFactory.ref` is called:
{code}
  private synchronized void ref(RSCConf config) throws IOException {
    if (refCount.get() != 0) {
      refCount.incrementAndGet();
      return;
    }

    Utils.checkState(server == null, ""Server already running but ref count is 0."");
    if (server == null) {
      try {
        server = new RpcServer(config); // <- this is line 98, where the IOException occurs
        ...
{code}
(In the `RpcServer` constructor, the code that is called is
{code} 
    int [] portData = getPortNumberAndRange();
    int startingPortNumber = portData[PortRangeSchema.START_PORT.ordinal()];
    int endPort = portData[PortRangeSchema.END_PORT.ordinal()];
    boolean isContected = false;
    for(int tries = startingPortNumber ; tries<=endPort ; tries++){
      try {
        this.channel = getChannel(tries);
        isContected = true;
        break;
      } catch(SocketException e){
        LOG.debug(""RPC not able to connect port "" + tries + "" "" + e.getMessage());
      }
    }
    if(!isContected) {
      throw new IOException(""Unable to connect to provided ports "" + this.portRange);
    }
{code}
leading to the IOException.)
`RSCClientFactory.ref` is synchronized. It is designed to reuse an existing server if it is running. The fact that `RSCClientFactory.ref` is getting called repeatedly with `refCount.get() == 0` and `server == null` means that different RSCClientFactory instances are being called.
The cause of the problem is that more than one `ServiceLoader<LivyClientFactory>` gets created (in `LivyClientBuilder`) and each one instantiates a `RSCClientFactory`. The problem is solved if there is only one `ServiceLoader<LivyClientFactory>`. I have tested this and it fixes the problem.
;;;","17/Jan/20 22:24;wypoon;I created a PR with the fix.;;;","15/Feb/20 21:44;mgaido;Issue resolved by pull requestÂ https://github.com/apache/incubator-livy/pull/275.;;;","03/Aug/20 05:04;aajisaka;Hi [~mgaido], would you fix the Fix Version/s? Livy 0.7.0 does not contain this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Livy third-party library license generating issue,LIVY-738,13277654,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,jerryshao,jerryshao,jerryshao,06/Jan/20 09:04,07/Jan/20 12:11,19/Dec/25 04:15,07/Jan/20 12:11,0.8.0,,,0.7.0,0.8.0,Build,,,,,,,,,,0,,,,,,"Some dependencies' licenses are missing, so it cannot correctly generate the license information, here propose to fix them manually.",,"jerryshao commented on pull request #272: [LIVY-738][BUILD] Fix Livy third-party library license generating issue
URL: https://github.com/apache/incubator-livy/pull/272
 
 
   Some libraries' license file are missing, so Livy cannot correctly generate the license file, here fixing them manually.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/20 09:07;githubbot;600","jerryshao commented on pull request #272: [LIVY-738][BUILD] Fix Livy third-party library license generating issue
URL: https://github.com/apache/incubator-livy/pull/272
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 12:10;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 07 12:11:14 UTC 2020,,,,,,,,,,"0|z0a7og:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Jan/20 12:11;jerryshao;Issue resolved by pull request 272
https://github.com/apache/incubator-livy/pull/272;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix RPC Channel Closed When Multi Clients Connect to One Driver ,LIVY-735,13275428,13271157,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,yihengw,yihengw,19/Dec/19 07:27,08/Jan/20 09:16,19/Dec/25 04:15,08/Jan/20 09:16,,,,0.8.0,,,,,,,,,,,,0,,,,,,"Currently, the driver tries to support communicating with multi-clients, by registering each client at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/driver/RSCDriver.java#L220.

But actually, if multi-clients connect to one driver, the rpc channel will close, the reason are as follows.

1. In every communication, client sends two packages to driver: header\{type, id}, and payload at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/rpc/RpcDispatcher.java#L144.

2. If client1 sends header1, payload1, and client2 sends header2, payload2 at the same time. 
 The driver receives the package in the order: header1, header2, payload1, payload2.

3. When driver receives header1, driver assigns lastHeader at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/rpc/RpcDispatcher.java#L73.

4. Then driver receives header2, driver process it as a payload at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/rpc/RpcDispatcher.java#L78 which cause exception and rpc channel closed.

In the muti-active HA mode, the design doc is at: https://docs.google.com/document/d/1bD3qYZpw14_NuCcSGUOfqQ0pqvSbCQsOLFuZp26Ohjc/edit?usp=sharing, the session is allocated among servers by consistent hashing. If a new livy joins, some session will be migrated from old livy to new livy. If the session client in new livy connect to driver before stoping session client in old livy, then two session clients will both connect to driver, and rpc channel close. In this case, it's hard to ensure only one client connect to one driver at any time. So it's better to support multi-clients connect to one driver, which has no side effects.

How to fix:
1. Move the code of processing client message from `RpcDispatcher` to each `Rpc`.
2. Each `Rpc` registers itself to `channelRpc` in RpcDispatcher.
3. `RpcDispatcher` dispatches each message to `Rpc` according to `ctx.channel()`.",,"runzhiwang commented on pull request #268: [LIVY-735][RSC] Fix rpc channel closed when multi clients connect to one driver
URL: https://github.com/apache/incubator-livy/pull/268
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the driver tries to support communicating with multi-clients, by registering each client at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/driver/RSCDriver.java#L220.
   
   But actually, if multi-clients connect to one driver, the rpc channel will close, the reason are as follows.
   
   1.  In every communication, client sends two packages to driver: header{type, id}, and body at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/rpc/RpcDispatcher.java#L144. 
   
   2. If client1 sends header1, body1, and client2 sends header2, body2 at the same time. 
     The driver receives the package in the order: header1, header2, body1, body2.
   
   3. When driver receives header1, driver assigns lastHeader at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/rpc/RpcDispatcher.java#L73.
   
   4. Then driver receives header2, driver process it as a body at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/rpc/RpcDispatcher.java#L78 which cause exception and rpc channel closed.
   
   In the HA mode: active-standy or active-active (the active-active pr will be submitted after this pr), driver will communicate with multi-clients, and this pr can achieve this target.
   
   ## How was this patch tested?
   
   I will add a test to cover this PR.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 03:26;githubbot;600","runzhiwang commented on pull request #268: [LIVY-735][RSC] Fix rpc channel closed when multi clients connect to one driver
URL: https://github.com/apache/incubator-livy/pull/268
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the driver tries to support communicating with multi-clients, by registering each client at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/driver/RSCDriver.java#L220.
   
   But actually, if multi-clients connect to one driver, the rpc channel will close, the reason are as follows.
   
   1.  In every communication, client sends two packages to driver: header{type, id}, and payload at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/rpc/RpcDispatcher.java#L144. 
   
   2. If client1 sends header1, payload1, and client2 sends header2, payload2 at the same time. 
     The driver receives the package in the order: header1, header2, payload1, payload2.
   
   3. When driver receives header1, driver assigns lastHeader at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/rpc/RpcDispatcher.java#L73.
   
   4. Then driver receives header2, driver process it as a payload at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/rpc/RpcDispatcher.java#L78 which cause exception and rpc channel closed.
   
   In the muti-active HA mode, the design doc is at: https://docs.google.com/document/d/1bD3qYZpw14_NuCcSGUOfqQ0pqvSbCQsOLFuZp26Ohjc/edit?usp=sharing, the session is allocated among servers by consistent hashing. If a new livy joins, some session will be migrated from old livy to new livy. If the session client in new livy connect to driver before stoping session client in old livy, then two session clients will both connect to driver, and rpc channel close.  In this case, it's hard to ensure only one client connect to one driver at any time. So it's better to support multi-clients connect to one driver, which has no side effects.
   
   How to fix:
   1. Move the code of processing client message from `RpcDispatcher` to each `Rpc`.
   2. Each `Rpc` registers itself to `channelRpc` in RpcDispatcher.
   3. `RpcDispatcher` dispatches each message to `Rpc` according to  `ctx.channel()`.
   
   ## How was this patch tested?
   
   I will add a test to cover this PR.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 04:20;githubbot;600","runzhiwang commented on pull request #268: [LIVY-735][RSC] Fix rpc channel closed when multi clients connect to one driver
URL: https://github.com/apache/incubator-livy/pull/268
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/20 07:48;githubbot;600","runzhiwang commented on pull request #268: [LIVY-735][RSC] Fix rpc channel closed when multi clients connect to one driver
URL: https://github.com/apache/incubator-livy/pull/268
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the driver tries to support communicating with multi-clients, by registering each client at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/driver/RSCDriver.java#L220.
   
   But actually, if multi-clients connect to one driver, the rpc channel will close, the reason are as follows.
   
   1.  In every communication, client sends two packages to driver: header{type, id}, and payload at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/rpc/RpcDispatcher.java#L144. 
   
   2. If client1 sends header1, payload1, and client2 sends header2, payload2 at the same time. 
     The driver receives the package in the order: header1, header2, payload1, payload2.
   
   3. When driver receives header1, driver assigns lastHeader at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/rpc/RpcDispatcher.java#L73.
   
   4. Then driver receives header2, driver process it as a payload at https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/rpc/RpcDispatcher.java#L78 which cause exception and rpc channel closed.
   
   In the muti-active HA mode, the design doc is at: https://docs.google.com/document/d/1bD3qYZpw14_NuCcSGUOfqQ0pqvSbCQsOLFuZp26Ohjc/edit?usp=sharing, the session is allocated among servers by consistent hashing. If a new livy joins, some session will be migrated from old livy to new livy. If the session client in new livy connect to driver before stoping session client in old livy, then two session clients will both connect to driver, and rpc channel close.  In this case, it's hard to ensure only one client connect to one driver at any time. So it's better to support multi-clients connect to one driver, which has no side effects.
   
   How to fix:
   1. Move the code of processing client message from `RpcDispatcher` to each `Rpc`.
   2. Each `Rpc` registers itself to `channelRpc` in RpcDispatcher.
   3. `RpcDispatcher` dispatches each message to `Rpc` according to  `ctx.channel()`.
   
   ## How was this patch tested?
   
   Existed UT and IT
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/20 07:48;githubbot;600","jerryshao commented on pull request #268: [LIVY-735][RSC] Fix rpc channel closed when multi clients connect to one driver
URL: https://github.com/apache/incubator-livy/pull/268
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Jan/20 09:15;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 08 09:16:34 UTC 2020,,,,,,,,,,"0|z09tyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,"08/Jan/20 09:16;jerryshao;Issue resolved by pull request 268
https://github.com/apache/incubator-livy/pull/268;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A Common Zookeeper Wrapper Utility ,LIVY-732,13275421,13271157,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,yihengw,yihengw,19/Dec/19 06:53,07/Jan/20 12:20,19/Dec/25 04:15,07/Jan/20 12:20,,,,0.8.0,,,,,,,,,,,,0,,,,,,"Currently, the utilities of zookeeper mixed with ZooKeeperStateStore. To use the utility of zookeeper, the instance of ZooKeeperStateStore has to be created , which looks weird.

This Jira aims to achieve two targets:

1.Â  Extract the utilities of zookeeper fromÂ ZooKeeperStateStore to support such as distributed lock, service discovery and so on.

2.Â  ZooKeeperManager which contains the utilities of zookeeper should be a single instance.",,"runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the utilities of zookeeper mixed with ZooKeeperStateStore. To use the utility of zookeeper, the instance of ZooKeeperStateStore has to be created , which looks weird.
   
   This PR aims to achieve two targets:
   
   1.  Extract the utilities of zookeeper from ZooKeeperStateStore to support such as distributed lock, service discovery and so on.
   
   2.  ZooKeeperManager which contains the utilities of zookeeper should be a single instance.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 02:49;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the utilities of zookeeper mixed with ZooKeeperStateStore. To use the utility of zookeeper, the instance of ZooKeeperStateStore has to be created , which looks weird.
   
   This PR aims to achieve two targets:
   
   1.  Extract the utilities of zookeeper from ZooKeeperStateStore to support such as distributed lock, service discovery and so on.
   
   2.  ZooKeeperManager which contains the utilities of zookeeper should be a single instance.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Dec/19 04:20;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Dec/19 23:35;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the utilities of zookeeper mixed with ZooKeeperStateStore. To use the utility of zookeeper, the instance of ZooKeeperStateStore has to be created , which looks weird.
   
   This PR aims to achieve the follow target:
   
   1.  Extract the utilities of zookeeper from ZooKeeperStateStore to support such as distributed lock, service discovery and so on.
   
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Dec/19 23:35;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Dec/19 01:20;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the utilities of zookeeper mixed with ZooKeeperStateStore. To use the utility of zookeeper, the instance of ZooKeeperStateStore has to be created , which looks weird.
   
   This PR aims to achieve the follow target:
   
   1.  Extract the utilities of zookeeper from ZooKeeperStateStore to support such as distributed lock, service discovery and so on.
   
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Dec/19 01:20;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Dec/19 07:12;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the utilities of zookeeper mixed with ZooKeeperStateStore. To use the utility of zookeeper, the instance of ZooKeeperStateStore has to be created , which looks weird.
   
   This PR aims to achieve the follow target:
   
   1.  Extract the utilities of zookeeper from ZooKeeperStateStore to support such as distributed lock, service discovery and so on.
   
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Dec/19 07:12;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Dec/19 23:53;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the utilities of zookeeper mixed with ZooKeeperStateStore. To use the utility of zookeeper, the instance of ZooKeeperStateStore has to be created , which looks weird.
   
   This PR aims to achieve the follow target:
   
   1.  Extract the utilities of zookeeper from ZooKeeperStateStore to support such as distributed lock, service discovery and so on.
   
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Dec/19 23:53;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Dec/19 06:14;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the utilities of zookeeper mixed with ZooKeeperStateStore. To use the utility of zookeeper, the instance of ZooKeeperStateStore has to be created , which looks weird.
   
   This PR aims to achieve the follow target:
   
   1.  Extract the utilities of zookeeper from ZooKeeperStateStore to support such as distributed lock, service discovery and so on.
   
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Dec/19 06:14;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 00:39;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the utilities of zookeeper mixed with ZooKeeperStateStore. To use the utility of zookeeper, the instance of ZooKeeperStateStore has to be created , which looks weird.
   
   This PR aims to achieve the follow target:
   
   1.  Extract the utilities of zookeeper from ZooKeeperStateStore to support such as distributed lock, service discovery and so on.
   
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Dec/19 00:39;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Dec/19 06:35;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the utilities of zookeeper mixed with ZooKeeperStateStore. To use the utility of zookeeper, the instance of ZooKeeperStateStore has to be created , which looks weird.
   
   This PR aims to achieve the follow target:
   
   1.  Extract the utilities of zookeeper from ZooKeeperStateStore to support such as distributed lock, service discovery and so on.
   
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Dec/19 06:35;githubbot;600","jerryshao commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/20 09:34;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the utilities of zookeeper mixed with ZooKeeperStateStore. To use the utility of zookeeper, the instance of ZooKeeperStateStore has to be created , which looks weird.
   
   This PR aims to achieve the follow target:
   
   1.  Extract the utilities of zookeeper from ZooKeeperStateStore to support such as distributed lock, service discovery and so on.
   
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/20 09:35;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/20 12:34;githubbot;600","runzhiwang commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   ## What changes were proposed in this pull request?
   
   Currently, the utilities of zookeeper mixed with ZooKeeperStateStore. To use the utility of zookeeper, the instance of ZooKeeperStateStore has to be created , which looks weird.
   
   This PR aims to achieve the follow target:
   
   1.  Extract the utilities of zookeeper from ZooKeeperStateStore to support such as distributed lock, service discovery and so on.
   
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/20 12:34;githubbot;600","jerryshao commented on pull request #267: [LIVY-732][SERVER] A common zookeeper wrapper utility
URL: https://github.com/apache/incubator-livy/pull/267
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Jan/20 12:19;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,12600,,,0,12600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 07 12:20:41 UTC 2020,,,,,,,,,,"0|z09tww:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Jan/20 12:20;jerryshao;Issue resolved by pull request 267
https://github.com/apache/incubator-livy/pull/267;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy should not recover the killed session,LIVY-729,13275123,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,runzhiwang,runzhiwang,18/Dec/19 00:56,19/Dec/19 02:52,19/Dec/25 04:15,19/Dec/19 02:52,0.6.0,,,0.8.0,,Server,,,,,,,,,,0,,,,,,"Follows are steps to reproduce the problem:
 # Set livy.server.recovery.mode=recovery, and create a session: session0 in yarn-cluster
 # kill the yarn application of the session
 # restart livy
 # livy try to recover session0, but application has been killed and driver does not exist, so client can not connect to driver, and exception was thrown as the image.
 # If the ip:port of the driver was reused by session1, client of session0 will try to connect to driver of session1, then driver will throw exception:Â Unexpected client ID.
 # Both the exception threw by livy and driver will confused the user, and recover a lot of killed sessions will delay the recover of alive session.

!image-2019-12-18-08-56-46-925.png!",,"runzhiwang commented on pull request #266: [LIVY-729] Fix livy should not recover the killed session
URL: https://github.com/apache/incubator-livy/pull/266
 
 
   ## What changes were proposed in this pull request?
   
   Follows are steps to reproduce the problem:
   
   1. Set livy.server.recovery.mode=recovery, and create a session: session0 in yarn-cluster
   2. kill the yarn application of the session
   3. restart livy
   4. livy try to recover session0, but application has been killed and driver does not exist, so client can not connect to driver, and exception was thrown as the image.
   5. If the ip:port of the driver was reused by session1, client of session0 will try to connect to driver of session1, then driver will throw exception: Unexpected client ID.
   6. Both the exception threw by livy and driver will confused the user, and recover a lot of killed sessions will delay the recover of alive session.
   ![image](https://user-images.githubusercontent.com/51938049/71066615-f0216280-21ad-11ea-9559-0cc0bd7d7546.png)
   
   How to fix:
   When session was killed or dead, remove the session from the store
   
   ## How was this patch tested?
   
   Existed IT and UT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/19 07:49;githubbot;600","runzhiwang commented on pull request #266: [LIVY-729] Fix livy recover the killed session
URL: https://github.com/apache/incubator-livy/pull/266
 
 
   ## What changes were proposed in this pull request?
   
   Follows are steps to reproduce the problem:
   
   1. Set livy.server.recovery.mode=recovery, and create an interactive session: session0 in yarn-cluster
   2. kill the yarn application of the session
   3. restart livy
   4. livy try to recover session0, but application has been killed and driver does not exist, so client can not connect to driver, and exception was thrown as the image.
   5. If the ip:port of the driver was reused by session1, client of session0 will try to connect to driver of session1, then driver will throw exception: Unexpected client ID.
   6. Both the exception threw by livy and driver will confuse the user, and recover a lot of killed sessions will delay the recover of alive session.
   ![image](https://user-images.githubusercontent.com/51938049/71066615-f0216280-21ad-11ea-9559-0cc0bd7d7546.png)
   
   How to fix:
   When session was killed or dead, remove the session from the store
   
   ## How was this patch tested?
   
   Existed IT and UT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Dec/19 09:27;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/19 00:56;runzhiwang;image-2019-12-18-08-56-46-925.png;https://issues.apache.org/jira/secure/attachment/12989054/image-2019-12-18-08-56-46-925.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 19 02:52:40 UTC 2019,,,,,,,,,,"0|z09s2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Dec/19 01:06;runzhiwang;working on it;;;","19/Dec/19 02:52;jerryshao;Issue resolved by pull request 266
https://github.com/apache/incubator-livy/pull/266;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Session state always be idle though the yarn application has been killed after  restart livy.,LIVY-727,13272897,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,runzhiwang,runzhiwang,07/Dec/19 05:42,19/Dec/19 06:30,19/Dec/25 04:15,19/Dec/19 06:30,0.6.0,0.7.0,,0.8.0,,,,,,,,,,,,0,,,,,,"# SetÂ livy.server.recovery.mode=recovery, and create a session in yarn-cluster
 # Restart livy,then kill the yarn application of the session.
 # The session state will always be idle and never change to killed or dead.",,"runzhiwang commented on pull request #265: [LIVY-727] Fix session state always be idle though the yarn application has been killed after restart livy
URL: https://github.com/apache/incubator-livy/pull/265
 
 
   ## What changes were proposed in this pull request?
   
   [LIVY-727] Fix session state always be idle though the yarn application has been killed after restart livy.
   
   Follows are steps to reproduce the problem:
   1. Set livy.server.recovery.mode=recovery, and create a session in yarn-cluster
   2. Restart livy
   3. Kill the yarn application.
   4. The session state will always be idle and never change to killed or dead. Just as the image, livy-session-16 has been killed in yarn, but the state is still idle.
   ![image](https://user-images.githubusercontent.com/51938049/70371088-92695c80-1909-11ea-875c-73696db693ce.png)
   
   The cause of the problem are as follows:
   1. Because when recover session, livy will not startDriver again, so the driverProcess is None.
   2. SparkYarnApp will not be created in `driverProcess.map { _ => SparkApp.create(appTag, appId, driverProcess, livyConf, Some(this)) }` when driverProcess is None.
   3. So yarnAppMonitorThread of the session will never start, and the session state will never change.
   
   How to fix the bug:
   1. If livy run in yarn, SparkApp will create even though the driverProcess is None
   2. If not run in yarn, SparkApp will not create, because the code require driverProcess is not None, 
     and I don't want to change the behavior.
   
   ## How was this patch tested?
   
   1. Set livy.server.recovery.mode=recovery, and create a session in yarn-cluster
   2. Restart livy
   3. Kill the yarn application.
   4. The session state will change to killed
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Dec/19 07:53;githubbot;600","jerryshao commented on pull request #265: [LIVY-727] Fix session state always be idle though the yarn application has been killed after restart livy 
URL: https://github.com/apache/incubator-livy/pull/265
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 03:17;githubbot;600","runzhiwang commented on pull request #265: [LIVY-727] Fix session state always be idle though the yarn application has been killed after restart livy 
URL: https://github.com/apache/incubator-livy/pull/265
 
 
   ## What changes were proposed in this pull request?
   
   [LIVY-727] Fix session state always be idle though the yarn application has been killed after restart livy.
   
   Follows are steps to reproduce the problem:
   1. Set livy.server.recovery.mode=recovery, and create a session in yarn-cluster
   2. Restart livy
   3. Kill the yarn application of the session.
   4. The session state will always be idle and never change to killed or dead. Just as the image, livy-session-16 has been killed in yarn, but the state is still idle.
   ![image](https://user-images.githubusercontent.com/51938049/70371088-92695c80-1909-11ea-875c-73696db693ce.png)
   
   The cause of the problem are as follows:
   1. Because when recover session, livy will not startDriver again, so the driverProcess is None.
   2. SparkYarnApp will not be created in `driverProcess.map { _ => SparkApp.create(appTag, appId, driverProcess, livyConf, Some(this)) }` when driverProcess is None.
   3. So yarnAppMonitorThread of the session will never start, and the session state will never change.
   
   How to fix the bug:
   1. If livy run in yarn, SparkApp will create even though the driverProcess is None
   2. If not run in yarn, SparkApp will not create, because the code require driverProcess is not None at https://github.com/apache/incubator-livy/blob/master/server/src/main/scala/org/apache/livy/utils/SparkApp.scala#L93, 
     and I don't want to change the behavior.
   
   ## How was this patch tested?
   
   1. Set livy.server.recovery.mode=recovery, and create a session in yarn-cluster
   2. Restart livy
   3. Kill the yarn application of the session.
   4. The session state will change to killed
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Dec/19 03:17;githubbot;600","jerryshao commented on pull request #265: [LIVY-727] Fix session state always be idle though the yarn application has been killed after restart livy 
URL: https://github.com/apache/incubator-livy/pull/265
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 02:08;githubbot;600","runzhiwang commented on pull request #265: [LIVY-727] Fix session state always be idle though the yarn application has been killed after restart livy 
URL: https://github.com/apache/incubator-livy/pull/265
 
 
   ## What changes were proposed in this pull request?
   
   [LIVY-727] Fix session state always be idle though the yarn application has been killed after restart livy.
   
   Follows are steps to reproduce the problem:
   1. Set livy.server.recovery.mode=recovery, and create a session in yarn-cluster
   2. Restart livy
   3. Kill the yarn application of the session.
   4. The session state will always be idle and never change to killed or dead. Just as the image, livy-session-16 has been killed in yarn, but the state is still idle.
   ![image](https://user-images.githubusercontent.com/51938049/70371088-92695c80-1909-11ea-875c-73696db693ce.png)
   
   The cause of the problem are as follows:
   1. Because when recover session, livy will not startDriver again, so the driverProcess is None.
   2. SparkYarnApp will not be created in `driverProcess.map { _ => SparkApp.create(appTag, appId, driverProcess, livyConf, Some(this)) }` when driverProcess is None.
   3. So yarnAppMonitorThread of the session will never start, and the session state will never change.
   
   How to fix the bug:
   1. If livy run in yarn, SparkApp will create even though the driverProcess is None
   2. If not run in yarn, SparkApp will not create, because the code require driverProcess is not None at https://github.com/apache/incubator-livy/blob/master/server/src/main/scala/org/apache/livy/utils/SparkApp.scala#L93, 
     and I don't want to change the behavior.
   
   ## How was this patch tested?
   
   1. Set livy.server.recovery.mode=recovery, and create a session in yarn-cluster
   2. Restart livy
   3. Kill the yarn application of the session.
   4. The session state will change to killed
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 02:08;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 19 06:30:01 UTC 2019,,,,,,,,,,"0|z09ebs:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Dec/19 05:43;runzhiwang;Iâ€˜m working on it.;;;","19/Dec/19 06:30;jerryshao;Issue resolved by pull request 272
https://github.com/apache/incubator-livy/pull/272;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specify explicit ZooKeeper version in maven,LIVY-717,13269855,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,symat,symat,symat,21/Nov/19 13:50,26/Nov/19 02:25,19/Dec/25 04:15,26/Nov/19 02:25,,,,0.7.0,,,,,,,,,,,,0,,,,,,"Hadoop trunk was updated recently to use Curator 4.0.2 and ZooKeeper 3.5.6, see [HADOOP-16579|https://issues.apache.org/jira/browse/HADOOP-16579]. Now I want to test Livy in a cluster where we have a new ZooKeeper 3.5 deployed. 

When we want to use Livy in a cluster where a newer ZooKeeper server version is used, we might run into run-time errors if we compile Livy using the current Curator / Hadoop versions. The Curator version can already explicitly set with the {{curator.version}} maven property in build time, but we were still missed the same parameter for ZooKeeper.

In this PR I added a new maven parameter called {{zookeeper.version}} and after analyzing the maven dependency tree, I made sure that the Curator and ZooKeeper versions used in compile time are always harmonized and controlled by the maven parameters.

I set the zookeeper.version in maven to {{3.4.6}} to be backward compatible with the current Livy dependencies.
",,"symat commented on pull request #262: [LIVY-717] introduce maven property to set ZooKeeper version
URL: https://github.com/apache/incubator-livy/pull/262
 
 
   When we want to use Livy in a cluster where a newer ZooKeeper server version is used, we might run into run-time errors if we compile Livy using the current Curator / Hadoop versions. The Curator version can already explicitly set with the `curator.version` maven property in build time, but we were still missed the same parameter for ZooKeeper. 
   
   In this PR I added a new maven parameter called `zookeeper.version` and after analyzing the maven dependency tree, I made sure that the Curator and ZooKeeper versions used in compile time are always harmonized and controlled by the maven parameters.
   
   I set the `zookeeper.version` in maven to `3.4.6` to be backward compatible with the current Livy dependencies.
   
   see https://issues.apache.org/jira/browse/LIVY-717
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Nov/19 13:36;githubbot;600","jerryshao commented on pull request #262: [LIVY-717] introduce maven property to set ZooKeeper version
URL: https://github.com/apache/incubator-livy/pull/262
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Nov/19 02:22;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 26 02:25:05 UTC 2019,,,,,,,,,,"0|z08vk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Nov/19 02:25;jerryshao;Issue resolved by pull request 262
https://github.com/apache/incubator-livy/pull/262;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The configuration in the livy.conf.template is inconsistent with LivyConf.scala,LIVY-715,13269581,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,micahzhao,micahzhao,micahzhao,20/Nov/19 12:24,22/Nov/19 02:40,19/Dec/25 04:15,22/Nov/19 02:40,0.6.0,,,0.7.0,,Docs,,,,,,,,,,0,,,,,,"Â  Â  When I test livy impersonation found that, in livy.conf.template the value of livy.impersonation.enabled is true. So I thought impersonation was enabled by default.Â  Â  Â  Â  Â  Â  Â 

Â  Â  Â However, impersonation was not turned on when we test. I foundÂ that the real configuration in LivyConf. scala is false. This can mislead users.",,"captainzmc commented on pull request #261: [LIVY-715][DOC]The configuration in the template is inconsistent with LivyConf.scala
URL: https://github.com/apache/incubator-livy/pull/261
 
 
   ## What changes were proposed in this pull request?
   
       When I test livy impersonation found that, in livy.conf.template the value of livy.impersonation.enabled is true. So I thought impersonation was enabled by default.             
       However, impersonation was not turned on when we test. I found that the real configuration in LivyConf. scala is false. 
       This can mislead users.
   
   ## How was this patch tested?
   
   no need
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 12:32;githubbot;600","captainzmc commented on pull request #261: [LIVY-715][DOC] The configuration in the template is inconsistent with LivyConf.scala
URL: https://github.com/apache/incubator-livy/pull/261
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Nov/19 02:17;githubbot;600","captainzmc commented on pull request #261: [LIVY-715][DOC] The configuration in the template is inconsistent with LivyConf.scala
URL: https://github.com/apache/incubator-livy/pull/261
 
 
   ## What changes were proposed in this pull request?
   
       When I test livy impersonation found that, in livy.conf.template the value of livy.impersonation.enabled is true. So I thought impersonation was enabled by default.             
       However, impersonation was not turned on when we test. I found that the real configuration in LivyConf. scala is false. 
       This can mislead users.
   
   ## How was this patch tested?
   
   no need
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Nov/19 02:18;githubbot;600","jerryshao commented on pull request #261: [LIVY-715][DOC] The configuration in the template is inconsistent with LivyConf.scala
URL: https://github.com/apache/incubator-livy/pull/261
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Nov/19 02:39;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 22 02:40:59 UTC 2019,,,,,,,,,,"0|z08tvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Nov/19 12:25;micahzhao;I'll fix that.;;;","22/Nov/19 02:40;jerryshao;Issue resolved by pull request 261
https://github.com/apache/incubator-livy/pull/261;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot remove the app in leakedAppTags when timeout,LIVY-714,13269461,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,runzhiwang,runzhiwang,20/Nov/19 01:49,18/Apr/25 14:38,19/Dec/25 04:15,25/Nov/19 03:49,,,,0.7.0,,,,,,,,,,,,0,,,,,,"# var isRemoved = false should be in while(iter.hasNext), otherwise if there are two apps, the first app will be removed and the second app will timeout in this loop, and after remove the first app, isRemoved = true, and the second app cannot pass the if(!isRemoved) and only will be delete in the next loop.
 # entry.getValue - now is negative, and never greater thanÂ sessionLeakageCheckTimeout.

!image-2019-11-20-09-50-52-316.png!",,"runzhiwang commented on pull request #259: [LIVY-714] Fix cannot remove the app in leakedAppTags when timeout
URL: https://github.com/apache/incubator-livy/pull/259
 
 
   ## What changes were proposed in this pull request?
   1.`var isRemoved = false` should be in `while(iter.hasNext),` otherwise if there are two apps, the first app will be killApplication and the second app will timeout in this loop, and after remove the first app,` isRemoved = true`, and the second app cannot pass the` if(!isRemoved)` and only will be deleted in the next loop.
   
   2.`entry.getValue - now` is negative, and never greater than `sessionLeakageCheckTimeout`.
   
   ![image](https://user-images.githubusercontent.com/51938049/69202431-99a81080-0b7c-11ea-8084-9801af5a75bd.png)
   
   
   ## How was this patch tested?
   
   Existed IT and UT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Nov/19 02:02;githubbot;600","jerryshao commented on pull request #259: [LIVY-714][SERVER] Fix cannot remove the app in leakedAppTags when timeout
URL: https://github.com/apache/incubator-livy/pull/259
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Nov/19 03:44;githubbot;600","codecov-commenter commented on PR #259:
URL: https://github.com/apache/incubator-livy/pull/259#issuecomment-2815570333

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/259?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   Attention: Patch coverage is `76.92308%` with `3 lines` in your changes missing coverage. Please review.
   > Project coverage is 68.04%. Comparing base [(`6261c57`)](https://app.codecov.io/gh/apache/incubator-livy/commit/6261c57be8df66d5f3fc3ccdaa15f8c4e1989d1d?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) to head [(`fac880c`)](https://app.codecov.io/gh/apache/incubator-livy/commit/fac880ceef41991fbb69505123b95e7609b4c0ae?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   > Report is 69 commits behind head on master.
   
   | [Files with missing lines](https://app.codecov.io/gh/apache/incubator-livy/pull/259?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) | Patch % | Lines |
   |---|---|---|
   | [...ain/scala/org/apache/livy/utils/SparkYarnApp.scala](https://app.codecov.io/gh/apache/incubator-livy/pull/259?src=pr&el=tree&filepath=server%2Fsrc%2Fmain%2Fscala%2Forg%2Fapache%2Flivy%2Futils%2FSparkYarnApp.scala&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya1lhcm5BcHAuc2NhbGE=) | 76.92% | [1 Missing and 2 partials :warning: ](https://app.codecov.io/gh/apache/incubator-livy/pull/259?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) |
   
   <details><summary>Additional details and impacted files</summary>
   
   
   ```diff
   @@             Coverage Diff              @@
   ##             master     #259      +/-   ##
   ============================================
   + Coverage     67.85%   68.04%   +0.19%     
     Complexity      937      937              
   ============================================
     Files           102      102              
     Lines          5876     5883       +7     
     Branches        891      893       +2     
   ============================================
   + Hits           3987     4003      +16     
   + Misses         1315     1303      -12     
   - Partials        574      577       +3     
   ```
   
   </details>
   
   [:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/apache/incubator-livy/pull/259?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).   
   :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   
   <details><summary> :rocket: New features to boost your workflow: </summary>
   
   - :snowflake: [Test Analytics](https://docs.codecov.com/docs/test-analytics): Detect flaky tests, report on failures, and find test suite problems.
   - :package: [JS Bundle Analysis](https://docs.codecov.com/docs/javascript-bundle-analysis): Save yourself from yourself by tracking and limiting bundle sizes in JS merges.
   </details>


;18/Apr/25 14:38;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/19 01:50;runzhiwang;image-2019-11-20-09-50-52-316.png;https://issues.apache.org/jira/secure/attachment/12986290/image-2019-11-20-09-50-52-316.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 25 03:49:12 UTC 2019,,,,,,,,,,"0|z08t4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Nov/19 01:57;runzhiwang;I'm working on it.;;;","25/Nov/19 03:49;jerryshao;Issue resolved by pull request 259
https://github.com/apache/incubator-livy/pull/259;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EMR 5.23/5.27 - Livy does not recognise that Spark job failed,LIVY-712,13267748,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Workaround,,sankotm,sankotm,12/Nov/19 15:38,28/Nov/19 14:09,19/Dec/25 04:15,28/Nov/19 14:09,0.5.0,0.6.0,,,,API,,,,,,,,,,0,api,EMR,spark,,,"We've upgraded from AWS EMR 5.13 -> 5.23 (Livy 0.4.0 -> 0.5.0, Spark 2.3.0 -> 2.4.0) and an issue appears that when there is an exception thrown during Spark job execution, Spark shuts down as if there was no problem and job appears as Completed in EMR. So we're not notified when system crashes. The same problem appears in EMR 5.27 (Livy 0.6.0, Spark 2.4.4).

Is it something with Spark? Or a known issue with Livy?

In Livy logs I see that spark-submit exists with error codeÂ 1
{quote}{{05:34:59 WARN BatchSession$: spark-submit exited with code 1}}
{quote}
Â And then Livy API states that batch state is
{quote}{{""state"": ""success""}}
{quote}
How can it be made work again?","AWS EMR 5.23/5.27, Scala",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 28 14:09:09 UTC 2019,,,,,,,,,,"0|z08ikg:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Nov/19 17:59;sankotm;As I search though the code i see a change inÂ SparkYarnApp.scala line 295 on between 0.4.0 and 0.5.0 (same in 0.6.0).

Â 

0.4.0:

{{Â } catch {}}
 {{Â  case e: InterruptedException =>}}
 {{Â  Â  yarnDiagnostics = ArrayBuffer(""Session stopped by user."")}}
 {{Â  Â  changeState(SparkApp.State.KILLED)}}
 {{Â  case e: Throwable =>}}
 {{Â  Â  error(s""Error whiling refreshing YARN state: $e"")}}
 {{Â  Â  yarnDiagnostics = ArrayBuffer(e.toString, e.getStackTrace().mkString("" ""))}}
 {{Â  Â  changeState(SparkApp.State.FAILED)}}

Â  }

0.5.0/0.6.0:

{{Â } catch {}}
 {{Â  case _: InterruptedException =>}}
 {{Â  Â  yarnDiagnostics = ArrayBuffer(""Session stopped by user."")}}
 {{Â  Â  changeState(SparkApp.State.KILLED)}}
 {{Â  case NonFatal(e) =>}}
 {{Â  Â  error(s""Error whiling refreshing YARN state"", e)}}
 {{Â  Â  yarnDiagnostics = ArrayBuffer(e.getMessage)}}
 {{Â  Â  changeState(SparkApp.State.FAILED)}}

Â  }

So it seems that in 0.5.0+ all Fatal exceptions are ignored and don't make App go into FAILED state. That looks like a bug. Is it so?;;;","21/Nov/19 13:28;yihengw;This code is changed in this patch:
https://github.com/apache/incubator-livy/commit/ca4cad22968e1a2f88fa0ec262c1088812e3d251

[~jshao] Any suggestion about this?;;;","21/Nov/19 13:30;yihengw;Can you provide a way to reproduce the issue?;;;","25/Nov/19 11:33;sankotm;Sure,
 * create a Scala Spark job that throws NullPointerException (to make sure job fails)
 * submit job through Livy to AWS EMR 5.23/5.27
 * check state of job execution through Livy API

Instead of failed state it should stateÂ ""state"": ""success"".

Are those steps sufficient to reproduce it or would you need more detail?;;;","26/Nov/19 15:15;sankotm;After further investigation, it seems that problem is unrelated to Livy.

It seems to be related to an issue of AWS-custom hadoop-* libraries used in EMR. I'll keep you posted with results of further investigations.;;;","28/Nov/19 14:06;sankotm;It seems that issue was present in EMR 5.23/5.27 (hadoop libraries 2.8.5-amzn-3/2.8.5-amzn-4) and is not present in EMR 5.28 anynore (hadoop libraries 2.8.5-amzn-5).

I'm thus closing the issue.;;;","28/Nov/19 14:09;sankotm;Issue seems to be external - concretely a problem in AWS EMR customizations of hadoop libraries.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Travis fails to build on Ubuntu16.04,LIVY-711,13267596,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,runzhiwang,runzhiwang,12/Nov/19 02:17,12/Nov/19 07:37,19/Dec/25 04:15,12/Nov/19 07:37,0.6.0,,,0.7.0,,Build,,,,,,,,,,0,,,,,,!image-2019-11-12-14-25-37-189.png!!image-2019-11-12-10-16-27-108.png!,,"runzhiwang commented on pull request #257: [LIVY-711] Fix Travis fails to build on Ubuntu16.04
URL: https://github.com/apache/incubator-livy/pull/257
 
 
   ## What changes were proposed in this pull request?
   
   Fix Travis fails to build on Ubuntu16.04
   
   ## How was this patch tested?
   
   1. Previously the `dist `of .travis.yml is `xenial ` which points to Ubuntu 14.04.5 LTS. Travis build successfully.
   ![image](https://user-images.githubusercontent.com/51938049/68647534-10c81e00-0559-11ea-9152-362711b30946.png)
   
   ![image](https://user-images.githubusercontent.com/51938049/68647646-669cc600-0559-11ea-9413-9c29860d63f5.png)
   
   2.Recently the `xenial ` points Ubuntu 16.04.6 LTS which needs jdk > 8, but 8 is needed by Livy, so travis build failed.
   ![image](https://user-images.githubusercontent.com/51938049/68647880-edea3980-0559-11ea-8135-f5a68b3d303d.png)
   ![image](https://user-images.githubusercontent.com/51938049/68647919-0e19f880-055a-11ea-94b9-4f19099654cb.png)
   
   3. So I change the dist to `trusty` which points to Ubuntu 14.04.5 LTS according to the travis doc, and travis build successfully.
   ![image](https://user-images.githubusercontent.com/51938049/68648018-4d484980-055a-11ea-8358-c4234d7cf56c.png)
   ![image](https://user-images.githubusercontent.com/51938049/68648028-5507ee00-055a-11ea-8f6e-7c7efcbc6390.png)
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Nov/19 06:40;githubbot;600","jerryshao commented on pull request #257: [LIVY-711][TEST] Fix Travis fails to build on Ubuntu16.04
URL: https://github.com/apache/incubator-livy/pull/257
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Nov/19 07:35;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/19 02:16;runzhiwang;image-2019-11-12-10-16-27-108.png;https://issues.apache.org/jira/secure/attachment/12985574/image-2019-11-12-10-16-27-108.png","12/Nov/19 06:25;runzhiwang;image-2019-11-12-14-25-37-189.png;https://issues.apache.org/jira/secure/attachment/12985585/image-2019-11-12-14-25-37-189.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 12 07:37:09 UTC 2019,,,,,,,,,,"0|z08hmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Nov/19 02:17;runzhiwang;Iâ€˜m working on it.;;;","12/Nov/19 07:37;jerryshao;Issue resolved by pull request 257
https://github.com/apache/incubator-livy/pull/257;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The version of curator jars are not aligned,LIVY-708,13266819,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,yihengw,yihengw,yihengw,07/Nov/19 12:13,12/Nov/19 01:57,19/Dec/25 04:15,12/Nov/19 01:57,0.6.0,,,0.7.0,,Server,,,,,,,,,,0,,,,,,"Livy server has dependency of Apache Curator through hadoop client. However, the versions of the curator jars are not aligned. Here're the curator jars after build

* curator-client-2.7.1.jar
* curator-framework-2.7.1.jar
* curator-recipes-2.6.0.jar

This will cause Method not found issue in some case:
Exception in thread ""main"" java.lang.NoSuchMethodError: 
{code:bash}
org.apache.curator.utils.PathUtils.validatePath(Ljava/lang/String;)V
{code}",,"yiheng commented on pull request #256: [LIVY-708][Server] Align curator jars version
URL: https://github.com/apache/incubator-livy/pull/256
 
 
   ## What changes were proposed in this pull request?
   Livy server has dependency of Apache Curator through hadoop client. However, the versions of the curator jars are not aligned. Here're the curator jars after build
   
   * curator-client-2.7.1.jar
   * curator-framework-2.7.1.jar
   * curator-recipes-2.6.0.jar
   This will cause Method not found issue in some case:
   ```
   Exception in thread ""main"" java.lang.NoSuchMethodError:
   org.apache.curator.utils.PathUtils.validatePath(Ljava/lang/String;)V
   ```
   This patch specify the version of curator-recipes to 2.7.1. 
   
   ## How was this patch tested?
   Manually test in the env where no such method exception are thrown.
   Existing UT/IT
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 12:21;githubbot;600","jerryshao commented on pull request #256: [LIVY-708][Server] Align curator jars version
URL: https://github.com/apache/incubator-livy/pull/256
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Nov/19 01:54;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 12 01:57:13 UTC 2019,,,,,,,,,,"0|z08cu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Nov/19 01:57;jerryshao;Issue resolved by pull request 256
https://github.com/apache/incubator-livy/pull/256;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add audit log for SqlJobs from ThriftServer,LIVY-707,13266530,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,Bone An,Bone An,Bone An,06/Nov/19 09:14,14/Nov/19 01:53,19/Dec/25 04:15,14/Nov/19 01:53,,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,"The audit Log style is below


{code:java}
19/11/06 16:38:30 INFO ThriftServerAudit$: user: test ipAddress: 10.25.22.46 query: select count(*) from test1 beforeExecute: 1573029416951 afterExecute: 1573029510972 time spent: 94021
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 14 01:53:48 UTC 2019,,,,,,,,,,"0|z08b1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Nov/19 01:53;jerryshao;Issue resolved by pull request 255
https://github.com/apache/incubator-livy/pull/255;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support getting keystore password from Hadoop credential provider for Thrift server,LIVY-705,13265826,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,wypoon,wypoon,wypoon,01/Nov/19 20:11,06/Nov/19 09:57,19/Dec/25 04:15,06/Nov/19 09:56,0.6.0,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,LIVY-475 added support for getting the keystore password and key password from a Hadoop credential provider file. The keystore password is also needed for SSL/TLS support in the Thrift server. We should extend the support for getting the keystore password from the Hadoop credential provider to the Thrift server as well.,,"wypoon commented on pull request #253: [LIVY-705][THRIFT] Support getting keystore password from Hadoop credential provider
URL: https://github.com/apache/incubator-livy/pull/253
 
 
   ## What changes were proposed in this pull request?
   
   https://issues.apache.org/jira/browse/LIVY-705
   
   LIVY-475 added support for getting the keystore password and key password from a Hadoop credential provider file. The keystore password is also needed for SSL/TLS support in the Thrift server. In this change, we extend the support for getting the keystore password from the Hadoop credential provider to the Thrift server as well.
   
   ## How was this patch tested?
   
   Manually tested a Livy Thrift server that has livy.server.thrift.use.SSL=true, using both binary and http mode. Configured keystore password in a Hadoop credential provider file and provided the path to this file in livy.hadoop.security.credential.provider.path.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Nov/19 20:57;githubbot;600","mgaido91 commented on pull request #253: [LIVY-705][THRIFT] Support getting keystore password from Hadoop credential provider
URL: https://github.com/apache/incubator-livy/pull/253
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Nov/19 09:54;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 06 09:56:45 UTC 2019,,,,,,,,,,"0|z086pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Nov/19 09:56;mgaido;Issue resolved by PR https://github.com/apache/incubator-livy/pull/253.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Submit Spark apps to Kubernetes,LIVY-702,13264626,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,asifkhatri,jahstreet,jahstreet,26/Oct/19 21:41,21/Mar/25 07:55,19/Dec/25 04:15,10/Jul/24 09:06,,,,0.9.0,,,,,,,,,,,,4,,,,,,"Provide a way to submit Spark apps to Kubernetes. Points to cover:
 * Support Batch sessions
 * Support Interactive sessions
 * Collect logs and diagnostics information
 * Restore sessions monitoring after Livy restarts
 * GC created Kubernetes resources",Kubernetes,"cyliu0204 commented on issue #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-617649746


   We use spark magic in jupyter to connect livy to start the Spark cluster in kubernetes. We have started to use this patch in our work. As more and more applications migrate towards the cloud, this patch is definitely very valuable.  hopefully this feature can be merged asapï¼Œso that more people can see and use this feature


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Apr/20 09:00;githubbot;600","cyliu0204 edited a comment on issue #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-617649746


   In our case, we use spark magic in jupyter to connect livy to start the Spark cluster in kubernetes. We have started to use this patch in our work. As more and more applications migrate towards the cloud, this patch is definitely very valuable.  hopefully this feature can be merged asapï¼Œso that more people can see and use this feature


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Apr/20 09:01;githubbot;600","jahstreet commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-626326229


   > I can take a chance to review this, but I'm not an export of k8s, may not fully understand the pros and cons of the implementation.
   
   @jerryshao , have you given it a try already?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/May/20 13:13;githubbot;600","lukatera commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-644585957


   I had some issues with using ZooKeeper as a recovery method. When one of the zookeepers is not up in Kubernetes, its address is not published (i.e. there is none), and thus Livy cannot determine the zookeeper host. With the current version of ZooKeeper, Livy crashes because of a bug in ZooKeeper client. This has been fixed though in more recent versions (3.5.2+): [ZOOKEEPER-1576](https://zookeeper.apache.org/doc/r3.5.2-alpha/releasenotes.html).
   
   In short, Livy cannot start if 2 out of 3 ZooKeepers are up at the moment, even though it should be able to do so. Bumping the ZooKeeper client version should resolve the issue.
   
   @jahstreet Can we update ZooKeeper version in this PR as well?
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jun/20 07:28;githubbot;600","jahstreet commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-644612554


   Hi @lukatera , upgrading ZooKeeper client is out of scope of this PR and is a good candidate for a separate one. Feel free to open it. Later we can rebase this one to master once it is merged and you can cherry-pick it locally (or to your fork) to add it to this PR code. Will it work for you?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jun/20 08:20;githubbot;600","lukatera commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-644650074


   > Hi @lukatera , upgrading ZooKeeper client is out of scope of this PR and is a good candidate for a separate one. Feel free to open it. Later we can rebase this one to master once it is merged and you can cherry-pick it locally (or to your fork) to add it to this PR code. Will it work for you?
   
   Sure, makes sense. Thanks :)
   Is there a single branch somewhere where all the K8S functionality is present so I can try to build it locally?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jun/20 09:29;githubbot;600","jahstreet commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-644914983


   @lukatera , you can check that PR: https://github.com/apache/incubator-livy/pull/167 . Basically this PR is the clean backport from the former one.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jun/20 17:50;githubbot;600","gmcoringa commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-658342256


   Thanks @jahstreet for your effort. 
   Tested with:
   * kubernetes 1.15.11
   * Spark 2.4.5
   
   What is holding this pr and https://github.com/apache/incubator-livy/pull/252?
   
   Also, @jahstreet if you could add me in the threads I can explain our use case, where we use jupyter to schedule spark jobs on kubernetes.
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jul/20 18:32;githubbot;600","jahstreet commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-667501084


   @gmcoringa , thank you for the feedback. Can I have your e-mail to add you to the thread?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/20 09:16;githubbot;600","jahstreet edited a comment on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-667501084


   @gmcoringa , thank you for the feedback. Can I have your e-mail to add you to the thread?
   
   > What is holding this pr and #252?
   
   It should be reviewed and approved by the maintainers. No luck to attract enough attention so far and I'm continuing maintaining the fork.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/20 09:17;githubbot;600","gmcoringa commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-667694334


   > @gmcoringa , thank you for the feedback. Can I have your e-mail to add you to the thread?
   santosfabianov@gmail.com
   
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/20 16:20;githubbot;600","gmcoringa edited a comment on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-667694334


   > @gmcoringa , thank you for the feedback. Can I have your e-mail to add you to the thread?
   Sure @jahstreet santosfabianov@gmail.com
   
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/20 16:21;githubbot;600","jesinity commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-692028122


   is there any hope to see this merged anytime soonish?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/20 12:45;githubbot;600","idzikovsky commented on a change in pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#discussion_r493348458



##########
File path: rsc/src/main/java/org/apache/livy/rsc/driver/RSCDriver.java
##########
@@ -169,6 +169,13 @@ private void initializeServer() throws Exception {
     // on the cluster, it would be tricky to solve that problem in a generic way.
     livyConf.set(RPC_SERVER_ADDRESS, null);
 
+    // If we are running on Kubernetes, set RPC_SERVER_ADDRESS from ""spark.driver.host"" option,
+    // which is set in class org.apache.spark.deploy.k8s.features.DriverServiceFeatureStep:
+    // line 61: val driverHostname = s""$resolvedServiceName.${kubernetesConf.namespace()}.svc""
+    if (livyConf.isRunningOnKubernetes()) {
+      livyConf.set(RPC_SERVER_ADDRESS, conf.get(""spark.driver.host""));
+    }
+

Review comment:
       By some reason this version does not work for me like it was with the same piece from https://github.com/apache/incubator-livy/pull/167.
   
   I got the same exception as here: https://github.com/apache/incubator-livy/pull/167#issuecomment-485878996
   
   I'm not sure why, but it seems like it's because the `livy.spark.master` property is not set inside of Spark Driver of Livy Session on my env.
   While the same piece from https://github.com/apache/incubator-livy/pull/167 works fine:
   ```
       if (conf.get(""spark.master"").startsWith(""k8s"")) {
         livyConf.set(RPC_SERVER_ADDRESS, conf.get(""spark.driver.host""));
       }
   ```
   
   But I'm not using code from your branch, but rather backporting your patch to our Livy build which is based on older Livy version, so maybe it's the cause.
   
   On the other hand, during quick lookup I've not found any code that bypass `livy.spark.master` property into driver, so I see no reason why it should work here.
   If I get a chance to test this issue on build from this PR, I'll share what get.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Sep/20 09:11;githubbot;600","jahstreet commented on a change in pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#discussion_r494899201



##########
File path: rsc/src/main/java/org/apache/livy/rsc/driver/RSCDriver.java
##########
@@ -169,6 +169,13 @@ private void initializeServer() throws Exception {
     // on the cluster, it would be tricky to solve that problem in a generic way.
     livyConf.set(RPC_SERVER_ADDRESS, null);
 
+    // If we are running on Kubernetes, set RPC_SERVER_ADDRESS from ""spark.driver.host"" option,
+    // which is set in class org.apache.spark.deploy.k8s.features.DriverServiceFeatureStep:
+    // line 61: val driverHostname = s""$resolvedServiceName.${kubernetesConf.namespace()}.svc""
+    if (livyConf.isRunningOnKubernetes()) {
+      livyConf.set(RPC_SERVER_ADDRESS, conf.get(""spark.driver.host""));
+    }
+

Review comment:
       Might be related to the backporting indeed. Will be happy to help once you get more debug info.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Sep/20 10:33;githubbot;600","jahstreet commented on a change in pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#discussion_r494899201



##########
File path: rsc/src/main/java/org/apache/livy/rsc/driver/RSCDriver.java
##########
@@ -169,6 +169,13 @@ private void initializeServer() throws Exception {
     // on the cluster, it would be tricky to solve that problem in a generic way.
     livyConf.set(RPC_SERVER_ADDRESS, null);
 
+    // If we are running on Kubernetes, set RPC_SERVER_ADDRESS from ""spark.driver.host"" option,
+    // which is set in class org.apache.spark.deploy.k8s.features.DriverServiceFeatureStep:
+    // line 61: val driverHostname = s""$resolvedServiceName.${kubernetesConf.namespace()}.svc""
+    if (livyConf.isRunningOnKubernetes()) {
+      livyConf.set(RPC_SERVER_ADDRESS, conf.get(""spark.driver.host""));
+    }
+

Review comment:
       Might be related to the backporting indeed. Will be happy to help once you get more debug info.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Sep/20 13:46;githubbot;600","jahstreet commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-704213459


   Good news, yesterday I've upgraded the helm charts to Spark 3.0.1 which unlocks K8s API 1.18.x usage. Feel free to try it out with [this guide](https://github.com/JahstreetOrg/spark-on-kubernetes-helm/blob/master/DOCUMENTATION.md).
   In the meantime I'm going to backport the required changes to this PR.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Oct/20 11:45;githubbot;600","jahstreet commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-704427266


   Tested with 1.18.0 only, so yeah, 1.18.0 would be better to say. Though I haven't tried with higher versions, in case you can modify the chart locally and remove the kubeVersion restrictions you can try it out on 1.18.9.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Oct/20 17:21;githubbot;600","kyprifog commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-704463818


   After modifying the limit bounds on chart locally I just tested and it worked!
   
   ![image](https://user-images.githubusercontent.com/991756/95244173-9d1da800-07df-11eb-9327-9887b6c4bd0f.png)
   ![image (1)](https://user-images.githubusercontent.com/991756/95244183-9f800200-07df-11eb-927e-10623e18df20.png)
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Oct/20 18:24;githubbot;600","jahstreet commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-704464910


   > After modifying the limit bounds on chart locally I just tested and it worked!
   
   Thx for the test, I'll patch the chart then.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Oct/20 18:26;githubbot;600","jahstreet edited a comment on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-704464910


   > After modifying the limit bounds on chart locally I just tested and it worked!
   
   Thx for the test, I'll patch the chart then (also feel free to create a PR in case you're interested in contributions).


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Oct/20 18:26;githubbot;600","kyprifog commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-705142269


   Sure thing, here you go:  https://github.com/JahstreetOrg/spark-on-kubernetes-helm/pull/43/files


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Oct/20 19:19;githubbot;600","zhenlohuang commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-788837988


   It is a very nice feature! Thanks for your efforts. 
   
   @ Committees  Do we have a plan to merge this feature?
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Mar/21 11:52;githubbot;600","idzikovsky commented on a change in pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#discussion_r631094385



##########
File path: rsc/src/main/java/org/apache/livy/rsc/driver/RSCDriver.java
##########
@@ -169,6 +169,13 @@ private void initializeServer() throws Exception {
     // on the cluster, it would be tricky to solve that problem in a generic way.
     livyConf.set(RPC_SERVER_ADDRESS, null);
 
+    // If we are running on Kubernetes, set RPC_SERVER_ADDRESS from ""spark.driver.host"" option,
+    // which is set in class org.apache.spark.deploy.k8s.features.DriverServiceFeatureStep:
+    // line 61: val driverHostname = s""$resolvedServiceName.${kubernetesConf.namespace()}.svc""
+    if (livyConf.isRunningOnKubernetes()) {
+      livyConf.set(RPC_SERVER_ADDRESS, conf.get(""spark.driver.host""));
+    }
+

Review comment:
       I've tried to use the latest Livy with your patches and the issue has not appeared. So it seems like I got that problem because I've backported something wrong.
   Anyway, thank you for help!




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/May/21 14:28;githubbot;600","jmcgrath207 commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-923591706


   Any plans to merge this?


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Sep/21 03:34;githubbot;600","butaozhang commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-927217815


   Good feature! Why didn't the livy community merge this feature?  Livy community seems very inactive and no code updates since  the last year. . .


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Sep/21 02:41;githubbot;600","naru014 commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-947484929


   Please let know, anytime soon Livy on K8 support PRs going to be merged?


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Oct/21 09:22;githubbot;600","naru014 commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-948305856


   @jahstreet All these are already part of the master branch in your repo right?
   https://github.com/JahstreetOrg/spark-on-kubernetes-helm
   
   Please confirm. 
   I have a use case for this and am trying to use it in my kubernetes cluster.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Oct/21 06:44;githubbot;600","jahstreet edited a comment on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-948387027


   Hi @naru014 , yes. To be precise - this is the branch from which my setup is build https://github.com/jahstreet/incubator-livy/tree/merge/first. And this is how I build it:
   - https://github.com/JahstreetOrg/spark-on-kubernetes-helm/blob/master/charts/livy/values.yaml#L7-L9
   - https://github.com/JahstreetOrg/spark-on-kubernetes-docker/blob/master/livy-spark/0.8.0-incubating-spark_3.0.1_2.12-hadoop_3.2.0_cloud/Dockerfile#L7-L19
   - 
   Hope it helps, let me know in case of issues.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Oct/21 08:38;githubbot;600","jahstreet commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-948387027


   Hi @naru014 , yes. To be precise - this is the branch from which my setup is build https://github.com/jahstreet/incubator-livy/tree/merge/first. And this is how I build it:
   - https://github.com/JahstreetOrg/spark-on-kubernetes-helm/blob/master/charts/livy/values.yaml#L7-L9
   - https://github.com/JahstreetOrg/spark-on-kubernetes-docker/blob/master/livy-spark/0.8.0-incubating-spark_3.0.1_2.12-hadoop_3.2.0_cloud/Dockerfile#L7-L19
   Hope it helps, let me know in case of issues.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Oct/21 08:38;githubbot;600","PerilousApricot commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1005081554


   This looks very useful! Any plans to merge?


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Jan/22 18:50;githubbot;600","jahstreet commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1005495942


   @PerilousApricot thx for the feedback. FYI I do keep an eye on it waiting for the reviewers and at least 1 approval from the repo admin so that someone could press the merge button in the end. Your review and (hopefully) approval would help a lot ðŸ™ . Also the short story about how you and/or your team would benefit from this feature might be helpful to attract more attention to the PR.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Jan/22 08:57;githubbot;600","PerilousApricot commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1005558192


   @jahstreet I've only done some preliminary messing around with your branch, but it works on k8s for me.
   
   I'm in academia, which means two things -- we have on-prem deployments, and we have chaotic use patterns. We have 200 users trying to use 2k cores via notebooks. Traditional spark deployments are inefficient because we have to statically partition the cluster, which means that a) cores stay idle b) there is no shared caches between user applications. 
   
   Really livy is the only good solution for this form of multitenancy, but it doesn't support k8s in master, so it's not good


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Jan/22 10:22;githubbot;600","haripriya-b commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1009275174


   Hi @jahstreet , are there any plans for adding support for Spark 3.2 in this PR? 


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/22 19:35;githubbot;600","jahstreet commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1011105187


   Hi @haripriya-b ,
   
   Eventually yes. Going to get to it in the coming months to sync with the latest updates in the world.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jan/22 14:32;githubbot;600","nanaones commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1025901205


   > 
   
   also very excited about the release of this feature. ðŸ‘


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jan/22 15:32;githubbot;600","Rustem commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1039720868


   Really great work @jahstreet . Any plans to upgrade with most recent kuber version ? 


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Feb/22 00:37;githubbot;600","jahstreet commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1040088807


   Hi @Rustem , thanks for your feedback. Hope to get to it in the coming month or 2.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Feb/22 10:11;githubbot;600","jahstreet commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1040088807


   Hi @Rustem , thanks for your feedback. Hope to get to it in the coming month or 2.


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Feb/22 18:34;githubbot;600","Rustem commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1039720868


   Really great work @jahstreet . Any plans to upgrade with most recent kuber version ? 


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Feb/22 18:36;githubbot;600","alexlang74 commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1080300578


   I agree - this is excellent work, @jahstreet ! Too bad that the Livy community is dormant, and not able to pick up your changes... I echo @Rustem 's request: I could easily live with Spark 3.0 (or even ""backlevel"" Spark 2.4), but would be great to be able to support a newer Kubernetes version (e.g., 1.23)...


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Mar/22 07:35;githubbot;600","nanaones commented on pull request #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1085293078


   Why is this important feature PR still open?
   Any problem?
   @jahstreet 


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Apr/22 01:16;githubbot;600","jdcohen220 commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1105272603

   We would love to start using this once its released.


;21/Apr/22 14:15;githubbot;600","ringtail commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1166967294

   Hey guys, are there still block issues left?


;27/Jun/22 07:10;githubbot;600","cometta commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1259469874

   any estimated time frame this ticket can be merged?


;27/Sep/22 12:58;githubbot;600","anistal commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1370695688

   I agree with all the comments: a lot of effort that should be at least considered


;04/Jan/23 09:41;githubbot;600","jinhaichen commented on code in PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#discussion_r1136699910


##########
rsc/src/main/java/org/apache/livy/rsc/RSCConf.java:
##########
@@ -151,6 +152,12 @@ public String findLocalAddress() throws IOException {
     return address.getCanonicalHostName();
   }
 
+  public boolean isRunningOnKubernetes() {
+    return Optional.ofNullable(get(""livy.spark.master""))
+            .filter(s -> s.startsWith(""k8s""))
+            .isPresent();
+  }
+

Review Comment:
   this function may always return false, since ""livy.spark.master"" will not get by RSCConf



;15/Mar/23 08:38;githubbot;600","idzikovsky commented on code in PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#discussion_r1137058321


##########
rsc/src/main/java/org/apache/livy/rsc/RSCConf.java:
##########
@@ -151,6 +152,12 @@ public String findLocalAddress() throws IOException {
     return address.getCanonicalHostName();
   }
 
+  public boolean isRunningOnKubernetes() {
+    return Optional.ofNullable(get(""livy.spark.master""))
+            .filter(s -> s.startsWith(""k8s""))
+            .isPresent();
+  }
+

Review Comment:
   Yeah, seems like that's what I've faced in https://github.com/apache/incubator-livy/pull/249#discussion_r493348458
   
   However it seems like it's not affecting functionality, as this function is used while setting `RPC_SERVER_ADDRESS` here:
   https://github.com/apache/incubator-livy/pull/249/files/b87c0cebb65ce7f34e6b4b6b738095be6254cf69#diff-43114318c4b009c2404f7eb326a84c184fb1501a3237c49a771df851d0f6f328R172-R178
   
   And the value of `RPC_SERVER_ADDRESS` is not used anyway since Livy 0.7 because of things I've explained in #388.
   



;15/Mar/23 13:21;githubbot;600","idzikovsky commented on code in PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#discussion_r1137058321


##########
rsc/src/main/java/org/apache/livy/rsc/RSCConf.java:
##########
@@ -151,6 +152,12 @@ public String findLocalAddress() throws IOException {
     return address.getCanonicalHostName();
   }
 
+  public boolean isRunningOnKubernetes() {
+    return Optional.ofNullable(get(""livy.spark.master""))
+            .filter(s -> s.startsWith(""k8s""))
+            .isPresent();
+  }
+

Review Comment:
   Yeah, seems like that's what I've faced in https://github.com/apache/incubator-livy/pull/249#discussion_r493348458
   
   However, it seems like it's not affecting functionality, as this function is used while setting `RPC_SERVER_ADDRESS` here:
   https://github.com/apache/incubator-livy/pull/249/files/b87c0cebb65ce7f34e6b4b6b738095be6254cf69#diff-43114318c4b009c2404f7eb326a84c184fb1501a3237c49a771df851d0f6f328R172-R178
   
   And the value of `RPC_SERVER_ADDRESS` is not used anyway since Livy 0.7 because of things I've explained in #388.
   



;15/Mar/23 13:22;githubbot;600","askhatri commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1487365329

   I have validated this fix in [the new git branch](https://github.com/askhatri/incubator-livy/tree/validating-livy-702-fix). I found that the fix is working as expected. The detailed steps used during the validation are documented at [README.md](https://github.com/askhatri/incubator-livy/tree/validating-livy-702-fix/dev/kubernetes). Should we consider merging the fix?


;28/Mar/23 17:55;githubbot;600","lmccay commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1487607032

   @askhatri - I would say that you can merge this given your review and testing.


;28/Mar/23 21:19;githubbot;600","gyogal commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1488124617

   Thank you @lmccay! This LGTM as well and based on @askhatri 's testing, if there are no objections, I think we should go ahead and merge this. We can address the remaining issues in separate tickets.


;29/Mar/23 08:05;githubbot;600","jahstreet commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1488133810

   Happy to see things going in this PR ðŸŽ‰ . Thank you a lot folks for putting the effort in reviewing and testing the change. 
   
   @askhatri can I help you with anything to test the work?
   
   I'd love to pay my time to finalize the activity here and then I have something to offer on top of it to contribute with.


;29/Mar/23 08:11;githubbot;600","askhatri commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1488180192

   > Happy to see things going in this PR ðŸŽ‰ . Thank you a lot folks for putting the effort in reviewing and testing the change.
   > 
   > @askhatri can I help you with anything to test the work?
   > 
   > I'd love to pay my time to finalize the activity here and then I have something to offer on top of it to contribute with.
   
   Thank you, @jahstreet for offering your help in testing.
   
   During my initial testing, I found that the code is working as expected. Only one observation is that we might need to upgrade [spark-on-kubernetes-helm](https://github.com/JahstreetOrg/spark-on-kubernetes-helm) to support Helm Chart 3.x and Kubernetes latest version 1.24 or higher. 


;29/Mar/23 08:45;githubbot;600","jahstreet commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1494052366

   > > Happy to see things going in this PR ðŸŽ‰ . Thank you a lot folks for putting the effort in reviewing and testing the change.
   > > @askhatri can I help you with anything to test the work?
   > > I'd love to pay my time to finalize the activity here and then I have something to offer on top of it to contribute with.
   > 
   > Thank you, @jahstreet for offering your help in testing.
   > 
   > During my initial testing, I found that the code is working as expected. Only one observation is that we might need to upgrade [spark-on-kubernetes-helm](https://github.com/JahstreetOrg/spark-on-kubernetes-helm) to support Helm Chart 3.x and Kubernetes latest version 1.24 or higher.
   
   Looking into it, not 100% sure we can get latest K8s version, given latest Spark 3.3.2 works with Fabric8 client 5.12.2 which aims for K8s <= 1.23.13 (as per [compatibility matrix](https://github.com/fabric8io/kubernetes-client#compatibility-matrix)). Will share my findings ...


;03/Apr/23 10:13;githubbot;600","askhatri commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1494134410

   > > > Happy to see things going in this PR ðŸŽ‰ . Thank you a lot folks for putting the effort in reviewing and testing the change.
   > > > @askhatri can I help you with anything to test the work?
   > > > I'd love to pay my time to finalize the activity here and then I have something to offer on top of it to contribute with.
   > > 
   > > 
   > > Thank you, @jahstreet for offering your help in testing.
   > > During my initial testing, I found that the code is working as expected. Only one observation is that we might need to upgrade [spark-on-kubernetes-helm](https://github.com/JahstreetOrg/spark-on-kubernetes-helm) to support Helm Chart 3.x and Kubernetes latest version 1.24 or higher.
   > 
   > Looking into it, not 100% sure we can get latest K8s version, given latest Spark 3.3.2 works with Fabric8 client 5.12.2 which aims for K8s <= 1.23.13 (as per [compatibility matrix](https://github.com/fabric8io/kubernetes-client#compatibility-matrix)). Will share my findings ...
   
   Thank you @jahstreet 


;03/Apr/23 11:15;githubbot;600","satishdalli commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1540199116

   Absolutely great work @jahstreet. We are waiting for PR with Helm 3 and k8s latest versions support. currently, we are using livy with consistent resources. we want to try this in AKS, EKS, and on-prem k8s clusters as a server-less livy.


;09/May/23 14:08;githubbot;600","lmccay commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1541032091

   Hey Folks - I noticed this JIRA is listed for 0.8.0 and would like to get a sense for how far out this may be.
   I'm trying to burn down the release blockers so that we can tackle the process of the first release since the reboot.
   If I don't hear otherwise, I will move it to 0.9.0 and we can discuss pulling it back in on the JIRA - if need be.


;09/May/23 23:53;githubbot;600","jahstreet commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1545280399

   Hi @lmccay , thank you for keeping eye on it.
   
   I think this PR is already battle tested and good to go. There is already a chain of work done on top of it by me and other people who left feedback on this chunk of work. If we make it a part of master I'm 100% sure I won't be the only one pushing Livy project to the world of K8s. Besides that, seeing the progress after the years of waiting would boost the motivation to continue contributions... So, including the upgrade of dependencies to support latest K8s and Spark versions, I propose to tackle those in separate PRs. How do you feel about merging it to 0.8 and is there anything formal we should do to make it happen?
   
   (resolving the merge conflicts in the meantime)


;12/May/23 07:09;githubbot;600","idzikovsky commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1545469410

   Yeah. We've been using Livy on K8s with fixes from this PR and PR #252 for near 2 years now, in different configurations. Including different Spark versions starting from 2.4.4 and up to 3.3.1 (however some fixes where made in Livy itself to make it compatible with Spark 3.3).
   Everything seems to be fine by now, except for the setup with Istio.
   To resolve those issues, we applied fix from #388 and reverted the thing which I described in https://github.com/apache/incubator-livy/pull/249#discussion_r1137058321 and https://github.com/apache/incubator-livy/pull/249#discussion_r493348458.
   
   But in general everything works fantastic.
   Thank you @jahstreet 


;12/May/23 09:40;githubbot;600","lmccay commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1545720672

   @jahstreet and @idzikovsky  - if we merge this before branching from 0.8.0 and it doesn't cause any issues due to other work not being there yet then I would have no problem with doing so. I'm personally not in a position to +1 the merge. Would someone that has tested it and/or tested with it inplace but not necessarily exercised be able to +1 it as a review?


;12/May/23 13:08;githubbot;600","jahstreet commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1661828216

   @lmccay I think we need to summon a maintainer with K8s expertise, is that something you expect? Do you know any name(s) we can put here and follow-up on that together?


;02/Aug/23 09:07;githubbot;600","lmouhib commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1728322850

   @jahstreet The implementation is great. I got couple of remarks/questions on the way it handle the logs and the ingress, the current implementation is opinionated for ngnix and lokki, while both are widely adopted, how can someone with another ingress controller use their own? I understand the log might be a bit more challenging to provide the native UI integration, in that case maybe offer the possibility to use a sidecar for log shipping?


;20/Sep/23 19:39;githubbot;600","ozsoyler commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-1768483364

   Hi. Is that possible to merge this into apache-incubator-livy officially on new version ""0.9.0""? I guess if it merges, we will be able to use livy on kubernetes for launching our awesome spark jobs! Thanks..


;18/Oct/23 13:39;githubbot;600","askhatri commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-2154741245

   Adding Kubernetes support to Apache Livy is a valuable enhancement for the next release. Should we consider merging this into the master branch?


;07/Jun/24 12:31;githubbot;600","lmccay commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-2155322653

   @askhatri - yes, I think we should push for an active approval on the merge of this. We can likely mark this as an important contribution for the next release. Let's move this forward now. 


;07/Jun/24 18:29;githubbot;600","jesinity commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-2155376786

   I've been following this issue since forever,  as it would have been highly beneficial somewhat like 2 companies ago.
   The work has been done but it was never merged. Is there maybe some passive obstruction against it? Looks like so.


;07/Jun/24 19:11;githubbot;600","idzikovsky commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-2155455988

   @jesinity there is no passive obstruction against it here.
   The problem is that there are no active Livy maintainers who are willing to review this and/or familiar enough with K8s.


;07/Jun/24 19:49;githubbot;600","devstein commented on PR #249:
URL: https://github.com/apache/incubator-livy/pull/249#issuecomment-2155668921

   To help with the K8s review/perspective, I know there are teams that have been using this branch in production for years (cc @jpugliesi). 


;07/Jun/24 22:52;githubbot;600","vikas-saxena02 commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2216690590

   @gyogal @lmccay , can you please take a look at this PR?


;09/Jul/24 06:28;githubbot;600","jshmchenxi commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2216893596

   @askhatri Thanks for pushing this feature forward!
   
   As a side note, could you please add the original author, @jahstreet, to the author list? We've been using this feature for a year and truly appreciate Alex's initial effort in bringing it to Livy.


;09/Jul/24 08:09;githubbot;600","askhatri commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2217023939

   Hi @jshmchenxi , I have added the original author, @jahstreet, to the author list as suggested by you. Please let me know incase if any further change or correction required


;09/Jul/24 08:57;githubbot;600","jahstreet commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2217098628

   > Hi @jshmchenxi , I have added the original author, @jahstreet, to the author list as suggested by you. Please let me know incase if any further change or correction required
   
   Thx for mentioning mate, appreciate the credits ðŸ™ .


;09/Jul/24 09:10;githubbot;600","jahstreet commented on code in PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#discussion_r1670095313


##########
server/src/main/scala/org/apache/livy/utils/SparkKubernetesApp.scala:
##########
@@ -0,0 +1,739 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.livy.utils
+
+import java.net.URLEncoder
+import java.util.Collections
+import java.util.concurrent.TimeoutException
+
+import scala.annotation.tailrec
+import scala.collection.mutable.ArrayBuffer
+import scala.concurrent._
+import scala.concurrent.duration._
+import scala.language.postfixOps
+import scala.util.{Failure, Success, Try}
+import scala.util.control.NonFatal
+
+import io.fabric8.kubernetes.api.model._
+import io.fabric8.kubernetes.api.model.networking.v1.{Ingress, IngressBuilder}
+import io.fabric8.kubernetes.client.{Config, ConfigBuilder, _}
+import org.apache.commons.lang.StringUtils
+
+import org.apache.livy.{LivyConf, Logging, Utils}
+
+object SparkKubernetesApp extends Logging {
+
+  private val leakedAppTags = new java.util.concurrent.ConcurrentHashMap[String, Long]()
+
+  private val leakedAppsGCThread = new Thread() {
+    override def run(): Unit = {
+      import KubernetesExtensions._
+      while (true) {
+        if (!leakedAppTags.isEmpty) {
+          // kill the app if found it and remove it if exceeding a threshold
+          val iter = leakedAppTags.entrySet().iterator()
+          var isRemoved = false
+          val now = System.currentTimeMillis()
+          val apps = withRetry(kubernetesClient.getApplications())
+          while (iter.hasNext) {
+            val entry = iter.next()
+            apps.find(_.getApplicationTag.contains(entry.getKey))
+              .foreach({
+                app =>
+                  info(s""Kill leaked app ${app.getApplicationId}"")
+                  withRetry(kubernetesClient.killApplication(app))
+                  iter.remove()
+                  isRemoved = true
+              })
+            if (!isRemoved) {
+              if ((entry.getValue - now) > sessionLeakageCheckTimeout) {
+                iter.remove()
+                info(s""Remove leaked Kubernetes app tag ${entry.getKey}"")
+              }
+            }
+          }
+        }
+        Thread.sleep(sessionLeakageCheckInterval)
+      }
+    }
+  }
+
+  val RefreshServiceAccountTokenThread = new Thread() {
+    override def run(): Unit = {
+      while (true) {
+        var currentContext = new Context()
+        var currentContextName = new String
+        val config = kubernetesClient.getConfiguration
+        if (config.getCurrentContext != null) {
+          currentContext = config.getCurrentContext.getContext
+          currentContextName = config.getCurrentContext.getName
+        }
+
+        var newAccessToken = new String
+        val newestConfig = Config.autoConfigure(currentContextName)
+        newAccessToken = newestConfig.getOauthToken
+        info(s""Refresh a new token ${newAccessToken}"")
+
+        config.setOauthToken(newAccessToken)
+        kubernetesClient = new DefaultKubernetesClient(config)
+
+        // Token will expire 1 hour default, community recommend to update every 5 minutes
+        Thread.sleep(300000)
+      }
+    }
+  }
+
+  private var livyConf: LivyConf = _
+
+  private var cacheLogSize: Int = _
+  private var appLookupTimeout: FiniteDuration = _
+  private var pollInterval: FiniteDuration = _
+
+  private var sessionLeakageCheckTimeout: Long = _
+  private var sessionLeakageCheckInterval: Long = _
+
+  var kubernetesClient: DefaultKubernetesClient = _
+
+  def init(livyConf: LivyConf): Unit = {
+    this.livyConf = livyConf
+
+    // KubernetesClient is thread safe. Create once, share it across threads.
+    kubernetesClient =
+      KubernetesClientFactory.createKubernetesClient(livyConf)
+
+    cacheLogSize = livyConf.getInt(LivyConf.SPARK_LOGS_SIZE)
+    appLookupTimeout = livyConf.getTimeAsMs(LivyConf.KUBERNETES_APP_LOOKUP_TIMEOUT).milliseconds
+    pollInterval = livyConf.getTimeAsMs(LivyConf.KUBERNETES_POLL_INTERVAL).milliseconds
+
+    sessionLeakageCheckInterval =
+      livyConf.getTimeAsMs(LivyConf.KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL)
+    sessionLeakageCheckTimeout = livyConf.getTimeAsMs(LivyConf.KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT)
+
+    leakedAppsGCThread.setDaemon(true)
+    leakedAppsGCThread.setName(""LeakedAppsGCThread"")
+    leakedAppsGCThread.start()
+
+    RefreshServiceAccountTokenThread.
+      setName(""RefreshServiceAccountTokenThread"")
+    RefreshServiceAccountTokenThread.setDaemon(true)
+    RefreshServiceAccountTokenThread.start()
+  }
+
+  // Returning T, throwing the exception on failure
+  // When istio-proxy restarts, the access to K8s API from livy could be down
+  // until envoy comes back, which could take upto 30 seconds

Review Comment:
   Is istio/envoy or any ingress controller a prerequisite to using Livy with K8s?



##########
server/src/main/scala/org/apache/livy/LivyConf.scala:
##########
@@ -258,6 +258,63 @@ object LivyConf {
   // value specifies max attempts to retry when safe mode is ON in hdfs filesystem
   val HDFS_SAFE_MODE_MAX_RETRY_ATTEMPTS = Entry(""livy.server.hdfs.safe-mode.max.retry.attempts"", 12)
 
+  // Kubernetes oauth token file path.
+  val KUBERNETES_OAUTH_TOKEN_FILE = Entry(""livy.server.kubernetes.oauthTokenFile"", """")
+  // Kubernetes oauth token string value.
+  val KUBERNETES_OAUTH_TOKEN_VALUE = Entry(""livy.server.kubernetes.oauthTokenValue"", """")
+  // Kubernetes CA cert file path.
+  val KUBERNETES_CA_CERT_FILE = Entry(""livy.server.kubernetes.caCertFile"", """")
+  // Kubernetes client key file path.
+  val KUBERNETES_CLIENT_KEY_FILE = Entry(""livy.server.kubernetes.clientKeyFile"", """")
+  // Kubernetes client cert file path.
+  val KUBERNETES_CLIENT_CERT_FILE = Entry(""livy.server.kubernetes.clientCertFile"", """")
+
+  // If Livy can't find the Kubernetes app within this time, consider it lost.
+  val KUBERNETES_APP_LOOKUP_TIMEOUT = Entry(""livy.server.kubernetes.app-lookup-timeout"", ""600s"")
+  // How often Livy polls Kubernetes to refresh Kubernetes app state.
+  val KUBERNETES_POLL_INTERVAL = Entry(""livy.server.kubernetes.poll-interval"", ""15s"")
+
+  // How long to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT =
+    Entry(""livy.server.kubernetes.app-leakage.check-timeout"", ""600s"")
+  // How often to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL =
+    Entry(""livy.server.kubernetes.app-leakage.check-interval"", ""60s"")
+
+  // Weather to create Kubernetes Nginx Ingress for Spark UI.
+  val KUBERNETES_INGRESS_CREATE = Entry(""livy.server.kubernetes.ingress.create"", false)
+  // Kubernetes Ingress class name.
+  val KUBERNETES_INGRESS_CLASS_NAME = Entry(""livy.server.kubernetes.ingress.className"", """")
+  // Kubernetes Nginx Ingress protocol.
+  val KUBERNETES_INGRESS_PROTOCOL = Entry(""livy.server.kubernetes.ingress.protocol"", ""http"")
+  // Kubernetes Nginx Ingress host.
+  val KUBERNETES_INGRESS_HOST = Entry(""livy.server.kubernetes.ingress.host"", ""localhost"")
+  // Kubernetes Nginx Ingress additional configuration snippet.
+  val KUBERNETES_INGRESS_ADDITIONAL_CONF_SNIPPET =
+    Entry(""livy.server.kubernetes.ingress.additionalConfSnippet"", """")
+  // Kubernetes Nginx Ingress additional annotations: key1=value1;key2=value2;... .
+  val KUBERNETES_INGRESS_ADDITIONAL_ANNOTATIONS =
+    Entry(""livy.server.kubernetes.ingress.additionalAnnotations"", """")
+  // Kubernetes secret name for Nginx Ingress TLS.
+  // Is omitted if 'livy.server.kubernetes.ingress.protocol' value doesn't end with 's'
+  val KUBERNETES_INGRESS_TLS_SECRET_NAME =
+    Entry(""livy.server.kubernetes.ingress.tls.secretName"", ""spark-cluster-tls"")
+
+  val KUBERNETES_GRAFANA_LOKI_ENABLED = Entry(""livy.server.kubernetes.grafana.loki.enabled"", false)
+  val KUBERNETES_GRAFANA_URL = Entry(""livy.server.kubernetes.grafana.url"", ""http://localhost:3000"")
+  val KUBERNETES_GRAFANA_LOKI_DATASOURCE =
+    Entry(""livy.server.kubernetes.grafana.loki.datasource"", ""loki"")
+  val KUBERNETES_GRAFANA_TIME_RANGE = Entry(""livy.server.kubernetes.grafana.timeRange"", ""6h"")
+
+  // side car container for spark pods enabled?
+  val KUBERNETES_SPARK_SIDECAR_ENABLED =
+    Entry(""livy.server.kubernetes.spark.sidecar.enabled"", true)

Review Comment:
   What is the use case for this flag?



##########
server/src/main/scala/org/apache/livy/LivyConf.scala:
##########
@@ -258,6 +258,63 @@ object LivyConf {
   // value specifies max attempts to retry when safe mode is ON in hdfs filesystem
   val HDFS_SAFE_MODE_MAX_RETRY_ATTEMPTS = Entry(""livy.server.hdfs.safe-mode.max.retry.attempts"", 12)
 
+  // Kubernetes oauth token file path.
+  val KUBERNETES_OAUTH_TOKEN_FILE = Entry(""livy.server.kubernetes.oauthTokenFile"", """")
+  // Kubernetes oauth token string value.
+  val KUBERNETES_OAUTH_TOKEN_VALUE = Entry(""livy.server.kubernetes.oauthTokenValue"", """")
+  // Kubernetes CA cert file path.
+  val KUBERNETES_CA_CERT_FILE = Entry(""livy.server.kubernetes.caCertFile"", """")
+  // Kubernetes client key file path.
+  val KUBERNETES_CLIENT_KEY_FILE = Entry(""livy.server.kubernetes.clientKeyFile"", """")
+  // Kubernetes client cert file path.
+  val KUBERNETES_CLIENT_CERT_FILE = Entry(""livy.server.kubernetes.clientCertFile"", """")
+
+  // If Livy can't find the Kubernetes app within this time, consider it lost.
+  val KUBERNETES_APP_LOOKUP_TIMEOUT = Entry(""livy.server.kubernetes.app-lookup-timeout"", ""600s"")
+  // How often Livy polls Kubernetes to refresh Kubernetes app state.
+  val KUBERNETES_POLL_INTERVAL = Entry(""livy.server.kubernetes.poll-interval"", ""15s"")
+
+  // How long to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT =
+    Entry(""livy.server.kubernetes.app-leakage.check-timeout"", ""600s"")
+  // How often to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL =
+    Entry(""livy.server.kubernetes.app-leakage.check-interval"", ""60s"")
+
+  // Weather to create Kubernetes Nginx Ingress for Spark UI.
+  val KUBERNETES_INGRESS_CREATE = Entry(""livy.server.kubernetes.ingress.create"", false)
+  // Kubernetes Ingress class name.
+  val KUBERNETES_INGRESS_CLASS_NAME = Entry(""livy.server.kubernetes.ingress.className"", """")
+  // Kubernetes Nginx Ingress protocol.
+  val KUBERNETES_INGRESS_PROTOCOL = Entry(""livy.server.kubernetes.ingress.protocol"", ""http"")
+  // Kubernetes Nginx Ingress host.
+  val KUBERNETES_INGRESS_HOST = Entry(""livy.server.kubernetes.ingress.host"", ""localhost"")
+  // Kubernetes Nginx Ingress additional configuration snippet.
+  val KUBERNETES_INGRESS_ADDITIONAL_CONF_SNIPPET =
+    Entry(""livy.server.kubernetes.ingress.additionalConfSnippet"", """")
+  // Kubernetes Nginx Ingress additional annotations: key1=value1;key2=value2;... .
+  val KUBERNETES_INGRESS_ADDITIONAL_ANNOTATIONS =
+    Entry(""livy.server.kubernetes.ingress.additionalAnnotations"", """")
+  // Kubernetes secret name for Nginx Ingress TLS.
+  // Is omitted if 'livy.server.kubernetes.ingress.protocol' value doesn't end with 's'
+  val KUBERNETES_INGRESS_TLS_SECRET_NAME =
+    Entry(""livy.server.kubernetes.ingress.tls.secretName"", ""spark-cluster-tls"")
+
+  val KUBERNETES_GRAFANA_LOKI_ENABLED = Entry(""livy.server.kubernetes.grafana.loki.enabled"", false)
+  val KUBERNETES_GRAFANA_URL = Entry(""livy.server.kubernetes.grafana.url"", ""http://localhost:3000"")
+  val KUBERNETES_GRAFANA_LOKI_DATASOURCE =
+    Entry(""livy.server.kubernetes.grafana.loki.datasource"", ""loki"")
+  val KUBERNETES_GRAFANA_TIME_RANGE = Entry(""livy.server.kubernetes.grafana.timeRange"", ""6h"")

Review Comment:
   Great to add some docs about expectations to Grafana and Loki installation to make use of this configs.
   Same about installing Livy on K8s, we should give some guide to the community on how to set this up at least locally to play with. That will help to raise the adoption.
   I can help you with connecting the dots, ping me if you wanna discuss.



;09/Jul/24 09:22;githubbot;600","askhatri commented on code in PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#discussion_r1670118431


##########
server/src/main/scala/org/apache/livy/utils/SparkKubernetesApp.scala:
##########
@@ -0,0 +1,739 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.livy.utils
+
+import java.net.URLEncoder
+import java.util.Collections
+import java.util.concurrent.TimeoutException
+
+import scala.annotation.tailrec
+import scala.collection.mutable.ArrayBuffer
+import scala.concurrent._
+import scala.concurrent.duration._
+import scala.language.postfixOps
+import scala.util.{Failure, Success, Try}
+import scala.util.control.NonFatal
+
+import io.fabric8.kubernetes.api.model._
+import io.fabric8.kubernetes.api.model.networking.v1.{Ingress, IngressBuilder}
+import io.fabric8.kubernetes.client.{Config, ConfigBuilder, _}
+import org.apache.commons.lang.StringUtils
+
+import org.apache.livy.{LivyConf, Logging, Utils}
+
+object SparkKubernetesApp extends Logging {
+
+  private val leakedAppTags = new java.util.concurrent.ConcurrentHashMap[String, Long]()
+
+  private val leakedAppsGCThread = new Thread() {
+    override def run(): Unit = {
+      import KubernetesExtensions._
+      while (true) {
+        if (!leakedAppTags.isEmpty) {
+          // kill the app if found it and remove it if exceeding a threshold
+          val iter = leakedAppTags.entrySet().iterator()
+          var isRemoved = false
+          val now = System.currentTimeMillis()
+          val apps = withRetry(kubernetesClient.getApplications())
+          while (iter.hasNext) {
+            val entry = iter.next()
+            apps.find(_.getApplicationTag.contains(entry.getKey))
+              .foreach({
+                app =>
+                  info(s""Kill leaked app ${app.getApplicationId}"")
+                  withRetry(kubernetesClient.killApplication(app))
+                  iter.remove()
+                  isRemoved = true
+              })
+            if (!isRemoved) {
+              if ((entry.getValue - now) > sessionLeakageCheckTimeout) {
+                iter.remove()
+                info(s""Remove leaked Kubernetes app tag ${entry.getKey}"")
+              }
+            }
+          }
+        }
+        Thread.sleep(sessionLeakageCheckInterval)
+      }
+    }
+  }
+
+  val RefreshServiceAccountTokenThread = new Thread() {
+    override def run(): Unit = {
+      while (true) {
+        var currentContext = new Context()
+        var currentContextName = new String
+        val config = kubernetesClient.getConfiguration
+        if (config.getCurrentContext != null) {
+          currentContext = config.getCurrentContext.getContext
+          currentContextName = config.getCurrentContext.getName
+        }
+
+        var newAccessToken = new String
+        val newestConfig = Config.autoConfigure(currentContextName)
+        newAccessToken = newestConfig.getOauthToken
+        info(s""Refresh a new token ${newAccessToken}"")
+
+        config.setOauthToken(newAccessToken)
+        kubernetesClient = new DefaultKubernetesClient(config)
+
+        // Token will expire 1 hour default, community recommend to update every 5 minutes
+        Thread.sleep(300000)
+      }
+    }
+  }
+
+  private var livyConf: LivyConf = _
+
+  private var cacheLogSize: Int = _
+  private var appLookupTimeout: FiniteDuration = _
+  private var pollInterval: FiniteDuration = _
+
+  private var sessionLeakageCheckTimeout: Long = _
+  private var sessionLeakageCheckInterval: Long = _
+
+  var kubernetesClient: DefaultKubernetesClient = _
+
+  def init(livyConf: LivyConf): Unit = {
+    this.livyConf = livyConf
+
+    // KubernetesClient is thread safe. Create once, share it across threads.
+    kubernetesClient =
+      KubernetesClientFactory.createKubernetesClient(livyConf)
+
+    cacheLogSize = livyConf.getInt(LivyConf.SPARK_LOGS_SIZE)
+    appLookupTimeout = livyConf.getTimeAsMs(LivyConf.KUBERNETES_APP_LOOKUP_TIMEOUT).milliseconds
+    pollInterval = livyConf.getTimeAsMs(LivyConf.KUBERNETES_POLL_INTERVAL).milliseconds
+
+    sessionLeakageCheckInterval =
+      livyConf.getTimeAsMs(LivyConf.KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL)
+    sessionLeakageCheckTimeout = livyConf.getTimeAsMs(LivyConf.KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT)
+
+    leakedAppsGCThread.setDaemon(true)
+    leakedAppsGCThread.setName(""LeakedAppsGCThread"")
+    leakedAppsGCThread.start()
+
+    RefreshServiceAccountTokenThread.
+      setName(""RefreshServiceAccountTokenThread"")
+    RefreshServiceAccountTokenThread.setDaemon(true)
+    RefreshServiceAccountTokenThread.start()
+  }
+
+  // Returning T, throwing the exception on failure
+  // When istio-proxy restarts, the access to K8s API from livy could be down
+  // until envoy comes back, which could take upto 30 seconds

Review Comment:
   istio/envoy is optional for using Livy with K8s. It is just to collected the Livy logs for audit purpose. 



;09/Jul/24 09:25;githubbot;600","jahstreet commented on code in PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#discussion_r1670123773


##########
server/src/main/scala/org/apache/livy/LivyConf.scala:
##########
@@ -258,6 +258,63 @@ object LivyConf {
   // value specifies max attempts to retry when safe mode is ON in hdfs filesystem
   val HDFS_SAFE_MODE_MAX_RETRY_ATTEMPTS = Entry(""livy.server.hdfs.safe-mode.max.retry.attempts"", 12)
 
+  // Kubernetes oauth token file path.
+  val KUBERNETES_OAUTH_TOKEN_FILE = Entry(""livy.server.kubernetes.oauthTokenFile"", """")
+  // Kubernetes oauth token string value.
+  val KUBERNETES_OAUTH_TOKEN_VALUE = Entry(""livy.server.kubernetes.oauthTokenValue"", """")
+  // Kubernetes CA cert file path.
+  val KUBERNETES_CA_CERT_FILE = Entry(""livy.server.kubernetes.caCertFile"", """")
+  // Kubernetes client key file path.
+  val KUBERNETES_CLIENT_KEY_FILE = Entry(""livy.server.kubernetes.clientKeyFile"", """")
+  // Kubernetes client cert file path.
+  val KUBERNETES_CLIENT_CERT_FILE = Entry(""livy.server.kubernetes.clientCertFile"", """")
+
+  // If Livy can't find the Kubernetes app within this time, consider it lost.
+  val KUBERNETES_APP_LOOKUP_TIMEOUT = Entry(""livy.server.kubernetes.app-lookup-timeout"", ""600s"")
+  // How often Livy polls Kubernetes to refresh Kubernetes app state.
+  val KUBERNETES_POLL_INTERVAL = Entry(""livy.server.kubernetes.poll-interval"", ""15s"")
+
+  // How long to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT =
+    Entry(""livy.server.kubernetes.app-leakage.check-timeout"", ""600s"")
+  // How often to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL =
+    Entry(""livy.server.kubernetes.app-leakage.check-interval"", ""60s"")
+
+  // Weather to create Kubernetes Nginx Ingress for Spark UI.
+  val KUBERNETES_INGRESS_CREATE = Entry(""livy.server.kubernetes.ingress.create"", false)
+  // Kubernetes Ingress class name.
+  val KUBERNETES_INGRESS_CLASS_NAME = Entry(""livy.server.kubernetes.ingress.className"", """")
+  // Kubernetes Nginx Ingress protocol.
+  val KUBERNETES_INGRESS_PROTOCOL = Entry(""livy.server.kubernetes.ingress.protocol"", ""http"")
+  // Kubernetes Nginx Ingress host.
+  val KUBERNETES_INGRESS_HOST = Entry(""livy.server.kubernetes.ingress.host"", ""localhost"")
+  // Kubernetes Nginx Ingress additional configuration snippet.
+  val KUBERNETES_INGRESS_ADDITIONAL_CONF_SNIPPET =
+    Entry(""livy.server.kubernetes.ingress.additionalConfSnippet"", """")
+  // Kubernetes Nginx Ingress additional annotations: key1=value1;key2=value2;... .
+  val KUBERNETES_INGRESS_ADDITIONAL_ANNOTATIONS =
+    Entry(""livy.server.kubernetes.ingress.additionalAnnotations"", """")
+  // Kubernetes secret name for Nginx Ingress TLS.
+  // Is omitted if 'livy.server.kubernetes.ingress.protocol' value doesn't end with 's'
+  val KUBERNETES_INGRESS_TLS_SECRET_NAME =
+    Entry(""livy.server.kubernetes.ingress.tls.secretName"", ""spark-cluster-tls"")
+
+  val KUBERNETES_GRAFANA_LOKI_ENABLED = Entry(""livy.server.kubernetes.grafana.loki.enabled"", false)
+  val KUBERNETES_GRAFANA_URL = Entry(""livy.server.kubernetes.grafana.url"", ""http://localhost:3000"")
+  val KUBERNETES_GRAFANA_LOKI_DATASOURCE =
+    Entry(""livy.server.kubernetes.grafana.loki.datasource"", ""loki"")
+  val KUBERNETES_GRAFANA_TIME_RANGE = Entry(""livy.server.kubernetes.grafana.timeRange"", ""6h"")

Review Comment:
   Ahh, I see it in https://github.com/askhatri/livycluster . Great at least to leave somewhere a link to it.



;09/Jul/24 09:28;githubbot;600","askhatri commented on code in PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#discussion_r1670128669


##########
server/src/main/scala/org/apache/livy/LivyConf.scala:
##########
@@ -258,6 +258,63 @@ object LivyConf {
   // value specifies max attempts to retry when safe mode is ON in hdfs filesystem
   val HDFS_SAFE_MODE_MAX_RETRY_ATTEMPTS = Entry(""livy.server.hdfs.safe-mode.max.retry.attempts"", 12)
 
+  // Kubernetes oauth token file path.
+  val KUBERNETES_OAUTH_TOKEN_FILE = Entry(""livy.server.kubernetes.oauthTokenFile"", """")
+  // Kubernetes oauth token string value.
+  val KUBERNETES_OAUTH_TOKEN_VALUE = Entry(""livy.server.kubernetes.oauthTokenValue"", """")
+  // Kubernetes CA cert file path.
+  val KUBERNETES_CA_CERT_FILE = Entry(""livy.server.kubernetes.caCertFile"", """")
+  // Kubernetes client key file path.
+  val KUBERNETES_CLIENT_KEY_FILE = Entry(""livy.server.kubernetes.clientKeyFile"", """")
+  // Kubernetes client cert file path.
+  val KUBERNETES_CLIENT_CERT_FILE = Entry(""livy.server.kubernetes.clientCertFile"", """")
+
+  // If Livy can't find the Kubernetes app within this time, consider it lost.
+  val KUBERNETES_APP_LOOKUP_TIMEOUT = Entry(""livy.server.kubernetes.app-lookup-timeout"", ""600s"")
+  // How often Livy polls Kubernetes to refresh Kubernetes app state.
+  val KUBERNETES_POLL_INTERVAL = Entry(""livy.server.kubernetes.poll-interval"", ""15s"")
+
+  // How long to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT =
+    Entry(""livy.server.kubernetes.app-leakage.check-timeout"", ""600s"")
+  // How often to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL =
+    Entry(""livy.server.kubernetes.app-leakage.check-interval"", ""60s"")
+
+  // Weather to create Kubernetes Nginx Ingress for Spark UI.
+  val KUBERNETES_INGRESS_CREATE = Entry(""livy.server.kubernetes.ingress.create"", false)
+  // Kubernetes Ingress class name.
+  val KUBERNETES_INGRESS_CLASS_NAME = Entry(""livy.server.kubernetes.ingress.className"", """")
+  // Kubernetes Nginx Ingress protocol.
+  val KUBERNETES_INGRESS_PROTOCOL = Entry(""livy.server.kubernetes.ingress.protocol"", ""http"")
+  // Kubernetes Nginx Ingress host.
+  val KUBERNETES_INGRESS_HOST = Entry(""livy.server.kubernetes.ingress.host"", ""localhost"")
+  // Kubernetes Nginx Ingress additional configuration snippet.
+  val KUBERNETES_INGRESS_ADDITIONAL_CONF_SNIPPET =
+    Entry(""livy.server.kubernetes.ingress.additionalConfSnippet"", """")
+  // Kubernetes Nginx Ingress additional annotations: key1=value1;key2=value2;... .
+  val KUBERNETES_INGRESS_ADDITIONAL_ANNOTATIONS =
+    Entry(""livy.server.kubernetes.ingress.additionalAnnotations"", """")
+  // Kubernetes secret name for Nginx Ingress TLS.
+  // Is omitted if 'livy.server.kubernetes.ingress.protocol' value doesn't end with 's'
+  val KUBERNETES_INGRESS_TLS_SECRET_NAME =
+    Entry(""livy.server.kubernetes.ingress.tls.secretName"", ""spark-cluster-tls"")
+
+  val KUBERNETES_GRAFANA_LOKI_ENABLED = Entry(""livy.server.kubernetes.grafana.loki.enabled"", false)
+  val KUBERNETES_GRAFANA_URL = Entry(""livy.server.kubernetes.grafana.url"", ""http://localhost:3000"")
+  val KUBERNETES_GRAFANA_LOKI_DATASOURCE =
+    Entry(""livy.server.kubernetes.grafana.loki.datasource"", ""loki"")
+  val KUBERNETES_GRAFANA_TIME_RANGE = Entry(""livy.server.kubernetes.grafana.timeRange"", ""6h"")
+
+  // side car container for spark pods enabled?
+  val KUBERNETES_SPARK_SIDECAR_ENABLED =
+    Entry(""livy.server.kubernetes.spark.sidecar.enabled"", true)

Review Comment:
   A sidecar container in Kubernetes is a secondary container that runs alongside the main application container in the same pod. We can set the sidecar configuration in Livy using this flag. 



;09/Jul/24 09:31;githubbot;600","askhatri commented on code in PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#discussion_r1670136188


##########
server/src/main/scala/org/apache/livy/LivyConf.scala:
##########
@@ -258,6 +258,63 @@ object LivyConf {
   // value specifies max attempts to retry when safe mode is ON in hdfs filesystem
   val HDFS_SAFE_MODE_MAX_RETRY_ATTEMPTS = Entry(""livy.server.hdfs.safe-mode.max.retry.attempts"", 12)
 
+  // Kubernetes oauth token file path.
+  val KUBERNETES_OAUTH_TOKEN_FILE = Entry(""livy.server.kubernetes.oauthTokenFile"", """")
+  // Kubernetes oauth token string value.
+  val KUBERNETES_OAUTH_TOKEN_VALUE = Entry(""livy.server.kubernetes.oauthTokenValue"", """")
+  // Kubernetes CA cert file path.
+  val KUBERNETES_CA_CERT_FILE = Entry(""livy.server.kubernetes.caCertFile"", """")
+  // Kubernetes client key file path.
+  val KUBERNETES_CLIENT_KEY_FILE = Entry(""livy.server.kubernetes.clientKeyFile"", """")
+  // Kubernetes client cert file path.
+  val KUBERNETES_CLIENT_CERT_FILE = Entry(""livy.server.kubernetes.clientCertFile"", """")
+
+  // If Livy can't find the Kubernetes app within this time, consider it lost.
+  val KUBERNETES_APP_LOOKUP_TIMEOUT = Entry(""livy.server.kubernetes.app-lookup-timeout"", ""600s"")
+  // How often Livy polls Kubernetes to refresh Kubernetes app state.
+  val KUBERNETES_POLL_INTERVAL = Entry(""livy.server.kubernetes.poll-interval"", ""15s"")
+
+  // How long to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT =
+    Entry(""livy.server.kubernetes.app-leakage.check-timeout"", ""600s"")
+  // How often to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL =
+    Entry(""livy.server.kubernetes.app-leakage.check-interval"", ""60s"")
+
+  // Weather to create Kubernetes Nginx Ingress for Spark UI.
+  val KUBERNETES_INGRESS_CREATE = Entry(""livy.server.kubernetes.ingress.create"", false)
+  // Kubernetes Ingress class name.
+  val KUBERNETES_INGRESS_CLASS_NAME = Entry(""livy.server.kubernetes.ingress.className"", """")
+  // Kubernetes Nginx Ingress protocol.
+  val KUBERNETES_INGRESS_PROTOCOL = Entry(""livy.server.kubernetes.ingress.protocol"", ""http"")
+  // Kubernetes Nginx Ingress host.
+  val KUBERNETES_INGRESS_HOST = Entry(""livy.server.kubernetes.ingress.host"", ""localhost"")
+  // Kubernetes Nginx Ingress additional configuration snippet.
+  val KUBERNETES_INGRESS_ADDITIONAL_CONF_SNIPPET =
+    Entry(""livy.server.kubernetes.ingress.additionalConfSnippet"", """")
+  // Kubernetes Nginx Ingress additional annotations: key1=value1;key2=value2;... .
+  val KUBERNETES_INGRESS_ADDITIONAL_ANNOTATIONS =
+    Entry(""livy.server.kubernetes.ingress.additionalAnnotations"", """")
+  // Kubernetes secret name for Nginx Ingress TLS.
+  // Is omitted if 'livy.server.kubernetes.ingress.protocol' value doesn't end with 's'
+  val KUBERNETES_INGRESS_TLS_SECRET_NAME =
+    Entry(""livy.server.kubernetes.ingress.tls.secretName"", ""spark-cluster-tls"")
+
+  val KUBERNETES_GRAFANA_LOKI_ENABLED = Entry(""livy.server.kubernetes.grafana.loki.enabled"", false)
+  val KUBERNETES_GRAFANA_URL = Entry(""livy.server.kubernetes.grafana.url"", ""http://localhost:3000"")
+  val KUBERNETES_GRAFANA_LOKI_DATASOURCE =
+    Entry(""livy.server.kubernetes.grafana.loki.datasource"", ""loki"")
+  val KUBERNETES_GRAFANA_TIME_RANGE = Entry(""livy.server.kubernetes.grafana.timeRange"", ""6h"")

Review Comment:
   Sure, I will try to add this as part of [LIVY-979](https://issues.apache.org/jira/browse/LIVY-979). I will connect with you via email.



;09/Jul/24 09:35;githubbot;600","vikas-saxena02 commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2217192672

   @askhatri the latest push was forced and I can see many of the checks have failed, can you please look into this?


;09/Jul/24 09:49;githubbot;600","vikas-saxena02 commented on code in PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#discussion_r1670162997


##########
server/src/main/scala/org/apache/livy/LivyConf.scala:
##########
@@ -258,6 +258,63 @@ object LivyConf {
   // value specifies max attempts to retry when safe mode is ON in hdfs filesystem
   val HDFS_SAFE_MODE_MAX_RETRY_ATTEMPTS = Entry(""livy.server.hdfs.safe-mode.max.retry.attempts"", 12)
 
+  // Kubernetes oauth token file path.
+  val KUBERNETES_OAUTH_TOKEN_FILE = Entry(""livy.server.kubernetes.oauthTokenFile"", """")
+  // Kubernetes oauth token string value.
+  val KUBERNETES_OAUTH_TOKEN_VALUE = Entry(""livy.server.kubernetes.oauthTokenValue"", """")
+  // Kubernetes CA cert file path.
+  val KUBERNETES_CA_CERT_FILE = Entry(""livy.server.kubernetes.caCertFile"", """")
+  // Kubernetes client key file path.
+  val KUBERNETES_CLIENT_KEY_FILE = Entry(""livy.server.kubernetes.clientKeyFile"", """")
+  // Kubernetes client cert file path.
+  val KUBERNETES_CLIENT_CERT_FILE = Entry(""livy.server.kubernetes.clientCertFile"", """")
+
+  // If Livy can't find the Kubernetes app within this time, consider it lost.
+  val KUBERNETES_APP_LOOKUP_TIMEOUT = Entry(""livy.server.kubernetes.app-lookup-timeout"", ""600s"")
+  // How often Livy polls Kubernetes to refresh Kubernetes app state.
+  val KUBERNETES_POLL_INTERVAL = Entry(""livy.server.kubernetes.poll-interval"", ""15s"")
+
+  // How long to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT =
+    Entry(""livy.server.kubernetes.app-leakage.check-timeout"", ""600s"")
+  // How often to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL =
+    Entry(""livy.server.kubernetes.app-leakage.check-interval"", ""60s"")
+
+  // Weather to create Kubernetes Nginx Ingress for Spark UI.
+  val KUBERNETES_INGRESS_CREATE = Entry(""livy.server.kubernetes.ingress.create"", false)
+  // Kubernetes Ingress class name.
+  val KUBERNETES_INGRESS_CLASS_NAME = Entry(""livy.server.kubernetes.ingress.className"", """")
+  // Kubernetes Nginx Ingress protocol.
+  val KUBERNETES_INGRESS_PROTOCOL = Entry(""livy.server.kubernetes.ingress.protocol"", ""http"")
+  // Kubernetes Nginx Ingress host.
+  val KUBERNETES_INGRESS_HOST = Entry(""livy.server.kubernetes.ingress.host"", ""localhost"")
+  // Kubernetes Nginx Ingress additional configuration snippet.
+  val KUBERNETES_INGRESS_ADDITIONAL_CONF_SNIPPET =
+    Entry(""livy.server.kubernetes.ingress.additionalConfSnippet"", """")
+  // Kubernetes Nginx Ingress additional annotations: key1=value1;key2=value2;... .
+  val KUBERNETES_INGRESS_ADDITIONAL_ANNOTATIONS =
+    Entry(""livy.server.kubernetes.ingress.additionalAnnotations"", """")
+  // Kubernetes secret name for Nginx Ingress TLS.
+  // Is omitted if 'livy.server.kubernetes.ingress.protocol' value doesn't end with 's'
+  val KUBERNETES_INGRESS_TLS_SECRET_NAME =
+    Entry(""livy.server.kubernetes.ingress.tls.secretName"", ""spark-cluster-tls"")
+
+  val KUBERNETES_GRAFANA_LOKI_ENABLED = Entry(""livy.server.kubernetes.grafana.loki.enabled"", false)
+  val KUBERNETES_GRAFANA_URL = Entry(""livy.server.kubernetes.grafana.url"", ""http://localhost:3000"")
+  val KUBERNETES_GRAFANA_LOKI_DATASOURCE =
+    Entry(""livy.server.kubernetes.grafana.loki.datasource"", ""loki"")
+  val KUBERNETES_GRAFANA_TIME_RANGE = Entry(""livy.server.kubernetes.grafana.timeRange"", ""6h"")
+
+  // side car container for spark pods enabled?
+  val KUBERNETES_SPARK_SIDECAR_ENABLED =
+    Entry(""livy.server.kubernetes.spark.sidecar.enabled"", true)

Review Comment:
   what could be the intention for running sidecar?



;09/Jul/24 09:51;githubbot;600","vikas-saxena02 commented on code in PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#discussion_r1670164751


##########
server/src/main/scala/org/apache/livy/utils/SparkKubernetesApp.scala:
##########
@@ -0,0 +1,739 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.livy.utils
+
+import java.net.URLEncoder
+import java.util.Collections
+import java.util.concurrent.TimeoutException
+
+import scala.annotation.tailrec
+import scala.collection.mutable.ArrayBuffer
+import scala.concurrent._
+import scala.concurrent.duration._
+import scala.language.postfixOps
+import scala.util.{Failure, Success, Try}
+import scala.util.control.NonFatal
+
+import io.fabric8.kubernetes.api.model._
+import io.fabric8.kubernetes.api.model.networking.v1.{Ingress, IngressBuilder}
+import io.fabric8.kubernetes.client.{Config, ConfigBuilder, _}
+import org.apache.commons.lang.StringUtils
+
+import org.apache.livy.{LivyConf, Logging, Utils}
+
+object SparkKubernetesApp extends Logging {
+
+  private val leakedAppTags = new java.util.concurrent.ConcurrentHashMap[String, Long]()
+
+  private val leakedAppsGCThread = new Thread() {
+    override def run(): Unit = {
+      import KubernetesExtensions._
+      while (true) {
+        if (!leakedAppTags.isEmpty) {
+          // kill the app if found it and remove it if exceeding a threshold
+          val iter = leakedAppTags.entrySet().iterator()
+          var isRemoved = false
+          val now = System.currentTimeMillis()
+          val apps = withRetry(kubernetesClient.getApplications())
+          while (iter.hasNext) {
+            val entry = iter.next()
+            apps.find(_.getApplicationTag.contains(entry.getKey))
+              .foreach({
+                app =>
+                  info(s""Kill leaked app ${app.getApplicationId}"")
+                  withRetry(kubernetesClient.killApplication(app))
+                  iter.remove()
+                  isRemoved = true
+              })
+            if (!isRemoved) {
+              if ((entry.getValue - now) > sessionLeakageCheckTimeout) {
+                iter.remove()
+                info(s""Remove leaked Kubernetes app tag ${entry.getKey}"")
+              }
+            }
+          }
+        }
+        Thread.sleep(sessionLeakageCheckInterval)
+      }
+    }
+  }
+
+  val RefreshServiceAccountTokenThread = new Thread() {
+    override def run(): Unit = {
+      while (true) {
+        var currentContext = new Context()
+        var currentContextName = new String
+        val config = kubernetesClient.getConfiguration
+        if (config.getCurrentContext != null) {
+          currentContext = config.getCurrentContext.getContext
+          currentContextName = config.getCurrentContext.getName
+        }
+
+        var newAccessToken = new String
+        val newestConfig = Config.autoConfigure(currentContextName)
+        newAccessToken = newestConfig.getOauthToken
+        info(s""Refresh a new token ${newAccessToken}"")
+
+        config.setOauthToken(newAccessToken)
+        kubernetesClient = new DefaultKubernetesClient(config)
+
+        // Token will expire 1 hour default, community recommend to update every 5 minutes
+        Thread.sleep(300000)
+      }
+    }
+  }
+
+  private var livyConf: LivyConf = _
+
+  private var cacheLogSize: Int = _
+  private var appLookupTimeout: FiniteDuration = _
+  private var pollInterval: FiniteDuration = _
+
+  private var sessionLeakageCheckTimeout: Long = _
+  private var sessionLeakageCheckInterval: Long = _
+
+  var kubernetesClient: DefaultKubernetesClient = _
+
+  def init(livyConf: LivyConf): Unit = {
+    this.livyConf = livyConf
+
+    // KubernetesClient is thread safe. Create once, share it across threads.
+    kubernetesClient =
+      KubernetesClientFactory.createKubernetesClient(livyConf)
+
+    cacheLogSize = livyConf.getInt(LivyConf.SPARK_LOGS_SIZE)
+    appLookupTimeout = livyConf.getTimeAsMs(LivyConf.KUBERNETES_APP_LOOKUP_TIMEOUT).milliseconds
+    pollInterval = livyConf.getTimeAsMs(LivyConf.KUBERNETES_POLL_INTERVAL).milliseconds
+
+    sessionLeakageCheckInterval =
+      livyConf.getTimeAsMs(LivyConf.KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL)
+    sessionLeakageCheckTimeout = livyConf.getTimeAsMs(LivyConf.KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT)
+
+    leakedAppsGCThread.setDaemon(true)
+    leakedAppsGCThread.setName(""LeakedAppsGCThread"")
+    leakedAppsGCThread.start()
+
+    RefreshServiceAccountTokenThread.
+      setName(""RefreshServiceAccountTokenThread"")
+    RefreshServiceAccountTokenThread.setDaemon(true)
+    RefreshServiceAccountTokenThread.start()
+  }
+
+  // Returning T, throwing the exception on failure
+  // When istio-proxy restarts, the access to K8s API from livy could be down
+  // until envoy comes back, which could take upto 30 seconds

Review Comment:
   great functionality, this will surely be beneficial



;09/Jul/24 09:52;githubbot;600","askhatri commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2217198759

   Hi @vikas-saxena02, I have re-triggered the checks now by adding ACTIONS_ALLOW_USE_UNSECURE_NODE_VERSION=true.


;09/Jul/24 09:52;githubbot;600","askhatri commented on code in PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#discussion_r1670177308


##########
server/src/main/scala/org/apache/livy/LivyConf.scala:
##########
@@ -258,6 +258,63 @@ object LivyConf {
   // value specifies max attempts to retry when safe mode is ON in hdfs filesystem
   val HDFS_SAFE_MODE_MAX_RETRY_ATTEMPTS = Entry(""livy.server.hdfs.safe-mode.max.retry.attempts"", 12)
 
+  // Kubernetes oauth token file path.
+  val KUBERNETES_OAUTH_TOKEN_FILE = Entry(""livy.server.kubernetes.oauthTokenFile"", """")
+  // Kubernetes oauth token string value.
+  val KUBERNETES_OAUTH_TOKEN_VALUE = Entry(""livy.server.kubernetes.oauthTokenValue"", """")
+  // Kubernetes CA cert file path.
+  val KUBERNETES_CA_CERT_FILE = Entry(""livy.server.kubernetes.caCertFile"", """")
+  // Kubernetes client key file path.
+  val KUBERNETES_CLIENT_KEY_FILE = Entry(""livy.server.kubernetes.clientKeyFile"", """")
+  // Kubernetes client cert file path.
+  val KUBERNETES_CLIENT_CERT_FILE = Entry(""livy.server.kubernetes.clientCertFile"", """")
+
+  // If Livy can't find the Kubernetes app within this time, consider it lost.
+  val KUBERNETES_APP_LOOKUP_TIMEOUT = Entry(""livy.server.kubernetes.app-lookup-timeout"", ""600s"")
+  // How often Livy polls Kubernetes to refresh Kubernetes app state.
+  val KUBERNETES_POLL_INTERVAL = Entry(""livy.server.kubernetes.poll-interval"", ""15s"")
+
+  // How long to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT =
+    Entry(""livy.server.kubernetes.app-leakage.check-timeout"", ""600s"")
+  // How often to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL =
+    Entry(""livy.server.kubernetes.app-leakage.check-interval"", ""60s"")
+
+  // Weather to create Kubernetes Nginx Ingress for Spark UI.
+  val KUBERNETES_INGRESS_CREATE = Entry(""livy.server.kubernetes.ingress.create"", false)
+  // Kubernetes Ingress class name.
+  val KUBERNETES_INGRESS_CLASS_NAME = Entry(""livy.server.kubernetes.ingress.className"", """")
+  // Kubernetes Nginx Ingress protocol.
+  val KUBERNETES_INGRESS_PROTOCOL = Entry(""livy.server.kubernetes.ingress.protocol"", ""http"")
+  // Kubernetes Nginx Ingress host.
+  val KUBERNETES_INGRESS_HOST = Entry(""livy.server.kubernetes.ingress.host"", ""localhost"")
+  // Kubernetes Nginx Ingress additional configuration snippet.
+  val KUBERNETES_INGRESS_ADDITIONAL_CONF_SNIPPET =
+    Entry(""livy.server.kubernetes.ingress.additionalConfSnippet"", """")
+  // Kubernetes Nginx Ingress additional annotations: key1=value1;key2=value2;... .
+  val KUBERNETES_INGRESS_ADDITIONAL_ANNOTATIONS =
+    Entry(""livy.server.kubernetes.ingress.additionalAnnotations"", """")
+  // Kubernetes secret name for Nginx Ingress TLS.
+  // Is omitted if 'livy.server.kubernetes.ingress.protocol' value doesn't end with 's'
+  val KUBERNETES_INGRESS_TLS_SECRET_NAME =
+    Entry(""livy.server.kubernetes.ingress.tls.secretName"", ""spark-cluster-tls"")
+
+  val KUBERNETES_GRAFANA_LOKI_ENABLED = Entry(""livy.server.kubernetes.grafana.loki.enabled"", false)
+  val KUBERNETES_GRAFANA_URL = Entry(""livy.server.kubernetes.grafana.url"", ""http://localhost:3000"")
+  val KUBERNETES_GRAFANA_LOKI_DATASOURCE =
+    Entry(""livy.server.kubernetes.grafana.loki.datasource"", ""loki"")
+  val KUBERNETES_GRAFANA_TIME_RANGE = Entry(""livy.server.kubernetes.grafana.timeRange"", ""6h"")
+
+  // side car container for spark pods enabled?
+  val KUBERNETES_SPARK_SIDECAR_ENABLED =
+    Entry(""livy.server.kubernetes.spark.sidecar.enabled"", true)

Review Comment:
   A sidecar container can be used to collect logs from the Spark application and forward them to a centralized logging system like Elasticsearch, Fluentd, and Kibana (EFK) stack.



;09/Jul/24 09:58;githubbot;600","jahstreet commented on code in PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#discussion_r1670183566


##########
server/src/main/scala/org/apache/livy/LivyConf.scala:
##########
@@ -258,6 +258,63 @@ object LivyConf {
   // value specifies max attempts to retry when safe mode is ON in hdfs filesystem
   val HDFS_SAFE_MODE_MAX_RETRY_ATTEMPTS = Entry(""livy.server.hdfs.safe-mode.max.retry.attempts"", 12)
 
+  // Kubernetes oauth token file path.
+  val KUBERNETES_OAUTH_TOKEN_FILE = Entry(""livy.server.kubernetes.oauthTokenFile"", """")
+  // Kubernetes oauth token string value.
+  val KUBERNETES_OAUTH_TOKEN_VALUE = Entry(""livy.server.kubernetes.oauthTokenValue"", """")
+  // Kubernetes CA cert file path.
+  val KUBERNETES_CA_CERT_FILE = Entry(""livy.server.kubernetes.caCertFile"", """")
+  // Kubernetes client key file path.
+  val KUBERNETES_CLIENT_KEY_FILE = Entry(""livy.server.kubernetes.clientKeyFile"", """")
+  // Kubernetes client cert file path.
+  val KUBERNETES_CLIENT_CERT_FILE = Entry(""livy.server.kubernetes.clientCertFile"", """")
+
+  // If Livy can't find the Kubernetes app within this time, consider it lost.
+  val KUBERNETES_APP_LOOKUP_TIMEOUT = Entry(""livy.server.kubernetes.app-lookup-timeout"", ""600s"")
+  // How often Livy polls Kubernetes to refresh Kubernetes app state.
+  val KUBERNETES_POLL_INTERVAL = Entry(""livy.server.kubernetes.poll-interval"", ""15s"")
+
+  // How long to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT =
+    Entry(""livy.server.kubernetes.app-leakage.check-timeout"", ""600s"")
+  // How often to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL =
+    Entry(""livy.server.kubernetes.app-leakage.check-interval"", ""60s"")
+
+  // Weather to create Kubernetes Nginx Ingress for Spark UI.
+  val KUBERNETES_INGRESS_CREATE = Entry(""livy.server.kubernetes.ingress.create"", false)
+  // Kubernetes Ingress class name.
+  val KUBERNETES_INGRESS_CLASS_NAME = Entry(""livy.server.kubernetes.ingress.className"", """")
+  // Kubernetes Nginx Ingress protocol.
+  val KUBERNETES_INGRESS_PROTOCOL = Entry(""livy.server.kubernetes.ingress.protocol"", ""http"")
+  // Kubernetes Nginx Ingress host.
+  val KUBERNETES_INGRESS_HOST = Entry(""livy.server.kubernetes.ingress.host"", ""localhost"")
+  // Kubernetes Nginx Ingress additional configuration snippet.
+  val KUBERNETES_INGRESS_ADDITIONAL_CONF_SNIPPET =
+    Entry(""livy.server.kubernetes.ingress.additionalConfSnippet"", """")
+  // Kubernetes Nginx Ingress additional annotations: key1=value1;key2=value2;... .
+  val KUBERNETES_INGRESS_ADDITIONAL_ANNOTATIONS =
+    Entry(""livy.server.kubernetes.ingress.additionalAnnotations"", """")
+  // Kubernetes secret name for Nginx Ingress TLS.
+  // Is omitted if 'livy.server.kubernetes.ingress.protocol' value doesn't end with 's'
+  val KUBERNETES_INGRESS_TLS_SECRET_NAME =
+    Entry(""livy.server.kubernetes.ingress.tls.secretName"", ""spark-cluster-tls"")
+
+  val KUBERNETES_GRAFANA_LOKI_ENABLED = Entry(""livy.server.kubernetes.grafana.loki.enabled"", false)
+  val KUBERNETES_GRAFANA_URL = Entry(""livy.server.kubernetes.grafana.url"", ""http://localhost:3000"")
+  val KUBERNETES_GRAFANA_LOKI_DATASOURCE =
+    Entry(""livy.server.kubernetes.grafana.loki.datasource"", ""loki"")
+  val KUBERNETES_GRAFANA_TIME_RANGE = Entry(""livy.server.kubernetes.grafana.timeRange"", ""6h"")
+
+  // side car container for spark pods enabled?
+  val KUBERNETES_SPARK_SIDECAR_ENABLED =
+    Entry(""livy.server.kubernetes.spark.sidecar.enabled"", true)

Review Comment:
   Understood, thx for explaining. Does Livy add this sidecar or should Livy be handling the Spark Pods with a sidecar differently? What would happen if Spark Pods have sidecar but this flag is false and how it is different from setting it to true?



##########
server/src/main/scala/org/apache/livy/LivyConf.scala:
##########
@@ -258,6 +258,63 @@ object LivyConf {
   // value specifies max attempts to retry when safe mode is ON in hdfs filesystem
   val HDFS_SAFE_MODE_MAX_RETRY_ATTEMPTS = Entry(""livy.server.hdfs.safe-mode.max.retry.attempts"", 12)
 
+  // Kubernetes oauth token file path.
+  val KUBERNETES_OAUTH_TOKEN_FILE = Entry(""livy.server.kubernetes.oauthTokenFile"", """")
+  // Kubernetes oauth token string value.
+  val KUBERNETES_OAUTH_TOKEN_VALUE = Entry(""livy.server.kubernetes.oauthTokenValue"", """")
+  // Kubernetes CA cert file path.
+  val KUBERNETES_CA_CERT_FILE = Entry(""livy.server.kubernetes.caCertFile"", """")
+  // Kubernetes client key file path.
+  val KUBERNETES_CLIENT_KEY_FILE = Entry(""livy.server.kubernetes.clientKeyFile"", """")
+  // Kubernetes client cert file path.
+  val KUBERNETES_CLIENT_CERT_FILE = Entry(""livy.server.kubernetes.clientCertFile"", """")
+
+  // If Livy can't find the Kubernetes app within this time, consider it lost.
+  val KUBERNETES_APP_LOOKUP_TIMEOUT = Entry(""livy.server.kubernetes.app-lookup-timeout"", ""600s"")
+  // How often Livy polls Kubernetes to refresh Kubernetes app state.
+  val KUBERNETES_POLL_INTERVAL = Entry(""livy.server.kubernetes.poll-interval"", ""15s"")
+
+  // How long to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT =
+    Entry(""livy.server.kubernetes.app-leakage.check-timeout"", ""600s"")
+  // How often to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL =
+    Entry(""livy.server.kubernetes.app-leakage.check-interval"", ""60s"")
+
+  // Weather to create Kubernetes Nginx Ingress for Spark UI.
+  val KUBERNETES_INGRESS_CREATE = Entry(""livy.server.kubernetes.ingress.create"", false)
+  // Kubernetes Ingress class name.
+  val KUBERNETES_INGRESS_CLASS_NAME = Entry(""livy.server.kubernetes.ingress.className"", """")
+  // Kubernetes Nginx Ingress protocol.
+  val KUBERNETES_INGRESS_PROTOCOL = Entry(""livy.server.kubernetes.ingress.protocol"", ""http"")
+  // Kubernetes Nginx Ingress host.
+  val KUBERNETES_INGRESS_HOST = Entry(""livy.server.kubernetes.ingress.host"", ""localhost"")
+  // Kubernetes Nginx Ingress additional configuration snippet.
+  val KUBERNETES_INGRESS_ADDITIONAL_CONF_SNIPPET =
+    Entry(""livy.server.kubernetes.ingress.additionalConfSnippet"", """")
+  // Kubernetes Nginx Ingress additional annotations: key1=value1;key2=value2;... .
+  val KUBERNETES_INGRESS_ADDITIONAL_ANNOTATIONS =
+    Entry(""livy.server.kubernetes.ingress.additionalAnnotations"", """")
+  // Kubernetes secret name for Nginx Ingress TLS.
+  // Is omitted if 'livy.server.kubernetes.ingress.protocol' value doesn't end with 's'
+  val KUBERNETES_INGRESS_TLS_SECRET_NAME =
+    Entry(""livy.server.kubernetes.ingress.tls.secretName"", ""spark-cluster-tls"")
+
+  val KUBERNETES_GRAFANA_LOKI_ENABLED = Entry(""livy.server.kubernetes.grafana.loki.enabled"", false)
+  val KUBERNETES_GRAFANA_URL = Entry(""livy.server.kubernetes.grafana.url"", ""http://localhost:3000"")
+  val KUBERNETES_GRAFANA_LOKI_DATASOURCE =
+    Entry(""livy.server.kubernetes.grafana.loki.datasource"", ""loki"")
+  val KUBERNETES_GRAFANA_TIME_RANGE = Entry(""livy.server.kubernetes.grafana.timeRange"", ""6h"")
+
+  // side car container for spark pods enabled?
+  val KUBERNETES_SPARK_SIDECAR_ENABLED =
+    Entry(""livy.server.kubernetes.spark.sidecar.enabled"", true)

Review Comment:
   Understood, thx for explaining. Does Livy add this sidecar or should Livy be handling the Spark Pods with a sidecar differently? What would happen if Spark Pods have sidecar but this flag is false and how it is different from setting it to true? Just trying to understand it better.



;09/Jul/24 10:02;githubbot;600","askhatri commented on code in PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#discussion_r1670200824


##########
server/src/main/scala/org/apache/livy/LivyConf.scala:
##########
@@ -258,6 +258,63 @@ object LivyConf {
   // value specifies max attempts to retry when safe mode is ON in hdfs filesystem
   val HDFS_SAFE_MODE_MAX_RETRY_ATTEMPTS = Entry(""livy.server.hdfs.safe-mode.max.retry.attempts"", 12)
 
+  // Kubernetes oauth token file path.
+  val KUBERNETES_OAUTH_TOKEN_FILE = Entry(""livy.server.kubernetes.oauthTokenFile"", """")
+  // Kubernetes oauth token string value.
+  val KUBERNETES_OAUTH_TOKEN_VALUE = Entry(""livy.server.kubernetes.oauthTokenValue"", """")
+  // Kubernetes CA cert file path.
+  val KUBERNETES_CA_CERT_FILE = Entry(""livy.server.kubernetes.caCertFile"", """")
+  // Kubernetes client key file path.
+  val KUBERNETES_CLIENT_KEY_FILE = Entry(""livy.server.kubernetes.clientKeyFile"", """")
+  // Kubernetes client cert file path.
+  val KUBERNETES_CLIENT_CERT_FILE = Entry(""livy.server.kubernetes.clientCertFile"", """")
+
+  // If Livy can't find the Kubernetes app within this time, consider it lost.
+  val KUBERNETES_APP_LOOKUP_TIMEOUT = Entry(""livy.server.kubernetes.app-lookup-timeout"", ""600s"")
+  // How often Livy polls Kubernetes to refresh Kubernetes app state.
+  val KUBERNETES_POLL_INTERVAL = Entry(""livy.server.kubernetes.poll-interval"", ""15s"")
+
+  // How long to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT =
+    Entry(""livy.server.kubernetes.app-leakage.check-timeout"", ""600s"")
+  // How often to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL =
+    Entry(""livy.server.kubernetes.app-leakage.check-interval"", ""60s"")
+
+  // Weather to create Kubernetes Nginx Ingress for Spark UI.
+  val KUBERNETES_INGRESS_CREATE = Entry(""livy.server.kubernetes.ingress.create"", false)
+  // Kubernetes Ingress class name.
+  val KUBERNETES_INGRESS_CLASS_NAME = Entry(""livy.server.kubernetes.ingress.className"", """")
+  // Kubernetes Nginx Ingress protocol.
+  val KUBERNETES_INGRESS_PROTOCOL = Entry(""livy.server.kubernetes.ingress.protocol"", ""http"")
+  // Kubernetes Nginx Ingress host.
+  val KUBERNETES_INGRESS_HOST = Entry(""livy.server.kubernetes.ingress.host"", ""localhost"")
+  // Kubernetes Nginx Ingress additional configuration snippet.
+  val KUBERNETES_INGRESS_ADDITIONAL_CONF_SNIPPET =
+    Entry(""livy.server.kubernetes.ingress.additionalConfSnippet"", """")
+  // Kubernetes Nginx Ingress additional annotations: key1=value1;key2=value2;... .
+  val KUBERNETES_INGRESS_ADDITIONAL_ANNOTATIONS =
+    Entry(""livy.server.kubernetes.ingress.additionalAnnotations"", """")
+  // Kubernetes secret name for Nginx Ingress TLS.
+  // Is omitted if 'livy.server.kubernetes.ingress.protocol' value doesn't end with 's'
+  val KUBERNETES_INGRESS_TLS_SECRET_NAME =
+    Entry(""livy.server.kubernetes.ingress.tls.secretName"", ""spark-cluster-tls"")
+
+  val KUBERNETES_GRAFANA_LOKI_ENABLED = Entry(""livy.server.kubernetes.grafana.loki.enabled"", false)
+  val KUBERNETES_GRAFANA_URL = Entry(""livy.server.kubernetes.grafana.url"", ""http://localhost:3000"")
+  val KUBERNETES_GRAFANA_LOKI_DATASOURCE =
+    Entry(""livy.server.kubernetes.grafana.loki.datasource"", ""loki"")
+  val KUBERNETES_GRAFANA_TIME_RANGE = Entry(""livy.server.kubernetes.grafana.timeRange"", ""6h"")
+
+  // side car container for spark pods enabled?
+  val KUBERNETES_SPARK_SIDECAR_ENABLED =
+    Entry(""livy.server.kubernetes.spark.sidecar.enabled"", true)

Review Comment:
   Livy does not add a sidecar container for Spark. Instead, Livy uses a flag to determine the status of the Spark pod. If the Spark pod is running with a sidecar, the flag is set to true; otherwise, it is set to false.



;09/Jul/24 10:10;githubbot;600","vikas-saxena02 commented on code in PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#discussion_r1670285245


##########
server/src/main/scala/org/apache/livy/LivyConf.scala:
##########
@@ -258,6 +258,63 @@ object LivyConf {
   // value specifies max attempts to retry when safe mode is ON in hdfs filesystem
   val HDFS_SAFE_MODE_MAX_RETRY_ATTEMPTS = Entry(""livy.server.hdfs.safe-mode.max.retry.attempts"", 12)
 
+  // Kubernetes oauth token file path.
+  val KUBERNETES_OAUTH_TOKEN_FILE = Entry(""livy.server.kubernetes.oauthTokenFile"", """")
+  // Kubernetes oauth token string value.
+  val KUBERNETES_OAUTH_TOKEN_VALUE = Entry(""livy.server.kubernetes.oauthTokenValue"", """")
+  // Kubernetes CA cert file path.
+  val KUBERNETES_CA_CERT_FILE = Entry(""livy.server.kubernetes.caCertFile"", """")
+  // Kubernetes client key file path.
+  val KUBERNETES_CLIENT_KEY_FILE = Entry(""livy.server.kubernetes.clientKeyFile"", """")
+  // Kubernetes client cert file path.
+  val KUBERNETES_CLIENT_CERT_FILE = Entry(""livy.server.kubernetes.clientCertFile"", """")
+
+  // If Livy can't find the Kubernetes app within this time, consider it lost.
+  val KUBERNETES_APP_LOOKUP_TIMEOUT = Entry(""livy.server.kubernetes.app-lookup-timeout"", ""600s"")
+  // How often Livy polls Kubernetes to refresh Kubernetes app state.
+  val KUBERNETES_POLL_INTERVAL = Entry(""livy.server.kubernetes.poll-interval"", ""15s"")
+
+  // How long to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT =
+    Entry(""livy.server.kubernetes.app-leakage.check-timeout"", ""600s"")
+  // How often to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL =
+    Entry(""livy.server.kubernetes.app-leakage.check-interval"", ""60s"")
+
+  // Weather to create Kubernetes Nginx Ingress for Spark UI.
+  val KUBERNETES_INGRESS_CREATE = Entry(""livy.server.kubernetes.ingress.create"", false)
+  // Kubernetes Ingress class name.
+  val KUBERNETES_INGRESS_CLASS_NAME = Entry(""livy.server.kubernetes.ingress.className"", """")
+  // Kubernetes Nginx Ingress protocol.
+  val KUBERNETES_INGRESS_PROTOCOL = Entry(""livy.server.kubernetes.ingress.protocol"", ""http"")
+  // Kubernetes Nginx Ingress host.
+  val KUBERNETES_INGRESS_HOST = Entry(""livy.server.kubernetes.ingress.host"", ""localhost"")
+  // Kubernetes Nginx Ingress additional configuration snippet.
+  val KUBERNETES_INGRESS_ADDITIONAL_CONF_SNIPPET =
+    Entry(""livy.server.kubernetes.ingress.additionalConfSnippet"", """")
+  // Kubernetes Nginx Ingress additional annotations: key1=value1;key2=value2;... .
+  val KUBERNETES_INGRESS_ADDITIONAL_ANNOTATIONS =
+    Entry(""livy.server.kubernetes.ingress.additionalAnnotations"", """")
+  // Kubernetes secret name for Nginx Ingress TLS.
+  // Is omitted if 'livy.server.kubernetes.ingress.protocol' value doesn't end with 's'
+  val KUBERNETES_INGRESS_TLS_SECRET_NAME =
+    Entry(""livy.server.kubernetes.ingress.tls.secretName"", ""spark-cluster-tls"")
+
+  val KUBERNETES_GRAFANA_LOKI_ENABLED = Entry(""livy.server.kubernetes.grafana.loki.enabled"", false)
+  val KUBERNETES_GRAFANA_URL = Entry(""livy.server.kubernetes.grafana.url"", ""http://localhost:3000"")
+  val KUBERNETES_GRAFANA_LOKI_DATASOURCE =
+    Entry(""livy.server.kubernetes.grafana.loki.datasource"", ""loki"")
+  val KUBERNETES_GRAFANA_TIME_RANGE = Entry(""livy.server.kubernetes.grafana.timeRange"", ""6h"")

Review Comment:
   @askhatri better to discuss it on LIVY-979 itself so that whole community is across it.



;09/Jul/24 10:56;githubbot;600","askhatri commented on code in PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#discussion_r1670308350


##########
server/src/main/scala/org/apache/livy/LivyConf.scala:
##########
@@ -258,6 +258,63 @@ object LivyConf {
   // value specifies max attempts to retry when safe mode is ON in hdfs filesystem
   val HDFS_SAFE_MODE_MAX_RETRY_ATTEMPTS = Entry(""livy.server.hdfs.safe-mode.max.retry.attempts"", 12)
 
+  // Kubernetes oauth token file path.
+  val KUBERNETES_OAUTH_TOKEN_FILE = Entry(""livy.server.kubernetes.oauthTokenFile"", """")
+  // Kubernetes oauth token string value.
+  val KUBERNETES_OAUTH_TOKEN_VALUE = Entry(""livy.server.kubernetes.oauthTokenValue"", """")
+  // Kubernetes CA cert file path.
+  val KUBERNETES_CA_CERT_FILE = Entry(""livy.server.kubernetes.caCertFile"", """")
+  // Kubernetes client key file path.
+  val KUBERNETES_CLIENT_KEY_FILE = Entry(""livy.server.kubernetes.clientKeyFile"", """")
+  // Kubernetes client cert file path.
+  val KUBERNETES_CLIENT_CERT_FILE = Entry(""livy.server.kubernetes.clientCertFile"", """")
+
+  // If Livy can't find the Kubernetes app within this time, consider it lost.
+  val KUBERNETES_APP_LOOKUP_TIMEOUT = Entry(""livy.server.kubernetes.app-lookup-timeout"", ""600s"")
+  // How often Livy polls Kubernetes to refresh Kubernetes app state.
+  val KUBERNETES_POLL_INTERVAL = Entry(""livy.server.kubernetes.poll-interval"", ""15s"")
+
+  // How long to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_TIMEOUT =
+    Entry(""livy.server.kubernetes.app-leakage.check-timeout"", ""600s"")
+  // How often to check livy session leakage.
+  val KUBERNETES_APP_LEAKAGE_CHECK_INTERVAL =
+    Entry(""livy.server.kubernetes.app-leakage.check-interval"", ""60s"")
+
+  // Weather to create Kubernetes Nginx Ingress for Spark UI.
+  val KUBERNETES_INGRESS_CREATE = Entry(""livy.server.kubernetes.ingress.create"", false)
+  // Kubernetes Ingress class name.
+  val KUBERNETES_INGRESS_CLASS_NAME = Entry(""livy.server.kubernetes.ingress.className"", """")
+  // Kubernetes Nginx Ingress protocol.
+  val KUBERNETES_INGRESS_PROTOCOL = Entry(""livy.server.kubernetes.ingress.protocol"", ""http"")
+  // Kubernetes Nginx Ingress host.
+  val KUBERNETES_INGRESS_HOST = Entry(""livy.server.kubernetes.ingress.host"", ""localhost"")
+  // Kubernetes Nginx Ingress additional configuration snippet.
+  val KUBERNETES_INGRESS_ADDITIONAL_CONF_SNIPPET =
+    Entry(""livy.server.kubernetes.ingress.additionalConfSnippet"", """")
+  // Kubernetes Nginx Ingress additional annotations: key1=value1;key2=value2;... .
+  val KUBERNETES_INGRESS_ADDITIONAL_ANNOTATIONS =
+    Entry(""livy.server.kubernetes.ingress.additionalAnnotations"", """")
+  // Kubernetes secret name for Nginx Ingress TLS.
+  // Is omitted if 'livy.server.kubernetes.ingress.protocol' value doesn't end with 's'
+  val KUBERNETES_INGRESS_TLS_SECRET_NAME =
+    Entry(""livy.server.kubernetes.ingress.tls.secretName"", ""spark-cluster-tls"")
+
+  val KUBERNETES_GRAFANA_LOKI_ENABLED = Entry(""livy.server.kubernetes.grafana.loki.enabled"", false)
+  val KUBERNETES_GRAFANA_URL = Entry(""livy.server.kubernetes.grafana.url"", ""http://localhost:3000"")
+  val KUBERNETES_GRAFANA_LOKI_DATASOURCE =
+    Entry(""livy.server.kubernetes.grafana.loki.datasource"", ""loki"")
+  val KUBERNETES_GRAFANA_TIME_RANGE = Entry(""livy.server.kubernetes.grafana.timeRange"", ""6h"")

Review Comment:
   Sure @vikas-saxena02.



;09/Jul/24 11:08;githubbot;600","vikas-saxena02 commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2217392729

   @jahstreet @askhatri is there any documentation that you can share that explains how to use livy run jobs on spark on kubernetes?


;09/Jul/24 11:22;githubbot;600","vikas-saxena02 commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2217393059

   /approve


;09/Jul/24 11:22;githubbot;600","jahstreet commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2217776530

   > @jahstreet @askhatri is there any documentation that you can share that explains how to use livy run jobs on spark on kubernetes?
   
   Yes, we will [talk about packaging](https://github.com/apache/incubator-livy/pull/451#discussion_r1670136188) it.
   Until then, people can refer:
   - (newer linked to this PR @askhatri 's setup) https://github.com/askhatri/livycluster
   - (older my setup) https://github.com/JahstreetOrg/spark-on-kubernetes-helm


;09/Jul/24 13:39;githubbot;600","vikas-saxena02 commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2218321321

   /assign gyogal 


;09/Jul/24 17:53;githubbot;600","askhatri commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2219589101

   Hi @vikas-saxena02, I have assigned to @gyogal as suggested by you...!


;10/Jul/24 05:25;githubbot;600","vikas-saxena02 commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2219623561

   Thanks @askhatri !! Hopefully this should be merged soon.


;10/Jul/24 05:57;githubbot;600","gyogal commented on PR #451:
URL: https://github.com/apache/incubator-livy/pull/451#issuecomment-2219890050

   Thanks everyone for your input! It seems like there are no objections and the overall feedback is positive. If you find any issues once this PR is merged, please feel free to raise a ticket.


;10/Jul/24 08:32;githubbot;600","gyogal merged PR #451:
URL: https://github.com/apache/incubator-livy/pull/451


;10/Jul/24 09:05;githubbot;600","jahstreet closed pull request #249: [LIVY-702]: Submit Spark apps to Kubernetes
URL: https://github.com/apache/incubator-livy/pull/249


;21/Mar/25 07:55;githubbot;600",,0,57600,,,0,57600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,LIVY-701,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 25 13:53:59 UTC 2024,,,,,,,,,,"0|z07zao:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Jan/22 07:44;rahul26goyal;Hi [~jahstreet]Â 

Is there any plans for working on this ticket by Livy community?Â ;;;","09/May/23 23:55;lmccay;There certainly seems to be activity on the PR: [https://github.com/apache/incubator-livy/pull/249]

I am uncertain what timing looks like and I would really like to burn down the remaining blockers to get 0.8.0 release process moving.

If I do not hear otherwise in a day or so, I will move to 0.9.0 and we can discuss pulling it back in if need be here in the JIRA.;;;","10/May/23 12:28;lmccay;[~jahstreet]Â  - Since I didn't hear anything yet, I've moved this out to 0.9.0 to begin the release discussion for 0.8.0.

We can discuss pulling it back in - if this is required and should block the 0.8.0 release but let's justify why that would be if more than a week or so.;;;","25/Jun/24 13:53;gyogal;A new PR https://github.com/apache/incubator-livy/pull/451 has just been submitted.

It is based on https://github.com/apache/incubator-livy/pull/249;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[LIVY-699][THRIFT] Fix resultSet.getBigDecimal throw java.sql.SQLException: Illegal conversion,LIVY-699,13263549,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,runzhiwang,runzhiwang,21/Oct/19 13:33,12/Nov/19 10:28,19/Dec/25 04:15,12/Nov/19 10:28,0.6.0,,,0.7.0,,,,,,,,,,,,0,,,,,,"LIVY-699[THRIFT] Fix resultSet.getBigDecimal throw java.sql.SQLException: Illegal conversion.

Follows are steps to reproduce the problem:
 # {{create table test(id decimal)}}.
 # ThenÂ {{resultSet.getBigDecimal(1)}}Â will throw:{{Â java.sql.SQLException: Illegal conversion}}. The reason isÂ {{getSchema().getColumnDescriptorAt(columnIndex - 1).getType();}}Â atÂ [https://github.com/apache/hive/blob/master/jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java#L415]Â return string, so cannot pass the checkÂ {{val instanceof BigDecimal atÂ [https://github.com/apache/hive/blob/master/jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java#L133], so throw java.sql.SQLException: Illegal conversion}}Â atÂ [https://github.com/apache/hive/blob/master/jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java#L137]

Additionally, SparkThrift return decimal instead of string in the same case, so it is necessary to return decimal instead of string in livy. The same to timestamp and date.",,"runzhiwang commented on pull request #247: [LIVY-699] Support DateType: decimal, timestamp, date
URL: https://github.com/apache/incubator-livy/pull/247
 
 
   ## What changes were proposed in this pull request?
   
   [LIVY-699] Support DateType: decimal, timestamp, date
   
   ## How was this patch tested?
   
   1. CREATE TABLE default.test (
   TIMESTAMPCOLUMN TIMESTAMP,
   DECIMALCOLUMN DECIMAL(22,2),
   DATECOLUMN DATE
   );
   
   2.In Client:
   ```
               ResultSet res = stmt.executeQuery(""select * from test "");
               ResultSetMetaData rsMetaData = res.getMetaData();
               int numberOfColumns = rsMetaData.getColumnCount();
               for (int i = 1; i <= numberOfColumns; i++) {
                   System.out.println(rsMetaData.getColumnTypeName(i));
               }
   
   ```
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Oct/19 13:41;githubbot;600","runzhiwang commented on pull request #247: [LIVY-699][THRIFT] Fix resultSet.getBigDecimal throw java.sql.SQLException: Illegal conversion
URL: https://github.com/apache/incubator-livy/pull/247
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 14:51;githubbot;600","runzhiwang commented on pull request #247: [LIVY-699][THRIFT] Fix resultSet.getBigDecimal throw java.sql.SQLException: Illegal conversion
URL: https://github.com/apache/incubator-livy/pull/247
 
 
   ## What changes were proposed in this pull request?
   
   [LIVY-699][THRIFT] Fix resultSet.getBigDecimal throw java.sql.SQLException: Illegal conversion. 
   
   Follows are steps to reproduce the problem:
   1. `create table test(id decimal)`.
   2. Then `resultSet.getBigDecimal(1)` will throw:` java.sql.SQLException: Illegal conversion`. The reason is `getSchema().getColumnDescriptorAt(columnIndex - 1).getType();` at https://github.com/apache/hive/blob/master/jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java#L415 return string, so cannot pass the check `val instanceof BigDecimal `at https://github.com/apache/hive/blob/master/jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java#L133, so throw `java.sql.SQLException: Illegal conversion` at https://github.com/apache/hive/blob/master/jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java#L137
   3. So the root cause is the error return of `getType()`, which should return decimal other than string.
   4. Regarding to date and timestamp, though the return type is string, hive-jdbc has done the transformation from string to date and timestamp in the following links. But I think it is necessary to return the right type.
   https://github.com/apache/hive/blob/master/jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java#L255
   https://github.com/apache/hive/blob/master/jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java#L571
   
   Additionally, SparkThrift return decimal type instead of string in the same case, if user use `getBigDecimal` and migrate from SparkThrift to Livy, it will throw exception. So it is necessary to return decimal instead of string in livy. 
   
   ## How was this patch tested?
   
   Add `SchemaIT.scala` to test the return of column type. 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 14:52;githubbot;600","mgaido91 commented on pull request #247: [LIVY-699][THRIFT] Fix resultSet.getBigDecimal throw java.sql.SQLException: Illegal conversion
URL: https://github.com/apache/incubator-livy/pull/247
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Nov/19 10:27;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 12 10:28:20 UTC 2019,,,,,,,,,,"0|z07snc:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Nov/19 10:28;mgaido;Issue resolved by PR https://github.com/apache/incubator-livy/pull/247.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster support for Livy,LIVY-698,13262754,,New Feature,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,xilangyan,xilangyan,17/Oct/19 03:31,13/Dec/19 01:03,19/Dec/25 04:15,13/Dec/19 01:03,0.6.0,,,,,Server,,,,,,,,,,0,,,,,,"This is a proposal for livy cluster support, it is designed to be a light solution for livy cluster which can compatible with non-cluster livy and different HA level.Â 

Server ID:Â  integer configured by livy.server.id, with default value 0 which is standalone mode. Lagest server id is 213, reason described below.

Session ID: session ID is generated as 3 digit server ID and 7 digit auto-increment integer. As the biggest integer isÂ  2,147,483,647, so largest server ID is 213 and each server can have 9,999,999 sessions. Limitation here is each cluster can have most 213 instance which I think is enough. For standalone mode, as server id is 0, so works the same.

Zookeeper: as zookeeper is required by config livy.server.zookeeper.quorum

Server Registration: each server should register themself to zookeeper path /livy/server/Â 

Leader: one of livy server is elected as leader of cluster on zookeeper path /livy/leader/

Coordination between servers:Â  servers don't talk with earch other directly, server just detect which livy server the session lives on, and send http 307 redirect to the correct livy server.Â  For example, if server A receive a requestÂ  [http://serverA:8999/sessions/1010000001|http://servera:8999/sessions/1010000001], server A know session is on server B, then it send a 307 redirect toÂ [http://serverB:8999/sessions/1010000001|http://serverb:8999/sessions/1010000001].Â 

Â 

Session HA: consider server failure case, user should be able to decide if want to keep sessions on failure server. This lead to two different mode:

Non session HA mode:

With this mode, session lost when sever failed(but it can still work with session-store, recover session when server get well)

request redirect: Sever detect session's correct server just by get the first 3 digit of session id.
 Session HA mode:

With this mode,Â  session information will be persistent to ZK store(reuse current session-store code), and recover in another server when server failed.

session registration:Â  all sessions are registered to zk path /livy/session/type/

request redirect: each server detect correct server of session by read zk and then send 307 redirect, server may cache session-server pair for a while

server failure: cluster leader should detect server failure, reassign session on failure server to other servers, other server should recover session by read session information in ZK

server notification: server need to send msg to other server, for example leader send command to ask server to recover session. all such msgs are sent through zk, in path /livy/notification/

Â 

Thrift Server: Thrift session has thrift session ID, thrift session can be restored with same session ID, however current hive-jdbc doesn't support restore session. So for thrift server, just register server to ZK to implement same server discovery as hive have. Later when hive-jdbc support restore session, thrift server can use exisitng livy cluster solution to adapt hive-jdbc.

Â ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-718,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2019-10-17 03:31:13.0,,,,,,,,,,"0|z07o7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rsc client cannot resolve the hostname of driver in yarn-cluster mode,LIVY-697,13261979,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,runzhiwang,runzhiwang,13/Oct/19 04:44,18/Apr/25 14:43,19/Dec/25 04:15,21/Oct/19 08:17,0.6.0,,,0.7.0,,RSC,,,,,,,,,,0,,,,,,!image-2019-10-13-12-44-41-861.png!,,"runzhiwang commented on pull request #246: [LIVY-697] Rsc client cannot resolve the hostname of driver in yarn-cluster mode
URL: https://github.com/apache/incubator-livy/pull/246
 
 
   
   
   ## What changes were proposed in this pull request?
   
   [LIVY-697] Rsc client cannot resolve the hostname of driver in yarn-cluster mode
   
   1. The content of Driver in /etc/hosts are as follows:
      127.0.0.1 localhost
      10.56.215.106 tdw-10-56-215-106
   
   2. The content of Driver in /etc/hostname are as follows:
      tdw-10-56-215-106
   
   3. The findLocalAddress method in livy cannot return 10.56.215.106, but return tdw-10-56-215-106.
     Because the hostname: tdw-10-56-215-106 point to 10.56.215.106,  which does pass the check 
      address.isLoopbackAddress(), so findLocalAddress return tdw-10-56-215-106.
   
   4. The rsc client cannot resolve the tdw-10-56-215-106, which cause rsc client cannot connect to 
      driver.
   
   5. Though I can modify the findLocalAddress method to return 10.56.215.106. But I do not think it's a
      good idea to get ip from host, which maybe cause error if the machine has multiple  network 
      cards. Also, rsc client can get the driver ip from the connection.
   
   ## How was this patch tested?
   
   1. The content of Driver in /etc/hosts are as follows:
      127.0.0.1 localhost
      10.56.215.106 tdw-10-56-215-106
   
   2. The content of Driver in /etc/hostname are as follows:
      tdw-10-56-215-106
   
   3. rsc client can get the driver ip in connection.
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Oct/19 13:13;githubbot;600","runzhiwang commented on pull request #246: [LIVY-697] Rsc client cannot resolve the hostname of driver in yarn-cluster mode
URL: https://github.com/apache/incubator-livy/pull/246
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Oct/19 13:33;githubbot;600","runzhiwang commented on pull request #246: [LIVY-697] Rsc client cannot resolve the hostname of driver in yarn-cluster mode
URL: https://github.com/apache/incubator-livy/pull/246
 
 
   ## What changes were proposed in this pull request?
   
   [LIVY-697] Rsc client cannot resolve the hostname of driver in yarn-cluster mode
   
   1. The content of Driver in /etc/hosts are as follows:
      127.0.0.1 localhost
      10.56.215.106 tdw-10-56-215-106
   
   2. The content of Driver in /etc/hostname are as follows:
      tdw-10-56-215-106
   
   3. The findLocalAddress method in livy cannot return 10.56.215.106, but return tdw-10-56-215-106.
      Because the hostname: tdw-10-56-215-106 point to 10.56.215.106,  which does pass the check 
      address.isLoopbackAddress(), so findLocalAddress return tdw-10-56-215-106.
   
   4. The findLocalAddress method in livy can return 10.56.215.106 as expected if the content 
     of Driver in  /etc/hosts are as follows, which is not correct in our environment.
      127.0.0.1 localhost
      127.0.0.1 tdw-10-56-215-106
   
   5. The rsc client cannot resolve the tdw-10-56-215-106, which cause rsc client cannot connect to 
      driver.
   
   6. Though I can modify the findLocalAddress method to return 10.56.215.106, but it maybe cause 
     error if the machine has multiple  network  cards.  So, rsc client gets the driver ip from the 
     connection
   
   ## How was this patch tested?
   
   1. The content of Driver in /etc/hosts are as follows:
      127.0.0.1 localhost
      10.56.215.106 tdw-10-56-215-106
   
   2. The content of Driver in /etc/hostname are as follows:
      tdw-10-56-215-106
   
   3. rsc client can get the driver ip in connection.
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Oct/19 13:34;githubbot;600","jerryshao commented on pull request #246: [LIVY-697] Rsc client cannot resolve the hostname of driver in yarn-cluster mode
URL: https://github.com/apache/incubator-livy/pull/246
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Oct/19 08:16;githubbot;600","idzikovsky opened a new pull request, #388:
URL: https://github.com/apache/incubator-livy/pull/388

   The fix for the LIVY-697 (#246) works fine for the problem it was designed to solve. But it causes another problem:
   the host reported to Livy Server from the RSC Driver is never used.
   
   I know that this is kind of unsupported for now, but I'm trying to run Livy in Kubernetes in the namespace with Istio injections enabled.
   And after fix for LIVY-697 it stopped working (I use Livy 0.7 with patches from #249), howerver when I used patches from #167 applied on Livy 0.5 everything was fine.
   
   **Root cause**
   In the fix for #167 we are explicitly (setting the host)[https://github.com/apache/incubator-livy/pull/249/files#diff-43114318c4b009c2404f7eb326a84c184fb1501a3237c49a771df851d0f6f328R175-R177] that will be reported by RSC Driver back to Livy Server. And because of the fix for LIVY-697 a value of this property is ignored.
   
   And even in that case, everything works fine, until we try to enable Istio: as Istio wraps all TCP traffic through a proxy, the following piece of code would set the host property not to the host of the actual RSC Driver, but to the IP address of TCP proxy server which would be 127.0.0.6:
   ```java
           InetSocketAddress insocket = (InetSocketAddress) ctx.channel().remoteAddress();
           host = insocket.getAddress().getHostAddress();
   ```
   
   So it would be good to add some option to disable the workaround that was made in the scope of LIVY-697.
   Especially taking into account that initially Livy was designed to connect to the driver using host/port reported by the driver, and not by using hacks that get the host from a remote connection channel.
   
   In my fix, I'm providing an option to enable the fix for LIVY-697 and leave it enabled by default to keep the current behavior as is.
   
   ## How was this patch tested?
   
   Manual
   
   


;31/Jan/23 21:26;githubbot;600","codecov-commenter commented on PR #246:
URL: https://github.com/apache/incubator-livy/pull/246#issuecomment-2815578577

   ## [Codecov](https://app.codecov.io/gh/apache/incubator-livy/pull/246?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) Report
   All modified and coverable lines are covered by tests :white_check_mark:
   > Project coverage is 68.12%. Comparing base [(`0804c8e`)](https://app.codecov.io/gh/apache/incubator-livy/commit/0804c8ea8ece67d01ababec616c9ad8e3b15dc9f?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache) to head [(`4a58338`)](https://app.codecov.io/gh/apache/incubator-livy/commit/4a583385e43f743da51cfec6eb803a66cf4590b8?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   > Report is 79 commits behind head on master.
   
   <details><summary>Additional details and impacted files</summary>
   
   
   ```diff
   @@             Coverage Diff              @@
   ##             master     #246      +/-   ##
   ============================================
   - Coverage     68.45%   68.12%   -0.33%     
   - Complexity      927      939      +12     
   ============================================
     Files           100      101       +1     
     Lines          5729     5855     +126     
     Branches        870      886      +16     
   ============================================
   + Hits           3922     3989      +67     
   - Misses         1247     1295      +48     
   - Partials        560      571      +11     
   ```
   
   </details>
   
   [:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/apache/incubator-livy/pull/246?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).   
   :loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=apache).
   
   <details><summary> :rocket: New features to boost your workflow: </summary>
   
   - :snowflake: [Test Analytics](https://docs.codecov.com/docs/test-analytics): Detect flaky tests, report on failures, and find test suite problems.
   - :package: [JS Bundle Analysis](https://docs.codecov.com/docs/javascript-bundle-analysis): Save yourself from yourself by tracking and limiting bundle sizes in JS merges.
   </details>


;18/Apr/25 14:43;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/19 04:44;runzhiwang;image-2019-10-13-12-44-41-861.png;https://issues.apache.org/jira/secure/attachment/12982866/image-2019-10-13-12-44-41-861.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 21 08:17:35 UTC 2019,,,,,,,,,,"0|z07jfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Oct/19 04:45;runzhiwang;working on it;;;","21/Oct/19 08:17;jerryshao;Issue resolved by pull request 246
https://github.com/apache/incubator-livy/pull/246;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade JQuery to 3.4.1 and Bootstrap to 3.4.1,LIVY-695,13261310,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,,petertoth,petertoth,09/Oct/19 12:37,29/Oct/19 16:43,19/Dec/25 04:15,29/Oct/19 16:43,,,,0.7.0,,,,,,,,,,,,0,,,,,,Upgrade JQuery to 3.4.1 and Bootstrap to 3.4.1 to fix CVEs.,,"peter-toth commented on pull request #245: [LIVY-695] Upgrade JQuery to 3.4.1 and Bootstrap to 3.4.1
URL: https://github.com/apache/incubator-livy/pull/245
 
 
   ## What changes were proposed in this pull request?
   
   Upgrade JQuery to 3.4.1 and Bootstrap to 3.4.1 to fix CVEs.
   
   ## How was this patch tested?
   
   Manual tests:
   - Livy Server UI looks ok
   - Documentation had a minor issue of logo position after plain upgrade, but fix is included in this commit
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Oct/19 12:45;githubbot;600","asfgit commented on pull request #245: [LIVY-695] Upgrade JQuery to 3.4.1 and Bootstrap to 3.4.1
URL: https://github.com/apache/incubator-livy/pull/245
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Oct/19 17:28;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2019-10-09 12:37:46.0,,,,,,,,,,"0|z07faw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade jetty to 9.4.18.v20190429,LIVY-694,13261079,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,arunmahadevan,petertoth,petertoth,08/Oct/19 10:22,04/Nov/25 20:40,19/Dec/25 04:15,04/Nov/25 20:40,,,,,,,,,,,,,,,,0,,,,,,Upgrade the jetty dependency to 9.4.18.v20190429 to fix CVEs.,,"peter-toth commented on pull request #244: [LIVY-694] Upgrade Jetty to 9.4.18.v20190429
URL: https://github.com/apache/incubator-livy/pull/244
 
 
   ## What changes were proposed in this pull request?
   
   Upgrade the jetty dependency to 9.4.18.v20190429 to fix CVEs.
   
   ## How was this patch tested?
   
   Existing UTs.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Oct/19 10:34;githubbot;600","coheigea commented on pull request #244:
URL: https://github.com/apache/incubator-livy/pull/244#issuecomment-648213951


   This PR duplicates an existing PR: https://github.com/apache/incubator-livy/pull/175/files
   @peter-toth is there any status on the Jetty check? It would be good to merge the fix due to the outstanding CVE issue.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jun/20 14:51;githubbot;600","peter-toth commented on pull request #244:
URL: https://github.com/apache/incubator-livy/pull/244#issuecomment-648245292


   @coheigea , unfortunately I didn't have time to check http mode, but we are using version 9.4.26.v20200117 now.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Jun/20 15:40;githubbot;600","coheigea commented on pull request #244:
URL: https://github.com/apache/incubator-livy/pull/244#issuecomment-697307994


   This PR is superceded by https://github.com/apache/incubator-livy/pull/305


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Sep/20 11:39;githubbot;600","peter-toth closed pull request #244: [LIVY-694] Upgrade Jetty to 9.4.18.v20190429
URL: https://github.com/apache/incubator-livy/pull/244


;19/Apr/24 19:31;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,,,,,,,LIVY-599,,LIVY-851,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 04 20:40:39 UTC 2025,,,,,,,,,,"0|z07dvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Nov/25 20:40;gyogal;Jetty version is already at 9.4.56.v20240826, resolving this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift server Kerberos authentication does not use configured principal,LIVY-692,13260682,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,wypoon,wypoon,05/Oct/19 01:50,16/Oct/19 22:24,19/Dec/25 04:15,16/Oct/19 22:19,0.6.0,,,,,Thriftserver,,,,,,,,,,0,,,,,,"I tried to use the Thrift server in Livy 0.6. I have a secure cluster, and I set
{noformat}
livy.server.thrift.authentication=KERBEROS
{noformat}
on the server side. I also have livy.server.auth.kerberos.keytab and livy.server.auth.kerberos.principal configured. In particular, in my system, I have
{noformat}
livy.server.auth.kerberos.principal=HTTP/<livy_server>@<REALM>
{noformat}
When I try to connect to the Thrift server using the beeline shipped with Hive, with
{noformat}
beeline -u 'jdbc:hive2://<livy_server>:<thrift_port>/<db>;principal=HTTP/<livy_server>@<REALM>'
{noformat}
it fails with
{noformat}
Error: Could not open client transport with JDBC Uri: ... :: Peer indicated failure: GSS initiate failed (state=08S01,code=0)
{noformat}
On the server side, the log shows
{noformat}
2019-10-03 15:21:09,536 ERROR org.apache.thrift.transport.TSaslTransport: SASL negotiation failure
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: Failure unspecified at GSS-API level (Mechanism level: Invalid argument (400) - Cannot find key of appropriate type to decrypt AP REP - DES3 CBC mode with SHA1-KD)]
	at com.sun.security.sasl.gsskerb.GssKrb5Server.evaluateResponse(GssKrb5Server.java:199)
	at org.apache.thrift.transport.TSaslTransport$SaslParticipant.evaluateChallengeOrResponse(TSaslTransport.java:539)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:283)
	at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)
	at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)
	at org.apache.livy.thriftserver.auth.TUGIAssumingTransportFactory$$anon$1.run(AuthBridgeServer.scala:143)
	at org.apache.livy.thriftserver.auth.TUGIAssumingTransportFactory$$anon$1.run(AuthBridgeServer.scala:142)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:360)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1710)
	at org.apache.livy.thriftserver.auth.TUGIAssumingTransportFactory.getTransport(AuthBridgeServer.scala:142)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:269)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: GSSException: Failure unspecified at GSS-API level (Mechanism level: Invalid argument (400) - Cannot find key of appropriate type to decrypt AP REP - DES3 CBC mode with SHA1-KD)
	at sun.security.jgss.krb5.Krb5Context.acceptSecContext(Krb5Context.java:856)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:342)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:285)
	at com.sun.security.sasl.gsskerb.GssKrb5Server.evaluateResponse(GssKrb5Server.java:167)
	... 14 more
Caused by: KrbException: Invalid argument (400) - Cannot find key of appropriate type to decrypt AP REP - DES3 CBC mode with SHA1-KD
	at sun.security.krb5.KrbApReq.authenticate(KrbApReq.java:278)
	at sun.security.krb5.KrbApReq.<init>(KrbApReq.java:149)
	at sun.security.jgss.krb5.InitSecContextToken.<init>(InitSecContextToken.java:108)
	at sun.security.jgss.krb5.Krb5Context.acceptSecContext(Krb5Context.java:829)
	... 17 more
{noformat}
On debugging, I found that the UGI in the TUGIAssumingTransportFactory is not that of the configured HTTP principal but the UGI of the service principal (livy).
",,"wypoon commented on pull request #241: [LIVY-692][THRIFT] Use configured principal for Kerberos authentication
URL: https://github.com/apache/incubator-livy/pull/241
 
 
   ## What changes were proposed in this pull request?
   
   https://issues.apache.org/jira/browse/LIVY-692
   Currently, if Kerberos authentication is used in the Thrift server (livy.server.thrift.authentication=kerberos), the `TTransportFactory` used in the `ThriftCLIService` uses a `UserGroupInformation` for the service (livy) principal; instead it should use a `UserGroupInformation` for the principal specified in livy.server.auth.kerberos.principal.
   
   ## How was this patch tested?
   
   Manual testing with both a Livy Thrift server using Kerberos authentication and one using simple authentication.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Oct/19 02:36;githubbot;600","wypoon commented on pull request #241: [LIVY-692][THRIFT] Use configured principal for Kerberos authentication
URL: https://github.com/apache/incubator-livy/pull/241
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/19 22:24;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 16 22:19:29 UTC 2019,,,,,,,,,,"0|z07bfk:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Oct/19 22:09;wypoon;With my change, I am able to connect to the Thrift server from beeline (both with the same configuration and JDBC URL as described above) and run queries.;;;","09/Oct/19 01:05;wypoon;To clarify, the problem occurs when I am using binary mode (the default for livy.server.thrift.transport.mode). When I use http mode, then I am able to connect using
{noformat}
beeline -u 'jdbc:hive2://<livy_server>:<thrift_port>/<db>;principal=HTTP/<livy_server>@<REALM>;transportMode=http;httpPath=cliservice'
{noformat}
and am able to run queries.
However, in that case, there are a slew of exceptions in the Livy server log, like
{noformat}
2019-10-08 18:02:05,926 INFO org.apache.livy.thriftserver.cli.ThriftHttpServlet: Could not validate cookie sent, will try to generate a new cookie
2019-10-08 18:02:05,926 INFO org.apache.livy.thriftserver.cli.ThriftHttpServlet: Failed to authenticate with http/_HOST kerberos principal, trying with livy/_HOST kerberos principal
2019-10-08 18:02:05,926 ERROR org.apache.livy.thriftserver.cli.ThriftHttpServlet: Failed to authenticate with livy/_HOST kerberos principal
2019-10-08 18:02:05,926 ERROR org.apache.livy.thriftserver.cli.ThriftHttpServlet: Error: 
org.apache.hive.service.auth.HttpAuthenticationException: java.lang.reflect.UndeclaredThrowableException
	at org.apache.livy.thriftserver.cli.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.scala:314)
	at org.apache.livy.thriftserver.cli.ThriftHttpServlet.doPost(ThriftHttpServlet.scala:111)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:539)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.UndeclaredThrowableException
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1748)
	at org.apache.livy.thriftserver.cli.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.scala:310)
	... 24 more
Caused by: org.apache.hive.service.auth.HttpAuthenticationException: Authorization header received from the client is empty.
	at org.apache.livy.thriftserver.cli.ThriftHttpServlet$.getAuthHeader(ThriftHttpServlet.scala:407)
	at org.apache.livy.thriftserver.cli.HttpKerberosServerAction.run(ThriftHttpServlet.scala:461)
	at org.apache.livy.thriftserver.cli.HttpKerberosServerAction.run(ThriftHttpServlet.scala:430)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	... 25 more
{noformat}
These exceptions seem to be an unrelated issue. I also see them when using http mode with my patch.
;;;","16/Oct/19 22:19;wypoon;This issue is not a bug. It arose from two causes: (1) I was not using a pure upstream Livy 0.6 but an internal version based on Livy 0.6 with some internal changes which happened to cause the livy principal to be obtained from ticket cache instead of keytab (needed for the principal to be used for Kerberos authentication in the Thrift server in binary mode), and (2) a misunderstanding thatÂ livy.server.auth.kerberos.principal should be the principal to use in the JDBC URL for connecting to the Thrift server. I was led to (2) because using the livy principal in the JDBC URL failed for me due to (1).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude curator in thrift server pom to avoid conflict jars,LIVY-690,13259465,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,yihengw,yihengw,yihengw,29/Sep/19 02:28,29/Sep/19 08:26,19/Dec/25 04:15,29/Sep/19 08:26,0.6.0,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,"Currently, thrift server has a dependency of curator-client:2.12.0 through the hive service. After the build, a curator-client-2.12.0.jar file will be generated in the jars folder. It is conflicted with the curator-client-2.7.1.jar file, which is used by livy server.

We observed that in some JDK, the curator-client-2.12.0.jar is loaded before the curator-client-2.7.1.jar, and will crash the recovery enabled livy server.",,"jerryshao commented on pull request #239: [LIVY-690][Thrift] Exclude curator in thrift server pom to avoid conflict jars
URL: https://github.com/apache/incubator-livy/pull/239
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Sep/19 08:25;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Sep 29 08:26:13 UTC 2019,,,,,,,,,,"0|z074bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,"29/Sep/19 02:29;yihengw;This will be fixed by this patch:
https://github.com/apache/incubator-livy/pull/239;;;","29/Sep/19 08:26;jerryshao;Issue resolved by pull request 239
[https://github.com/apache/incubator-livy/pull/239];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error message of LivyClient only keep outer stackTrace but discard cause's stackTrace,LIVY-688,13258690,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,994184916@qq.com,994184916@qq.com,994184916@qq.com,25/Sep/19 04:32,29/Sep/19 08:22,19/Dec/25 04:15,29/Sep/19 08:20,0.6.0,,,0.7.0,,RSC,,,,,,,,,,0,,,,,,"1. SparkSession maybe failed when initialize ExternalCatalog at sometime. As SparkSession call _SharedState.reflect_ to instance anÂ ExternalCatalog, any exception happened during this process will wrapped byÂ InvocationTargetException.

IllegalArgumentException
 Â  â””â”€â”€InvocationTargetException
 Â  Â  Â  Â  Â  Â  Â â””â”€â”€the indeed Exception
 2. org.apache.livy.rsc.Utils.stackTraceAsString only keepÂ IllegalArgumentException'sÂ stackTrace but discardÂ the indeed Exception'sÂ stackTrace and message, which makes the finalÂ java.util.concurrent.ExecutionException's message ambiguous.Â Â ",,"WeiWenda commented on pull request #237: [LIVY-688] Error message of BypassJobStatus should contains cause information of Exception
URL: https://github.com/apache/incubator-livy/pull/237
 
 
   ## What changes were proposed in this pull request?
   Change the implement of org.apache.livy.rsc.Utils.stackTraceAsString to guava Throwables.getStackTraceAsString, so that user can receive details of error message by calling org.apache.livy.client.http.JobHandleImpl.get.
   
   https://issues.apache.org/jira/browse/LIVY-688
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Sep/19 05:20;githubbot;600","jerryshao commented on pull request #237: [LIVY-688] Error message of BypassJobStatus should contains cause information of Exception
URL: https://github.com/apache/incubator-livy/pull/237
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Sep/19 08:20;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Sep 29 08:20:33 UTC 2019,,,,,,,,,,"0|z06zj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Sep/19 05:21;994184916@qq.com;[https://github.com/apache/incubator-livy/pull/237];;;","29/Sep/19 08:20;jerryshao;Issue resolved by pull request 237
[https://github.com/apache/incubator-livy/pull/237];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy SQLInterpreter get empty array when extract date format rows to json,LIVY-683,13257032,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,Jassy,Jassy,17/Sep/19 06:29,21/Sep/19 17:07,19/Dec/25 04:15,21/Sep/19 17:07,,,,,,REPL,,,,,,,,,,0,,,,,,"I submit a sql with select date format:Â 
{code:java}
select to_date(update_time) from sec_ods.ods_binlog_task_list_d_whole where concat(YEAR, MONTH, DAY )=20190306 limit 10
{code}
in livy:

!image-2019-09-17-14-23-31-715.png|width=429,height=424!

we can see data is composed by some empty arrays.

and in spark this sql can get correct result:
{code:java}
to_date(sec_ods.ods_binlog_task_list_d_whole.`update_time`)
2019-03-04
2019-03-04
2019-03-04
2019-03-04
2019-03-04
2019-03-04
2019-03-04
2019-03-04
2019-03-04
2019-03-04
{code}",,"Jassy1994 commented on pull request #234: [LIVY-683] Livy SQLInterpreter get empty array when extract date format rows to json
URL: https://github.com/apache/incubator-livy/pull/234
 
 
   ## What changes were proposed in this pull request?
   
   The issue link: [LIVY-683](https://issues.apache.org/jira/browse/LIVY-683)
   to avoid this situation, we can transform date data to string data in rows, and this time rows can be extracted correctly.
   
   ## How was this patch tested?
   
   manual test
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Sep/19 06:57;githubbot;600","mgaido91 commented on pull request #234: [LIVY-683] Livy SQLInterpreter get empty array when extract date format rows to json
URL: https://github.com/apache/incubator-livy/pull/234
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Sep/19 17:06;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,LIVY-613,,,,,,,,"17/Sep/19 06:23;Jassy;image-2019-09-17-14-23-31-715.png;https://issues.apache.org/jira/secure/attachment/12980467/image-2019-09-17-14-23-31-715.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2019-09-17 06:29:43.0,,,,,,,,,,"0|z06pbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Livy Thrift-server Ordinary ldap authentication, based on ldap.url, basedn, domain",LIVY-678,13256876,13245964,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,micahzhao,micahzhao,micahzhao,16/Sep/19 13:18,31/Oct/19 02:37,19/Dec/25 04:15,31/Oct/19 02:37,,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,"Add providerÂ  to search for the user in the LDAP directory within the baseDN tree. Access is granted if user has been found, and denied otherwise.",,"captainzmc commented on pull request #236: [LIVY-678] Thrift ldap authentication, based on ldapurl, basedn, domain
URL: https://github.com/apache/incubator-livy/pull/236
 
 
   ## What changes were proposed in this pull request?
   
   [LIVY-609] subtask :
   [LIVY-678] Based on ldapUrl, basedn, domain implements LDAP authentication of Livy thriftserver.
   
   ## How was this patch tested?
   
   UTs tests for this part have been added. We can test in UTs
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Sep/19 07:46;githubbot;600","jerryshao commented on pull request #236: [LIVY-678] Thrift ldap authentication, based on ldapurl, basedn, domain
URL: https://github.com/apache/incubator-livy/pull/236
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Oct/19 02:35;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 31 02:37:27 UTC 2019,,,,,,,,,,"0|z06od4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"31/Oct/19 02:37;jerryshao;Issue resolved by pull request 236
https://github.com/apache/incubator-livy/pull/236;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LDAP Livy Server(Rest) support,LIVY-662,13255163,,Improvement,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,micahzhao,micahzhao,micahzhao,06/Sep/19 06:28,09/Sep/19 06:36,19/Dec/25 04:15,09/Sep/19 06:36,0.6.0,,,,,Server,,,,,,,,,,0,,,,,,Currently Livy server (Rest) does not have LDAP security authentication. We need to add support for it,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 09 06:36:52 UTC 2019,,,,,,,,,,"0|z06dsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Sep/19 06:37;micahzhao;We have this need and I would like to add this.;;;","09/Sep/19 06:36;micahzhao;The same jira already exists:Â 
https://issues.apache.org/jira/browse/LIVY-356;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Travis failed on ""can kill spark-submit while it's running""",LIVY-659,13254669,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,runzhiwang,runzhiwang,04/Sep/19 09:48,06/Sep/19 07:45,19/Dec/25 04:15,06/Sep/19 07:45,0.6.0,,,0.7.0,,Tests,,,,,,,,,,0,,,,,,"* can kill spark-submit while it's running *** FAILED *** (41 milliseconds)
 org.mockito.exceptions.verification.WantedButNotInvoked: Wanted but not invoked:
lineBufferedProcess.destroy();
-> at org.apache.livy.utils.SparkYarnAppSpec$$anonfun$1$$anonfun$apply$mcV$sp$13$$anonfun$apply$mcV$sp$15$$anonfun$apply$mcV$sp$16.apply$mcV$sp(SparkYarnAppSpec.scala:226)
Actually, there were zero interactions with this mock.
 at org.apache.livy.utils.SparkYarnAppSpec$$anonfun$1$$anonfun$apply$mcV$sp$13$$anonfun$apply$mcV$sp$15$$anonfun$apply$mcV$sp$16.apply$mcV$sp(SparkYarnAppSpec.scala:226)
 at org.apache.livy.utils.SparkYarnAppSpec.org$apache$livy$utils$SparkYarnAppSpec$$cleanupThread(SparkYarnAppSpec.scala:43)
 at org.apache.livy.utils.SparkYarnAppSpec$$anonfun$1$$anonfun$apply$mcV$sp$13$$anonfun$apply$mcV$sp$15.apply$mcV$sp(SparkYarnAppSpec.scala:224)
 at org.apache.livy.utils.Clock$.withSleepMethod(Clock.scala:31)
 at org.apache.livy.utils.SparkYarnAppSpec$$anonfun$1$$anonfun$apply$mcV$sp$13.apply$mcV$sp(SparkYarnAppSpec.scala:201)
 at org.apache.livy.utils.SparkYarnAppSpec$$anonfun$1$$anonfun$apply$mcV$sp$13.apply(SparkYarnAppSpec.scala:201)
 at org.apache.livy.utils.SparkYarnAppSpec$$anonfun$1$$anonfun$apply$mcV$sp$13.apply(SparkYarnAppSpec.scala:201)
 at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)

please reference to:Â https://travis-ci.org/captainzmc/incubator-livy/jobs/580596561",,"runzhiwang commented on pull request #226: [LIVY-659]Fix travis failed on can kill spark-submit while it's running
URL: https://github.com/apache/incubator-livy/pull/226
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""can kill spark-submit while it's running""
   
   The cause of failed is as follows:
   
   1. The test does not mock the method getApplicationReport of mockYarnClient, so yarnClient.getApplicationReport will return null.
   
   2. If the thread yarnAppMonitorThread run before the test executing app.kill(), yarnAppMonitorThread will throw NullPointerException when execute appReport.getDiagnostics.
   
   3. Then the NullPointerException was caught by yarnAppMonitorThread, and yarnAppMonitorThread changeState(SparkApp.State.FAILED).
   
   4. At last, the test execute app.kill(), but the app state is FAILED, so it won't execute process.foreach(_.destroy()), which cause verify(mockSparkSubmit, times(1)).destroy() failed.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 00:34;githubbot;600","jerryshao commented on pull request #226: [LIVY-659][TEST]Fix travis failed on can kill spark-submit while it's running
URL: https://github.com/apache/incubator-livy/pull/226
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Sep/19 07:44;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 06 07:45:43 UTC 2019,,,,,,,,,,"0|z06b9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Sep/19 09:48;runzhiwang;Iâ€˜m working on it.;;;","06/Sep/19 07:45;jerryshao;Issue resolved by pull request 226
https://github.com/apache/incubator-livy/pull/226;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RSCDriver should catch exception if cancel job failed during shutdown,LIVY-658,13254246,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,xilangyan,xilangyan,xilangyan,02/Sep/19 13:35,29/Sep/19 02:13,19/Dec/25 04:15,29/Sep/19 02:13,,,,0.7.0,,RSC,,,,,,,,,,0,,,,,,"Currently, if startup meet exception, exception will trigger spark to shutdown, then trigger cancel job, but cancel job will throw another exception due to spark is not initialized. The new exception will swallow the old exception.",,"yantzu commented on pull request #223: [LIVY-658]RSCDriver should catch exception if cancel job failed during shutdown
URL: https://github.com/apache/incubator-livy/pull/223
 
 
   ## What changes were proposed in this pull request?
   
   Currently, if startup meet exception, exception will trigger spark to shutdown, then trigger cancel job, but cancel job will throw another exception due to spark is not initialized. The new exception will swallow the old exception.
   
   Before changes:
   ![cancel job exception](https://user-images.githubusercontent.com/7855100/64118287-f0961900-cdc9-11e9-9b72-d051fb4bdbdf.jpg)
   
   After changes:
   ![cancel job exception after fix](https://user-images.githubusercontent.com/7855100/64118295-f4c23680-cdc9-11e9-9a2d-38efa0770a99.jpg)
   
   ## How was this patch tested?
   
   Tested manually.
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 13:38;githubbot;600","jerryshao commented on pull request #223: [LIVY-658]RSCDriver should catch exception if cancel job failed during shutdown
URL: https://github.com/apache/incubator-livy/pull/223
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Sep/19 06:53;githubbot;600","yantzu commented on pull request #223: [LIVY-658]RSCDriver should catch exception if cancel job failed during shutdown
URL: https://github.com/apache/incubator-livy/pull/223
 
 
   ## What changes were proposed in this pull request?
   
   Currently, if startup meet exception, exception will trigger spark to shutdown, then trigger cancel job, but cancel job will throw another exception due to spark is not initialized. The new exception will swallow the old exception.
   
   https://issues.apache.org/jira/browse/LIVY-658
   
   Before changes:
   ![cancel job exception](https://user-images.githubusercontent.com/7855100/64118287-f0961900-cdc9-11e9-9b72-d051fb4bdbdf.jpg)
   
   After changes:
   ![cancel job exception after fix](https://user-images.githubusercontent.com/7855100/64118295-f4c23680-cdc9-11e9-9a2d-38efa0770a99.jpg)
   
   ## How was this patch tested?
   
   Tested manuallyï¼Œ and  add unit test.
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Sep/19 06:53;githubbot;600","jerryshao commented on pull request #223: [LIVY-658]RSCDriver should catch exception if cancel job failed during shutdown
URL: https://github.com/apache/incubator-livy/pull/223
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Sep/19 02:09;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Sep 29 02:13:06 UTC 2019,,,,,,,,,,"0|z068zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,"29/Sep/19 02:13;jerryshao;Issue resolved by pull request 223
https://github.com/apache/incubator-livy/pull/223;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Travis failed on should not create sessions with duplicate names,LIVY-657,13254158,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,runzhiwang,runzhiwang,02/Sep/19 06:47,18/Sep/19 03:09,19/Dec/25 04:15,18/Sep/19 03:09,0.6.0,,,0.7.0,,Tests,,,,,,,,,,0,,,,,,"should not create sessions with duplicate names *** FAILED *** (17 milliseconds)
 session2.stopped was false (SessionManagerSpec.scala:96)

Â 

please reference toÂ https://travis-ci.org/apache/incubator-livy/jobs/579604782",,"runzhiwang commented on pull request #225: [LIVY-657]Fix travis failed on should not create sessions with duplicate names
URL: https://github.com/apache/incubator-livy/pull/225
 
 
   
   
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""should not create sessions with duplicate names""
   
   The cause of failed is as follows:
   
   1. When session2 was stopped, it will call stop() method in Session.scala asynchronously by Scala Future. So when assert(session2.stopped), if the stop() method has not been called, the test will fail.
   
   ## How was this patch tested?
   
   Existed UT and IT.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Sep/19 13:03;githubbot;600","jerryshao commented on pull request #225: [LIVY-657][TEST]Fix travis failed on should not create sessions with duplicate names
URL: https://github.com/apache/incubator-livy/pull/225
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Sep/19 08:10;githubbot;600","runzhiwang commented on pull request #225: [LIVY-657][TEST]Fix travis failed on should not create sessions with duplicate names
URL: https://github.com/apache/incubator-livy/pull/225
 
 
   
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""should not create sessions with duplicate names""
   
   The cause of failed is as follows:
   
   1. When session2 was stopped, it will call stop() method in Session.scala asynchronously by Scala Future. So when assert(session2.stopped), if the stop() method has not been executed asynchronously, the test will fail.
   
   ## How was this patch tested?
   
   Existed UT and IT.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Sep/19 08:10;githubbot;600","jerryshao commented on pull request #225: [LIVY-657][TEST]Fix travis failed on should not create sessions with duplicate names
URL: https://github.com/apache/incubator-livy/pull/225
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Sep/19 03:08;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 18 03:09:11 UTC 2019,,,,,,,,,,"0|z068g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"02/Sep/19 06:47;runzhiwang;I'm working on it.;;;","18/Sep/19 03:09;jerryshao;Issue resolved by pull request 225
https://github.com/apache/incubator-livy/pull/225;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to scalatest 3.0.8,LIVY-653,13253289,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Duplicate,,pj.fanning,pj.fanning,27/Aug/19 15:44,04/Nov/25 21:06,19/Dec/25 04:15,04/Nov/25 21:06,0.7.0,,,,,Tests,,,,,,,,,,0,,,,,,Might make it easier to look at supporting scala 2.12 (and ultimately scala 2.13),,"pjfanning closed pull request #219:
URL: https://github.com/apache/incubator-livy/pull/219


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jun/20 21:20;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 04 21:06:56 UTC 2025,,,,,,,,,,"0|z0633c:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Nov/22 22:09;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;","04/Nov/25 21:06;gyogal;The plugin was upgraded in LIVY-756.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrifserver doesn't set session name correctly,LIVY-652,13253171,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,xilangyan,xilangyan,xilangyan,27/Aug/19 03:11,04/Sep/19 03:14,19/Dec/25 04:15,04/Sep/19 03:14,0.6.0,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,"Thrift server doesn't set session name correctly, so not able to view session name in Livy Web UI",,"yantzu commented on pull request #218: [LIVY-652]Thrifserver doesn't set session name correctly
URL: https://github.com/apache/incubator-livy/pull/218
 
 
   ## What changes were proposed in this pull request?
   
   Thriftserver should set session name as the value passed in livy.session.name , but it current always set it NONE
   
   ## How was this patch tested?
   
   add IT
   
   https://issues.apache.org/jira/browse/LIVY-652
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Aug/19 03:18;githubbot;600","yantzu commented on pull request #218: [LIVY-652]Thrifserver doesn't set session name correctly
URL: https://github.com/apache/incubator-livy/pull/218
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Sep/19 08:13;githubbot;600","yantzu commented on pull request #218: [LIVY-652]Thrifserver doesn't set session name correctly
URL: https://github.com/apache/incubator-livy/pull/218
 
 
   ## What changes were proposed in this pull request?
   
   Thriftserver should set session name as the value passed in livy.session.name , but it current always set it NONE
   
   ## How was this patch tested?
   
   add IT
   
   https://issues.apache.org/jira/browse/LIVY-652
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Sep/19 08:13;githubbot;600","jerryshao commented on pull request #218: [LIVY-652]Thrifserver doesn't set session name correctly
URL: https://github.com/apache/incubator-livy/pull/218
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Sep/19 03:09;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 04 03:14:29 UTC 2019,,,,,,,,,,"0|z062d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Sep/19 03:14;jerryshao;Issue resolved by pull request 218
https://github.com/apache/incubator-livy/pull/218;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove schema from ResultSet,LIVY-650,13252873,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,mgaido,mgaido,mgaido,25/Aug/19 14:56,29/Aug/19 12:46,19/Dec/25 04:15,29/Aug/19 12:46,0.7.0,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,"The {{ResultSet}} class currently contains the schema despite it is never used.
We can get rid of it and this is a benefit because we don't serialize this string which may be pretty big.",,"mgaido91 commented on pull request #213: [LIVY-650][THRIFT] Remove schema from ResultSet
URL: https://github.com/apache/incubator-livy/pull/213
 
 
   ## What changes were proposed in this pull request?
   
   The class `ResultSet` is serialized and sent over the wire. Currently this class contains a JSON string representation of the spark schema, which is never used. Hence, the PR removes it in order to avoid serializing it uselessly.
   
   ## How was this patch tested?
   
   existing UTs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Aug/19 15:12;githubbot;600","mgaido91 commented on pull request #213: [LIVY-650][THRIFT] Remove schema from ResultSet
URL: https://github.com/apache/incubator-livy/pull/213
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Aug/19 12:44;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 29 12:46:10 UTC 2019,,,,,,,,,,"0|z060iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,"29/Aug/19 12:46;mgaido;Issue resolved by https://github.com/apache/incubator-livy/pull/213.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong return message in cancel statement documentation,LIVY-648,13252767,,Documentation,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,oshevchenko,oshevchenko,oshevchenko,23/Aug/19 21:59,25/Aug/19 01:41,19/Dec/25 04:15,25/Aug/19 01:37,,,,0.7.0,,,,,,,,,,,,0,,,,,,"A trivial mistake in the documentation. ""Canceled"" vs ""cancelled"" (two Ls).
[https://github.com/apache/incubator-livy/blob/80daadef02ae57b2a5487c6f92e0f7df558d4864/docs/rest-api.md#L308]

!image-2019-08-24-00-54-29-000.png|width=1529,height=278!

!image-2019-08-24-00-58-11-847.png!

We can just fix documentation or unify all names across the project.",,"o-shevchenko commented on pull request #210: [LIVY-648] Wrong return message in cancel statement documentation
URL: https://github.com/apache/incubator-livy/pull/210
 
 
   ## What changes were proposed in this pull request?
   Fix the trivial mistake in the documentation. ""Canceled"" vs ""cancelled"" (two Ls).
   
   ## How was this patch tested?
   not need any tests
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 22:02;githubbot;600","jerryshao commented on pull request #210: [LIVY-648][DOC] Wrong return message in cancel statement documentation
URL: https://github.com/apache/incubator-livy/pull/210
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Aug/19 01:37;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/19 21:54;oshevchenko;image-2019-08-24-00-54-29-000.png;https://issues.apache.org/jira/secure/attachment/12978458/image-2019-08-24-00-54-29-000.png","23/Aug/19 21:58;oshevchenko;image-2019-08-24-00-58-11-847.png;https://issues.apache.org/jira/secure/attachment/12978457/image-2019-08-24-00-58-11-847.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 25 01:37:44 UTC 2019,,,,,,,,,,"0|z05zvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Aug/19 01:37;jerryshao;Issue resolved by pull request 210
[https://github.com/apache/incubator-livy/pull/210];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Travis failed on ""batch session should not be gc-ed until application is finished""",LIVY-647,13252563,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,runzhiwang,runzhiwang,23/Aug/19 02:22,17/Sep/19 04:01,19/Dec/25 04:15,17/Sep/19 04:01,0.6.0,,,0.7.0,,Server,,,,,,,,,,0,,,,,,"- batch session should not be gc-ed until application is finished *** FAILED *** (50 milliseconds)
 org.mockito.exceptions.misusing.UnfinishedStubbingException: Unfinished stubbing detected here:
 -> at org.apache.livy.sessions.SessionManagerSpec$$anonfun$1.org$apache$livy$sessions$SessionManagerSpec$$anonfun$$changeStateAndCheck$1(SessionManagerSpec.scala:129)
 E.g. thenReturn() may be missing.
 Examples of correct stubbing:
 when(mock.isOk()).thenReturn(true);
 when(mock.isOk()).thenThrow(exception);
 doThrow(exception).when(mock).someVoidMethod();
 Hints:
 1. missing thenReturn()
 2. you are trying to stub a final method, you naughty developer!
 at org.apache.livy.sessions.SessionManagerSpec$$anonfun$1.org$apache$livy$sessions$SessionManagerSpec$$anonfun$$changeStateAndCheck$1(SessionManagerSpec.scala:129)
 at org.apache.livy.sessions.SessionManagerSpec$$anonfun$1$$anonfun$org$apache$livy$sessions$SessionManagerSpec$$anonfun$$testSessionGC$1$1.apply(SessionManagerSpec.scala:141)
 at org.apache.livy.sessions.SessionManagerSpec$$anonfun$1$$anonfun$org$apache$livy$sessions$SessionManagerSpec$$anonfun$$testSessionGC$1$1.apply(SessionManagerSpec.scala:135)
 at scala.collection.immutable.List.foreach(List.scala:392)
 at org.apache.livy.sessions.SessionManagerSpec$$anonfun$1.org$apache$livy$sessions$SessionManagerSpec$$anonfun$$testSessionGC$1(SessionManagerSpec.scala:135)
 at org.apache.livy.sessions.SessionManagerSpec$$anonfun$1$$anonfun$apply$mcV$sp$8.apply$mcV$sp(SessionManagerSpec.scala:108)
 at org.apache.livy.sessions.SessionManagerSpec$$anonfun$1$$anonfun$apply$mcV$sp$8.apply(SessionManagerSpec.scala:98)
 at org.apache.livy.sessions.SessionManagerSpec$$anonfun$1$$anonfun$apply$mcV$sp$8.apply(SessionManagerSpec.scala:98)
 at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
 at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
 ...

Â 

please reference toÂ https://travis-ci.org/runzhiwang/incubator-livy/builds/575627338",,"runzhiwang commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""batch session should not be gc-ed until application is finished""
   
   The cause of failed is as follows:
   
   1. When create BatchSessionManager, the GarbageCollector thread will be created, which check session.state. However the session was mocked, and the test thread has not execute doReturn(s).when(session).state, so cause exception.
   
   2. So the fix avoid collecting garbage of mocked session.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 07:28;githubbot;600","runzhiwang commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 08:36;githubbot;600","runzhiwang commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""batch session should not be gc-ed until application is finished""
   
   The cause of failed is as follows:
   
   1. When create BatchSessionManager, the GarbageCollector thread will be created, which collect garbage according to session.state. However the session was mocked, and the test thread change state by doReturn(s).when(session).state continuely, if the test thread has not finish doReturn(s).when(session).state, and GarbageCollector thread check session.state, the exception will threw by GarbageCollector.
   
   2. So the fix avoid collecting garbage of mocked session.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 08:49;githubbot;600","runzhiwang commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 08:57;githubbot;600","runzhiwang commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""batch session should not be gc-ed until application is finished""
   
   The cause of failed is as follows:
   
   1. When create BatchSessionManager, the GarbageCollector thread will be created, which collect garbage according to session.state. However the session was mocked, and the test thread change state by doReturn(s).when(session).state continuely, if the test thread do half of the statement doReturn(s).when(session).state, and GarbageCollector thread check session.state at this time, the exception will threw by GarbageCollector thread.
   
   2. So the fix avoid collecting garbage of mocked session.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 08:58;githubbot;600","runzhiwang commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 11:25;githubbot;600","runzhiwang commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""batch session should not be gc-ed until application is finished""
   
   The cause of failed is as follows:
   
   1. When create BatchSessionManager, the GarbageCollector thread will be created, which collect garbage according to session.state. However the session was mocked, and the test thread execute doReturn(s).when(session).state several times, if the test thread do half of the statement doReturn(s).when(session).state, and GarbageCollector thread check session.state at this time, the exception will threw by GarbageCollector thread.
   
   2. So the fix avoid collecting garbage of mocked session.
   
   3. Modify the test can also fix this bug, but it cannot avoid the similar bug happening in the future.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 11:25;githubbot;600","jerryshao commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Sep/19 07:52;githubbot;600","runzhiwang commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""batch session should not be gc-ed until application is finished""
   
   The cause of failed is as follows:
   
   1. When create BatchSessionManager, the GarbageCollector thread will be created, which collect garbage according to session.state. However the session was mocked, and the test thread execute doReturn(s).when(session).state several times, if the test thread do half of the statement doReturn(s).when(session).state, and GarbageCollector thread check session.state at this time, the exception will threw by GarbageCollector thread.
   
   2. So the fix avoid executing doReturn(s).when(session).state after session has been registered into SessionManager.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Sep/19 07:52;githubbot;600","jerryshao commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Sep/19 09:50;githubbot;600","runzhiwang commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""batch session should not be gc-ed until application is finished""
   
   The cause of failed is as follows:
   
   1. When create BatchSessionManager, the GarbageCollector thread will be created, which collect garbage according to session.state. However the session was mocked, and the test thread execute doReturn(s).when(session).state several times, if the test thread do half of the statement doReturn(s).when(session).state, and GarbageCollector thread check session.state at this time, the exception will threw by GarbageCollector thread.
   
   2. So the fix avoid executing doReturn(s).when(session).state after session has been registered into SessionManager.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Sep/19 09:50;githubbot;600","jerryshao commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 06:19;githubbot;600","runzhiwang commented on pull request #222: [LIVY-647]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""batch session should not be gc-ed until application is finished""
   
   The cause of failed is as follows:
   
   1. When create BatchSessionManager, the GarbageCollector thread will be created, which collect garbage according to session.state. However the session was mocked, and the test thread execute doReturn(s).when(session).state several times, if the test thread do half of the statement doReturn(s).when(session).state, and GarbageCollector thread check session.state at this time, the exception will threw by GarbageCollector thread.
   
   2. So the fix avoid executing doReturn(s).when(session).state after session has been registered into SessionManager.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 06:19;githubbot;600","jerryshao commented on pull request #222: [LIVY-647][TEST]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Sep/19 07:32;githubbot;600","runzhiwang commented on pull request #222: [LIVY-647][TEST]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""batch session should not be gc-ed until application is finished""
   
   The cause of failed is as follows:
   
   1. When create BatchSessionManager, the GarbageCollector thread will be created, which collect garbage according to session.state. However the session was mocked, and the test thread execute doReturn(s).when(session).state several times, if the test thread do half of the statement doReturn(s).when(session).state, and GarbageCollector thread check session.state at this time, the exception will threw by GarbageCollector thread.
   
   2. So the fix avoid executing doReturn(s).when(session).state after session has been registered into SessionManager.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Sep/19 07:32;githubbot;600","jerryshao commented on pull request #222: [LIVY-647][TEST]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Sep/19 08:31;githubbot;600","runzhiwang commented on pull request #222: [LIVY-647][TEST]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""batch session should not be gc-ed until application is finished""
   
   The cause of failed is as follows:
   
   1. When create BatchSessionManager, the GarbageCollector thread will be created, which collect garbage according to session.state. However the session was mocked, and the test thread execute doReturn(s).when(session).state several times, if the test thread do half of the statement doReturn(s).when(session).state, and GarbageCollector thread check session.state at this time, the exception will threw by GarbageCollector thread.
   
   2. So the fix avoid executing doReturn(s).when(session).state after session has been registered into SessionManager.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Sep/19 08:31;githubbot;600","jerryshao commented on pull request #222: [LIVY-647][TEST]Fix travis failed on batch session should not be gc-ed until application is finished
URL: https://github.com/apache/incubator-livy/pull/222
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Sep/19 03:59;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,10800,,,0,10800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 17 04:01:30 UTC 2019,,,,,,,,,,"0|z05ymg:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Aug/19 08:08;runzhiwang;I'm working on it.;;;","17/Sep/19 04:01;jerryshao;Issue resolved by pull request 222
https://github.com/apache/incubator-livy/pull/222;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add Name, Owner, Proxy User to web UI",LIVY-645,13252367,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,xilangyan,xilangyan,xilangyan,22/Aug/19 11:59,05/Sep/19 06:17,19/Dec/25 04:15,05/Sep/19 06:17,0.6.0,,,0.7.0,,Server,,,,,,,,,,0,,,,,,"In current web UI, Interactive Sessions list has no Name, Batch Sessions list has no Name, Owner and Proxy User. Should add those information so user can find their session quickly.",,"yantzu commented on pull request #207: [LIVY-645]Add Session Name, Owner, Proxy User information to Web UI
URL: https://github.com/apache/incubator-livy/pull/207
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 13:32;githubbot;600","yantzu commented on pull request #207: [LIVY-645]Add Session Name, Owner, Proxy User information to Web UI
URL: https://github.com/apache/incubator-livy/pull/207
 
 
   ## What changes were proposed in this pull request?
   
   1, Web UI, add Session Name to Interactive Sessions list
   2, Web UI, add Session Name, Owner, Proxy User to Batch Sessions list
   3, fix thrift server session doesn't set Session Name issue
   
   ## How was this patch tested?
   
   Update existing unit test, and have tested manually.
   
   ![AddName](https://user-images.githubusercontent.com/7855100/63513238-3de7d000-c518-11e9-8b18-613874ed635a.jpg)
   ![AddName2](https://user-images.githubusercontent.com/7855100/63513246-42ac8400-c518-11e9-9019-679bb82e4bb0.jpg)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 13:32;githubbot;600","yantzu commented on pull request #207: [LIVY-645]Add Session Name, Owner, Proxy User information to Web UI
URL: https://github.com/apache/incubator-livy/pull/207
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Aug/19 04:07;githubbot;600","yantzu commented on pull request #207: [LIVY-645]Add Session Name, Owner, Proxy User information to Web UI
URL: https://github.com/apache/incubator-livy/pull/207
 
 
   ## What changes were proposed in this pull request?
   
   1, Web UI, add Session Name to Interactive Sessions list
   2, Web UI, add Session Name, Owner, Proxy User to Batch Sessions list
   3, fix thrift server session doesn't set Session Name issue
   
   ## How was this patch tested?
   
   Update existing unit test, and have tested manually.
   
   ![AddName](https://user-images.githubusercontent.com/7855100/63513238-3de7d000-c518-11e9-8b18-613874ed635a.jpg)
   ![AddName2](https://user-images.githubusercontent.com/7855100/63513246-42ac8400-c518-11e9-9019-679bb82e4bb0.jpg)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Aug/19 04:07;githubbot;600","jerryshao commented on pull request #207: [LIVY-645]Add Session Name, Owner, Proxy User information to Web UI
URL: https://github.com/apache/incubator-livy/pull/207
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 06:15;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 05 06:17:10 UTC 2019,,,,,,,,,,"0|z05xew:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Sep/19 06:17;jerryshao;Issue resolved by pull request 207
https://github.com/apache/incubator-livy/pull/207;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report,LIVY-644,13252298,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,yihengw,yihengw,yihengw,22/Aug/19 07:27,19/Sep/19 02:38,19/Dec/25 04:15,19/Sep/19 02:38,,,,0.7.0,,Tests,,,,,,,,,,0,,,,,,"Recently a lot of Travis job failed when generating coverage report:
[https://travis-ci.org/apache/incubator-livy/jobs/575142847]
[https://travis-ci.org/apache/incubator-livy/jobs/561700903]
[https://travis-ci.org/apache/incubator-livy/jobs/508574433]
[https://travis-ci.org/apache/incubator-livy/jobs/508574435]
[https://travis-ci.org/apache/incubator-livy/jobs/508066760]
[https://travis-ci.org/apache/incubator-livy/jobs/507989073]
[https://travis-ci.org/apache/incubator-livy/jobs/574702251]
[https://travis-ci.org/apache/incubator-livy/jobs/574686891]
[https://travis-ci.org/apache/incubator-livy/jobs/574363881]
[https://travis-ci.org/apache/incubator-livy/jobs/574215174]
[https://travis-ci.org/apache/incubator-livy/jobs/573689926]
Â 

Here is the error stack:

Â 
[ERROR] Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report: An error has occurred in JaCoCo Aggregate report generation. Error while creating report: null: EOFException -> [Help 1]
2988org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report: An error has occurred in JaCoCo Aggregate report generation.
2989at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)
2990at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:154)
2991at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:146)
2992at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
2993at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
2994at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:51)
2995at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
2996at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:309)
2997at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:194)
2998at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:107)
2999at org.apache.maven.cli.MavenCli.execute (MavenCli.java:955)
3000at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:290)
3001at org.apache.maven.cli.MavenCli.main (MavenCli.java:194)
3002at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
3003at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
3004at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
3005at java.lang.reflect.Method.invoke (Method.java:498)
3006at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:289)
3007at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:229)
3008at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:415)
3009at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:356)
3010Caused by: org.apache.maven.plugin.MojoExecutionException: An error has occurred in JaCoCo Aggregate report generation.
3011at org.jacoco.maven.AbstractReportMojo.execute (AbstractReportMojo.java:167)
3012at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:134)
3013at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:208)
3014at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:154)
3015at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:146)
3016at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
3017at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
3018at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:51)
3019at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
3020at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:309)
3021at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:194)
3022at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:107)
3023at org.apache.maven.cli.MavenCli.execute (MavenCli.java:955)
3024at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:290)
3025at org.apache.maven.cli.MavenCli.main (MavenCli.java:194)
3026at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
3027at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
3028at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
3029at java.lang.reflect.Method.invoke (Method.java:498)
3030at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:289)
3031at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:229)
3032at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:415)
3033at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:356)
3034Caused by: org.apache.maven.reporting.MavenReportException: Error while creating report: null
3035at org.jacoco.maven.AbstractReportMojo.executeReport (AbstractReportMojo.java:183)
3036at org.jacoco.maven.AbstractReportMojo.execute (AbstractReportMojo.java:165)
3037at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:134)
3038at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:208)
3039at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:154)
3040at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:146)
3041at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
3042at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
3043at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:51)
3044at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
3045at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:309)
3046at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:194)
3047at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:107)
3048at org.apache.maven.cli.MavenCli.execute (MavenCli.java:955)
3049at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:290)
3050at org.apache.maven.cli.MavenCli.main (MavenCli.java:194)
3051at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
3052at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
3053at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
3054at java.lang.reflect.Method.invoke (Method.java:498)
3055at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:289)
3056at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:229)
3057at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:415)
3058at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:356)
3059Caused by: java.io.EOFException
3060at java.io.DataInputStream.readUnsignedShort (DataInputStream.java:340)
3061at java.io.DataInputStream.readUTF (DataInputStream.java:589)
3062at java.io.DataInputStream.readUTF (DataInputStream.java:564)
3063at org.jacoco.core.data.ExecutionDataReader.readExecutionData (ExecutionDataReader.java:148)
3064at org.jacoco.core.data.ExecutionDataReader.readBlock (ExecutionDataReader.java:115)
3065at org.jacoco.core.data.ExecutionDataReader.read (ExecutionDataReader.java:92)
3066at org.jacoco.core.tools.ExecFileLoader.load (ExecFileLoader.java:59)
3067at org.jacoco.core.tools.ExecFileLoader.load (ExecFileLoader.java:73)
3068at org.jacoco.maven.ReportSupport.loadExecutionData (ReportSupport.java:87)
3069at org.jacoco.maven.ReportAggregateMojo.loadExecutionData (ReportAggregateMojo.java:120)
3070at org.jacoco.maven.ReportAggregateMojo.loadExecutionData (ReportAggregateMojo.java:113)
3071at org.jacoco.maven.AbstractReportMojo.executeReport (AbstractReportMojo.java:177)
3072at org.jacoco.maven.AbstractReportMojo.execute (AbstractReportMojo.java:165)
3073at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:134)
3074at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:208)
3075at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:154)
3076at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:146)
3077at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
3078at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
3079at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:51)
3080at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
3081at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:309)
3082at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:194)
3083at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:107)
3084at org.apache.maven.cli.MavenCli.execute (MavenCli.java:955)
3085at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:290)
3086at org.apache.maven.cli.MavenCli.main (MavenCli.java:194)
3087at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
3088at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
3089at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
3090at java.lang.reflect.Method.invoke (Method.java:498)
3091at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:289)
3092at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:229)
3093at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:415)
3094at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:356)",,"yiheng commented on pull request #229: [LIVY-644][TEST] Flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report
URL: https://github.com/apache/incubator-livy/pull/229
 
 
   ## What changes were proposed in this pull request?
   This patch fixes the flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report.
   
   When JVM shutdown no gracefully in a test, the code coverage data file generated by jacoco may be corrupt. Jacoco will throw an exception when generate code coverage report.
   
   In Livy integration test, two test cases shut down no gracefully(one of them uses System.exit). We can find random failure when jacoco process code coverage data file generated by that test case.
   
   In this patch, we turn off the code coverage analysis on these two test cases.
   
   ## How was this patch tested?
   Compare the jacoco data file generated in the integration test. Before the fix, there're 18 files, and after the fix there're 16 files, which means the fix works.
   
   Run 10 builds on Travis each before and after the fix:
   1. Before the fix: 3 builds failed due to the jacoco code coverage exception
   2. After the fix: No build failed
   
   Existing UTs and ITs.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Sep/19 02:04;githubbot;600","jerryshao commented on pull request #229: [LIVY-644][TEST] Flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report
URL: https://github.com/apache/incubator-livy/pull/229
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Sep/19 07:11;githubbot;600","yiheng commented on pull request #229: [LIVY-644][TEST] Flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report
URL: https://github.com/apache/incubator-livy/pull/229
 
 
   ## What changes were proposed in this pull request?
   This patch fixes the flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report.
   
   When JVM shutdown no gracefully in a test, the code coverage data file generated by jacoco may be corrupt. Jacoco will throw an exception when generate code coverage report.
   
   In Livy integration test, two test cases shut down no gracefully(one of them uses System.exit). We can find random failure when jacoco process code coverage data file generated by that test case.
   
   In this patch, we turn off the code coverage analysis on these two test cases.
   
   ## How was this patch tested?
   Compare the jacoco data file generated in the integration test. Before the fix, there're 18 files, and after the fix there're 16 files, which means the fix works.
   
   Run 10 builds on Travis each before and after the fix:
   1. Before the fix: 3 builds failed due to the jacoco code coverage exception
   2. After the fix: No build failed
   
   Existing UTs and ITs.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Sep/19 07:11;githubbot;600","jerryshao commented on pull request #229: [LIVY-644][TEST] Flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report
URL: https://github.com/apache/incubator-livy/pull/229
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Sep/19 08:05;githubbot;600","yiheng commented on pull request #229: [LIVY-644][TEST] Flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report
URL: https://github.com/apache/incubator-livy/pull/229
 
 
   ## What changes were proposed in this pull request?
   This patch fixes the flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report.
   
   When JVM shutdown no gracefully in a test, the code coverage data file generated by jacoco may be corrupt. Jacoco will throw an exception when generate code coverage report.
   
   In Livy integration test, two test cases shut down no gracefully(one of them uses System.exit). We can find random failure when jacoco process code coverage data file generated by that test case.
   
   In this patch, we turn off the code coverage analysis on these two test cases.
   
   ## How was this patch tested?
   Compare the jacoco data file generated in the integration test. Before the fix, there're 18 files, and after the fix there're 16 files, which means the fix works.
   
   Run 10 builds on Travis each before and after the fix:
   1. Before the fix: 3 builds failed due to the jacoco code coverage exception
   2. After the fix: No build failed
   
   Existing UTs and ITs.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Sep/19 08:05;githubbot;600","jerryshao commented on pull request #229: [LIVY-644][TEST] Flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report
URL: https://github.com/apache/incubator-livy/pull/229
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Sep/19 02:42;githubbot;600","yiheng commented on pull request #229: [LIVY-644][TEST] Flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report
URL: https://github.com/apache/incubator-livy/pull/229
 
 
   ## What changes were proposed in this pull request?
   This patch fixes the flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report.
   
   When JVM shutdown no gracefully in a test, the code coverage data file generated by jacoco may be corrupt. Jacoco will throw an exception when generate code coverage report.
   
   In Livy integration test, two test cases shut down no gracefully(one of them uses System.exit). We can find random failure when jacoco process code coverage data file generated by that test case.
   
   In this patch, we turn off the code coverage analysis on these two test cases.
   
   ## How was this patch tested?
   Compare the jacoco data file generated in the integration test. Before the fix, there're 18 files, and after the fix there're 16 files, which means the fix works.
   
   Run 10 builds on Travis each before and after the fix:
   1. Before the fix: 3 builds failed due to the jacoco code coverage exception
   2. After the fix: No build failed
   
   Existing UTs and ITs.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Sep/19 02:42;githubbot;600","jerryshao commented on pull request #229: [LIVY-644][TEST] Flaky test: Failed to execute goal org.jacoco:jacoco-maven-plugin:0.8.2:report-aggregate (jacoco-report) on project livy-coverage-report
URL: https://github.com/apache/incubator-livy/pull/229
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Sep/19 02:35;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4800,,,0,4800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 19 02:38:30 UTC 2019,,,,,,,,,,"0|z05wzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,"19/Sep/19 02:38;jerryshao;Issue resolved by pull request 229
https://github.com/apache/incubator-livy/pull/229;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A rare status happened in yarn cause SparkApp change into error state,LIVY-642,13251781,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,Jassy,Jassy,Jassy,20/Aug/19 08:37,09/Mar/20 18:26,19/Dec/25 04:15,30/Aug/19 03:57,0.6.0,,,0.7.0,,Server,,,,,,,,,,0,,,,,,"Some batch session execute successfully but return error state. in livy logï¼Œwe find:
{quote}{{2019-08-08 20:11:37,678 ERROR [Logging.scala:56] - Unknown YARN state RUNNING for app application_1559632632227_39801506 with final status SUCCEEDED.}}
{quote}
and this situation with yarn state RUNNING and final status SUCCEEDED is a *correct* state in yarn, so this should not mapped to SparkApp.State.FAILED",,"Jassy1994 commented on pull request #204: [LIVY-642] Fix a rare status happened in yarn cause SparkApp change into error sâ€¦
URL: https://github.com/apache/incubator-livy/pull/204
 
 
   â€¦tate
   
   ## What changes were proposed in this pull request?
   
   to support this yarn state with YarnApplicationState.RUNNING and FinalApplicationStatus.SUCCEEDED instead return SparkApp.State.FAILED
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Aug/19 09:17;githubbot;600","jerryshao commented on pull request #204: [LIVY-642] Fix a rare status happened in yarn cause SparkApp change into error sâ€¦
URL: https://github.com/apache/incubator-livy/pull/204
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 02:22;githubbot;600","Jassy1994 commented on pull request #204: [LIVY-642] Fix a rare status happened in yarn cause SparkApp change into error sâ€¦
URL: https://github.com/apache/incubator-livy/pull/204
 
 
   â€¦tate
   
   ## What changes were proposed in this pull request?
   
   to support this yarn state with YarnApplicationState.RUNNING and FinalApplicationStatus.SUCCEEDED instead return SparkApp.State.FAILED
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 02:22;githubbot;600","jerryshao commented on pull request #204: [LIVY-642] Fix a rare status happened in yarn cause SparkApp change into error state
URL: https://github.com/apache/incubator-livy/pull/204
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 03:53;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,LIVY-576,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 30 03:57:27 UTC 2019,,,,,,,,,,"0|z05tsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Aug/19 09:21;Jassy;Add this state as a correct state in method mapYarnState can solve this problem

PRï¼š[https://github.com/apache/incubator-livy/pull/204];;;","27/Aug/19 04:46;runzhiwang;hi [~Jassy]Â , could you explain how to reproduce this problem?Â ;;;","27/Aug/19 08:21;Jassy;Hi [~runzhiwang]Â Sorry, this case happened in some of our batchsession. and it caused by yarn AM has not sync state and still RUNNING, butÂ final status has changed to SUCCEEDED. so i cannot reproduce this state. this state is illegal in livy and mapped toÂ SparkApp.State.FAILED;;;","30/Aug/19 03:57;jerryshao;Issue resolved by pull request 204
https://github.com/apache/incubator-livy/pull/204;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Travis failed on ""should end with status dead when batch session exits with no 0 return code""",LIVY-641,13251486,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,runzhiwang,runzhiwang,19/Aug/19 00:50,27/Aug/19 03:58,19/Dec/25 04:15,27/Aug/19 03:57,0.6.0,,,0.7.0,,Tests,,,,,,,,,,0,,,,,,"BatchSessionSpec:
 2463A Batch process
 2464- should create a process (2 seconds, 103 milliseconds)
 2465- should update appId and appInfo (30 milliseconds)
 {color:#FF0000}2466- should end with status dead when batch session exits with no 0 return code *** FAILED *** (121 milliseconds){color}
{color:#FF0000} 2467 false was not true (BatchSessionSpec.scala:138){color}
 2468- should recover session (name = None) (17 milliseconds)
 2469- should recover session (name = Some(Test Batch Session)) (4 milliseconds)
 2470- should recover session (name = null) (15 milliseconds)


 please reference to [https://travis-ci.org/apache/incubator-livy/builds/572562376?utm_source=github_status&utm_medium=notification]",,"runzhiwang commented on pull request #214: [LIVY-641] Fix travis failed on test: should end with status dead when batch session exits with no 0 return code
URL: https://github.com/apache/incubator-livy/pull/214
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on test: should end with status dead when batch session exits with no 0 return code
   
   Travis failed because of thread in SparkProcApp.scala and thead in BatchSession.scala change SessionState to different value when stopSession in test. 
   
   ## How was this patch tested?
   
   1. Existed UT and IT. 
   2. Create Batch Session In Yarn and kill SparkSubmit, then check the SessionState.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/19 04:57;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 27 03:58:00 UTC 2019,,,,,,,,,,"0|z05s3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Aug/19 09:50;runzhiwang;I'm working on it.;;;","27/Aug/19 03:58;jerryshao;Issue resolved by pull request 214
[https://github.com/apache/incubator-livy/pull/214|https://github.com/apache/incubator-livy/pull/214];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests for ThriftServer,LIVY-640,13251147,,Improvement,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,micahzhao,micahzhao,micahzhao,16/Aug/19 02:51,09/Sep/19 06:37,19/Dec/25 04:15,05/Sep/19 12:27,,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,"Compared to spark's ThriftServer, livy lacks some tests. We need to add tests to cover usage scenarios.",,"captainzmc commented on pull request #209: [LIVY-640] Add tests for ThriftServer
URL: https://github.com/apache/incubator-livy/pull/209
 
 
   ## What changes were proposed in this pull request?
   
   1ã€Added some tests that ThriftServer is currently missing.
   2ã€Set ENABLE_HIVE_CONTEXT=true in ThriftServerBaseTest to support the creation of Hive tables and the creation of udf in test.
   3ã€Upgrade spark2.2.0 to 2.2.3. To resolve the issue that session cannot be created during Travis test when ENABLE_HIVE_CONTEXT is set to true
   4ã€Travis ITs is divided into two parts to avoid test timeout problems.
   
   ## How was this patch tested?
   
   Test with Travis and see the results
   ![image](https://user-images.githubusercontent.com/13825159/63577482-10a32c80-c5c1-11e9-868a-0aab4b1af743.png)
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 08:14;githubbot;600","captainzmc commented on pull request #209: [LIVY-640] Add tests for ThriftServer
URL: https://github.com/apache/incubator-livy/pull/209
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 09:23;githubbot;600","captainzmc commented on pull request #209: [LIVY-640] Add tests for ThriftServer
URL: https://github.com/apache/incubator-livy/pull/209
 
 
   ## What changes were proposed in this pull request?
   
   1ã€Added some tests that ThriftServer is currently missing.
   2ã€Set ENABLE_HIVE_CONTEXT=true in ThriftServerBaseTest to support the creation of Hive tables and the creation of udf in test.
   3ã€Upgrade spark2.2.0 to 2.2.3. To resolve the issue that session cannot be created during Travis test when ENABLE_HIVE_CONTEXT is set to true
   4ã€Travis ITs is divided into two parts to avoid test timeout problems.
   
   ## How was this patch tested?
   
   Test with Travis and see the results
   ![image](https://user-images.githubusercontent.com/13825159/63577482-10a32c80-c5c1-11e9-868a-0aab4b1af743.png)
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 09:24;githubbot;600","captainzmc commented on pull request #209: [LIVY-640] Add tests for ThriftServer
URL: https://github.com/apache/incubator-livy/pull/209
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/19 02:59;githubbot;600","captainzmc commented on pull request #209: [LIVY-640] Add tests for ThriftServer
URL: https://github.com/apache/incubator-livy/pull/209
 
 
   ## What changes were proposed in this pull request?
   
   1ã€Added some tests in BinaryThriftServerSuite that ThriftServer is currently missingï¼ˆMoved from spark ThriftServer testsï¼‰.
   2ã€Set ENABLE_HIVE_CONTEXT=true in BinaryThriftServerSuite to support the creation of Hive tables and the creation of udf in test.
   3ã€Upgrade spark2.2.0 to 2.2.3. To resolve the issue that session cannot be created during Travis test when ENABLE_HIVE_CONTEXT is set to true
   4ã€Travis ITs is divided into two parts to avoid test timeout problems.
   
   ## How was this patch tested?
   
   Test with Travis and see the results
   ![image](https://user-images.githubusercontent.com/13825159/63577482-10a32c80-c5c1-11e9-868a-0aab4b1af743.png)
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Aug/19 06:19;githubbot;600","captainzmc commented on pull request #209: [LIVY-640] Add tests for ThriftServer
URL: https://github.com/apache/incubator-livy/pull/209
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Aug/19 06:47;githubbot;600","captainzmc commented on pull request #209: [LIVY-640] Add tests for ThriftServer
URL: https://github.com/apache/incubator-livy/pull/209
 
 
   ## What changes were proposed in this pull request?
   
   1ã€Added some tests in BinaryThriftServerSuite that ThriftServer is currently missingï¼ˆThe tests was Moved from spark ThriftServerï¼‰.
   2ã€Set ENABLE_HIVE_CONTEXT=true in BinaryThriftServerSuite to support the creation of Hive tables and the creation of udf in test.
   3ã€Upgrade spark2.2.0 to 2.2.3. To resolve the issue that session cannot be created during Travis test when ENABLE_HIVE_CONTEXT is set to true
   4ã€Travis ITs is divided into two parts to avoid test timeout problems.
   
   ## How was this patch tested?
   
   Test with Travis and see the results
   ![image](https://user-images.githubusercontent.com/13825159/63577482-10a32c80-c5c1-11e9-868a-0aab4b1af743.png)
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Aug/19 06:48;githubbot;600","captainzmc commented on pull request #209: [LIVY-640] Add tests for ThriftServer
URL: https://github.com/apache/incubator-livy/pull/209
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 11:17;githubbot;600","captainzmc commented on pull request #209: [LIVY-640] Add tests for ThriftServer
URL: https://github.com/apache/incubator-livy/pull/209
 
 
   ## What changes were proposed in this pull request?
   
   1ã€Added some tests in BinaryThriftServerSuite that ThriftServer is currently missingï¼ˆThe tests was Moved from spark ThriftServerï¼‰.
   2ã€Set ENABLE_HIVE_CONTEXT=true in BinaryThriftServerSuite to support the creation of Hive tables and the creation of udf in test.
   3ã€Upgrade spark2.2.0 to 2.2.3. To resolve the issue that session cannot be created during Travis test when ENABLE_HIVE_CONTEXT is set to true
   4ã€Travis ITs is divided into two parts to avoid test timeout problems.
   
   ## How was this patch tested?
   
   Test with Travis and see the results
   ![image](https://user-images.githubusercontent.com/13825159/63577482-10a32c80-c5c1-11e9-868a-0aab4b1af743.png)
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 11:17;githubbot;600","mgaido91 commented on pull request #209: [LIVY-640] Add tests for ThriftServer
URL: https://github.com/apache/incubator-livy/pull/209
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 12:26;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,6000,,,0,6000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 05 12:27:34 UTC 2019,,,,,,,,,,"0|z05q08:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/Aug/19 02:52;micahzhao;I would like to add this.;;;","05/Sep/19 12:27;mgaido;Issue resolved by https://github.com/apache/incubator-livy/pull/209.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Is it possible to add start time and completion time and duration to the statements web ui interface?,LIVY-639,13250962,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,openopen2,openopen2,openopen2,15/Aug/19 07:37,27/Aug/19 03:35,19/Dec/25 04:15,27/Aug/19 03:31,0.6.0,,,0.7.0,,REPL,RSC,Server,,,,,,,,0,,,,,,"Many times I need to know the execution time and start time of the statement, but now the livy web ui does not support this. I have improved livy and have written the code, I hope to get community support and submit PR.

Â ",,"OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 14:17;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   ## What changes were proposed in this pull request?
   
   add start time and completion time and duration to the statements web ui
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-639?filter=-2
   
   ## How was this patch tested?
   
   Existing test set and our test environment
   
   ![DeepinScreenshot_select-area_20190816153300](https://user-images.githubusercontent.com/23738217/63151060-2487e500-c03b-11e9-92d1-f3eb497c322c.png)
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 14:17;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 09:49;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   ## What changes were proposed in this pull request?
   
   add start time and completion time and duration to the statements web ui
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-639?filter=-2
   
   ## How was this patch tested?
   
   Existing test set and our test environment
   
   ![DeepinScreenshot_select-area_20190816153300](https://user-images.githubusercontent.com/23738217/63151060-2487e500-c03b-11e9-92d1-f3eb497c322c.png)
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 09:49;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 10:31;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   ## What changes were proposed in this pull request?
   
   add start time and completion time and duration to the statements web ui
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-639?filter=-2
   
   ## How was this patch tested?
   
   Existing test set and our test environment
   
   ![DeepinScreenshot_select-area_20190816153300](https://user-images.githubusercontent.com/23738217/63151060-2487e500-c03b-11e9-92d1-f3eb497c322c.png)
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/19 10:31;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Aug/19 01:14;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   ## What changes were proposed in this pull request?
   
   add start time and completion time and duration to the statements web ui
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-639?filter=-2
   
   ## How was this patch tested?
   
   Existing test set and our test environment
   
   ![DeepinScreenshot_select-area_20190816153300](https://user-images.githubusercontent.com/23738217/63151060-2487e500-c03b-11e9-92d1-f3eb497c322c.png)
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Aug/19 01:14;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Aug/19 03:35;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   ## What changes were proposed in this pull request?
   
   add start time and completion time and duration to the statements web ui
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-639?filter=-2
   
   ## How was this patch tested?
   
   Existing test set and our test environment
   
   ![DeepinScreenshot_select-area_20190816153300](https://user-images.githubusercontent.com/23738217/63151060-2487e500-c03b-11e9-92d1-f3eb497c322c.png)
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Aug/19 03:35;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Aug/19 05:24;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   ## What changes were proposed in this pull request?
   
   add start time and completion time and duration to the statements web ui
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-639?filter=-2
   
   ## How was this patch tested?
   
   Existing test set and our test environment
   
   ![DeepinScreenshot_select-area_20190816153300](https://user-images.githubusercontent.com/23738217/63151060-2487e500-c03b-11e9-92d1-f3eb497c322c.png)
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Aug/19 05:24;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Aug/19 09:51;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   ## What changes were proposed in this pull request?
   
   add start time and completion time and duration to the statements web ui
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-639?filter=-2
   
   ## How was this patch tested?
   
   Existing test set and our test environment
   
   ![DeepinScreenshot_select-area_20190816153300](https://user-images.githubusercontent.com/23738217/63151060-2487e500-c03b-11e9-92d1-f3eb497c322c.png)
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Aug/19 09:52;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/19 08:06;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   ## What changes were proposed in this pull request?
   
   add start time and completion time and duration to the statements web ui
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-639?filter=-2
   
   ## How was this patch tested?
   
   Existing test set and our test environment
   
   ![DeepinScreenshot_select-area_20190816153300](https://user-images.githubusercontent.com/23738217/63151060-2487e500-c03b-11e9-92d1-f3eb497c322c.png)
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/19 08:06;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/19 08:51;githubbot;600","OpenOpened commented on pull request #201: [LIVY-639] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   ## What changes were proposed in this pull request?
   
   add start time and completion time and duration to the statements web ui
   
   JIRA: https://issues.apache.org/jira/browse/LIVY-639?filter=-2
   
   ## How was this patch tested?
   
   Existing test set and our test environment
   
   ![DeepinScreenshot_select-area_20190816153300](https://user-images.githubusercontent.com/23738217/63151060-2487e500-c03b-11e9-92d1-f3eb497c322c.png)
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/19 08:51;githubbot;600","jerryshao commented on pull request #201: [LIVY-639][REPL] add start time and completion time and duration to the statements web ui
URL: https://github.com/apache/incubator-livy/pull/201
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Aug/19 03:29;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,11400,,,0,11400,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/19 07:34;openopen2;DeepinScreenshot_select-area_20190815153353.png;https://issues.apache.org/jira/secure/attachment/12977678/DeepinScreenshot_select-area_20190815153353.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,,Tue Aug 27 03:35:53 UTC 2019,,,,,,,,,,"0|z05ov4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Aug/19 03:35;jerryshao;Issue resolved by pull request 201
https://github.com/apache/incubator-livy/pull/201;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get sql.AnalysisException when create table using thriftserver,LIVY-638,13250550,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Done,,micahzhao,micahzhao,13/Aug/19 13:20,16/Aug/19 02:38,19/Dec/25 04:15,16/Aug/19 02:38,0.6.0,,,,,Thriftserver,,,,,,,,,,0,,,,,,"org.apache.spark.sql.AnalysisExceptionoccurs when I use thriftserver to execute the following SQL.Â When I do not use hive as metastore, thriftserver does not support create table ?

0: jdbc:hive2://localhost:10090> CREATE TABLE test(key INT, val STRING);
 Error: java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.spark.sql.AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);;
 'CreateTable `test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists

org.apache.spark.sql.execution.datasources.HiveOnlyCheck$$anonfun$apply$12.apply(rules.scala:392)
 org.apache.spark.sql.execution.datasources.HiveOnlyCheck$$anonfun$apply$12.apply(rules.scala:390)
 org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:117)
 org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:390)
 org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:388)
 org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$2.apply(CheckAnalysis.scala:386)
 org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$2.apply(CheckAnalysis.scala:386)
 scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
 scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
 org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:386)
 org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
 org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
 org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
 org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
 org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
 org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
 org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
 org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
 org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)
 org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
 org.apache.livy.thriftserver.session.SqlJob.executeSql(SqlJob.java:74)
 org.apache.livy.thriftserver.session.SqlJob.call(SqlJob.java:64)
 org.apache.livy.thriftserver.session.SqlJob.call(SqlJob.java:35)
 org.apache.livy.rsc.driver.JobWrapper.call(JobWrapper.java:64)
 org.apache.livy.rsc.driver.JobWrapper.call(JobWrapper.java:31)
 java.util.concurrent.FutureTask.run(FutureTask.java:266)
 java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 java.lang.Thread.run(Thread.java:748) (state=,code=0)
 0: jdbc:hive2://localhost:10090>

Â ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/19 13:21;micahzhao;create table.png;https://issues.apache.org/jira/secure/attachment/12977490/create+table.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 16 02:37:04 UTC 2019,,,,,,,,,,"0|z05mbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Aug/19 08:44;yihengw;It's weird that even I enable hive support in spark configuration, livy thrift server will still throw this exception...Â ;;;","16/Aug/19 02:37;micahzhao;In livy.conf, there is a configuration item of livy.repl.enable-hive-context, with the default false. In InteractiveSession.scala, it is defined as follows:
{code:java}
val confVal = if (enableHiveContext) ""hive"" else ""in-memory"" builderProperties.put(""spark.sql.catalogImplementation"", confVal)
{code}
So just set livy.repl.enable-hive-context to true
to enable the spark.sql.catalogImplementation = hive;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get NullPointerException when create database using thriftserver,LIVY-637,13250544,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,micahzhao,micahzhao,micahzhao,13/Aug/19 13:04,09/Sep/19 06:37,19/Dec/25 04:15,22/Aug/19 06:07,0.6.0,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,"When I connected thriftserver with spark beeline. NullPointerException occurs whenÂ  execute the following SQL. This exception does not affect the final execution result.

create database test;

use test;

drop database test;

0: jdbc:hive2://localhost:10090> create database test;
 java.lang.NullPointerException
 at org.apache.hive.service.cli.ColumnBasedSet.<init>(ColumnBasedSet.java:50)
 at org.apache.hive.service.cli.RowSetFactory.create(RowSetFactory.java:37)
 at org.apache.hive.jdbc.HiveQueryResultSet.next(HiveQueryResultSet.java:368)
 at org.apache.hive.beeline.BufferedRows.<init>(BufferedRows.java:42)
 at org.apache.hive.beeline.BeeLine.print(BeeLine.java:1794)
 at org.apache.hive.beeline.Commands.execute(Commands.java:860)
 at org.apache.hive.beeline.Commands.sql(Commands.java:713)
 at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
 at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
 at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
 at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
 at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)
 Error: Error retrieving next row (state=,code=0)

Â 0: jdbc:hive2://localhost:10090> use test;
 java.lang.NullPointerException
 at org.apache.hive.service.cli.ColumnBasedSet.<init>(ColumnBasedSet.java:50)
 at org.apache.hive.service.cli.RowSetFactory.create(RowSetFactory.java:37)
 at org.apache.hive.jdbc.HiveQueryResultSet.next(HiveQueryResultSet.java:368)
 at org.apache.hive.beeline.BufferedRows.<init>(BufferedRows.java:42)
 at org.apache.hive.beeline.BeeLine.print(BeeLine.java:1794)
 at org.apache.hive.beeline.Commands.execute(Commands.java:860)
 at org.apache.hive.beeline.Commands.sql(Commands.java:713)
 at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
 at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
 at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
 at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
 at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)
 Error: Error retrieving next row (state=,code=0)

0: jdbc:hive2://localhost:10090> drop database test;
 java.lang.NullPointerException
 at org.apache.hive.service.cli.ColumnBasedSet.<init>(ColumnBasedSet.java:50)
 at org.apache.hive.service.cli.RowSetFactory.create(RowSetFactory.java:37)
 at org.apache.hive.jdbc.HiveQueryResultSet.next(HiveQueryResultSet.java:368)
 at org.apache.hive.beeline.BufferedRows.<init>(BufferedRows.java:42)
 at org.apache.hive.beeline.BeeLine.print(BeeLine.java:1794)
 at org.apache.hive.beeline.Commands.execute(Commands.java:860)
 at org.apache.hive.beeline.Commands.sql(Commands.java:713)
 at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
 at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
 at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
 at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
 at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)
 Error: Error retrieving next row (state=,code=0)

Â ",,"captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
   ## What changes were proposed in this pull request?
   
   Spark beeline use old hive-jdbc-client doesnâ€™t do null point ref check. So  when new TRowSet, setColumes make sure column set not null.
   
   ## How was this patch tested?
   
   Connect to livy's thriftserver using spark beeline. And create/use/drop database will no longer have nullpointerexceptions after execution.
   
   ![image](https://user-images.githubusercontent.com/13825159/63147414-b0484400-c030-11e9-8d14-b22238306194.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 06:19;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 08:46;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
   ## What changes were proposed in this pull request?
   
   Spark beeline use old hive-jdbc-client doesnâ€™t do null point ref check. So  when new TRowSet, setColumes make sure column set not null.
   
   ## How was this patch tested?
   
   Connect to livy's thriftserver using spark beeline. And create/use/drop database will no longer have nullpointerexceptions after execution.
   
   ![image](https://user-images.githubusercontent.com/13825159/63147414-b0484400-c030-11e9-8d14-b22238306194.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 08:46;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 09:37;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
   ## What changes were proposed in this pull request?
   
   Spark beeline use old hive-jdbc-client doesnâ€™t do null point ref check. So  when new TRowSet, setColumes make sure column set not null.
   
   ## How was this patch tested?
   
   Connect to livy's thriftserver using spark beeline. And create/use/drop database will no longer have nullpointerexceptions after execution.
   
   ![image](https://user-images.githubusercontent.com/13825159/63147414-b0484400-c030-11e9-8d14-b22238306194.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 09:37;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 02:47;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
   ## What changes were proposed in this pull request?
   
   Spark beeline use old hive-jdbc-client doesnâ€™t do null point ref check. So  when new TRowSet, setColumes make sure column set not null.
   
   ## How was this patch tested?
   
   Connect to livy's thriftserver using spark beeline. And create/use/drop database will no longer have nullpointerexceptions after execution.
   
   ![image](https://user-images.githubusercontent.com/13825159/63147414-b0484400-c030-11e9-8d14-b22238306194.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 02:47;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 03:19;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
   ## What changes were proposed in this pull request?
   
   Spark beeline use old hive-jdbc-client doesnâ€™t do null point ref check. So  when new TRowSet, setColumes make sure column set not null.
   
   ## How was this patch tested?
   
   Connect to livy's thriftserver using spark beeline. And create/use/drop database will no longer have nullpointerexceptions after execution.
   
   ![image](https://user-images.githubusercontent.com/13825159/63147414-b0484400-c030-11e9-8d14-b22238306194.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 03:24;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 03:32;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
   ## What changes were proposed in this pull request?
   
   Spark beeline use old hive-jdbc-client doesnâ€™t do null point ref check. So  when new TRowSet, setColumes make sure column set not null.
   
   ## How was this patch tested?
   
   Connect to livy's thriftserver using spark beeline. And create/use/drop database will no longer have nullpointerexceptions after execution.
   
   ![image](https://user-images.githubusercontent.com/13825159/63147414-b0484400-c030-11e9-8d14-b22238306194.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 03:32;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 04:54;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
   ## What changes were proposed in this pull request?
   
   Spark beeline use old hive-jdbc-client doesnâ€™t do null point ref check. So  when new TRowSet, setColumes make sure column set not null.
   
   ## How was this patch tested?
   
   Connect to livy's thriftserver using spark beeline. And create/use/drop database will no longer have nullpointerexceptions after execution.
   
   ![image](https://user-images.githubusercontent.com/13825159/63147414-b0484400-c030-11e9-8d14-b22238306194.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 04:54;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Aug/19 05:02;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
   ## What changes were proposed in this pull request?
   
   Spark beeline use old hive-jdbc-client doesnâ€™t do null point ref check. So  when new TRowSet, setColumes make sure column set not null.
   
   ## How was this patch tested?
   
   Connect to livy's thriftserver using spark beeline. And create/use/drop database will no longer have nullpointerexceptions after execution.
   
   ![image](https://user-images.githubusercontent.com/13825159/63147414-b0484400-c030-11e9-8d14-b22238306194.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Aug/19 05:02;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Aug/19 07:08;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
   ## What changes were proposed in this pull request?
   
   Spark beeline use old hive-jdbc-client doesnâ€™t do null point ref check. So  when new TRowSet, setColumes make sure column set not null.
   
   ## How was this patch tested?
   
   Connect to livy's thriftserver using spark beeline. And create/use/drop database will no longer have nullpointerexceptions after execution.
   
   ![image](https://user-images.githubusercontent.com/13825159/63147414-b0484400-c030-11e9-8d14-b22238306194.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Aug/19 07:08;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Aug/19 07:35;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
   ## What changes were proposed in this pull request?
   
   Spark beeline use old hive-jdbc-client doesnâ€™t do null point ref check. So  when new TRowSet, setColumes make sure column set not null.
   
   ## How was this patch tested?
   
   Connect to livy's thriftserver using spark beeline. And create/use/drop database will no longer have nullpointerexceptions after execution.
   
   ![image](https://user-images.githubusercontent.com/13825159/63147414-b0484400-c030-11e9-8d14-b22238306194.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Aug/19 07:35;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Aug/19 10:46;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
   ## What changes were proposed in this pull request?
   
   Spark beeline use old hive-jdbc-client doesnâ€™t do null point ref check. So  when new TRowSet, setColumes make sure column set not null.
   
   ## How was this patch tested?
   
   Connect to livy's thriftserver using spark beeline. And create/use/drop database will no longer have nullpointerexceptions after execution.
   
   ![image](https://user-images.githubusercontent.com/13825159/63147414-b0484400-c030-11e9-8d14-b22238306194.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Aug/19 10:46;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Aug/19 12:46;githubbot;600","captainzmc commented on pull request #200: [LIVY-637]Fix NullPointerException when create database using thriftsâ€¦
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
   ## What changes were proposed in this pull request?
   
   Spark beeline use old hive-jdbc-client doesnâ€™t do null point ref check. So  when new TRowSet, setColumes make sure column set not null.
   
   ## How was this patch tested?
   
   Connect to livy's thriftserver using spark beeline. And create/use/drop database will no longer have nullpointerexceptions after execution.
   
   ![image](https://user-images.githubusercontent.com/13825159/63147414-b0484400-c030-11e9-8d14-b22238306194.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Aug/19 12:47;githubbot;600","jerryshao commented on pull request #200: [LIVY-637][ThriftServer]Fix NullPointerException when create database using thriftserver
URL: https://github.com/apache/incubator-livy/pull/200
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 06:03;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,14400,,,0,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/19 13:06;micahzhao;create.png;https://issues.apache.org/jira/secure/attachment/12977488/create.png","13/Aug/19 13:03;micahzhao;drop.png;https://issues.apache.org/jira/secure/attachment/12977483/drop.png","13/Aug/19 13:03;micahzhao;use.png;https://issues.apache.org/jira/secure/attachment/12977484/use.png",,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 22 06:07:31 UTC 2019,,,,,,,,,,"0|z05mag:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Aug/19 13:29;micahzhao;Â This exception does not affect the final result. I would like to fix this issues.;;;","14/Aug/19 03:46;micahzhao;SameÂ exception whenÂ CREATE andÂ DROPÂ TEMPORARY FUNCTION;;;","14/Aug/19 08:24;yihengw;Spark beeline uses an old hive-jdbc-client version, in which it will not check if the column is null in the RowSet returned by the thrift server.

Â 

See here:

[https://github.com/apache/hive/blob/release-1.2.1/service/src/java/org/apache/hive/service/cli/ColumnBasedSet.java#L50]

Â 

In the latest hive code, this has been fixed

[https://github.com/apache/hive/blob/master/service/src/java/org/apache/hive/service/cli/ColumnBasedSet.java#L82];;;","22/Aug/19 06:07;jerryshao;Issue resolved by pull request 200
https://github.com/apache/incubator-livy/pull/200;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Travis failed to build: Could not resolve dependencies,LIVY-635,13250477,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,runzhiwang,runzhiwang,13/Aug/19 09:00,27/Aug/19 07:39,19/Dec/25 04:15,15/Aug/19 07:14,0.6.0,,,0.7.0,,Build,Tests,Thriftserver,,,,,,,,0,,,,,,"[ERROR] Failed to execute goal on project livy-thriftserver: Could not resolve dependencies for project org.apache.livy:livy-thriftserver:jar:0.7.0-incubating-SNAPSHOT: Failed to collect dependencies at org.apache.hive:hive-jdbc:jar:3.0.0 -> org.apache.hive:hive-service:jar:3.0.0 -> org.apache.hive:hive-llap-server:jar:3.0.0 -> org.apache.hbase:hbase-server:jar:2.0.0-alpha4 -> org.glassfish.web:javax.servlet.jsp:jar:2.3.2 -> org.glassfish:javax.el:jar:3.0.1-b08-SNAPSHOT: Failed to read artifact descriptor for org.glassfish:javax.el:jar:3.0.1-b08-SNAPSHOT: Could not transfer artifact org.glassfish:javax.el:pom:3.0.1-b08-SNAPSHOT from/to apache-snapshots (https://repository.apache.org/snapshots/): Connect to repository.apache.org:443 [repository.apache.org/207.244.88.140] failed: Connection timed out (Connection timed out) -> [Help 1]
2258org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal on project livy-thriftserver: Could not resolve dependencies for project org.apache.livy:livy-thriftserver:jar:0.7.0-incubating-SNAPSHOT: Failed to collect dependencies at org.apache.hive:hive-jdbc:jar:3.0.0 -> org.apache.hive:hive-service:jar:3.0.0 -> org.apache.hive:hive-llap-server:jar:3.0.0 -> org.apache.hbase:hbase-server:jar:2.0.0-alpha4 -> org.glassfish.web:javax.servlet.jsp:jar:2.3.2 -> org.glassfish:javax.el:jar:3.0.1-b08-SNAPSHOT
2259 at org.apache.maven.lifecycle.internal.LifecycleDependencyResolver.getDependencies (LifecycleDependencyResolver.java:249)
2260 at org.apache.maven.lifecycle.internal.LifecycleDependencyResolver.resolveProjectDependencies (LifecycleDependencyResolver.java:145)
2261 at org.apache.maven.lifecycle.internal.MojoExecutor.ensureDependenciesAreResolved (MojoExecutor.java:246)
2262 at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:200)
2263 at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:154)
2264 at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:146)
2265 at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
2266 at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
2267 at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:51)
2268 at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
2269 at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:309)
2270 at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:194)
2271 at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:107)
2272 at org.apache.maven.cli.MavenCli.execute (MavenCli.java:955)
2273 at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:290)
2274 at org.apache.maven.cli.MavenCli.main (MavenCli.java:194)
2275 at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
2276 at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
2277 at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
2278 at java.lang.reflect.Method.invoke (Method.java:498)
2279 at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:289)
2280 at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:229)
2281 at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:415)
2282 at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:356)",,"runzhiwang commented on pull request #198: [LIVY-635] Fix travis fail to build by excluding the dependency of hbase
URL: https://github.com/apache/incubator-livy/pull/198
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis fail to build by excluding the dependency of hbase.
   
   ## How was this patch tested?
   
   
   existed UT and IT.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 06:12;githubbot;600","runzhiwang commented on pull request #198: [LIVY-635] Fix travis fail to build by excluding the dependency of hbase
URL: https://github.com/apache/incubator-livy/pull/198
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 07:28;githubbot;600","runzhiwang commented on pull request #198: [LIVY-635] Fix travis fail to build by excluding the dependency of hbase
URL: https://github.com/apache/incubator-livy/pull/198
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis fail to build by excluding the dependency of hbase.
   
   ## How was this patch tested?
   
   
   existed UT and IT.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 07:28;githubbot;600","jerryshao commented on pull request #198: [LIVY-635][BUILD] Fix travis fail to build by excluding the dependency of hbase
URL: https://github.com/apache/incubator-livy/pull/198
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 07:14;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 15 07:14:32 UTC 2019,,,,,,,,,,"0|z05lvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,"15/Aug/19 07:14;jerryshao;Issue resolved by pull request 198
[https://github.com/apache/incubator-livy/pull/198];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy ThriftServer should be able to reuse hive PasswdAuthenticationProvider,LIVY-634,13249837,,Improvement,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,,xilangyan,xilangyan,09/Aug/19 07:24,17/Oct/19 01:04,19/Dec/25 04:15,17/Oct/19 01:04,0.6.0,,,,,Thriftserver,,,,,,,,,,0,,,,,,"Currently Livy share sameÂ PasswdAuthenticationProvider interface with HiveServer2, but not able to reuseÂ PasswdAuthenticationProvider implementation likeÂ LdapAuthenticationProviderImpl andÂ PamAuthenticationProviderImpl.

Â 

We can extend livy CustomAuthenticationProvider to support hiveÂ PasswdAuthenticationProviderÂ  implementation. For example, to useÂ LdapAuthenticationProviderImplÂ  we can add such configuration in livy.conf

Â 

livy.server.thrift.authentication = CUSTOM
livy.server.thrift.custom.authentication.class = org.apache.hive.service.auth.LdapAuthenticationProviderImpl
livy.server.thrift.authentication.custom.hive.server2.authentication.ldap.url =Â xxxx
livy.server.thrift.authentication.custom.hive.server2.authentication.ldap.Domain =Â xxxx

Â ",,"yantzu commented on pull request #195: [LIVY-634] To support reuse hive PasswdAuthenticationProvider implementation in ThriftServer
URL: https://github.com/apache/incubator-livy/pull/195
 
 
   ## What changes were proposed in this pull request?
   
   Currently Livy share same PasswdAuthenticationProvider interface with HiveServer2, but not able to reuse PasswdAuthenticationProvider implementation like LdapAuthenticationProviderImpl and PamAuthenticationProviderImpl. 
   
   We can extend livy CustomAuthenticationProvider to support hive PasswdAuthenticationProvider  implementation. For example, to use LdapAuthenticationProviderImpl  we can add such configuration in livy.conf:
   livy.server.thrift.authentication = CUSTOM
   livy.server.thrift.custom.authentication.class = org.apache.hive.service.auth.LdapAuthenticationProviderImpl
   livy.server.thrift.authentication.custom.hive.server2.authentication.ldap.url = xxxx
   livy.server.thrift.authentication.custom.hive.server2.authentication.ldap.Domain = xxxx
   
   https://issues.apache.org/jira/browse/LIVY-634
   
   ## How was this patch tested?
   
   Tested manually.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 07:29;githubbot;600","yantzu commented on pull request #195: [LIVY-634] To support reuse hive PasswdAuthenticationProvider implementation in ThriftServer
URL: https://github.com/apache/incubator-livy/pull/195
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 01:45;githubbot;600","yantzu commented on pull request #195: [LIVY-634] To support reuse hive PasswdAuthenticationProvider implementation in ThriftServer
URL: https://github.com/apache/incubator-livy/pull/195
 
 
   ## What changes were proposed in this pull request?
   
   Currently Livy share same PasswdAuthenticationProvider interface with HiveServer2, but not able to reuse PasswdAuthenticationProvider implementation like LdapAuthenticationProviderImpl and PamAuthenticationProviderImpl. 
   
   We can extend livy CustomAuthenticationProvider to support hive PasswdAuthenticationProvider  implementation. For example, to use LdapAuthenticationProviderImpl  we can add such configuration in livy.conf:
   livy.server.thrift.authentication = CUSTOM
   livy.server.thrift.custom.authentication.class = org.apache.hive.service.auth.LdapAuthenticationProviderImpl
   livy.server.thrift.authentication.custom.hive.server2.authentication.ldap.url = xxxx
   livy.server.thrift.authentication.custom.hive.server2.authentication.ldap.Domain = xxxx
   
   https://issues.apache.org/jira/browse/LIVY-634
   
   ## How was this patch tested?
   
   Tested manually.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 01:46;githubbot;600","yantzu commented on pull request #195: [LIVY-634] To support reuse hive PasswdAuthenticationProvider implementation in ThriftServer
URL: https://github.com/apache/incubator-livy/pull/195
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 04:06;githubbot;600","yantzu commented on pull request #195: [LIVY-634] To support reuse hive PasswdAuthenticationProvider implementation in ThriftServer
URL: https://github.com/apache/incubator-livy/pull/195
 
 
   ## What changes were proposed in this pull request?
   
   Currently Livy share same PasswdAuthenticationProvider interface with HiveServer2, but not able to reuse PasswdAuthenticationProvider implementation like LdapAuthenticationProviderImpl and PamAuthenticationProviderImpl. 
   
   We can extend livy CustomAuthenticationProvider to support hive PasswdAuthenticationProvider  implementation. For example, to use LdapAuthenticationProviderImpl  we can add such configuration in livy.conf:
   livy.server.thrift.authentication = CUSTOM
   livy.server.thrift.custom.authentication.class = org.apache.hive.service.auth.LdapAuthenticationProviderImpl
   livy.server.thrift.authentication.custom.hive.server2.authentication.ldap.url = xxxx
   livy.server.thrift.authentication.custom.hive.server2.authentication.ldap.Domain = xxxx
   
   https://issues.apache.org/jira/browse/LIVY-634
   
   ## How was this patch tested?
   
   Tested manually.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 04:06;githubbot;600","yantzu commented on pull request #195: [LIVY-634] To support reuse hive PasswdAuthenticationProvider implementation in ThriftServer
URL: https://github.com/apache/incubator-livy/pull/195
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 08:05;githubbot;600","yantzu commented on pull request #195: [LIVY-634] To support reuse hive PasswdAuthenticationProvider implementation in ThriftServer
URL: https://github.com/apache/incubator-livy/pull/195
 
 
   ## What changes were proposed in this pull request?
   
   Currently Livy share same PasswdAuthenticationProvider interface with HiveServer2, but not able to reuse PasswdAuthenticationProvider implementation like LdapAuthenticationProviderImpl and PamAuthenticationProviderImpl. 
   
   We can extend livy CustomAuthenticationProvider to support hive PasswdAuthenticationProvider  implementation. For example, to use LdapAuthenticationProviderImpl  we can add such configuration in livy.conf:
   livy.server.thrift.authentication = CUSTOM
   livy.server.thrift.custom.authentication.class = org.apache.hive.service.auth.LdapAuthenticationProviderImpl
   livy.server.thrift.authentication.custom.hive.server2.authentication.ldap.url = xxxx
   livy.server.thrift.authentication.custom.hive.server2.authentication.ldap.Domain = xxxx
   
   https://issues.apache.org/jira/browse/LIVY-634
   
   ## How was this patch tested?
   
   Tested manually.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 08:05;githubbot;600","yantzu commented on pull request #195: [LIVY-634] To support reuse hive PasswdAuthenticationProvider implementation in ThriftServer
URL: https://github.com/apache/incubator-livy/pull/195
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 01:03;githubbot;600","yantzu commented on pull request #195: [LIVY-634] To support reuse hive PasswdAuthenticationProvider implementation in ThriftServer
URL: https://github.com/apache/incubator-livy/pull/195
 
 
   ## What changes were proposed in this pull request?
   
   Currently Livy share same PasswdAuthenticationProvider interface with HiveServer2, but not able to reuse PasswdAuthenticationProvider implementation like LdapAuthenticationProviderImpl and PamAuthenticationProviderImpl. 
   
   We can extend livy CustomAuthenticationProvider to support hive PasswdAuthenticationProvider  implementation. For example, to use LdapAuthenticationProviderImpl  we can add such configuration in livy.conf:
   livy.server.thrift.authentication = CUSTOM
   livy.server.thrift.custom.authentication.class = org.apache.hive.service.auth.LdapAuthenticationProviderImpl
   livy.server.thrift.authentication.custom.hive.server2.authentication.ldap.url = xxxx
   livy.server.thrift.authentication.custom.hive.server2.authentication.ldap.Domain = xxxx
   
   https://issues.apache.org/jira/browse/LIVY-634
   
   ## How was this patch tested?
   
   Tested manually.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 01:03;githubbot;600","yantzu commented on pull request #195: [LIVY-634] To support reuse hive PasswdAuthenticationProvider implementation in ThriftServer
URL: https://github.com/apache/incubator-livy/pull/195
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Oct/19 02:13;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,6000,,,0,6000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2019-08-09 07:24:12.0,,,,,,,,,,"0|z05hxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
session should not be gc-ed for long running queries,LIVY-633,13249817,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,yihengw,lijubjohn,lijubjohn,09/Aug/19 05:54,17/Sep/19 09:23,19/Dec/25 04:15,17/Sep/19 09:23,0.6.0,,,0.7.0,,Server,,,,,,,,,,2,,,,,,"If you have set a relatively small session timeout eg 15 mins and query execution is taking > 15 mins , the session gets gc-ed ,Â which is incorrect wrt user experience as the user was still active on session and waiting for result",,"yiheng commented on pull request #224: [LIVY-633][Server] session should not be gc-ed for long running queries
URL: https://github.com/apache/incubator-livy/pull/224
 
 
   ## What changes were proposed in this pull request?
   Currently, Livy records the last activity time of the session before statement execution. If a statement runs too long, exceeding then the session timeout, the session will be garbage collected after the statement execution.
   
   This should not be the expected behavior. The statement execution time should not be count into idle. We should update the last activity time after the statement execution.
   
   In this patch, we add a replLastActivity field into the rscClient, which will be updated when the repl state changes. So when session changes its state from busy to idle, this field will catch the time and finally reflect on the session last activity.
   
   ## How was this patch tested?
   Manual test. Also, add a new unit test.
   
   Existing unit tests and integration tests.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Sep/19 11:17;githubbot;600","yiheng commented on pull request #224: [LIVY-633][Server] session should not be gc-ed for long running queries
URL: https://github.com/apache/incubator-livy/pull/224
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 02:06;githubbot;600","yiheng commented on pull request #224: [LIVY-633][Server] session should not be gc-ed for long running queries
URL: https://github.com/apache/incubator-livy/pull/224
 
 
   ## What changes were proposed in this pull request?
   Currently, Livy records the last activity time of the session before statement execution. If a statement runs too long, exceeding then the session timeout, the session will be garbage collected after the statement execution.
   
   This should not be the expected behavior. The statement execution time should not be count into idle. We should update the last activity time after the statement execution.
   
   We cannot be updated when session changes state from busy to idle in the Session class. So in this patch, we add a replLastActivity field into the rscClient, which will be updated when the repl state changes. So when session changes its state from busy to idle, this field will catch the time and finally reflect on the session last activity.
   
   ## How was this patch tested?
   Manual test. Also, add a new unit test.
   
   Existing unit tests and integration tests.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 02:06;githubbot;600","yiheng commented on pull request #224: [LIVY-633][Server] session should not be gc-ed for long running queries
URL: https://github.com/apache/incubator-livy/pull/224
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 03:23;githubbot;600","yiheng commented on pull request #224: [LIVY-633][Server] session should not be gc-ed for long running queries
URL: https://github.com/apache/incubator-livy/pull/224
 
 
   ## What changes were proposed in this pull request?
   Currently, Livy records the last activity time of the session before statement execution. If a statement runs too long, exceeding then the session timeout, the session will be garbage collected after the statement execution.
   
   This should not be the expected behavior. The statement execution time should not be count into idle. We should update the last activity time after the statement execution.
   
   We cannot be updated when session changes state from busy to idle in the Session class. So in this patch, we add a replLastActivity field into the rscClient, which will be updated when the repl state changes. So when session changes its state from busy to idle, this field will catch the time and finally reflect on the session last activity.
   
   ## How was this patch tested?
   Manual test. Also, add a new unit test.
   
   Existing unit tests and integration tests.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Sep/19 03:23;githubbot;600","yiheng commented on pull request #224: [LIVY-633][Server] session should not be gc-ed for long running queries
URL: https://github.com/apache/incubator-livy/pull/224
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Sep/19 03:46;githubbot;600","yiheng commented on pull request #224: [LIVY-633][Server] session should not be gc-ed for long running queries
URL: https://github.com/apache/incubator-livy/pull/224
 
 
   ## What changes were proposed in this pull request?
   Currently, Livy records the last activity time of the session before statement execution. If a statement runs too long, exceeding then the session timeout, the session will be garbage collected after the statement execution.
   
   This should not be the expected behavior. The statement execution time should not be count into idle. We should update the last activity time after the statement execution.
   
   We cannot be updated when session changes state from busy to idle in the Session class. So in this patch, we add a replLastActivity field into the rscClient, which will be updated when the repl state changes. So when session changes its state from busy to idle, this field will catch the time and finally reflect on the session last activity.
   
   ## How was this patch tested?
   Manual test. Also, add a new unit test.
   
   Existing unit tests and integration tests.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Sep/19 03:46;githubbot;600","jerryshao commented on pull request #224: [LIVY-633][Server] session should not be gc-ed for long running queries
URL: https://github.com/apache/incubator-livy/pull/224
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Sep/19 09:22;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4800,,,0,4800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 17 09:23:43 UTC 2019,,,,,,,,,,"0|z05ht4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"15/Aug/19 11:06;yihengw;Is this related to this JIRA?Â [https://issues.apache.org/jira/projects/LIVY/issues/LIVY-547?filter=allissues];;;","27/Aug/19 18:03;igor.calabria;[~yihengw] I believe this is a separate issue. The one you linked fixed the case where even busy sessions were killed. In this one, the session is killed after it completes a statement that takes longer than the timeout. It seems that `Session#recordActivity()`Â  [https://github.com/apache/incubator-livy/blob/master/server/src/main/scala/org/apache/livy/sessions/Session.scala#L176] is not called when the statement finishes, so when the session transitions from busy to idle, it'll be instantly destroyed.;;;","03/Sep/19 07:25;yihengw;Yes, it's a different problem. It should be a bug in Livy. I'm working on a patch to fix it.;;;","17/Sep/19 09:23;jerryshao;Issue resolved by pull request 224
https://github.com/apache/incubator-livy/pull/224;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement GetFunctions metadata operation,LIVY-625,13249004,13223490,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,yihengw,yihengw,yihengw,06/Aug/19 03:06,16/Aug/19 02:36,19/Dec/25 04:15,16/Aug/19 02:34,,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,We should support GetFunctions metadata operation in Livy thrift server.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 16 02:34:45 UTC 2019,,,,,,,,,,"0|z05cso:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Aug/19 12:17;yihengw;Working on it.;;;","08/Aug/19 13:37;yihengw;[https://github.com/apache/incubator-livy/pull/194];;;","16/Aug/19 02:34;jerryshao;Issue resolved by pull request 194
[https://github.com/apache/incubator-livy/pull/194];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement GetColumns metadata operation,LIVY-624,13249003,13223490,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,yihengw,yihengw,yihengw,06/Aug/19 03:06,16/Aug/19 02:36,19/Dec/25 04:15,16/Aug/19 02:33,,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,We should support GetColumns metadata operation in Livy thrift server.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 16 02:33:32 UTC 2019,,,,,,,,,,"0|z05csg:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Aug/19 12:17;yihengw;Working on it.;;;","08/Aug/19 13:37;yihengw;[https://github.com/apache/incubator-livy/pull/194];;;","16/Aug/19 02:33;jerryshao;Issue resolved by pull request 194
[https://github.com/apache/incubator-livy/pull/194];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement GetTables metadata operation,LIVY-623,13249002,13223490,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,yihengw,yihengw,yihengw,06/Aug/19 03:05,16/Aug/19 02:36,19/Dec/25 04:15,16/Aug/19 02:32,,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,We should supportÂ GetTables metadata operation in Livy thrift server.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 16 02:32:58 UTC 2019,,,,,,,,,,"0|z05cs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Aug/19 12:17;yihengw;Working on it.;;;","08/Aug/19 13:37;yihengw;[https://github.com/apache/incubator-livy/pull/194];;;","16/Aug/19 02:32;jerryshao;Issue resolved by pull request 194
[https://github.com/apache/incubator-livy/pull/194];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement GetSchemas metadata operation,LIVY-622,13249001,13223490,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,yihengw,yihengw,yihengw,06/Aug/19 03:03,30/Aug/19 12:06,19/Dec/25 04:15,16/Aug/19 02:32,,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,We should support GetSchemas metadata operation in Livy thrift server.,,"yiheng commented on pull request #194: [LIVY-622][LIVY-623][LIVY-624][LIVY-625][Thrift]Support GetFunctions, GetSchemas, GetTables, GetColumns in Livy thrift server
URL: https://github.com/apache/incubator-livy/pull/194
 
 
   ## What changes were proposed in this pull request?
   In this patch, we add the implementations of GetSchemas, GetFunctions, GetTables, and GetColumns in Livy Thrift server.
   
   https://issues.apache.org/jira/browse/LIVY-622
   https://issues.apache.org/jira/browse/LIVY-623
   https://issues.apache.org/jira/browse/LIVY-624
   https://issues.apache.org/jira/browse/LIVY-625 
   
   ## How was this patch tested?
   Add new unit tests and integration test. Run them with existing tests.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Aug/19 13:20;githubbot;600","yiheng commented on pull request #194: [LIVY-622][LIVY-623][LIVY-624][LIVY-625][Thrift]Support GetFunctions, GetSchemas, GetTables, GetColumns in Livy thrift server
URL: https://github.com/apache/incubator-livy/pull/194
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 11:20;githubbot;600","yiheng commented on pull request #194: [LIVY-622][LIVY-623][LIVY-624][LIVY-625][Thrift]Support GetFunctions, GetSchemas, GetTables, GetColumns in Livy thrift server
URL: https://github.com/apache/incubator-livy/pull/194
 
 
   ## What changes were proposed in this pull request?
   In this patch, we add the implementations of GetSchemas, GetFunctions, GetTables, and GetColumns in Livy Thrift server.
   
   https://issues.apache.org/jira/browse/LIVY-622
   https://issues.apache.org/jira/browse/LIVY-623
   https://issues.apache.org/jira/browse/LIVY-624
   https://issues.apache.org/jira/browse/LIVY-625 
   
   ## How was this patch tested?
   Add new unit tests and integration test. Run them with existing tests.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 11:20;githubbot;600","yiheng commented on pull request #194: [LIVY-622][LIVY-623][LIVY-624][LIVY-625][Thrift]Support GetFunctions, GetSchemas, GetTables, GetColumns in Livy thrift server
URL: https://github.com/apache/incubator-livy/pull/194
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 13:53;githubbot;600","yiheng commented on pull request #194: [LIVY-622][LIVY-623][LIVY-624][LIVY-625][Thrift]Support GetFunctions, GetSchemas, GetTables, GetColumns in Livy thrift server
URL: https://github.com/apache/incubator-livy/pull/194
 
 
   ## What changes were proposed in this pull request?
   In this patch, we add the implementations of GetSchemas, GetFunctions, GetTables, and GetColumns in Livy Thrift server.
   
   https://issues.apache.org/jira/browse/LIVY-622
   https://issues.apache.org/jira/browse/LIVY-623
   https://issues.apache.org/jira/browse/LIVY-624
   https://issues.apache.org/jira/browse/LIVY-625 
   
   ## How was this patch tested?
   Add new unit tests and integration test. Run them with existing tests.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 13:53;githubbot;600","jerryshao commented on pull request #194: [LIVY-622][LIVY-623][LIVY-624][LIVY-625][Thrift]Support GetFunctions, GetSchemas, GetTables, GetColumns in Livy thrift server
URL: https://github.com/apache/incubator-livy/pull/194
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 02:32;githubbot;600","mgaido91 commented on pull request #217:  [LIVY-622][LIVY-623][LIVY-624][LIVY-625][THRIFT][FOLLOWUP] Use ResultSet in catalog operations
URL: https://github.com/apache/incubator-livy/pull/217
 
 
   ## What changes were proposed in this pull request?
   
   This is a followup of #194 which addresses all the remaining concerns. The main changes are:
   
    - reverting the introduction of a state specific for catalog operations;
    - usage of `ResultSet` to send over the wire the data for catalog operations too.
   
   ## How was this patch tested?
   
   existing modified UTs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/19 13:06;githubbot;600","mgaido91 commented on pull request #217:  [LIVY-622][LIVY-623][LIVY-624][LIVY-625][THRIFT][FOLLOWUP] Use ResultSet in catalog operations
URL: https://github.com/apache/incubator-livy/pull/217
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 08:23;githubbot;600","mgaido91 commented on pull request #217:  [LIVY-622][LIVY-623][LIVY-624][LIVY-625][THRIFT][FOLLOWUP] Use ResultSet in catalog operations
URL: https://github.com/apache/incubator-livy/pull/217
 
 
   ## What changes were proposed in this pull request?
   
   This is a followup of #194 which addresses all the remaining concerns. The main changes are:
   
    - reverting the introduction of a state specific for catalog operations;
    - usage of `ResultSet` to send over the wire the data for catalog operations too.
   
   ## How was this patch tested?
   
   existing modified UTs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 08:23;githubbot;600","mgaido91 commented on pull request #217:  [LIVY-622][LIVY-623][LIVY-624][LIVY-625][THRIFT][FOLLOWUP] Use ResultSet in catalog operations
URL: https://github.com/apache/incubator-livy/pull/217
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 09:19;githubbot;600","mgaido91 commented on pull request #217:  [LIVY-622][LIVY-623][LIVY-624][LIVY-625][THRIFT][FOLLOWUP] Use ResultSet in catalog operations
URL: https://github.com/apache/incubator-livy/pull/217
 
 
   ## What changes were proposed in this pull request?
   
   This is a followup of #194 which addresses all the remaining concerns. The main changes are:
   
    - reverting the introduction of a state specific for catalog operations;
    - usage of `ResultSet` to send over the wire the data for catalog operations too.
   
   ## How was this patch tested?
   
   existing modified UTs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 09:19;githubbot;600","mgaido91 commented on pull request #217:  [LIVY-622][LIVY-623][LIVY-624][LIVY-625][THRIFT][FOLLOWUP] Use ResultSet in catalog operations
URL: https://github.com/apache/incubator-livy/pull/217
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 12:06;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,7200,,,0,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 16 02:32:44 UTC 2019,,,,,,,,,,"0|z05cs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Aug/19 12:17;yihengw;Working on it.;;;","16/Aug/19 02:32;jerryshao;Issue resolved by pull request 194
[https://github.com/apache/incubator-livy/pull/194];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark batch session always ends with success when configuration is master yarn and deploy-mode client,LIVY-620,13248432,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,gumartinm,gumartinm,gumartinm,01/Aug/19 21:01,27/Aug/19 03:56,19/Dec/25 04:15,15/Aug/19 03:53,0.5.0,,,0.7.0,,Batch,,,,,,,,,,0,,,,,,"In AWS emr-5.23.0 with Livy 0.5.0 and the following configuration in /etc/livy/conf/livy.conf:
{noformat}
livy.spark.master                yarn
livy.spark.deploy-mode           client
{noformat}
Batch session always ends with success because yarn always ends with status Succeeded. Even if spark fails for some reason (exceptions or whatever) batch session ends with success.
 Not sure, but the issue about yarn always ending with success when client deploy-mode might be related to this Jira (see linked comment):Â https://issues.apache.org/jira/browse/SPARK-11058?focusedCommentId=16052520&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16052520

When client deploy-mode and having spark errors yarn always ends with status Succeeded but the process launched by Livy (the one running org.apache.spark.deploy.SparkSubmit) is killed and exits with no 0 return code. So even if in this case yarn always ends with success livy can find out if it ended with error and end with error itself.

I have already implemented a patch (in master branch) that could fix this issue:

PR:Â [https://github.com/apache/incubator-livy/pull/192]
{noformat}
diff --git a/server/src/main/scala/org/apache/livy/server/batch/BatchSession.scala b/server/src/main/scala/org/apache/livy/server/batch/BatchSession.scala
index 4b27058..c215a8e 100644
--- a/server/src/main/scala/org/apache/livy/server/batch/BatchSession.scala
+++ b/server/src/main/scala/org/apache/livy/server/batch/BatchSession.scala
@@ -93,6 +93,7 @@ object BatchSession extends Logging {
 
       val file = resolveURIs(Seq(request.file), livyConf)(0)
       val sparkSubmit = builder.start(Some(file), request.args)
 
       Utils.startDaemonThread(s""batch-session-process-$id"") {
         childProcesses.incrementAndGet()
@@ -101,6 +102,7 @@ object BatchSession extends Logging {
             case 0 =>
             case exitCode =>
               warn(s""spark-submit exited with code $exitCode"")
+              s.stateChanged(SparkApp.State.KILLED)
           }
         } finally {
           childProcesses.decrementAndGet()
@@ -182,6 +184,14 @@ class BatchSession(
   override def stateChanged(oldState: SparkApp.State, newState: SparkApp.State): Unit = {
     synchronized {
       debug(s""$this state changed from $oldState to $newState"")
+      if (_state != SessionState.Dead()) {
+        stateChanged(newState)
+      }
+    }
+  }
+
+  private def stateChanged(newState: SparkApp.State): Unit = {
+    synchronized {
       newState match {
         case SparkApp.State.RUNNING =>
           _state = SessionState.Running
{noformat}",,"gumartinm commented on pull request #192: [LIVY-620] Spark batch session always ends with success when configuration is master yarn and deploy-mode client
URL: https://github.com/apache/incubator-livy/pull/192
 
 
   ## What changes were proposed in this pull request?
   
   Batch session should end with state killed when process exits with no 0 return code.
   https://issues.apache.org/jira/browse/LIVY-620
   
   ## How was this patch tested?
   
   Unit Test (included in this PR)
   Submit batch session that runs forever, wait 2 seconds, kill that batch session and expect for state killed.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Aug/19 23:38;githubbot;600","gumartinm commented on pull request #192: [LIVY-620] Spark batch session always ends with success when configuration is master yarn and deploy-mode client
URL: https://github.com/apache/incubator-livy/pull/192
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 01:30;githubbot;600","gumartinm commented on pull request #192: [LIVY-620] Spark batch session always ends with success when configuration is master yarn and deploy-mode client
URL: https://github.com/apache/incubator-livy/pull/192
 
 
   ## What changes were proposed in this pull request?
   
   Batch session should end with state killed when process exits with no 0 return code.
   https://issues.apache.org/jira/browse/LIVY-620
   
   ## How was this patch tested?
   
   Unit Test (included in this PR)
   Submit batch session that runs forever, wait 2 seconds, kill that batch session and expect for state killed.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Aug/19 01:30;githubbot;600","gumartinm commented on pull request #192: [LIVY-620] Spark batch session always ends with success when configuration is master yarn and deploy-mode client
URL: https://github.com/apache/incubator-livy/pull/192
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Aug/19 08:53;githubbot;600","gumartinm commented on pull request #192: [LIVY-620] Spark batch session always ends with success when configuration is master yarn and deploy-mode client
URL: https://github.com/apache/incubator-livy/pull/192
 
 
   ## What changes were proposed in this pull request?
   
   Batch session should end with state dead when process exits with no 0 return code.
   https://issues.apache.org/jira/browse/LIVY-620
   
   ## How was this patch tested?
   
   1. Unit Test (included in this PR)
   Submit batch session that runs forever, wait 2 seconds, kill that batch session and expect for state dead.
   
   2. Also currently used in production environment.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Aug/19 08:53;githubbot;600","gumartinm commented on pull request #192: [LIVY-620] Spark batch session always ends with success when configuration is master yarn and deploy-mode client
URL: https://github.com/apache/incubator-livy/pull/192
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Aug/19 15:33;githubbot;600","gumartinm commented on pull request #192: [LIVY-620] Spark batch session always ends with success when configuration is master yarn and deploy-mode client
URL: https://github.com/apache/incubator-livy/pull/192
 
 
   ## What changes were proposed in this pull request?
   
   Batch session should end with state dead when process exits with no 0 return code.
   https://issues.apache.org/jira/browse/LIVY-620
   
   ## How was this patch tested?
   
   1. Unit Test (included in this PR)
   Submit batch session that runs forever, wait 2 seconds, kill that batch session and expect for state dead.
   
   2. Also currently used in production environment.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Aug/19 15:33;githubbot;600","jerryshao commented on pull request #192: [LIVY-620] Spark batch session always ends with success when configuration is master yarn and deploy-mode client
URL: https://github.com/apache/incubator-livy/pull/192
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 05:58;githubbot;600","gumartinm commented on pull request #192: [LIVY-620] Spark batch session always ends with success when configuration is master yarn and deploy-mode client
URL: https://github.com/apache/incubator-livy/pull/192
 
 
   ## What changes were proposed in this pull request?
   
   Batch session should end with state dead when process exits with no 0 return code.
   https://issues.apache.org/jira/browse/LIVY-620
   
   ## How was this patch tested?
   
   1. Unit Test (included in this PR)
   Submit batch session that runs forever, wait 2 seconds, kill that batch session and expect for state dead.
   
   2. Also currently used in production environment.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Aug/19 05:58;githubbot;600","jerryshao commented on pull request #192: [LIVY-620] Spark batch session always ends with success when configuration is master yarn and deploy-mode client
URL: https://github.com/apache/incubator-livy/pull/192
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 02:59;githubbot;600","gumartinm commented on pull request #192: [LIVY-620] Spark batch session always ends with success when configuration is master yarn and deploy-mode client
URL: https://github.com/apache/incubator-livy/pull/192
 
 
   ## What changes were proposed in this pull request?
   
   Batch session should end with state dead when process exits with no 0 return code.
   https://issues.apache.org/jira/browse/LIVY-620
   
   ## How was this patch tested?
   
   1. Unit Test (included in this PR)
   Submit batch session that runs forever, wait 2 seconds, kill that batch session and expect for state dead.
   
   2. Also currently used in production environment.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 03:00;githubbot;600","jerryshao commented on pull request #192: [LIVY-620] Spark batch session always ends with success when configuration is master yarn and deploy-mode client
URL: https://github.com/apache/incubator-livy/pull/192
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 03:53;githubbot;600","jerryshao commented on pull request #214: [LIVY-620][LIVY-641] Fix travis failed on test: should end with status dead when batch session exits with no 0 return code
URL: https://github.com/apache/incubator-livy/pull/214
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Aug/19 03:13;githubbot;600","runzhiwang commented on pull request #214: [LIVY-620][LIVY-641] Fix travis failed on test: should end with status dead when batch session exits with no 0 return code
URL: https://github.com/apache/incubator-livy/pull/214
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on test: should end with status dead when batch session exits with no 0 return code
   
   1. Travis failed because of thread in SparkProcApp.scala and thead in BatchSession.scala change SessionState to different value when stopSession in test. 
   
   2. The changes of BatchSession.scala is to revert the commit of https://github.com/apache/incubator-livy/commit/01da43dba07aee1e4d13a2a19f233a38546ddec0.
   
   3. The changes of SparkYarnApp.scala and BatchSessionSpec.scala are the new changes
   
   ## How was this patch tested?
   
   1. Existed UT and IT. 
   2. Create Batch Session In Yarn and kill SparkSubmit, then check the SessionState.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Aug/19 03:13;githubbot;600","jerryshao commented on pull request #214: [LIVY-620][LIVY-641] Fix travis failed on test: should end with status dead when batch session exits with no 0 return code
URL: https://github.com/apache/incubator-livy/pull/214
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Aug/19 03:56;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,9000,,,0,9000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 15 03:53:55 UTC 2019,,,,,,,,,,"0|z0599k:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Aug/19 01:19;timmiboi;Are there any work-arounds for this?;;;","15/Aug/19 03:53;jerryshao;Issue resolved by pull request 192
[https://github.com/apache/incubator-livy/pull/192];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy session leak on Yarn when creating session duplicated names,LIVY-617,13247951,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,shanyu,shanyu,shanyu,30/Jul/19 21:57,28/Aug/19 11:11,19/Dec/25 04:15,28/Aug/19 11:11,0.6.0,,,0.7.0,,Server,,,,,,,,,,1,,,,,,"When running Livy on Yarn and try to create session with duplicated names, LivyÂ server sends response to client ""Duplicate session name: xxx"" but it doesn'tÂ stop the session. The session creation failed, however, the Yarn application got started and keeps running forever.

This is because during livy session register method, exception ""IllegalArgumentException"" is thrown without stopping the session:
{code:java}
def register(session: S): S = {
Â Â Â  info(s""Registering new session ${session.id}"")
Â Â Â  synchronized {
Â Â Â Â Â  session.name.foreach { sessionName =>
Â Â Â Â Â Â Â  if (sessionsByName.contains(sessionName)) {
Â Â Â Â Â Â Â Â  Â throw new IllegalArgumentException(s""Duplicate session name: ${session.name}"")
Â Â Â Â Â Â Â  } else {
Â Â Â Â Â Â Â Â Â  sessionsByName.put(sessionName, session)
Â Â Â Â Â Â Â  }
Â Â Â Â Â  }
Â Â Â Â Â  sessions.put(session.id, session)
Â Â Â Â Â  session.start()
Â Â Â  }
Â Â Â  session
Â  }{code}
Â 

Reproduction scripts:

curl -s -k -u username:password -X POST --data '\{""name"": ""duplicatedname"", ""kind"": ""pyspark""}' -H ""Content-Type: application/json"" 'https://myserver/livy/v1/sessions'",,"shanyu commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   ## What changes were proposed in this pull request?
   When creating a session with duplicated name, instead of throw exception in SessionManager.register() method, we should stop the session. Otherwise the session driver process will keep running and end up creating a leaked Yarn application.
   
   https://issues.apache.org/jira/browse/LIVY-617
   
   ## How was this patch tested?
   
   This is just a simple fix and verified with manual end to end test.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 01:31;githubbot;600","jerryshao commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 09:49;githubbot;600","shanyu commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   ## What changes were proposed in this pull request?
   When creating a session with duplicated name, instead of throw exception in SessionManager.register() method, we should stop the session. Otherwise the session driver process will keep running and end up creating a leaked Yarn application.
   
   https://issues.apache.org/jira/browse/LIVY-617
   
   ## How was this patch tested?
   
   This is just a simple fix and verified with manual end to end test.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 09:49;githubbot;600","jerryshao commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 03:07;githubbot;600","shanyu commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   ## What changes were proposed in this pull request?
   When creating a session with duplicated name, instead of throw exception in SessionManager.register() method, we should stop the session. Otherwise the session driver process will keep running and end up creating a leaked Yarn application.
   
   https://issues.apache.org/jira/browse/LIVY-617
   
   ## How was this patch tested?
   
   This is just a simple fix and verified with manual end to end test.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 03:07;githubbot;600","jerryshao commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 07:06;githubbot;600","shanyu commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   ## What changes were proposed in this pull request?
   When creating a session with duplicated name, instead of throw exception in SessionManager.register() method, we should stop the session. Otherwise the session driver process will keep running and end up creating a leaked Yarn application.
   
   https://issues.apache.org/jira/browse/LIVY-617
   
   ## How was this patch tested?
   
   This is just a simple fix and verified with manual end to end test.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/19 07:06;githubbot;600","jerryshao commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 02:09;githubbot;600","shanyu commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   ## What changes were proposed in this pull request?
   When creating a session with duplicated name, instead of throw exception in SessionManager.register() method, we should stop the session. Otherwise the session driver process will keep running and end up creating a leaked Yarn application.
   
   https://issues.apache.org/jira/browse/LIVY-617
   
   ## How was this patch tested?
   
   This is just a simple fix and verified with manual end to end test.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/19 02:09;githubbot;600","jerryshao commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 02:48;githubbot;600","shanyu commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   ## What changes were proposed in this pull request?
   When creating a session with duplicated name, instead of throw exception in SessionManager.register() method, we should stop the session. Otherwise the session driver process will keep running and end up creating a leaked Yarn application.
   
   https://issues.apache.org/jira/browse/LIVY-617
   
   ## How was this patch tested?
   
   This is just a simple fix and verified with manual end to end test.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 02:48;githubbot;600","jerryshao commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Aug/19 07:56;githubbot;600","shanyu commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   ## What changes were proposed in this pull request?
   When creating a session with duplicated name, instead of throw exception in SessionManager.register() method, we should stop the session. Otherwise the session driver process will keep running and end up creating a leaked Yarn application.
   
   https://issues.apache.org/jira/browse/LIVY-617
   
   ## How was this patch tested?
   
   This is just a simple fix and verified with manual end to end test.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Aug/19 07:56;githubbot;600","jerryshao commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/19 08:25;githubbot;600","shanyu commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   ## What changes were proposed in this pull request?
   When creating a session with duplicated name, instead of throw exception in SessionManager.register() method, we should stop the session. Otherwise the session driver process will keep running and end up creating a leaked Yarn application.
   
   https://issues.apache.org/jira/browse/LIVY-617
   
   ## How was this patch tested?
   
   This is just a simple fix and verified with manual end to end test.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/19 08:25;githubbot;600","jerryshao commented on pull request #187:  LIVY-617: Livy session leak on Yarn when creating session duplicated names
URL: https://github.com/apache/incubator-livy/pull/187
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/19 11:05;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,9600,,,0,9600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 28 11:11:24 UTC 2019,,,,,,,,,,"0|z056b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Aug/19 11:11;jerryshao;Issue resolved by pull request 187
https://github.com/apache/incubator-livy/pull/187;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy can't handle the java.sql.Date type correctly,LIVY-613,13246790,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,397090770,397090770,397090770,24/Jul/19 11:11,21/Sep/19 17:07,19/Dec/25 04:15,26/Jul/19 11:35,0.7.0,,,0.7.0,,REPL,,,,,,,,,,0,,,,,,"When Spark table has java.sql.Date type column,Â Livy can't handle the java.sql.Date type correctly. e.g

{code:java}
create table test(
    name string,
    birthday date
);

insert into test values ('Livy', '2019-07-24')

curl -H ""Content-Type:application/json"" -X POST -d '{""code"":""select * from test"", ""kind"":""sql""}' 192.168.1.6:8998/sessions/48/statements
{""id"":1,""code"":""select * from test"",""state"":""waiting"",""output"":null,""progress"":0.0}

curl 192.168.1.6:8998/sessions/48/statements/1
{""id"":1,""code"":""select * from test"",""state"":""available"",""output"":{""status"":""ok"",""execution_count"":1,""data"":{""application/json"":{""schema"":{""type"":""struct"",""fields"":[{""name"":""name"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""birthday"",""type"":""date"",""nullable"":true,""metadata"":{}}]},""data"":[[""Livy"",{}]]}}},""progress"":1.0}{code}
as you can see, the output of `select * from test` isÂ [""Livy"",{}],Â birthdayÂ column's value isn'tÂ handleÂ Â correctly.

The reason is that json4j can't handle java.sql.Date, so we should define theÂ CustomSerializer forÂ java.sql.Date.

Â ",,"397090770 commented on pull request #186: [LIVY-613] Livy can't handle the java.sql.Date type correctly.
URL: https://github.com/apache/incubator-livy/pull/186
 
 
   ## What changes were proposed in this pull request?
   
   When Spark table has java.sql.Date type column, Livy can't handle the java.sql.Date type correctly. e.g
   ```
   create table test(
       name string,
       birthday date
   );
   
   insert into test1 values ('Livy', '2019-07-24')
   
   curl -H ""Content-Type:application/json"" -X POST -d '{""code"":""select * from test1"", ""kind"":""sql""}' 192.168.1.6:8998/sessions/48/statements
   {""id"":1,""code"":""select * from test1"",""state"":""waiting"",""output"":null,""progress"":0.0}
   
   curl 192.168.1.6:8998/sessions/48/statements/1
   {""id"":1,""code"":""select * from test1"",""state"":""available"",""output"":{""status"":""ok"",""execution_count"":1,""data"":{""application/json"":{""schema"":{""type"":""struct"",""fields"":[{""name"":""name"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""birthday"",""type"":""date"",""nullable"":true,""metadata"":{}}]},""data"":[[""Livy"",{}]]}}},""progress"":1.0}
   ```
   as you can see, the output of `select * from test1` is [""Livy"",{}], birthday column's value isn't handle  correctly.
   
   The reason is that json4j can't handle java.sql.Date, so we should define the CustomSerializer for java.sql.Date.
   
   This PR  add a `DateSerializer` to support java.sql.Date parser.
   
   ## How was this patch tested?
   
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Jul/19 11:43;githubbot;600","jerryshao commented on pull request #186: [LIVY-613] Livy can't handle the java.sql.Date type correctly.
URL: https://github.com/apache/incubator-livy/pull/186
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 03:30;githubbot;600","397090770 commented on pull request #186: [LIVY-613] Livy can't handle the java.sql.Date type correctly.
URL: https://github.com/apache/incubator-livy/pull/186
 
 
   ## What changes were proposed in this pull request?
   
   When Spark table has `java.sql.Date` type column, Livy can't handle the `java.sql.Date` type correctly. e.g
   ```
   create table test(
       name string,
       birthday date
   );
   
   insert into test values ('Livy', '2019-07-24')
   
   curl -H ""Content-Type:application/json"" -X POST -d '{""code"":""select * from test"", ""kind"":""sql""}' 192.168.1.6:8998/sessions/48/statements
   {""id"":1,""code"":""select * from test"",""state"":""waiting"",""output"":null,""progress"":0.0}
   
   curl 192.168.1.6:8998/sessions/48/statements/1
   {""id"":1,""code"":""select * from test"",""state"":""available"",""output"":{""status"":""ok"",""execution_count"":1,""data"":{""application/json"":{""schema"":{""type"":""struct"",""fields"":[{""name"":""name"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""birthday"",""type"":""date"",""nullable"":true,""metadata"":{}}]},""data"":[[""Livy"",{}]]}}},""progress"":1.0}
   ```
   as you can see, the output of `select * from test` is `[""Livy"",{}]` , birthday column's value isn't handle  correctly.
   
   The reason is that json4j can't handle `java.sql.Date`, so we should define the `CustomSerializer` for `java.sql.Date`.
   
   This PR  add a `DateSerializer` to support `java.sql.Date` parser.
   
   ## How was this patch tested?
   
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 03:30;githubbot;600","jerryshao commented on pull request #186: [LIVY-613][REPL] Livy can't handle the java.sql.Date type correctly.
URL: https://github.com/apache/incubator-livy/pull/186
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Jul/19 11:35;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,LIVY-683,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 26 11:35:21 UTC 2019,,,,,,,,,,"0|z04z5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Jul/19 11:35;jerryshao;Issue resolved by pull request 186
[https://github.com/apache/incubator-livy/pull/186];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
optimization for windows environment build.,LIVY-610,13245968,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,Charo Zhang,Charo Zhang,Charo Zhang,19/Jul/19 10:49,22/Jul/19 06:24,19/Dec/25 04:15,22/Jul/19 06:21,0.6.0,,,0.7.0,,Build,,,,,,,,,,0,,,,,,* we can remove requireOS restriction for windows building.,,"dockerzhang commented on pull request #184: [LIVY-610] optimization for windows environment build.
URL: https://github.com/apache/incubator-livy/pull/184
 
 
   ## What changes were proposed in this pull request?
   
   we can remove requireOS restriction for windows building, and add execute permission for bin/livy-server.
   https://issues.apache.org/jira/browse/LIVY-610
   ## How was this patch tested?
    tested manually.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jul/19 10:55;githubbot;600","jerryshao commented on pull request #184: [LIVY-610][BUILD] optimization for windows environment build.
URL: https://github.com/apache/incubator-livy/pull/184
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 06:21;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 22 06:21:21 UTC 2019,,,,,,,,,,"0|z04u2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Jul/19 06:21;jerryshao;Issue resolved by pull request 184
[https://github.com/apache/incubator-livy/pull/184];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When Spark table contains date column Livy will throw NoClassDefFoundError,LIVY-608,13245662,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Not A Bug,,397090770,397090770,18/Jul/19 03:36,18/Jul/19 04:03,19/Dec/25 04:15,18/Jul/19 04:03,0.6.0,,,,,REPL,,,,,,,,,,0,,,,,,"My Spark table creation statement is as follows
{code:java}
create table test(
 id bigint,
 times date
);
{code}
when we use livy to select data from test, there willÂ throw java.lang.NoClassDefFoundError:
{code:java}
curl -H ""Content-Type:application/json"" -X POST -d '{""code"":""select * from test"", ""kind"":""sql""}' 127.0.0.1:8998/sessions/1/statements
{""id"":1,""code"":""select * from test"",""state"":""waiting"",""output"":null,""progress"":0.0}


curl 127.0.0.1:8998/sessions/1/statements/1
{""id"":1,""code"":""select * from test"",""state"":""available"",""output"":{""status"":""error"",""execution_count"":1,""ename"":""Internal Error: java.lang.NoClassDefFoundError"",""evalue"":""org/apache/livy/shaded/json4s/scalap/scalasig/SymbolInfoSymbol"",""traceback"":[]},""progress"":1.0}
{code}
because livy repl model need json4s_scalap library, but we did not provide.

Â ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2019-07-18 03:36:53.0,,,,,,,,,,"0|z04s7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrade build spark version to 2.4.3,LIVY-603,13243315,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,yihengw,xilangyan,xilangyan,05/Jul/19 08:54,12/Jul/19 02:28,19/Dec/25 04:15,12/Jul/19 02:12,0.6.0,,,0.7.0,,Build,,,,,,,,,,0,,,,,,config with current pom.xml will fail because 2.4.0 spark is removed fromÂ [http://mirrors.advancedhosters.com/apache/spark/],,"yiheng commented on pull request #179: [LIVY-603] Upgrade build spark version to 2.4.3
URL: https://github.com/apache/incubator-livy/pull/179
 
 
   ## What changes were proposed in this pull request?
   Bump Spark 2.4 minor version to 2.4.3. 
   
   ## How was this patch tested?
   Existing test
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 02:40;githubbot;600","jerryshao commented on pull request #179: [LIVY-603][BUILD] Upgrade build spark version to 2.4.3
URL: https://github.com/apache/incubator-livy/pull/179
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 09:06;githubbot;600","yiheng commented on pull request #179: [LIVY-603][BUILD] Upgrade build spark version to 2.4.3
URL: https://github.com/apache/incubator-livy/pull/179
 
 
   ## What changes were proposed in this pull request?
   Bump Spark 2.4 minor version to 2.4.3. 
   
   ## How was this patch tested?
   Existing test
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 09:06;githubbot;600","yantzu commented on pull request #176: [LIVY-603]Upgrade spark 2.4 from 2.4.0 to 2.4.3
URL: https://github.com/apache/incubator-livy/pull/176
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jul/19 13:50;githubbot;600","jerryshao commented on pull request #179: [LIVY-603][BUILD] Upgrade build spark version to 2.4.3
URL: https://github.com/apache/incubator-livy/pull/179
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Jul/19 02:12;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 12 02:12:44 UTC 2019,,,,,,,,,,"0|z04ea8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Jul/19 02:12;jerryshao;Issue resolved by pull request 179
[https://github.com/apache/incubator-livy/pull/179];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Livy jetty version to 9.4.18.v20190429,LIVY-599,13238052,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,arunmahadevan,arunmahadevan,06/Jun/19 21:34,04/Nov/25 20:40,19/Dec/25 04:15,04/Nov/25 20:40,,,,,,,,,,,,,,,,0,,,,,,"Upgrade the jetty version to 9.4.18.v20190429.

This has fixes for CVE-2019-10247 and spark has recently upgraded to this version",,"arunmahadevan commented on pull request #175: [LIVY-599] Upgrade jetty version to 9.4.18.v20190429
URL: https://github.com/apache/incubator-livy/pull/175
 
 
   ## What changes were proposed in this pull request?
   
   Upgrade to jetty version 9.4.18.v20190429.
   This has fixes for CVE-2019-10247 and spark has recently upgraded to this version
   
   https://issues.apache.org/jira/browse/LIVY-599
   
   ## How was this patch tested?
   
   Existing unit tests.
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jun/19 21:40;githubbot;600","coheigea commented on pull request #175:
URL: https://github.com/apache/incubator-livy/pull/175#issuecomment-697308007


   This PR is superceded by https://github.com/apache/incubator-livy/pull/305


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Sep/20 11:39;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,LIVY-694,,,,LIVY-851,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 04 20:40:26 UTC 2025,,,,,,,,,,"0|z03hwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Nov/22 22:10;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;","04/Nov/25 20:40;gyogal;Jetty version is already at 9.4.56.v20240826, resolving this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Livy jackson dependency to 2.9.9,LIVY-598,13238047,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,arunmahadevan,arunmahadevan,arunmahadevan,06/Jun/19 21:22,08/Oct/19 09:18,19/Dec/25 04:15,21/Jul/19 14:02,,,,0.7.0,,,,,,,,,,,,0,,,,,,"Upgrade the jackson dependency to 2.9.9 which fixes CVE-2019-12086. 
Spark has also recently upgraded to jackson version 2.9.9.",,"arunmahadevan commented on pull request #174: [LIVY-598] Upgrade jackson to 2.9.9
URL: https://github.com/apache/incubator-livy/pull/174
 
 
   ## What changes were proposed in this pull request?
   
   Upgrade the jackson dependency to 2.9.9 which has fixes related to CVEs.  Spark has also recently upgraded to jackson version 2.9.9.
   
   ## How was this patch tested?
   
   Existing unit tests
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jun/19 21:26;githubbot;600","jerryshao commented on pull request #174: [LIVY-598] Upgrade jackson to 2.9.9
URL: https://github.com/apache/incubator-livy/pull/174
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/19 06:38;githubbot;600","arunmahadevan commented on pull request #174: [LIVY-598] Upgrade jackson to 2.9.9
URL: https://github.com/apache/incubator-livy/pull/174
 
 
   ## What changes were proposed in this pull request?
   
   Upgrade the jackson dependency to 2.9.9 which fixes CVE-2019-12086.  Spark has also recently upgraded to jackson version 2.9.9.
   
   ## How was this patch tested?
   
   Existing unit tests
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/19 06:38;githubbot;600","mgaido91 commented on pull request #174: [LIVY-598] Upgrade jackson to 2.9.9
URL: https://github.com/apache/incubator-livy/pull/174
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jul/19 14:00;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jul 21 14:02:15 UTC 2019,,,,,,,,,,"0|z03hvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,"21/Jul/19 14:02;mgaido;Issue resolved by https://github.com/apache/incubator-livy/pull/174.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Livy guava dependency,LIVY-597,13234396,,Improvement,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,arunmahadevan,arunmahadevan,20/May/19 21:07,29/Jul/19 08:41,19/Dec/25 04:15,29/Jul/19 08:33,,,,,,,,,,,,,,,,0,,,,,,"The guava 15.0 that Livy is using is affected by CVE-2018-10237.

It seems Livy's guava usage is limited and we can upgrade the version
seamlessly.",,"arunmahadevan commented on pull request #173: [LIVY-597] Upgrade guava version to latest
URL: https://github.com/apache/incubator-livy/pull/173
 
 
   ## What changes were proposed in this pull request?
   
   The guava 15.0 that Livy is using is affected by CVE-2018-10237.
   
   It seems Livy's guava usage is limited and we can upgrade the version
   seamlessly.
   
   https://issues.apache.org/jira/browse/LIVY-597
   
   ## How was this patch tested?
   
   Existing unit tests.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/May/19 21:10;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,LIVY-587,,,,LIVY-587,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 10 08:50:01 UTC 2019,,,,,,,,,,"0|z02vf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/Jul/19 08:17;runzhiwang;HiÂ [~arunmahadevan],Â  [LIVY-587|https://issues.apache.org/jira/browse/LIVY-587]Â conflicts withÂ [LIVY-597|https://issues.apache.org/jira/browse/LIVY-597] Â about guava. Because guava was not used by livy any more,Â  how about removing the dependency instead of upgrading guava?;;;","10/Jul/19 08:50;zjffdu;I agree to remove guava, I don't think guava isÂ indispensable for livy.

Â Â ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace DEGEST-MED5 with GSSAPI(Kerberos) in the RPC sasl,LIVY-595,13232645,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Fixed,,yc_huawei,yc_huawei,10/May/19 11:40,11/May/19 10:06,19/Dec/25 04:15,11/May/19 10:06,0.5.0,,,,,RSC,Server,,,,,,,,,0,,,,,,"This is aÂ English version

DIGEST-MD5 has been considered as a non-secure encryption mechanism in the industry, so according to the company's security requirements, it is replaced by GSSAPI (kerberos authentication);

Initially, I just changed the configuration value of livy. rsc. rpc. sasl. mechanisms to GSSAPI, but reported an error: Failed to find any Kerberos credentails; so I started my painful journey to modify the source code (thank you very much if you have a feasible configuration plan to inform). The specific steps are as follows:

Â 

1) In the Rpc and RpcServer classes, create LoginContext and login when creating client and server for sasl, and encapsulate Sasl. createSaslServer and Sasl. createSaslClient with Subject. doAs.

2) The parameters of Sasl. createSaslServer and Sasl. createSaslClient mainly change protocol to the user name of principal (i.e. the first paragraph of principal), and server Name to the qualified name of principal (i.e. the second paragraph of principal). Other parameters remain unchanged and login succeeds.

Â 

Question: Client and server can communicate, the first sendHello can succeed, but the second time Livy returns token to driver, driver unwrap error: {color:#FF0000}Caused by GSSException: Defective token detection (Mechanism level: Wrap Token (new format): Cannot read all 12 bytes needed to form this token!){color}

Â 

My analysis: I tracked livy's log. The byte array returned to driver is null and sent to driver by Chanel Rpc.SaslMessage object, when unwrap, \{data is [20, 1, 0, 0], offset is 0, len is 4}, driver unwrap will report an error.

Â 
Â 
The problem is too difficult to solve, IÂ needÂ helpÂ now. thinks everyone.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,,,Sat May 11 10:06:42 UTC 2019,,,,,,,,,,"0|z02kls:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/May/19 11:48;yc_huawei;*livy log:*

19/05/10 17:15:48 RPC-Handler-4 INFO RpcServer: server challenge payload byte is : [5, 4, 0, -1, 0, 12, 0, 0, 0, 0, 0, 0, 14, 12, 88, 110, 4, 1, 0, 0, 91, -60, -82, 68, 104, 45, -96, -54, 76, -95, 0, 57].
Krb5Context.unwrap: token=[05 04 00 ff 00 0c 00 00 00 00 00 00 0e 0c 58 6e 04 01 00 00 5b c4 ae 44 68 2d a0 ca 4c a1 00 39 ]
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: 2019-05-10 17:15:48,059 | INFO | RPC-Handler-3 | yc add : SASL confidentiality enabled, and class is org.apache.livy.rsc.rpc.Rpc$SaslClientHandler | org.apache.livy.rsc.rpc.SaslHandler.channelRead0(SaslHandler.java:90)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: 2019-05-10 17:15:48,059 | INFO | RPC-Handler-3 | yc add onComplete | org.apache.livy.rsc.rpc.SaslHandler.channelRead0(SaslHandler.java:95)
Krb5Context.unwrap: data=[04 01 00 00 ]
19/05/10 17:15:48 RPC-Handler-4 INFO RpcServer: AuthorizeCallback set true
19/05/10 17:15:48 RPC-Handler-4 INFO{color:#FF0000} RpcServer: after server evaluate response byte is : null.{color}
19/05/10 17:15:48 RPC-Handler-4 INFO RpcServer$SaslServerHandler: Sending SASL challenge response clientId is null, payload is null.
19/05/10 17:15:48 RPC-Handler-4 INFO KryoMessageCodec: {color:#FF0000}Encoded message of type org.apache.livy.rsc.rpc.Rpc$SaslMessage (4 bytes){color}
19/05/10 17:15:48 RPC-Handler-4 INFO KryoMessageCodec: Encoded ByteBuf class io.netty.buffer.UnpooledUnsafeNoCleanerDirectByteBuf
19/05/10 17:15:48 RPC-Handler-4 DEBUG Rpc: [id: 0x36c6e919, L:/192.168.100.25:10000 - R:/192.168.100.25:59218] WRITE: 8B
 +-------------------------------------------------+
 | 0 1 2 3 4 5 6 7 8 9 a b c d e f |
+--------+-------------------------------------------------+----------------+
|00000000| 00 00 00 04 14 01 00 00 |........ |

+--------+-------------------------------------------------+----------------+
19/05/10 17:15:48 RPC-Handler-4 DEBUG Rpc: [id: 0x36c6e919, L:/192.168.100.25:10000 - R:/192.168.100.25:59218] FLUSH
19/05/10 17:15:48 RPC-Handler-4 INFO RpcServer$SaslServerHandler: ended writeAndFlush!
19/05/10 17:15:48 RPC-Handler-4 INFO RpcServer: server isComplete true
19/05/10 17:15:48 RPC-Handler-4 INFO RpcServer$SaslServerHandler: yc add : SASL confidentiality enabled, and class is org.apache.livy.rsc.rpc.RpcServer$SaslServerHandler
19/05/10 17:15:48 RPC-Handler-4 INFO RpcServer$SaslServerHandler: yc add onComplete
19/05/10 17:15:48 RPC-Handler-4 INFO RpcServer: onComplete.

Â 

*driver log:*

19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: 2019-05-10 17:15:48,062 | DEBUG | RPC-Handler-3 | [id: 0xc76a1550, L:/192.168.100.25:59218 - R:/192.168.100.25:10000] RECEIVED: 8B
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: +-------------------------------------------------+
19/05/10 17:15:48 Thread-122 INFO LineBufferedStream: stdout: Krb5Context.unwrap: token=[14 01 00 00 ]
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: | 0 1 2 3 4 5 6 7 8 9 a b c d e f |
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: +--------+-------------------------------------------------+----------------+
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: |00000000| 00 00 00 04 14 01 00 00 |........ |
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: +--------+-------------------------------------------------+----------------+ | io.netty.util.internal.logging.Slf4JLogger.debug(Slf4JLogger.java:71)
{color:#FF0000}19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: 2019-05-10 17:15:48,062 | INFO | RPC-Handler-3 | unwrap data is [20, 1, 0, 0], offset is 0, len is 4. | org.apache.livy.rsc.rpc.Rpc$SaslClientHandler.unwrap(Rpc.java:480){color}
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: 2019-05-10 17:15:48,064 | INFO | RPC-Handler-3 | [ReplDriver] Caught exception in channel pipeline. | org.apache.livy.rsc.rpc.RpcDispatcher.exceptionCaught(RpcDispatcher.java:177)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: io.netty.handler.codec.DecoderException: javax.security.sasl.SaslException:{color:#FF0000} Problems unwrapping SASL buffer [Caused by GSSException: Defective token detected (Mechanism level: Wrap Token (new format):Cannot read all 12 bytes needed to form this token!)]{color}
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:442)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.handler.codec.ByteToMessageCodec.channelRead(ByteToMessageCodec.java:103)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:240)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at java.lang.Thread.run(Thread.java:748)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: Caused by: javax.security.sasl.SaslException: Problems unwrapping SASL buffer [Caused by GSSException: Defective token detected (Mechanism level: Wrap Token (new format):Cannot read all 12 bytes needed to form this token!)]
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at com.sun.security.sasl.gsskerb.GssKrb5Base.unwrap(GssKrb5Base.java:86)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at org.apache.livy.rsc.rpc.Rpc$SaslClientHandler.unwrap(Rpc.java:481)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at org.apache.livy.rsc.rpc.KryoMessageCodec.doWrapOrUnWrap(KryoMessageCodec.java:146)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at org.apache.livy.rsc.rpc.KryoMessageCodec.maybeDecrypt(KryoMessageCodec.java:121)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at org.apache.livy.rsc.rpc.KryoMessageCodec.decode(KryoMessageCodec.java:76)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.handler.codec.ByteToMessageCodec$1.decode(ByteToMessageCodec.java:42)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:411)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: ... 24 more
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: Caused by: GSSException: Defective token detected (Mechanism level: Wrap Token (new format):Cannot read all 12 bytes needed to form this token!)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at sun.security.jgss.krb5.MessageToken_v2.<init>(MessageToken_v2.java:258)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at sun.security.jgss.krb5.MessageToken_v2.<init>(MessageToken_v2.java:165)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at sun.security.jgss.krb5.WrapToken_v2.<init>(WrapToken_v2.java:71)
19/05/10 17:15:48 Thread-123 INFO LineBufferedStream: stdout: at sun.security.jgss.krb5.Krb5Context.unwrap(Krb5Context.java:1056);;;","11/May/19 10:06;yc_huawei;this problem is livy bugï¼Œwhen GSSAPIÂ negotiation is complete will return null byte,and livy send this null byte to driver, so we should not send null byte by the channel then remove this handler from the pipeline. thinks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
livyé‡‡ç”¨yarn-clusteræ¨¡å¼ï¼Œå°†driverä¸Žlivyçš„SASLæœºåˆ¶ç”±DIGEST-MD5æ›¿æ¢ä¸ºGSSAPIï¼ˆKerberosï¼‰,LIVY-594,13232640,,Improvement,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Duplicate,,yc_huawei,yc_huawei,10/May/19 11:28,10/May/19 11:51,19/Dec/25 04:15,10/May/19 11:51,0.5.0,,,0.5.0,,RSC,Server,,,,,,,,,0,security,,,,,"DIGEST-MD5å·²ç»è¢«ä¸šç•Œè®¤ä¸ºæ˜¯éžå®‰å…¨çš„åŠ å¯†æœºåˆ¶ï¼Œå› æ­¤æ ¹æ®å…¬å¸å®‰å…¨è¦æ±‚ï¼Œæ›¿æ¢ä¸ºGSSAPIï¼ˆå³kerberosè®¤è¯ï¼‰ï¼›

èµ·åˆï¼Œåªæ˜¯å°†livy.rsc.rpc.sasl.mechanismsé…ç½®å€¼æ”¹ä¸ºGSSAPIï¼Œä½†æ˜¯æŠ¥é”™è¯¯:Â Failed to find any Kerberos credentailsï¼›äºŽæ˜¯å¼€å§‹äº†æˆ‘ä¿®æ”¹æºç çš„ç—›è‹¦ä¹‹æ—…ï¼ˆå¦‚æžœå¤§å®¶æœ‰å¯è¡Œçš„é…ç½®æ–¹æ¡ˆå¸Œæœ›å‘ŠçŸ¥ï¼Œä¸‡åˆ†æ„Ÿè°¢ï¼‰ï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

1ï¼‰åœ¨Rpcå’ŒRpcServerç±»ä¸­ï¼Œå°†saslçš„åˆ›å»ºclientå’Œserveræ—¶åˆ›å»ºLoginContextå¹¶è¿›è¡Œloginï¼Œé‡‡ç”¨Subject.doAså°è£…Sasl.createSaslServerå’ŒSasl.createSaslClientã€‚

2ï¼‰Sasl.createSaslServerå’ŒSasl.createSaslClientçš„å‚æ•°ï¼Œä¸»è¦æ˜¯å°†protocolä¿®æ”¹ä¸ºprincipalçš„useråç§°ï¼ˆå³principalçš„ç¬¬ä¸€æ®µï¼‰ï¼ŒserverNameä¿®æ”¹ä¸ºä¸»æœºé™å®šåï¼ˆå³principalçš„ç¬¬äºŒæ®µï¼‰ã€‚å…¶ä»–å‚æ•°ä¸å˜ï¼Œèƒ½å¤ŸloginæˆåŠŸã€‚

é—®é¢˜ï¼šclientå’Œserverèƒ½å¤Ÿè¿›è¡Œé€šä¿¡ï¼Œç¬¬ä¸€æ¬¡sendHelloèƒ½å¤ŸæˆåŠŸï¼Œä½†æ˜¯ç¬¬äºŒæ¬¡ç”±livyè¿”å›žç»™driverçš„tokenï¼Œdriver unwrapæŠ¥é”™ï¼šCaused by GSSException: Defective token detected (Mechanism level: Wrap Token (new format):Cannot read all 12 bytes needed to form this token!)

åˆ†æžï¼šæˆ‘è·Ÿè¸ªäº†livyçš„æ—¥å¿—ï¼Œåœ¨è¿”å›žç»™driverçš„byteæ•°ç»„ä¸ºnullï¼Œç»è¿‡chanelå‘é€ç»™driverçš„

Rpc.SaslMessageå¯¹è±¡ï¼Œåœ¨unwrapæ—¶ï¼Œ\{data is [20, 1, 0, 0], offset is 0, len is 4}ï¼Œdriver unwrapä¾¿æŠ¥é”™ã€‚

å®žåœ¨æ˜¯è°ƒä¸é€šäº†ï¼Œæ±‚ç¤¾åŒºçš„å¤§ç¥žå¸®å¿™ã€‚

Â ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,,,Fri May 10 11:51:57 UTC 2019,,,,,,,,,,"0|z02kko:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/May/19 11:51;yc_huawei;see the LIVY-595 Problem;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding Scala 2.12 support,LIVY-593,13231215,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ksumit,Sacha,Sacha,02/May/19 09:45,17/May/23 06:31,19/Dec/25 04:15,17/May/23 06:31,0.6.0,,,0.8.0,,,,,,,,,,,,2,,,,,,"Spark 2.4.2 is build with Scala 2.12 however Apache Livy 0.6.0 do not support Scala 2.12.

I am writing to let you know that Livy 0.6.0 do not work with Spark 2.4.2 Scala 2.12.

Could you tell me when Livy will support Scala 2.12 ?",,"ksumit opened a new pull request, #394:
URL: https://github.com/apache/incubator-livy/pull/394

   ## What changes were proposed in this pull request?
   Added support for specific flags to support hadoop/spark/scala version specific builds.
   
   ## How was this patch tested?
   
   Ran `mvn install` with the new flags to confirm they work.
   


;14/Mar/23 16:38;githubbot;600","ayushtkn commented on code in PR #394:
URL: https://github.com/apache/incubator-livy/pull/394#discussion_r1147898935


##########
README.md:
##########
@@ -88,3 +88,13 @@ between different Spark versions.
 
 The Livy package itself does not contain a Spark distribution. It will work with any supported
 version of Spark without needing to rebuild.
+
+### Build Profiles
+
+| Flag        | Purpose                                                            |
+|-------------|--------------------------------------------------------------------|
+| -Phadoop2   | Choose Hadoop2 based build dependencies (default configuration)    |
+| -Pspark2    | Choose Spark 2.x based build dependencies (default configuration)  |
+| -Pscala-2.11 | Choose Scala 2.11 based build dependencies (default configuration) |        
+| -Pscala-2.12 | Choose scala 2.12 based build dependencies                         |
+| -Pspark3 | Choose Spark 3.x based build dependencies |                                                

Review Comment:
   This sounds tough to do and very hard coded, Do we have an option like -Dhadoop.version=3.3.5 and have a way to provide them at runtime, if we can do that 



;24/Mar/23 17:50;githubbot;600","ayushtkn commented on code in PR #394:
URL: https://github.com/apache/incubator-livy/pull/394#discussion_r1147898935


##########
README.md:
##########
@@ -88,3 +88,13 @@ between different Spark versions.
 
 The Livy package itself does not contain a Spark distribution. It will work with any supported
 version of Spark without needing to rebuild.
+
+### Build Profiles
+
+| Flag        | Purpose                                                            |
+|-------------|--------------------------------------------------------------------|
+| -Phadoop2   | Choose Hadoop2 based build dependencies (default configuration)    |
+| -Pspark2    | Choose Spark 2.x based build dependencies (default configuration)  |
+| -Pscala-2.11 | Choose Scala 2.11 based build dependencies (default configuration) |        
+| -Pscala-2.12 | Choose scala 2.12 based build dependencies                         |
+| -Pspark3 | Choose Spark 3.x based build dependencies |                                                

Review Comment:
   This sounds tough to do and very hard coded, Do we have an option like -Dhadoop.version=3.3.5 and have a way to provide them at runtime, if we can do that I think we are sorted



;24/Mar/23 18:08;githubbot;600","ksumit commented on code in PR #394:
URL: https://github.com/apache/incubator-livy/pull/394#discussion_r1149793135


##########
README.md:
##########
@@ -88,3 +88,13 @@ between different Spark versions.
 
 The Livy package itself does not contain a Spark distribution. It will work with any supported
 version of Spark without needing to rebuild.
+
+### Build Profiles
+
+| Flag        | Purpose                                                            |
+|-------------|--------------------------------------------------------------------|
+| -Phadoop2   | Choose Hadoop2 based build dependencies (default configuration)    |
+| -Pspark2    | Choose Spark 2.x based build dependencies (default configuration)  |
+| -Pscala-2.11 | Choose Scala 2.11 based build dependencies (default configuration) |        
+| -Pscala-2.12 | Choose scala 2.12 based build dependencies                         |
+| -Pspark3 | Choose Spark 3.x based build dependencies |                                                

Review Comment:
   It is hardcoded intentionally, getting different versions of hadoop, hive and spark to play with a version of livy and making it work across their dependency versions is a hard problem. you would notice that the functionality to pass and override [hadoop](https://github.com/apache/incubator-livy/blob/master/pom.xml#L82), [scala](https://github.com/apache/incubator-livy/blob/master/pom.xml#LL109-L111C57), [spark](https://github.com/apache/incubator-livy/blob/master/pom.xml#LL86-L86C63) and other versions is already available but we still need these profiles to resolve version incompatibilities in dependencies.



;27/Mar/23 21:08;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,LIVY-969,,,,,,,,,,,,,,,,,LIVY-423,LIVY-562,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,,Wed May 17 06:31:21 UTC 2023,,,,,,,,,,"0|z02bt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Jan/23 19:01;ksumit;I discovered few issues building livy against scala 2.12 so reopening. I will send out a PR that fixes the issue along with documentation on how to build with scala 2.12;;;","13/Mar/23 08:34;padang;[~ksumit] Shall scala 2.12 become defaultÂ  build after your PR ? Most fresh installs of Spark will pull Scala 2.12 onto the dev environment, from there the next step is to install Livy and would be simpler if it can work by default with what is already installed.;;;","14/Mar/23 00:40;ksumit;I'm open to ideas. From my experience in our internal dev environment, moving to scala 2.12 was significant. I was thinking the default build will still be using 2.11 but there would be option to build with 2.12 as well. The final distribution should contain both 2.11 and 2.12 artifacts.

[~lmccay], [~dacort] thoughts?;;;","14/Mar/23 09:45;padang;After researching further:

Â  Â - first set of users will be on Spark 2 (latest currently 2.4.8): it is based on Scala 2.11; build output of Spark says:Â 
exec: curl --progress-bar -L [https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz]

Â  Â - second set of users will be on Spark 3 (latest currently 3.2.2): it is based on Scala 2.12; build output of Spark says:Â 
exec: curl --silent --show-error -L [https://downloads.lightbend.com/scala/2.12.15/scala-2.12.15.tgz]

The most conservative / less error-prone option would thus be that 'mvn package' build both by default.

If for some reason we have to choose one version and cannot do both by default, then I'd use whatever '$SPARK_HOME/bin/spark-submit --version' returns (which also would be consistent with what sparkmagic does, as sparkmagic is I guess the most popular use case for livy-server it'd make sense they have both have the same strategy).Â 

Â ;;;","14/Mar/23 16:39;ksumit;Building both by default doesn't seem right because scala 2.11 and 2.12 are not compatible with each other and lead to issues. Ofcourse we can make things work somehow but it will lead to more ongoing support burden in the future. That said when pull requests are validated or new builds are created, we can have separate stages verifying that the supported combinations are never broken. Please see my attempt at this Jira at [[LIVY-593] Add flags to support hadoop, spark and scala version specific builds by ksumit Â· Pull Request #394 Â· apache/incubator-livy (github.com)|https://github.com/apache/incubator-livy/pull/394];;;","09/May/23 23:59;lmccay;Hi [~ksumit] - are we still targeting this for 0.8.0? Should we be considering this a blocker or can we move it out to 0.9.0?;;;","10/May/23 00:01;ksumit;Hi [~lmccay] I would like to put this into 0.8.0 Let me get this moving now that we have the pipelines revived.;;;","10/May/23 12:29;lmccay;[~ksumit]Â  - do you think we can get this solid within the next few days or a week tops?

Â ;;;","17/May/23 06:31;ksumit;This was completed as part of https://github.com/apache/incubator-livy/pull/405;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Proxy user cannot view its session log,LIVY-592,13230985,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,yihengw,zixu,zixu,30/Apr/19 19:15,22/Aug/19 02:44,19/Dec/25 04:15,22/Aug/19 02:43,,,,0.7.0,,Server,,,,,,,,,,1,,,,,,"Here is how to reproduce the issue.

--------------------------------------------------------------------------------------------

root@storage-0-0:~# kinit admin

Password for [admin@AZDATA.LOCAL|mailto:admin@AZDATA.LOCAL]:

Warning: Your password will expire in 41 days on Tue Jun 11 08:35:19 2019

root@storage-0-0:~#

root@storage-0-0:~# curl -k -X POST --negotiate -u : --data '\{""kind"": ""pyspark"", ""proxyUser"": ""admin""}' -H ""Content-Type: application/json"" 'https://gateway-0.azdata.local:8443/gateway/default/livy/v1/sessions'

{""id"":0,""name"":null,""appId"":null,""owner"":""knox"",""proxyUser"":""admin"",""state"":""starting"",""kind"":""pyspark"",""appInfo"":\{""driverLogUrl"":null,""sparkUiUrl"":null},""log"":[]}

Â 

root@storage-0-0:~# curl -k --negotiate -u : 'https://gateway-0.azdata.local:8443/gateway/default/livy/v1/sessions'

{""from"":0,""total"":2,""sessions"":[{""id"":0,""name"":null,""appId"":""application_1556613676830_0001"",""owner"":""knox"",""proxyUser"":""admin"",""state"":""starting"",""kind"":""pyspark"",""appInfo"":{""driverLogUrl"":""[http://storage-0-0.storage-0-svc.test.svc.cluster.local:8042/node/containerlogs/container_1556613676830_0001_01_000001/admin]"",""sparkUiUrl"":""[http://master-0.azdata.local:8088/proxy/application_1556613676830_0001/]""},""log"":[]},\{""id"":1,""name"":null,""appId"":null,""owner"":""knox"",""proxyUser"":""bob"",""state"":""starting"",""kind"":""pyspark"",""appInfo"":{""driverLogUrl"":null,""sparkUiUrl"":null},""log"":[]}]}

--------------------------------------------------------------------------------------------

From the result, you can see that the user admin can not view the log of its own session.Â ",Docker running on Kubernetes,"yiheng commented on pull request #202: [LIVY-592][Server] Allow proxy user to view or modify his session
URL: https://github.com/apache/incubator-livy/pull/202
 
 
   ## What changes were proposed in this pull request?
   This patch is ported from https://github.com/apache/incubator-livy/pull/172, with adding some unit tests.
   
   Allow proxy user to view or modify his session. Here is the link to the corresponding Jira
   https://issues.apache.org/jira/browse/LIVY-592
   
   ## How was this patch tested?
   New unit tests and existing unit tests.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 12:07;githubbot;600","jerryshao commented on pull request #202: [LIVY-592][Server] Allow proxy user to view or modify his session
URL: https://github.com/apache/incubator-livy/pull/202
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 02:41;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,LIVY-591,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 22 02:42:53 UTC 2019,,,,,,,,,,"0|z02ae8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/Apr/19 20:16;zixu;A pull request is sent

[https://github.com/apache/incubator-livy/pull/172];;;","22/Aug/19 02:42;jerryshao;Issue resolved by pull request 202
https://github.com/apache/incubator-livy/pull/202;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ACLs enforcement should occur on both session owner and proxy user,LIVY-591,13229929,,Improvement,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,ankur.gupta,ankur.gupta,24/Apr/19 17:32,22/Aug/19 02:44,19/Dec/25 04:15,22/Aug/19 02:44,0.6.0,,,,,Server,,,,,,,,,,0,,,,,,"Currently ACLs enforcement occurs only on session owner. So, a request is authorized if the request user is same as session owner or has correct ACLs configured.

Eg: https://github.com/apache/incubator-livy/blob/master/server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala#L70

In case of impersonation, proxy user is checked against session owner, instead he should be checked against session proxy. Otherwise, a proxy user who created the session will not be able to submit statements against it, if ACLs are not configured correctly.

Additionally, it seems there is no auth-check right now while creating a session. We should add that check as well (against modify-session acls).",,"ankuriitg commented on pull request #171: [LIVY-591] Changed ACLs enforcement to occur on proxy user, if available
URL: https://github.com/apache/incubator-livy/pull/171
 
 
   ## What changes were proposed in this pull request?
   ACLs enforcement occurs by checking whether the user requesting for a
   resource/access is owner of that resource or has sufficient permissions.
   
   This check used to occur against the session owner, even when a proxy user was
   specified during the creation of the resource. This can cause a issue where a
   proxy user can create a resource but can't access or modify it.
   
   The check has been modified to occur againt the session proxy user, defaulting
   to session owner if proxy user is not specified.
   
   ## How was this patch tested?
   Added a couple of unit tests
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Apr/19 20:56;githubbot;600","jerryshao commented on pull request #171: [LIVY-591] Changed ACLs enforcement to occur on proxy user, if available
URL: https://github.com/apache/incubator-livy/pull/171
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/19 02:44;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,LIVY-592,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2019-04-24 17:32:40.0,,,,,,,,,,"0|z023vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Guava dependency,LIVY-587,13226998,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,vanzin,vanzin,09/Apr/19 17:05,29/Jul/19 08:33,19/Dec/25 04:15,29/Jul/19 08:30,0.6.0,,,0.7.0,,Core,,,,,,,,,,0,,,,,,"It seems Guava has crept back into Livy at some point. Guava is kind of a pain to maintain and update. We should avoid using it, especially since it doesn't seem to be used for anything important.",,"runzhiwang commented on pull request #181: [LIVY-587] Remove unused guava dependency
URL: https://github.com/apache/incubator-livy/pull/181
 
 
   ## What changes were proposed in this pull request?
   
   Guava was unused any more, and it's too heavy to include, so remove the guava dependency.  
   
   ## How was this patch tested?
   
   Existing unit tests.
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 12:30;githubbot;600","jerryshao commented on pull request #181: [LIVY-587] Remove unused guava dependency
URL: https://github.com/apache/incubator-livy/pull/181
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/19 02:36;githubbot;600","runzhiwang commented on pull request #181: [LIVY-587] Remove unused guava dependency
URL: https://github.com/apache/incubator-livy/pull/181
 
 
   ## What changes were proposed in this pull request?
   
   Guava was unused any more, and it's too heavy to include, so remove the guava dependency.  
   
   ## How was this patch tested?
   
   Existing unit tests.
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/19 02:36;githubbot;600","jerryshao commented on pull request #181: [LIVY-587] Remove unused guava dependency
URL: https://github.com/apache/incubator-livy/pull/181
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/19 03:16;githubbot;600","runzhiwang commented on pull request #181: [LIVY-587] Remove unused guava dependency
URL: https://github.com/apache/incubator-livy/pull/181
 
 
   ## What changes were proposed in this pull request?
   
   Guava was unused any more, and it's too heavy to include, so remove the guava dependency.  
   
   ## How was this patch tested?
   
   Existing unit tests.
   
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/19 03:16;githubbot;600","jerryshao commented on pull request #181: [LIVY-587] Remove unused guava dependency
URL: https://github.com/apache/incubator-livy/pull/181
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Jul/19 08:30;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,LIVY-597,,,,LIVY-597,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 29 08:30:08 UTC 2019,,,,,,,,,,"0|z01lwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Jun/19 06:59;tison;Hi [~vanzin] from our current codebase it seems we don't use guava any more, just remains the dependency in the {{pom.xml}}. We can just remove the dependency.;;;","10/Jul/19 08:08;runzhiwang;HiÂ [~vanzin],Â [~Tison], Â [~arunmahadevan],Â  [livy-587|https://issues.apache.org/jira/browse/LIVY-587]Â conflicts with [livy-597|https://issues.apache.org/jira/browse/LIVY-597]Â  about guava. Because guava was not used by livy any more,Â  how about removing the dependency instead of upgrading guava?;;;","10/Jul/19 12:40;jerryshao;I would suggest to remove guava rather than upgrading guava version. Guava is too heavy to include, unless it is unavoidable, let's not include it unnecessarily.;;;","29/Jul/19 08:30;jerryshao;Issue resolved by pull request 181
[https://github.com/apache/incubator-livy/pull/181];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When a batch fails on startup, Livy continues to report the batch as ""starting"", even though it has failed",LIVY-586,13226394,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,sbrougher,sbrougher,05/Apr/19 16:03,30/Aug/19 08:04,19/Dec/25 04:15,30/Aug/19 08:04,0.5.0,,,0.7.0,,Batch,,,,,,,,,,0,,,,,,"When starting a Livy batch, I accidentally provided it a jar location in S3 that did not exist. Livy then continued to report that the job was ""starting"", even though it had clearly failed.

stdout:
{code:java}
2019-04-05 11:24:18,149 [main] WARN org.apache.hadoop.util.NativeCodeLoader [appName=] [jobId=] [clusterId=] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Warning: Skip remote jar s3://dev-dp-local/jars/develop-fix/ap5-app-transform-0.2-thread-pool-SNAPSHOT.jar.
2019-04-05 11:24:19,152 [main] INFO org.apache.hadoop.yarn.client.RMProxy [appName=] [jobId=] [clusterId=] - Connecting to ResourceManager at ip-10-25-30-127.dev.cainc.internal/10.25.30.127:8032
2019-04-05 11:24:19,453 [main] INFO org.apache.spark.deploy.yarn.Client [appName=] [jobId=] [clusterId=] - Requesting a new application from cluster with 6 NodeManagers
2019-04-05 11:24:19,532 [main] INFO org.apache.spark.deploy.yarn.Client [appName=] [jobId=] [clusterId=] - Verifying our application has not requested more than the maximum memory capability of the cluster (54272 MB per container)
2019-04-05 11:24:19,533 [main] INFO org.apache.spark.deploy.yarn.Client [appName=] [jobId=] [clusterId=] - Will allocate AM container, with 9011 MB memory including 819 MB overhead
2019-04-05 11:24:19,534 [main] INFO org.apache.spark.deploy.yarn.Client [appName=] [jobId=] [clusterId=] - Setting up container launch context for our AM
2019-04-05 11:24:19,537 [main] INFO org.apache.spark.deploy.yarn.Client [appName=] [jobId=] [clusterId=] - Setting up the launch environment for our AM container
2019-04-05 11:24:19,549 [main] INFO org.apache.spark.deploy.yarn.Client [appName=] [jobId=] [clusterId=] - Preparing resources for our AM container
2019-04-05 11:24:21,059 [main] WARN org.apache.spark.deploy.yarn.Client [appName=] [jobId=] [clusterId=] - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
2019-04-05 11:24:23,790 [main] INFO org.apache.spark.deploy.yarn.Client [appName=] [jobId=] [clusterId=] - Uploading resource file:/mnt/tmp/spark-b4e4a760-77a3-4554-a3f3-c3f82675d865/__spark_libs__3639879082942366045.zip -> hdfs://ip-10-25-30-127.dev.cainc.internal:8020/user/livy/.sparkStaging/application_1554234858331_0222/__spark_libs__3639879082942366045.zip
2019-04-05 11:24:26,817 [main] INFO org.apache.spark.deploy.yarn.Client [appName=] [jobId=] [clusterId=] - Uploading resource s3://dev-dp-local/jars/develop-fix/ap5-app-transform-0.2-thread-pool-SNAPSHOT.jar -> hdfs://ip-10-25-30-127.dev.cainc.internal:8020/user/livy/.sparkStaging/application_1554234858331_0222/ap5-app-transform-0.2-thread-pool-SNAPSHOT.jar
2019-04-05 11:24:26,940 [main] INFO org.apache.spark.deploy.yarn.Client [appName=] [jobId=] [clusterId=] - Deleted staging directory hdfs://ip-10-25-30-127.dev.cainc.internal:8020/user/livy/.sparkStaging/application_1554234858331_0222
Exception in thread ""main"" java.io.FileNotFoundException: No such file or directory 's3://dev-dp-local/jars/develop-fix/ap5-app-transform-0.2-thread-pool-SNAPSHOT.jar'
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:805)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:536)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:340)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:292)
	at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:356)
	at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:478)
	at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$10.apply(Client.scala:577)
	at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$10.apply(Client.scala:576)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:576)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:869)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:169)
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1152)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1520)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2019-04-05 11:24:26,964 [pool-1-thread-1] INFO org.apache.spark.util.ShutdownHookManager [appName=] [jobId=] [clusterId=] - Shutdown hook called
2019-04-05 11:24:26,965 [pool-1-thread-1] INFO org.apache.spark.util.ShutdownHookManager [appName=] [jobId=] [clusterId=] - Deleting directory /mnt/tmp/spark-aa8e8eff-ca2c-4358-a24f-19eb3863ef8f
2019-04-05 11:24:26,966 [pool-1-thread-1] INFO org.apache.spark.util.ShutdownHookManager [appName=] [jobId=] [clusterId=] - Deleting directory /mnt/tmp/spark-b4e4a760-77a3-4554-a3f3-c3f82675d865
{code}
stderr is empty

YARN Diagnostics eventually warns that the tag for the batch can't be found after 900 seconds.","AWS EMR, Livy submits batches to YARN in cluster mode","runzhiwang commented on pull request #215: [LIVY-586] Fix batch state from starting to dead when startup fail
URL: https://github.com/apache/incubator-livy/pull/215
 
 
   ## What changes were proposed in this pull request?
   
   Fix batch state from starting to dead when startup fail
   
   ## How was this patch tested?
   
   Create batch session with error parameter, the batch state will turn from starting to dead.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/19 09:16;githubbot;600","jerryshao commented on pull request #215: [LIVY-586] Fix batch state from starting to dead when startup fail
URL: https://github.com/apache/incubator-livy/pull/215
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 04:00;githubbot;600","runzhiwang commented on pull request #215: [LIVY-586] Fix batch state from starting to dead when startup fail
URL: https://github.com/apache/incubator-livy/pull/215
 
 
   ## What changes were proposed in this pull request?
   
   Fix batch state from starting to dead when startup fail
   
   ## How was this patch tested?
   
   Create batch session with error parameter, the batch state will turn from starting to dead.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 04:00;githubbot;600","runzhiwang commented on pull request #215: [LIVY-586] Fix batch state from starting to dead when startup fail
URL: https://github.com/apache/incubator-livy/pull/215
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 04:39;githubbot;600","runzhiwang commented on pull request #215: [LIVY-586] Fix batch state from starting to dead when startup fail
URL: https://github.com/apache/incubator-livy/pull/215
 
 
   ## What changes were proposed in this pull request?
   
   Fix batch state from starting to dead when startup fail
   
   ## How was this patch tested?
   
   Create batch session with error parameter, the batch state will turn from starting to dead.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 04:39;githubbot;600","runzhiwang commented on pull request #215: [LIVY-586] Fix batch state from starting to dead when startup fail
URL: https://github.com/apache/incubator-livy/pull/215
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 04:43;githubbot;600","runzhiwang commented on pull request #215: [LIVY-586] Fix batch state from starting to dead when startup fail
URL: https://github.com/apache/incubator-livy/pull/215
 
 
   ## What changes were proposed in this pull request?
   
   Fix batch state from starting to dead when startup fail
   
   ## How was this patch tested?
   
   Create batch session with error parameter, the batch state will turn from starting to dead.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 04:43;githubbot;600","jerryshao commented on pull request #215: [LIVY-586] Fix batch state from starting to dead when startup fail
URL: https://github.com/apache/incubator-livy/pull/215
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Aug/19 08:01;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4800,,,0,4800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 30 08:04:48 UTC 2019,,,,,,,,,,"0|z01i74:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Aug/19 03:37;runzhiwang;I'm working on it.;;;","30/Aug/19 08:04;jerryshao;Issue resolved by pull request 215
https://github.com/apache/incubator-livy/pull/215;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
python build needs configparser,LIVY-584,13225881,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,felixcheung,felixcheung,03/Apr/19 18:25,04/Apr/19 05:41,19/Dec/25 04:15,04/Apr/19 05:41,0.6.0,,,,,,,,,,,,,,,0,,,,,,"pipÂ install configparser Â just to build. it won't build until I manually pip install.

Â 

(run into this in my 2nd environment)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2019-04-03 18:25:27.0,,,,,,,,,,"0|z01fbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
python test_create_new_session_without_default_config test fails consistently,LIVY-582,13225879,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,yihengw,felixcheung,felixcheung,03/Apr/19 18:15,16/Jul/19 02:23,19/Dec/25 04:15,16/Jul/19 02:23,0.6.0,,,0.7.0,,,,,,,,,,,,0,,,,,,"{code:java}
test_create_new_session_without_default_config ________________

def test_create_new_session_without_default_config():
> mock_and_validate_create_new_session(False)

src/test/python/livy-tests/client_test.py:105:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
<string>:3: in wrapper
???
src/test/python/livy-tests/client_test.py:48: in mock_and_validate_create_new_session
load_defaults=defaults)
src/main/python/livy/client.py:88: in __init__
session_conf_dict).json()['id']
src/main/python/livy/client.py:388: in _create_new_session
headers=self._conn._JSON_HEADERS, data=data)
src/main/python/livy/client.py:500: in send_request
json=data, auth=self._spnego_auth())
.eggs/requests-2.21.0-py2.7.egg/requests/api.py:60: in request
return session.request(method=method, url=url, **kwargs)
.eggs/requests-2.21.0-py2.7.egg/requests/sessions.py:533: in request
resp = self.send(prep, **send_kwargs)
.eggs/requests-2.21.0-py2.7.egg/requests/sessions.py:646: in send
r = adapter.send(request, **kwargs)
.eggs/responses-0.10.6-py2.7.egg/responses.py:626: in unbound_on_send
return self._on_request(adapter, request, *a, **kwargs)

self = <responses.RequestsMock object at 0x10ce93490>
adapter = <requests.adapters.HTTPAdapter object at 0x10c9b31d0>
request = <PreparedRequest [POST]>
kwargs = {'cert': None, 'proxies': OrderedDict(), 'stream': False, 'timeout': 10, ...}
match = None, resp_callback = None
error_msg = ""Connection refused by Responses: POST http://machine:8998/sessions/ doesn't match Responses Mock""
response = ConnectionError(u""Connection refused by Responses: POST http://machine:8998/sessions/doesn't match Responses Mock"",)
{code}
Not sure why. this fails 100% and I don't see anything listening to this port. Need some help to troubleshoot this.",,"yiheng commented on pull request #180: [LIVY-582][Tests] Hostname in python-api test should be lower case to avoid test failures
URL: https://github.com/apache/incubator-livy/pull/180
 
 
   ## What changes were proposed in this pull request?
   In the python-API test code, when returning a mocked response, the mock lib will compare the URL with the predefined URLs case sensitive. However, the `Request` lib used in the Livy python API will change the URL to lower case. This will cause test failures on a machine with an upper case hostname.
   
   This patch turns the hostname in python-API test code into the lower case to avoid such test failures.
   
   ## How was this patch tested?
   Existing test. Run test specifically on a machine with an upper case hostname.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jul/19 07:06;githubbot;600","jerryshao commented on pull request #180: [LIVY-582][Tests] Hostname in python-api test should be lower case to avoid test failures
URL: https://github.com/apache/incubator-livy/pull/180
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jul/19 02:23;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 16 02:23:24 UTC 2019,,,,,,,,,,"0|z01faw:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Jul/19 12:40;yihengw;Just change the upper case charactersÂ in your hostname into lower case and the issueÂ will go.:P

The root cause is the python Request lib will change the request url to lower case, while the mock lib is case sensitive...;;;","16/Jul/19 02:23;jerryshao;Issue resolved by pull request 180
[https://github.com/apache/incubator-livy/pull/180];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unknown YARN state RUNNING for app with final status SUCCEEDED,LIVY-576,13223945,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,shanyu,shanyu,26/Mar/19 03:58,09/Mar/20 18:34,19/Dec/25 04:15,09/Mar/20 18:26,0.5.0,,,0.7.0,,Server,,,,,,,,,,0,,,,,,"Livy with Spark 2.3/2.4 on Yarn 2.9, there is a chance forÂ Yarn to return application reports with Yarn state RUNNING and final Yarn status SUCCEEDED, this means the Yarn application is finishing up and about to be successful. Livy's mapYarnSate() method does not have a valid mapping for this combination and therefore it render the session 'dead'.

I saw this in Livy server log:

19/03/25 20:04:28 ERROR utils.SparkYarnApp: Unknown YARN state RUNNING for app application_1553542555261_0063 with final status SUCCEEDED.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-642,,,,,,,,"26/Mar/19 04:04;shanyu;LIVY-576.patch;https://issues.apache.org/jira/secure/attachment/12963687/LIVY-576.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 09 18:34:17 UTC 2020,,,,,,,,,,"0|z013g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Feb/20 10:04;andrasbeni;I believe this is already solved as LIVY-642.;;;","24/Feb/20 22:22;shanyu;Thanks [~andrasbeni], yes LIVY-642 is basically a duplicate of this JIRA with exactly the same fix. We can close this JIRA now that LIVY-642 is checked in.

On the other hand, [~jerryshao], maybe we should be inspecting the JIRAs more often. This JIRA has been sitting there for almost 1 year now without anyone reviewing it. And a duplicate JIRA was reviewed and checked in after about 6 months.;;;","09/Mar/20 18:34;wypoon;I agree with [~shanyu] that it would be helpful if attention is paid to JIRAs when a fix is submitted. In this case, I see that a patch file is uploaded but I don't see a PR. Maybe that is why the fix was neglected?
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests for metadata operations,LIVY-574,13223489,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,yihengw,mgaido,mgaido,23/Mar/19 10:58,25/Aug/19 07:48,19/Dec/25 04:15,25/Aug/19 07:48,,,,0.7.0,,Tests,Thriftserver,,,,,,,,,0,,,,,,We do not have tests for metadata operations. We should add them at least for the ones which we have currently implemented.,,"yiheng commented on pull request #197: [LIVY-574][THRIFT SERVER][TEST] Add tests for metadata operations
URL: https://github.com/apache/incubator-livy/pull/197
 
 
   ## What changes were proposed in this pull request?
   Add unit test for existing meta operation: GetCatalogsOperation/GetTableTypesOperation/GetTypeInfoOperation.
   
   We also fix issues we met.
   
   ## How was this patch tested?
   Add new unit tests and existing test.
   
   We also use SquirrelSQL test relate operations.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/19 09:16;githubbot;600","yiheng commented on pull request #197: [LIVY-574][THRIFT SERVER][TEST] Add tests for metadata operations
URL: https://github.com/apache/incubator-livy/pull/197
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 11:36;githubbot;600","yiheng commented on pull request #197: [LIVY-574][THRIFT SERVER][TEST] Add tests for metadata operations
URL: https://github.com/apache/incubator-livy/pull/197
 
 
   ## What changes were proposed in this pull request?
   Add unit test for existing meta operation: GetCatalogsOperation/GetTableTypesOperation/GetTypeInfoOperation.
   
   We also fix issues we met.
   
   ## How was this patch tested?
   Add new unit tests and existing test.
   
   We also use SquirrelSQL test relate operations.
   ![image](https://user-images.githubusercontent.com/1297418/62930007-26a93400-bdee-11e9-9364-259308724db6.png)
   
   ![image](https://user-images.githubusercontent.com/1297418/62930073-42143f00-bdee-11e9-9409-62d07ad0eabd.png)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 11:36;githubbot;600","mgaido91 commented on pull request #197: [LIVY-574][TESTS][THRIFT] Add tests for metadata operations
URL: https://github.com/apache/incubator-livy/pull/197
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Aug/19 07:47;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Aug 25 07:48:49 UTC 2019,,,,,,,,,,"0|z010o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Aug/19 05:39;yihengw;I found some errors relate with existing meta operation when I test it with squirrel-SQL. I would like to take this one and fix these issues.;;;","25/Aug/19 07:48;mgaido;Issue resolved by PR https://github.com/apache/incubator-livy/pull/197.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests for operation logs retrieval,LIVY-573,13223488,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,yihengw,mgaido,mgaido,23/Mar/19 10:56,19/Aug/19 17:24,19/Dec/25 04:15,19/Aug/19 17:24,,,,0.7.0,,Tests,Thriftserver,,,,,,,,,0,,,,,,Our current tests do not cover the retrieval of operation logs. We should try and add coverage for it if possible.,,"yiheng commented on pull request #199: [LIVY-573] Add tests for operation logs retrieval
URL: https://github.com/apache/incubator-livy/pull/199
 
 
   ## What changes were proposed in this pull request?
   Add unit test for operation logs retrieval.
   
   ## How was this patch tested?
   New unit tests and existing unit test
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Aug/19 10:19;githubbot;600","yiheng commented on pull request #199: [LIVY-573] Add tests for operation logs retrieval
URL: https://github.com/apache/incubator-livy/pull/199
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 05:43;githubbot;600","yiheng commented on pull request #199: [LIVY-573] Add tests for operation logs retrieval
URL: https://github.com/apache/incubator-livy/pull/199
 
 
   ## What changes were proposed in this pull request?
   Add unit test for operation logs retrieval.
   
   ## How was this patch tested?
   New unit tests and existing unit test
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 05:43;githubbot;600","yiheng commented on pull request #199: [LIVY-573][TESTS][THRIFT] Add tests for operation logs retrieval
URL: https://github.com/apache/incubator-livy/pull/199
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 15:25;githubbot;600","yiheng commented on pull request #199: [LIVY-573][TESTS][THRIFT] Add tests for operation logs retrieval
URL: https://github.com/apache/incubator-livy/pull/199
 
 
   ## What changes were proposed in this pull request?
   Add unit test for operation logs retrieval.
   
   ## How was this patch tested?
   New unit tests and existing unit test
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 15:25;githubbot;600","mgaido91 commented on pull request #199: [LIVY-573][TESTS][THRIFT] Add tests for operation logs retrieval
URL: https://github.com/apache/incubator-livy/pull/199
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/19 17:21;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 19 17:24:22 UTC 2019,,,,,,,,,,"0|z010ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Aug/19 11:48;yihengw;Have created a patch. [~mgaido] can you please help review it? Thanks;;;","19/Aug/19 17:24;mgaido;Issue resolved by PR https://github.com/apache/incubator-livy/pull/199.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log operations fails because of dependencies on Spark,LIVY-572,13223483,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,mgaido,mgaido,mgaido,23/Mar/19 09:57,27/Mar/19 00:43,19/Dec/25 04:15,27/Mar/19 00:43,0.6.0,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,"When {{livy.server.thrift.logging.operation.enabled}} is {{true}}, which is by default, all queries run through beeline fail due to:

{code}
19/03/23 10:56:35 INFO ThriftBinaryCLIService: Closing the session: SessionHandle [eaf96260-ef2b-438e-9d12-3812f1095201]
Exception in thread ""LivyThriftserver-Handler-Pool: Thread-30"" java.lang.NoClassDefFoundError: org/apache/spark/sql/Row
	at org.apache.livy.thriftserver.session.ColumnBuffer.toHiveString(ColumnBuffer.java:296)
	at org.apache.livy.thriftserver.session.ColumnBuffer.add(ColumnBuffer.java:183)
	at org.apache.livy.thriftserver.serde.ColumnOrientedResultSet.addRow(ThriftResultSet.scala:108)
	at org.apache.livy.thriftserver.LivyOperationManager$$anonfun$getOperationLogRowSet$2.apply(LivyOperationManager.scala:125)
	at org.apache.livy.thriftserver.LivyOperationManager$$anonfun$getOperationLogRowSet$2.apply(LivyOperationManager.scala:124)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)
	at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)
	at org.apache.livy.thriftserver.LivyOperationManager.getOperationLogRowSet(LivyOperationManager.scala:124)
	at org.apache.livy.thriftserver.LivyOperationManager.fetchResults(LivyOperationManager.scala:229)
	at org.apache.livy.thriftserver.LivyCLIService.fetchResults(LivyCLIService.scala:353)
	at org.apache.livy.thriftserver.cli.ThriftCLIService.FetchResults(ThriftCLIService.scala:609)
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1837)
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1822)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.Row
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 21 more
{code}",,"mgaido91 commented on pull request #162: [LIVY-572] Avoid usage of spark classes in ColumnBuffer
URL: https://github.com/apache/incubator-livy/pull/162
 
 
   ## What changes were proposed in this pull request?
   
   The `ColumnBuffers` can be created both inside spark jobs and in the Livy server. The latter case happens when operation logs are returned and  fails before this patch because we are using Spark classes in this code after the refactor in LIVY-503. Unfortunately, we do not have test coverage for operation logs retrieval and this is the reason why this wasn't spot out earlier.
   
   Since operation logs are retrieved by beeline for each query, this means that every query run through beeline fails, unless `livy.server.thrift.logging.operation.enabled` is set to `false`.
   
   ## How was this patch tested?
   
   manual tests using beeline
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Mar/19 12:46;githubbot;600","asfgit commented on pull request #162: [LIVY-572] Avoid usage of spark classes in ColumnBuffer
URL: https://github.com/apache/incubator-livy/pull/162
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 00:43;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 27 00:43:34 UTC 2019,,,,,,,,,,"0|z010mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Mar/19 00:43;vanzin;Issue resolved by pull request 162
[https://github.com/apache/incubator-livy/pull/162];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When an exception happens running a query, we should report it to end user",LIVY-571,13223482,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,xilangyan,mgaido,mgaido,23/Mar/19 09:45,23/Jul/19 08:34,19/Dec/25 04:15,22/Jul/19 11:10,,,,0.7.0,,Thriftserver,,,,,,,,,,0,,,,,,"When a query fails with an exception on livy thriftserver, instead of reporting this exception to the end user, a meaningless one is reported. Eg, with Hive support not enabled on spark, the following query causes:

{code}
0: jdbc:hive2://localhost:10090/> create table test as select a.* from (select 1, ""2"") a;
Error: java.lang.RuntimeException: java.util.NoSuchElementException: Statement 820bb5c2-018b-46ea-9b7f-b0e3b9c31c46 not found in session acf3712b-1f08-4111-950f-559fc3f3f10c.
org.apache.livy.thriftserver.session.ThriftSessionState.statementNotFound(ThriftSessionState.java:118)
org.apache.livy.thriftserver.session.ThriftSessionState.cleanupStatement(ThriftSessionState.java:107)
org.apache.livy.thriftserver.session.CleanupStatementJob.call(CleanupStatementJob.java:43)
org.apache.livy.thriftserver.session.CleanupStatementJob.call(CleanupStatementJob.java:26)
org.apache.livy.rsc.driver.JobWrapper.call(JobWrapper.java:64)
org.apache.livy.rsc.driver.JobWrapper.call(JobWrapper.java:31)
java.util.concurrent.FutureTask.run(FutureTask.java:266)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
java.lang.Thread.run(Thread.java:748)
{code}

Looking at the logs, of course the real problem is:
{code}
19/03/23 10:40:32 ERROR LivyExecuteStatementOperation: Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.spark.sql.AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);;
'CreateTable `test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists
+- Project [1#1, 2#2]
   +- SubqueryAlias `a`
      +- Project [1 AS 1#1, 2 AS 2#2]
         +- OneRowRelation

org.apache.spark.sql.execution.datasources.HiveOnlyCheck$$anonfun$apply$12.apply(rules.scala:392)
org.apache.spark.sql.execution.datasources.HiveOnlyCheck$$anonfun$apply$12.apply(rules.scala:390)
org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:117)
org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:390)
org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:388)
org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$2.apply(CheckAnalysis.scala:386)
org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$2.apply(CheckAnalysis.scala:386)
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:386)
org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)
org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)
org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)
org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
org.apache.livy.thriftserver.session.SqlJob.executeSql(SqlJob.java:74)
org.apache.livy.thriftserver.session.SqlJob.call(SqlJob.java:64)
org.apache.livy.thriftserver.session.SqlJob.call(SqlJob.java:35)
org.apache.livy.rsc.driver.JobWrapper.call(JobWrapper.java:64)
org.apache.livy.rsc.driver.JobWrapper.call(JobWrapper.java:31)
java.util.concurrent.FutureTask.run(FutureTask.java:266)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
java.lang.Thread.run(Thread.java:748)
	at org.apache.livy.thriftserver.LivyExecuteStatementOperation.execute(LivyExecuteStatementOperation.scala:147)
	at org.apache.livy.thriftserver.LivyExecuteStatementOperation$$anon$1$$anon$2.run(LivyExecuteStatementOperation.scala:97)
	at org.apache.livy.thriftserver.LivyExecuteStatementOperation$$anon$1$$anon$2.run(LivyExecuteStatementOperation.scala:94)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.livy.thriftserver.LivyExecuteStatementOperation$$anon$1.run(LivyExecuteStatementOperation.scala:107)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
 ....
{code}

And this should be reported to the end user.",,"yantzu commented on pull request #182: [LIVY-571] cleanupStatement should not throw exception when statementId not exist
URL: https://github.com/apache/incubator-livy/pull/182
 
 
   ## What changes were proposed in this pull request?
   
   To fix LIVY-571. ThriftSessionState is used to store query result by statementId. When exception during execute query, no query result is stored. But when statement closed from beeline, a request is invoked to remove cached query result in ThriftSessionState.
   
   Remove statement query result from ThriftSessionState should just return if no item found, this could be the default behavior to avoid relevant issue.
   
   https://issues.apache.org/jira/browse/LIVY-571
   
   ## How was this patch tested?
   
   Existing unit tests.
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jul/19 08:17;githubbot;600","mgaido91 commented on pull request #182: [LIVY-571] cleanupStatement should not throw exception when statementId not exist
URL: https://github.com/apache/incubator-livy/pull/182
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jul/19 10:15;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 23 08:34:14 UTC 2019,,,,,,,,,,"0|z010mg:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/Jul/19 08:34;mgaido;Issue resolved by PR https://github.com/apache/incubator-livy/pull/182.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace test jar file in source repo with auto-generated file,LIVY-570,13222426,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Fixed,vanzin,vanzin,vanzin,18/Mar/19 22:06,18/Mar/19 23:38,19/Dec/25 04:15,18/Mar/19 23:38,0.6,,,0.6.0,,Tests,,,,,,,,,,0,,,,,,"We can't have jar files in source distributions, so {{python-api/src/test/python/livy-tests/resources/jar_file.jar}} has to go.",,"vanzin commented on pull request #159: [LIVY-570] Replace test archives with auto-generated files.
URL: https://github.com/apache/incubator-livy/pull/159
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/19 22:29;githubbot;600","asfgit commented on pull request #159: [LIVY-570] Replace test archives with auto-generated files.
URL: https://github.com/apache/incubator-livy/pull/159
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/19 23:38;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 18 23:38:44 UTC 2019,,,,,,,,,,"0|z00u48:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Mar/19 23:38;vanzin;Issue resolved by pull request 159
[https://github.com/apache/incubator-livy/pull/159];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade jackson version from 2.9.5 to 2.9.8,LIVY-568,13221133,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,gyogal,gyogal,gyogal,12/Mar/19 13:57,12/Mar/19 23:58,19/Dec/25 04:15,12/Mar/19 15:49,0.5.0,0.6.0,,0.6.0,,Build,,,,,,,,,,0,,,,,,Due to security issues of jackson databind module ([https://github.com/FasterXML/jackson-databind/issues/2186]) proposing to upgrade jackson version to 2.9.8.,,"gyogal commented on pull request #153: [LIVY-568][BUILD] Upgrade jackson version to 2.9.8
URL: https://github.com/apache/incubator-livy/pull/153
 
 
   ## What changes were proposed in this pull request?
   
   Due to security issues of jackson databind module (https://github.com/FasterXML/jackson-databind/issues/2186) proposing to upgrade jackson version to 2.9.8.
   
   ## How was this patch tested?
   
   Existing UTs.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Mar/19 14:02;githubbot;600","asfgit commented on pull request #153: [LIVY-568][BUILD] Upgrade jackson version to 2.9.8
URL: https://github.com/apache/incubator-livy/pull/153
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Mar/19 15:49;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 12 15:49:31 UTC 2019,,,,,,,,,,"0|z00m4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Mar/19 15:49;vanzin;Issue resolved by pull request 153
[https://github.com/apache/incubator-livy/pull/153];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When cancel a sql job through livy, actually the sparkjob still running ",LIVY-566,13220614,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,holmes.infra,holmes.infra,09/Mar/19 10:52,15/Mar/19 15:45,19/Dec/25 04:15,15/Mar/19 15:45,,,,0.6.0,,Server,,,,,,,,,,0,,,,,,"The cancel methodÂ  in LivyExecuteStatementOperation only clean the job's statement status in spark driverï¼Œactually the job is still running.The first causeÂ  is that there is no method to call driver to cancel sqlJob,the second is the statementId in sqlJob will cover the jobId of JobWrapper so that sqlJob can not be canceled.",,"holmes-infra commented on pull request #150: [LIVY-566] When cancel a sql job through livy, actually the sparkjob still running
URL: https://github.com/apache/incubator-livy/pull/150
 
 
   ## What changes were proposed in this pull request?
   https://issues.apache.org/jira/browse/LIVY-566
   
   (Please fill in changes proposed in this fix)
   (Include a link to the associated JIRA and make sure to add a link to this pr on the JIRA as well)
   
   ## How was this patch tested?
   
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/19 12:38;githubbot;600","holmes-infra commented on pull request #150: [LIVY-566] When cancel a sql job through livy, actually the sparkjob still running
URL: https://github.com/apache/incubator-livy/pull/150
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/19 14:58;githubbot;600","holmes-infra commented on pull request #151: [LIVY-566] When cancel a sql job through livy, actually the sparkjob still running 
URL: https://github.com/apache/incubator-livy/pull/151
 
 
   ## What changes were proposed in this pull request?
   https://issues.apache.org/jira/browse/LIVY-566
   
   (Please fill in changes proposed in this fix)
   (Include a link to the associated JIRA and make sure to add a link to this pr on the JIRA as well)
   
   ## How was this patch tested?
   
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/19 15:34;githubbot;600","vanzin commented on pull request #151: [LIVY-566] When cancel a sql job through livy, actually the sparkjob still running 
URL: https://github.com/apache/incubator-livy/pull/151
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Mar/19 19:23;githubbot;600","vanzin commented on pull request #156: [LIVY-566] Cancel Spark jobs related to statement on cancellation.
URL: https://github.com/apache/incubator-livy/pull/156
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Mar/19 19:56;githubbot;600","asfgit commented on pull request #156: [LIVY-566] Cancel Spark jobs related to statement on cancellation.
URL: https://github.com/apache/incubator-livy/pull/156
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Mar/19 15:45;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 15 15:45:40 UTC 2019,,,,,,,,,,"0|z00iy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,"15/Mar/19 15:45;vanzin;Issue resolved by pull request 156
[https://github.com/apache/incubator-livy/pull/156];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug that sql query row count is less than the actual row count of the table,LIVY-565,13220605,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,holmes.infra,holmes.infra,09/Mar/19 09:18,18/Mar/19 16:05,19/Dec/25 04:15,18/Mar/19 16:05,,,,0.6.0,,Server,,,,,,,,,,0,,,,,,"For example, there is a table with two columns ,which has two hundred rows ,and one of the column's data is all null .When the maxRows of sql query is 200 ,it will only return 100 rows.The cause is that the ColumnBuffer array ,only expands automatically when the inserted data size exceeds 100 , but does not expand if the one of column data is all null.",,"holmes-infra commented on pull request #149: [LIVY-565] Fix the bug that sql query row count is less than the actual row count of the table
URL: https://github.com/apache/incubator-livy/pull/149
 
 
   For example, there is a table with two columns ,which has two hundred rows ,and one of the column's data is all null .When the maxRows of sql query is 200 ,it will only return 100 rows.The cause is that the ColumnBuffer array ,only expands automatically when the inserted data size exceeds 100 , but does not expand if the one of column data is all null.
   
   https://issues.apache.org/jira/browse/LIVY-565
   
   ## What changes were proposed in this pull request?
   
   (Please fill in changes proposed in this fix)
   (Include a link to the associated JIRA and make sure to add a link to this pr on the JIRA as well)
   
   ## How was this patch tested?
   
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/19 09:26;githubbot;600","vanzin commented on pull request #149: [LIVY-565] Fix the bug that sql query row count is less than the actual row count of the table at some condition
URL: https://github.com/apache/incubator-livy/pull/149
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Mar/19 16:41;githubbot;600","vanzin commented on pull request #155: [LIVY-565] Ensure column buffer for string and binary contains all data.
URL: https://github.com/apache/incubator-livy/pull/155
 
 
   The code was failing to account for null values at the tail of the column,
   and could return a buffer that was too short for the number of rows being
   returned.
   
   Also added unit tests to verify the fix.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Mar/19 19:15;githubbot;600","asfgit commented on pull request #155: [LIVY-565] Ensure column buffer for string and binary returns all data.
URL: https://github.com/apache/incubator-livy/pull/155
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/19 16:04;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 18 16:05:02 UTC 2019,,,,,,,,,,"0|z00iw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Mar/19 09:29;holmes.infra;Submitted a patch for this issue https://github.com/apache/incubator-livy/pull/149;;;","18/Mar/19 16:05;vanzin;Issue resolved by pull request 155
[https://github.com/apache/incubator-livy/pull/155];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support to check the batch job running progress,LIVY-564,13220595,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,,holmes.infra,holmes.infra,09/Mar/19 06:50,20/Mar/19 18:08,19/Dec/25 04:15,20/Mar/19 18:08,0.5.0,,,,,Server,,,,,,,,,,0,,,,,,"When submitting a task,it's better to be able to view the batch job's progress",,"holmes-infra commented on pull request #148: [LIVY-564] Support to check the spark batch job running progress
URL: https://github.com/apache/incubator-livy/pull/148
 
 
   ## What changes were proposed in this pull request?
   When submitting a task,it's better to be able to view the batch job's progress
   https://issues.apache.org/jira/browse/LIVY-564
   
   (Please fill in changes proposed in this fix)
   (Include a link to the associated JIRA and make sure to add a link to this pr on the JIRA as well)
   
   ## How was this patch tested?
   
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Mar/19 06:53;githubbot;600","vanzin commented on pull request #148: [LIVY-564] Support to check the spark batch job running progress
URL: https://github.com/apache/incubator-livy/pull/148
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Mar/19 18:08;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 20 18:08:40 UTC 2019,,,,,,,,,,"0|z00its:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Mar/19 06:54;holmes.infra;Submitted a patch for this issueÂ https://github.com/apache/incubator-livy/pull/148;;;","20/Mar/19 18:08;vanzin;Spark doesn't publish this information to YARN, so there's no point in Livy exposing it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RSCConf take no effect in RSCDriver ,LIVY-563,13220453,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,holmes.infra,holmes.infra,08/Mar/19 12:40,11/Apr/19 16:30,19/Dec/25 04:15,11/Apr/19 16:30,0.5.0,,,0.7.0,,Server,,,,,,,,,,0,,,,,,"When create InterActiveSessin, prepareBuilderProp did not add all RSCConf(livy.rsc.)Â  to BuilderProp. AfterÂ RSCServer started, the livy.rsc.*Â related configure which modified by userÂ  take no effect. For example,Â livy.rsc.sql.num-rows and livy.rsc.rpc.max.size and so on

Â ",,"vanzin commented on pull request #168: [LIVY-563] Propagate RSC configuration when creating sessions.
URL: https://github.com/apache/incubator-livy/pull/168
 
 
   Even though not all RSC configs apply to the remote driver, a few do,
   so propagate all of them when starting a new session.
   
   Includes new unit test.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/19 19:17;githubbot;600","vanzin commented on pull request #147: [LIVY-563] Merge RSCConf to LivyConf when prepareBuilderProp
URL: https://github.com/apache/incubator-livy/pull/147
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Apr/19 19:23;githubbot;600","asfgit commented on pull request #168: [LIVY-563] Propagate RSC configuration when creating sessions.
URL: https://github.com/apache/incubator-livy/pull/168
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Apr/19 16:30;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,,,Thu Apr 11 16:30:09 UTC 2019,,,,,,,,,,"0|z00hy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,"08/Mar/19 12:51;holmes.infra;Submitted a patch for this issueÂ https://github.com/apache/incubator-livy/pull/147;;;","11/Apr/19 16:30;vanzin;Issue resolved by pull request 168
[https://github.com/apache/incubator-livy/pull/168];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support of scala 2.12,LIVY-562,13219113,,Improvement,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,tradunskih,tradunskih,02/Mar/19 21:03,28/Jan/23 19:01,19/Dec/25 04:15,02/Jul/20 07:52,,,,0.8.0,,,,,,,,,,,,2,,,,,,"I know that adding support of a new scala version is something that can be time consuming and would require some testing phase, but it would be great at least to have this support in integration-test module (no scala suffix supported there, yet).

We can use livy with spark 2.4.0 with scala 2.12 and livy java client, but we cannot use integration-test module for testing because it fails on scala compatibility issues, such asÂ 

{code:java}

java.lang.NoSuchMethodError: scala.Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps;
 at org.apache.livy.test.framework.MiniCluster.<init>(MiniCluster.scala:211)
 at org.apache.livy.test.framework.Cluster$.liftedTree1$1(Cluster.scala:102)
 at org.apache.livy.test.framework.Cluster$.cluster$lzycompute(Cluster.scala:100)
 at org.apache.livy.test.framework.Cluster$.cluster(Cluster.scala:98)
 at org.apache.livy.test.framework.Cluster$.get(Cluster.scala:125)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-593,,LIVY-423,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2019-03-02 21:03:20.0,,,,,,,,,,"0|z009qo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Travis-CI logs less verbose,LIVY-557,13214769,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,meisam,meisam,meisam,09/Feb/19 03:41,15/Feb/19 23:17,19/Dec/25 04:15,15/Feb/19 23:17,0.6.0,,,0.6.0,,Build,,,,,,,,,,0,,,,,,"Â Each build on travis generates 10K+ lines of log. Should we make build commands less verbose by passing --quiet to them?
 Â 
 As an example, apt-get installs and pip installs generate 3K+ lines on their own. Maven generates another 6K+ lines of log, but I am not sure if silencing maven is a good idea. Passing --quiet to Maven silences scalac warnings.",,"meisam commented on pull request #144: [LIVY-557] Make Travis-CI logs less verbose
URL: https://github.com/apache/incubator-livy/pull/144
 
 
   
   Task-url: https://issues.apache.org/jira/browse/LIVY-557
   
   ## What changes were proposed in this pull request?
   - Pass `-q` or `--quiet` to the build commands in `.travis.yml` file.
   - Set the SLF4J log level to warn for maven.
   
   ## How was this patch tested?
   Testing on Travis.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Feb/19 03:55;githubbot;600","meisam commented on pull request #144: [LIVY-557] Make Travis-CI logs less verbose
URL: https://github.com/apache/incubator-livy/pull/144
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Feb/19 08:08;githubbot;600","meisam commented on pull request #144: [LIVY-557] Make Travis-CI logs less verbose
URL: https://github.com/apache/incubator-livy/pull/144
 
 
   ## What changes were proposed in this pull request?
   - Pass `-q` or `--quiet` to the build commands in `.travis.yml` file.
   - Set the SLF4J log level to warn for maven.
   
   ## How was this patch tested?
   Testing on Travis.
   
   Task-url: https://issues.apache.org/jira/browse/LIVY-557
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Feb/19 08:09;githubbot;600","meisam commented on pull request #144: [LIVY-557] Make Travis-CI logs less verbose
URL: https://github.com/apache/incubator-livy/pull/144
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Feb/19 12:28;githubbot;600","meisam commented on pull request #144: [LIVY-557] Make Travis-CI logs less verbose
URL: https://github.com/apache/incubator-livy/pull/144
 
 
   ## What changes were proposed in this pull request?
   - Pass `-q` or `--quiet` to the build commands in `.travis.yml` file.
   - Set the SLF4J log level to warn for maven.
   
   ## How was this patch tested?
   Testing on Travis.
   
   Task-url: https://issues.apache.org/jira/browse/LIVY-557
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Feb/19 12:28;githubbot;600","asfgit commented on pull request #144: [LIVY-557] Make Travis-CI logs less verbose
URL: https://github.com/apache/incubator-livy/pull/144
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Feb/19 23:17;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 15 23:17:06 UTC 2019,,,,,,,,,,"0|yi0teo:",9223372036854775807,,,,,,,,,,,,,,,,,,,"15/Feb/19 23:17;vanzin;Issue resolved by pull request 144
[https://github.com/apache/incubator-livy/pull/144];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HearbeatExpired is not stubbed correctly in test cases,LIVY-556,13214492,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,meisam,meisam,meisam,07/Feb/19 23:47,15/Feb/19 23:19,19/Dec/25 04:15,15/Feb/19 23:19,0.6.0,,,0.6.0,,Server,Tests,,,,,,,,14/Feb/19 00:00,0,,,,,,"{{SessionHearbeatWatchdog.deleteExpiredSesions}} is called nondeterministically.
 Most of the time, the test cases finish without giving {{deleteExpiredSesions}}Â a chance to run. But occasionally {{deleteExpiredSesions}}Â is called before the test cases finishes, which causes the following error message:
{code:java}
Exception in thread ""HeartbeatWatchdog-org.apache.livy.sessions.InteractiveSessionManager"" org.mockito.exceptions.misusing.WrongTypeOfReturnValue: 
Idle$ cannot be returned by heartbeatExpired()
heartbeatExpired() should return boolean
***
If you're unsure why you're getting above error read on.
Due to the nature of the syntax above problem might occur because:
1. This exception *might* occur in wrongly written multi-threaded tests.
   Please refer to Mockito FAQ on limitations of concurrency testing.
2. A spy is stubbed using when(spy.foo()).then() syntax. It is safer to stub spies - 
   - with doReturn|Throw() family of methods. More in javadocs for Mockito.spy() method.

        at org.apache.livy.server.interactive.SessionHeartbeatWatchdog$$anonfun$deleteExpiredSessions$1.apply(SessionHeartbeat.scala:111)
        at org.apache.livy.server.interactive.SessionHeartbeatWatchdog$$anonfun$deleteExpiredSessions$1.apply(SessionHeartbeat.scala:110)
        at scala.collection.Iterator$class.foreach(Iterator.scala:891)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
        at scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:206)
        at org.apache.livy.server.interactive.SessionHeartbeatWatchdog$class.deleteExpiredSessions(SessionHeartbeat.scala:110)
        at org.apache.livy.sessions.InteractiveSessionManager.deleteExpiredSessions(SessionManager.scala:47)
        at org.apache.livy.server.interactive.SessionHeartbeatWatchdog$$anon$1.run(SessionHeartbeat.scala:92)
- interactive session should not gc-ed if session timeout check is off (1 second, 98 milliseconds)
{code}",test,"meisam commented on pull request #143: [LIVY-556] HearbeatExpired is not stubbed correctly in test cases
URL: https://github.com/apache/incubator-livy/pull/143
 
 
   ## What changes were proposed in this pull request?
   Add the proper stubbing to all mocked interactive sessions to make sure session heartbeats work correctly.
   
   ## How was this patch tested?
   This change only fixes the test cases. It is not changing production code.
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
   Task-url: https://issues.apache.org/jira/projects/LIVY/issues/LIVY-556
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Feb/19 23:59;githubbot;600","asfgit commented on pull request #143: [LIVY-556] HearbeatExpired is not stubbed correctly in test cases
URL: https://github.com/apache/incubator-livy/pull/143
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Feb/19 23:18;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7200,6000,1200,16%,7200,6000,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 15 23:19:02 UTC 2019,,,,,,,,,,"0|yi0rpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,"15/Feb/19 23:19;vanzin;Issue resolved by pull request 143
[https://github.com/apache/incubator-livy/pull/143];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add ""doAs"" impersonation support",LIVY-551,13210384,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,krisden,krisden,krisden,18/Jan/19 13:55,07/Feb/19 17:55,19/Dec/25 04:15,07/Feb/19 17:54,,,,0.6.0,,,,,,,,,,,,1,,,,,,"Currently, impersonation is limited to creating batches and sessions with ""proxyUser"". It would be better to fully support impersonation for all endpoints. This is only possible as a query parameter since many of the endpoints are get requests. Adding support for the ""doAs"" query parameter matches what Hadoop does.Â We can leverage the same types of configs as well. This change is backwards compatible since we can check for doAs before proxyUser and use that. This will not remove the proxyUser checking for batch and session creation.",,"risdenk commented on pull request #141: [LIVY-551] Add ""doAs"" impersonation support
URL: https://github.com/apache/incubator-livy/pull/141
 
 
   ## What changes were proposed in this pull request?
   
   Currently `proxyuser` is used in batches and sessions for impersonation. This should be extended to support impersonation for all endpoints. Adding the query parameter `doAs` matches others in the Hadoop ecosystem and will work across endpoints.
   
   https://issues.apache.org/jira/browse/LIVY-551
   
   ## How was this patch tested?
   
   Added unit and integration tests for this feature. Manually tested to ensure that `proxyuser` still works. Also checked that if both `proxyuser` and the new `doAs` parameter are specified that `doAs` takes precedence. 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jan/19 15:29;githubbot;600","risdenk commented on pull request #141: [LIVY-551] Add ""doAs"" impersonation support
URL: https://github.com/apache/incubator-livy/pull/141
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jan/19 18:45;githubbot;600","risdenk commented on pull request #141: [LIVY-551] Add ""doAs"" impersonation support
URL: https://github.com/apache/incubator-livy/pull/141
 
 
   ## What changes were proposed in this pull request?
   
   Currently `proxyuser` is used in batches and sessions for impersonation. This should be extended to support impersonation for all endpoints. Adding the query parameter `doAs` matches others in the Hadoop ecosystem and will work across endpoints.
   
   https://issues.apache.org/jira/browse/LIVY-551
   
   ## How was this patch tested?
   
   Added unit and integration tests for this feature. Manually tested to ensure that `proxyuser` still works. Also checked that if both `proxyuser` and the new `doAs` parameter are specified that `doAs` takes precedence. 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jan/19 18:45;githubbot;600","risdenk commented on pull request #141: [LIVY-551] Add ""doAs"" impersonation support
URL: https://github.com/apache/incubator-livy/pull/141
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jan/19 21:51;githubbot;600","risdenk commented on pull request #141: [LIVY-551] Add ""doAs"" impersonation support
URL: https://github.com/apache/incubator-livy/pull/141
 
 
   ## What changes were proposed in this pull request?
   
   Currently `proxyuser` is used in batches and sessions for impersonation. This should be extended to support impersonation for all endpoints. Adding the query parameter `doAs` matches others in the Hadoop ecosystem and will work across endpoints.
   
   https://issues.apache.org/jira/browse/LIVY-551
   
   ## How was this patch tested?
   
   Added unit and integration tests for this feature. Manually tested to ensure that `proxyuser` still works. Also checked that if both `proxyuser` and the new `doAs` parameter are specified that `doAs` takes precedence. 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jan/19 21:51;githubbot;600","risdenk commented on pull request #141: [LIVY-551] Add ""doAs"" impersonation support
URL: https://github.com/apache/incubator-livy/pull/141
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Feb/19 14:37;githubbot;600","risdenk commented on pull request #141: [LIVY-551] Add ""doAs"" impersonation support
URL: https://github.com/apache/incubator-livy/pull/141
 
 
   ## What changes were proposed in this pull request?
   
   Currently `proxyuser` is used in batches and sessions for impersonation. This should be extended to support impersonation for all endpoints. Adding the query parameter `doAs` matches others in the Hadoop ecosystem and will work across endpoints.
   
   https://issues.apache.org/jira/browse/LIVY-551
   
   ## How was this patch tested?
   
   Added unit and integration tests for this feature. Manually tested to ensure that `proxyuser` still works. Also checked that if both `proxyuser` and the new `doAs` parameter are specified that `doAs` takes precedence. 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Feb/19 14:37;githubbot;600","risdenk commented on pull request #141: [LIVY-551] Add ""doAs"" impersonation support
URL: https://github.com/apache/incubator-livy/pull/141
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Feb/19 14:25;githubbot;600","risdenk commented on pull request #141: [LIVY-551] Add ""doAs"" impersonation support
URL: https://github.com/apache/incubator-livy/pull/141
 
 
   ## What changes were proposed in this pull request?
   
   Currently `proxyuser` is used in batches and sessions for impersonation. This should be extended to support impersonation for all endpoints. Adding the query parameter `doAs` matches others in the Hadoop ecosystem and will work across endpoints.
   
   https://issues.apache.org/jira/browse/LIVY-551
   
   ## How was this patch tested?
   
   Added unit and integration tests for this feature. Manually tested to ensure that `proxyuser` still works. Also checked that if both `proxyuser` and the new `doAs` parameter are specified that `doAs` takes precedence. 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Feb/19 14:25;githubbot;600","asfgit commented on pull request #141: [LIVY-551] Add ""doAs"" impersonation support
URL: https://github.com/apache/incubator-livy/pull/141
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Feb/19 17:54;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,6000,,,0,6000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 07 17:54:47 UTC 2019,,,,,,,,,,"0|yi02g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Jan/19 13:55;krisden;I'll open a PR for this shortly. I was struggling with tests but think I figured out a way forward.;;;","18/Jan/19 15:31;krisden;FYI [~vanzin] since we talked about this in person earlier this week.;;;","07/Feb/19 17:54;vanzin;Issue resolved by pull request 141
[https://github.com/apache/incubator-livy/pull/141];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy submit application exist [hdfs delegation token] error,LIVY-549,13210340,,Question,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Done,,zoeminghong,zoeminghong,18/Jan/19 09:22,11/Feb/19 02:17,19/Dec/25 04:15,11/Feb/19 02:17,0.5.0,,,,,API,Batch,,,,,,,,,0,,,,,,"I have already used to try spark-submit way withÂ  --principal and --keytab parameters. This way was successd.Â 

I have a Spark job where part of the work done on the executors includes POSTing to a SPNEGO-enabled REST API endpoint, and unless I'm mistaken, when Spark is running a job as a proxy user, the doAs() chain doesn't have the necessary credentials to actually do a SPNEGO negotiation.

How can I use Livy toÂ achieve this function. Thank You.

spark version:2.3.1

https://issues.cloudera.org/browse/LIVY-44?jql=labels%20%3D%20HueÂ ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 11 02:16:36 UTC 2019,,,,,,,,,,"0|yi0268:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/Jan/19 22:59;zyork;If you're using impersonation with livy, wouldn't the job have the Livy principal and keytab? Does that not work? What error are you receiving?

Btw, this is probably a better question for the user or dev lists.;;;","04/Feb/19 17:18;Tagar;Is this related to LIVY-551 ?Â ;;;","11/Feb/19 02:16;zoeminghong;I got it.Â setÂ  livy.impersonation.enabled=false. spark.yarn.keytab andÂ spark.yarn.principal effecting.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy kills session after livy.server.session.timeout even if the session is active,LIVY-547,13208583,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,shanyu,Sandeep Nemuri,Sandeep Nemuri,09/Jan/19 17:13,09/Aug/19 06:56,19/Dec/25 04:15,09/Aug/19 06:53,,,,0.7.0,,Server,,,,,,,,,,0,,,,,,"Livy kills session after {{livy.server.session.timeout}} even if the session is active.

Code that runs more than the {{livy.server.session.timeout}} with intermediate sleeps.
{noformat}
%pyspark 
import time 
import datetime 
import random
def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1
NUM_SAMPLES=10
count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print ""Pi is roughly %f"" % (4.0 * count / NUM_SAMPLES)

print(""waiting for 100 s"") 
time.sleep(100) 
count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print ""Pi is roughly %f"" % (4.0 * count / NUM_SAMPLES)

print(""waiting for 200 s"") 
time.sleep(200) 
count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print ""Pi is roughly %f"" % (4.0 * count / NUM_SAMPLES)

print(""waiting for 300 s1"") 
time.sleep(300)
count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print ""Pi is roughly %f"" % (4.0 * count / NUM_SAMPLES)

print(""waiting for 300 s2"") 
time.sleep(300) 
count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print ""Pi is roughly %f"" % (4.0 * count / NUM_SAMPLES)

print(""waiting for 300 s3"") 
time.sleep(300) 
count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print ""Pi is roughly %f"" % (4.0 * count / NUM_SAMPLES)

print(""waiting for 300 s4"") 
time.sleep(300) 
count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print ""Pi is roughly %f"" % (4.0 * count / NUM_SAMPLES)
{noformat}
Livy log:
{noformat}
19/01/07 17:38:59 INFO InteractiveSession: Interactive session 14 created [appid: application_1546711709239_0002, owner: zeppelin-hwc327, proxyUser: Some(admin), state: idle, kind: shared, info: {driverLogUrl=http://hwc327-node3.hogwarts-labs.com:8042/node/containerlogs/container_e18_1546711709239_0002_01_000001/admin, sparkUiUrl=http://hwc327-node2.hogwarts-labs.com:8088/proxy/application_1546711709239_0002/}]
19/01/07 17:52:46 INFO InteractiveSession: Stopping InteractiveSession 14...
19/01/07 17:52:56 WARN RSCClient: Exception while waiting for end session reply.
java.util.concurrent.TimeoutException
        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49)
        at org.apache.livy.rsc.RSCClient.stop(RSCClient.java:223)
        at org.apache.livy.server.interactive.InteractiveSession$$anonfun$stopSession$1.apply(InteractiveSession.scala:471)
        at org.apache.livy.server.interactive.InteractiveSession$$anonfun$stopSession$1.apply(InteractiveSession.scala:471)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.livy.server.interactive.InteractiveSession.stopSession(InteractiveSession.scala:471)
        at org.apache.livy.sessions.Session$$anonfun$stop$1.apply$mcV$sp(Session.scala:174)
        at org.apache.livy.sessions.Session$$anonfun$stop$1.apply(Session.scala:171)
        at org.apache.livy.sessions.Session$$anonfun$stop$1.apply(Session.scala:171)
        at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
        at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
        at scala.concurrent.impl.ExecutionContextImpl$$anon$3.exec(ExecutionContextImpl.scala:107)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
19/01/07 17:52:56 WARN RpcDispatcher: [ClientProtocol] Closing RPC channel with 1 outstanding RPCs.
19/01/07 17:52:56 WARN InteractiveSession: Failed to stop RSCDriver. Killing it...
19/01/07 17:52:56 INFO YarnClientImpl: Killed application application_1546711709239_0002
19/01/07 17:52:56 INFO InteractiveSession: Stopped InteractiveSession 14.

{noformat}

 Below note indicates that the timeout applies to a inactive session.
{code:java|title=LivyConf.scala}
  // How long will an inactive session be gc-ed.
  val SESSION_TIMEOUT = Entry(""livy.server.session.timeout"", ""1h"")
{code}",,"Jassy1994 commented on pull request #138: [LIVY-547]do not check expire when session in state busy
URL: https://github.com/apache/incubator-livy/pull/138
 
 
   ## What changes were proposed in this pull request?
   add a logic in check expired function to ensure interactiveSession will not expired in state busyã€‚ 
   
   ## How was this patch tested?
   set livy.server.session.timeout to a short interval and submit a job by interactiveSession
   this session will not be recycled in state busyã€‚
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Jan/19 11:37;githubbot;600","shanyu commented on pull request #146: LIVY-547: Livy kills session after livy.server.session.timeout even iâ€¦
URL: https://github.com/apache/incubator-livy/pull/146
 
 
   â€¦f the session is active
   
   Signed-off-by: Shanyu Zhao <shzhao@microsoft.com>
   
   ## What changes were proposed in this pull request?
   
   (Please fill in changes proposed in this fix)
   (Include a link to the associated JIRA and make sure to add a link to this pr on the JIRA as well)
   
   ## How was this patch tested?
   
   (Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)
   (If this patch involves UI changes, please attach a screenshot; otherwise, remove this)
   
   Please review https://livy.incubator.apache.org/community/ before opening a pull request.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Mar/19 03:54;githubbot;600","shanyu commented on pull request #146: LIVY-547: Livy kills session after livy.server.session.timeout even iâ€¦
URL: https://github.com/apache/incubator-livy/pull/146
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 01:05;githubbot;600","shanyu commented on pull request #190: LIVY-547: Livy kills session after livy.server.session.timeout even if the session is active
URL: https://github.com/apache/incubator-livy/pull/190
 
 
   ## What changes were proposed in this pull request?
   Add a new configuration:
     livy.server.session.timeout-check.skip-busy
   To indicate whether or not to skip timeout check for a busy session. It defaults to false for 
   backward compatibility.
   
   https://issues.apache.org/jira/browse/LIVY-547
   
   ## How was this patch tested?
   
   Manually tested the configuration.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jul/19 20:17;githubbot;600","jerryshao commented on pull request #190: LIVY-547: Livy kills session after livy.server.session.timeout even if the session is active
URL: https://github.com/apache/incubator-livy/pull/190
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 02:53;githubbot;600","shanyu commented on pull request #190: LIVY-547: Livy kills session after livy.server.session.timeout even if the session is active
URL: https://github.com/apache/incubator-livy/pull/190
 
 
   ## What changes were proposed in this pull request?
   Add a new configuration:
     livy.server.session.timeout-check.skip-busy
   To indicate whether or not to skip timeout check for a busy session. It defaults to false for 
   backward compatibility.
   
   https://issues.apache.org/jira/browse/LIVY-547
   
   ## How was this patch tested?
   
   Manually tested the configuration.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 02:53;githubbot;600","jerryshao commented on pull request #190: [LIVY-547][SERVER] Livy kills session after livy.server.session.timeout even if the session is active
URL: https://github.com/apache/incubator-livy/pull/190
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 06:52;githubbot;600","jerryshao commented on pull request #138: [LIVY-547]do not check expire when session in state busy
URL: https://github.com/apache/incubator-livy/pull/138
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/19 06:56;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4800,,,0,4800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 09 06:53:28 UTC 2019,,,,,,,,,,"0|u00oa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Jan/19 02:58;Jassy;[~Sandeep Nemuri] Hi Sandeepï¼ŒI think your session is an interactiveSession and Livy gc thread will recycle your session when your job consume more time than livy.server.session.timeoutã€‚

the relavant codeï¼š
{code:java}
def expired(session: Session): Boolean = {
  session.state match {
    case s: FinishedSessionState =>
      val currentTime = System.nanoTime()
      currentTime - s.time > sessionStateRetainedInSec
    case _ =>
      if (!sessionTimeoutCheck) {
        false
      } else if (session.isInstanceOf[BatchSession]) {
        false
      } else {
        val currentTime = System.nanoTime()
        currentTime - session.lastActivity > sessionTimeout
      }
  }{code}
you can see in last elseï¼Œsession will be recycled when duration time exceed sessionTimeoutï¼Œand session.lastActivity will not update in state busy.

I think this can be modified as:
{code:java}
def expired(session: Session): Boolean = {
  session.state match {
    case s: FinishedSessionState =>
      val currentTime = System.nanoTime()
      currentTime - s.time > sessionStateRetainedInSec
    case _ =>
      if (!sessionTimeoutCheck) {
        false
      } else if (session.isInstanceOf[BatchSession]) {
        false
      } else if(""busy"".equals(session.state.state)) {
        false
      } else {
        val currentTime = System.nanoTime()
        currentTime - session.lastActivity > sessionTimeout
      }
  }
}{code}
Â ;;;","14/Jan/19 05:56;Sandeep Nemuri;Hi [~Jassy], Thanks for the quick fix. It makes sense to get the current state and check if session is busy before killing.Â 

You may want to submit a PR so that Livy community can review and post their comments(if any).;;;","28/Feb/19 02:41;shanyu;[~Jassy] are you still working on this issue?;;;","31/Jul/19 20:19;shanyu;Created new PR and added new configuration to change the behavior of timeout check and the default value has backward compatibility.

# Whether or not to skip timeout check for a busy session
# livy.server.session.timeout-check.skip-busy = false;;;","09/Aug/19 06:53;jerryshao;Issue resolved by pull request 190
https://github.com/apache/incubator-livy/pull/190;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix inconsistent Jetty dependencies,LIVY-543,13204648,13177985,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mgaido,mgaido,mgaido,15/Dec/18 12:49,19/Dec/18 00:29,19/Dec/25 04:15,19/Dec/18 00:29,,,,0.6.0,,Server,,,,,,,,,,0,,,,,,"LIVY-526 introduced a new version for jetty which is different from the one used by Hive. This can cause conflicts in the version used and cause problems. Since the reason of LIVY-526 was to have security fixes, rather than reverting it we should try and have our version of jetty independent from the one used by Hive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 19 00:29:10 UTC 2018,,,,,,,,,,"0|u0009k:",9223372036854775807,,,,,,,,,,,,,,,,,,,"19/Dec/18 00:29;vanzin;Issue resolved by pull request 134
[https://github.com/apache/incubator-livy/pull/134];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy Always connecting to ResourceManager at /0.0.0.0:8032 and not using resource manager address specified in yarn-site.xml ,LIVY-540,13202417,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Problem,,sdandey,sdandey,05/Dec/18 02:52,13/Dec/18 22:56,19/Dec/25 04:15,13/Dec/18 22:56,,,,,,Server,,,,,,,,,,0,,,,,,"We are using cloudera version of hadoop and installed livy manually using the tar ball.

When we start Livy using the kerberos launch key principal and keytab it runs succesfully.Â  However, it's always connecting to local Resource Manager (log shows RMProxy: Connecting to ResourceManager at /0.0.0.0:8032) though we have specified the yarn.configuration path in the livy-env.sh and try to submit job always to that 0.0.0.0:8032.Â 

Â 

Did any one ran into this issue before?Â  Why does livy doesn't read the yarn.resourcemanager.address from yarn-conf.xml. A'm I missing any configuraiton params here.

Â 

yarn-site.xml
{code:java}
<?xml version=""1.0"" encoding=""UTF-8""?> <!--Autogenerated by Cloudera Manager--> <configuration> <property> <name>yarn.acl.enable</name> <value>true</value> </property> <property> <name>yarn.admin.acl</name> <value>*</value> </property> <property> <name>yarn.resourcemanager.address</name> <value>hostname.myserver:8032</value> </property> <property> <name>yarn.resourcemanager.admin.address</name> <value>hostname.myserver:8033</value> </property> <property> <name>yarn.resourcemanager.scheduler.address</name> <value>hostname.myserver:8030</value> </property> <property> <name>yarn.resourcemanager.resource-tracker.address</name> <value>hostname.myserver:8031</value> </property> <property> <name>yarn.resourcemanager.webapp.address</name> <value>hostname.myserver:8088</value> </property> <property> <name>yarn.resourcemanager.webapp.https.address</name> <value>hostname.myserver:8090</value> </property> <property> <name>yarn.resourcemanager.client.thread-count</name> <value>50</value> </property> <property> <name>yarn.resourcemanager.scheduler.client.thread-count</name> <value>50</value> </property> <property> <name>yarn.resourcemanager.admin.client.thread-count</name> <value>1</value> </property> <property> <name>yarn.scheduler.minimum-allocation-mb</name> <value>1024</value> </property> <property> <name>yarn.scheduler.increment-allocation-mb</name> <value>512</value> </property> <property> <name>yarn.scheduler.maximum-allocation-mb</name> <value>329591</value> </property> <property> <name>yarn.scheduler.minimum-allocation-vcores</name> <value>1</value> </property> <property> <name>yarn.scheduler.increment-allocation-vcores</name> <value>1</value> </property> <property> <name>yarn.scheduler.maximum-allocation-vcores</name> <value>40</value> </property> <property> <name>yarn.resourcemanager.amliveliness-monitor.interval-ms</name> <value>1000</value> </property> <property> <name>yarn.am.liveness-monitor.expiry-interval-ms</name> <value>600000</value> </property> <property> <name>yarn.resourcemanager.am.max-attempts</name> <value>2</value> </property> <property> <name>yarn.resourcemanager.container.liveness-monitor.interval-ms</name> <value>600000</value> </property> <property> <name>yarn.resourcemanager.nm.liveness-monitor.interval-ms</name> <value>1000</value> </property> <property> <name>yarn.nm.liveness-monitor.expiry-interval-ms</name> <value>600000</value> </property> <property> <name>yarn.resourcemanager.resource-tracker.client.thread-count</name> <value>50</value> </property> <property> <name>yarn.application.classpath</name> <value>$HADOOP_CLIENT_CONF_DIR,$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,$HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*</value> </property> <property> <name>yarn.resourcemanager.scheduler.class</name> <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value> </property> <property> <name>yarn.resourcemanager.max-completed-applications</name> <value>10000</value> </property> <property> <name>yarn.nodemanager.remote-app-log-dir</name> <value>/tmp/logs</value> </property> <property> <name>yarn.nodemanager.remote-app-log-dir-suffix</name> <value>logs</value> </property> <property> <name>yarn.resourcemanager.principal</name> <value>yarn/_HOST@BDS.UBS.COM</value> </property> </configuration>
{code}
Â 

Â 

Â 

livy-env.sh
{code:java}
export JAVA_HOME=/app/bds/java 
export SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2 
export HADOOP_HOME=/opt/cloudera/parcels/CDH 
export LIVY_PID_DIR=${LIVY_HOME} 
export HADOOP_CONF_DIR=/etc/hadoop/conf export KRB5_CONFIG=/etc/krb5_bds.conf
export JAVA_TOOL_OPTIONS='-Djava.security.krb5.conf=/etc/krb5_bds.conf'
{code}
livy.conf
{code:java}
livy.spark.master = yarn livy.spark.deploy-mode = cluster livy.impersonation.enabled = true livy.repl.enable-hive-context = true livy.server.launch.kerberos.principal=livy@XXX.com livy.server.launch.kerberos.keytab=/app/bds/security/keytabs/livy.keytab livy.superusers = hdfs
{code}
livy-log
{code:java}
Picked up JAVA_TOOL_OPTIONS: -Djava.security.krb5.conf=/etc/krb5_bds.conf 18/12/04 14:52:19 INFO AccessManager: AccessControlManager acls disabled;users with view permission: ;users with modify permission: ;users with super permission: hdfs;other allowed users: * 18/12/04 14:52:19 INFO LineBufferedStream: stdout: /opt/cloudera/parcels/SPARK2/lib/spark2/conf/spark-env.sh: line 81: spark.yarn.appMasterEnv.PYSPARK3_PYTHON=/app/bds/apo/BDS_py36_ds_112018/bin/python: No such file or directory 18/12/04 14:52:19 INFO LineBufferedStream: stdout: WARNING: User-defined SPARK_HOME (/app/bds/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.458809/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2). 18/12/04 14:52:19 INFO LineBufferedStream: stdout: WARNING: Running spark-class from user-defined location. 18/12/04 14:52:19 INFO LineBufferedStream: stdout: /opt/cloudera/parcels/SPARK2/lib/spark2/conf/spark-env.sh: line 81: spark.yarn.appMasterEnv.PYSPARK3_PYTHON=/app/bds/apo/BDS_py36_ds_112018/bin/python: No such file or directory 18/12/04 14:52:20 INFO LineBufferedStream: stdout: Picked up JAVA_TOOL_OPTIONS: -Djava.security.krb5.conf=/etc/krb5_bds.conf 18/12/04 14:52:20 INFO LineBufferedStream: stdout: Picked up JAVA_TOOL_OPTIONS: -Djava.security.krb5.conf=/etc/krb5_bds.conf 18/12/04 14:52:20 INFO LineBufferedStream: stdout: Welcome to 18/12/04 14:52:20 INFO LineBufferedStream: stdout: ____ __ 18/12/04 14:52:20 INFO LineBufferedStream: stdout: / __/__ ___ _____/ /__ 18/12/04 14:52:20 INFO LineBufferedStream: stdout: _\ \/ _ \/ _ `/ __/ '_/ 18/12/04 14:52:20 INFO LineBufferedStream: stdout: /___/ .__/\_,_/_/ /_/\_\ version 2.3.0.cloudera3 18/12/04 14:52:20 INFO LineBufferedStream: stdout: /_/ 18/12/04 14:52:20 INFO LineBufferedStream: stdout: 18/12/04 14:52:20 INFO LineBufferedStream: stdout: Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_151 18/12/04 14:52:20 INFO LineBufferedStream: stdout: Branch HEAD 18/12/04 14:52:20 INFO LineBufferedStream: stdout: Compiled by user jenkins on 2018-07-05T03:50:33Z 18/12/04 14:52:20 INFO LineBufferedStream: stdout: Revision 04c773e19117d158cf917e60a6e98488a643d49e 18/12/04 14:52:20 INFO LineBufferedStream: stdout: Url git://github.mtv.cloudera.com/CDH/spark.git 18/12/04 14:52:20 INFO LineBufferedStream: stdout: Type --help for more information. 18/12/04 14:52:20 WARN LivySparkUtils$: Current Spark (2,3) is not verified in Livy, please use it carefully 18/12/04 14:52:21 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)]) 18/12/04 14:52:21 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)]) 18/12/04 14:52:21 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups]) 18/12/04 14:52:21 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics 18/12/04 14:52:21 DEBUG Shell: setsid exited with exit code 0 18/12/04 14:52:21 DEBUG Groups: Creating new Groups object 18/12/04 14:52:21 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping; cacheTimeout=300000; warningDeltaMs=5000 18/12/04 14:52:21 DEBUG LivyServer: Ran kinit command successfully. 18/12/04 14:52:21 DEBUG AbstractService: Service: org.apache.hadoop.yarn.client.api.impl.YarnClientImpl entered state INITED 18/12/04 14:52:21 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 18/12/04 14:52:21 DEBUG UserGroupInformation: hadoop login 18/12/04 14:52:21 DEBUG UserGroupInformation: hadoop login commit 18/12/04 14:52:21 DEBUG UserGroupInformation: using kerberos user:livy@BDS.UBS.COM 18/12/04 14:52:21 DEBUG UserGroupInformation: Using user: ""livy@BDS.UBS.COM"" with name livy@BDS.UBS.COM 18/12/04 14:52:21 DEBUG UserGroupInformation: User entry: ""livy@BDS.UBS.COM"" 18/12/04 14:52:21 DEBUG UserGroupInformation: UGI loginUser:livy@BDS.UBS.COM (auth:KERBEROS) 18/12/04 14:52:21 INFO StateStore$: Using BlackholeStateStore for recovery. 18/12/04 14:52:21 INFO BatchSessionManager: Recovered 0 batch sessions. Next session id: 0 18/12/04 14:52:21 DEBUG UserGroupInformation: Found tgt Ticket (hex) =
{code}",RHEL 7.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 13 22:56:02 UTC 2018,,,,,,,,,,"0|s016d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Dec/18 22:56;sdandey;Closing the ticket.Â  Issue is with the Cloudera gateway node configuration and not with LIVY.Â ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect default value for livy.rsc.launcher.port.range in livy-client.conf,LIVY-539,13202361,,Documentation,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,apetresc,apetresc,apetresc,04/Dec/18 19:56,04/Dec/18 22:37,19/Dec/25 04:15,04/Dec/18 22:37,0.5.0,,,0.6.0,,RSC,,,,,,,,,,0,,,,,,"The template forÂ {{livy-client.conf}} implies that the default value of {{livy.rsc.launcher.port.range}} is {{10000~10110}}, but this is in fact incorrect - the real default value is {{10000~10010}}, as you can [see here|https://github.com/apache/incubator-livy/blob/master/rsc/src/main/java/org/apache/livy/rsc/RSCConf.java#L54] This is really confusing, especially since:

Â  a) 10110 is wrong on the Spark side, the default configuration there won't open that many ports

Â  b) 10010 and 10110 look very similar to the human eye so it can take a long time to notice the discrepancy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Dec 04 22:37:10 UTC 2018,,,,,,,,,,"0|s0160w:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Dec/18 19:59;apetresc;Pull Request: https://github.com/apache/incubator-livy/pull/132;;;","04/Dec/18 22:37;vanzin;Issue resolved by pull request 132
[https://github.com/apache/incubator-livy/pull/132];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add integration tests for the thrift server,LIVY-538,13200943,13177985,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mgaido,vanzin,vanzin,27/Nov/18 20:32,03/Jan/19 20:33,19/Dec/25 04:15,03/Jan/19 20:33,0.6.0,,,0.6.0,,Server,Tests,,,,,,,,,0,,,,,,"Currently there are no integration tests for the thrift server part. The UTs are run on a different environment than ITs, which are more similar to a real cluster, and having ITs would allow us to catch issues in the code that are masked in UTs, such as serialization issues or referencing unavailable classes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 03 20:33:09 UTC 2019,,,,,,,,,,"0|s00xco:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Dec/18 15:21;mgaido;Created PR: https://github.com/apache/incubator-livy/pull/131.;;;","03/Jan/19 20:33;vanzin;Issue resolved by pull request 131
[https://github.com/apache/incubator-livy/pull/131];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add numExecutor configuration when creating session in thriftserver,LIVY-537,13200853,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mgaido,mgaido,mgaido,27/Nov/18 13:27,28/Nov/18 21:49,19/Dec/25 04:15,28/Nov/18 21:49,0.6.0,,,0.6.0,,Server,,,,,,,,,,0,,,,,,"When parsing the option provided by the user in order to create a new session using the thiftserver API, we are not setting the number of executors, which is a very important parameter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 28 21:49:19 UTC 2018,,,,,,,,,,"0|s00wsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Nov/18 13:32;mgaido;Created PR: https://github.com/apache/incubator-livy/pull/130.;;;","28/Nov/18 21:49;vanzin;Issue resolved by pull request 130
[https://github.com/apache/incubator-livy/pull/130];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Config option â€œlivy.server.session.max-creationâ€ doesn't work properly,LIVY-535,13200058,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,aromanenko,aromanenko,aromanenko,22/Nov/18 11:47,26/Nov/18 17:21,19/Dec/25 04:15,26/Nov/18 17:21,0.5.0,,,0.6.0,,,,,,,,,,,,0,,,,,,"Config option {{livy.server.session.max-creation}} doesn't work properly.

For example, I set the value of this option to â€œ1â€ and try to submit 5 batch jobs (example SparkPi) to Livy almost in the same time using a bash command like this: 
{code:bash}
for i in {1..5}; do curl -X POST --data '{""file"": ""/tmp/spark-examples-2.jar"", ""className"": ""org.apache.spark.examples.SparkPi""}' -H ""Content-Type: application/json"" http://localhost:8999/batches; done 
{code}

In such case, I expect that the only one job (first) will be submitted properly and others will be rejected with a response message â€œRejected, too many sessions are being created!â€. This is what I actually have when I submit them manually, one by one. Though, in the case given above, almost always all 5 jobs will be submitted properly (sometimes only one will be rejected) despite of the value of config option.

Looks like, that code, where Livy checks the number of already created sessions and launch new one, is not atomic:
https://github.com/apache/incubator-livy/blob/4cfb6bcb8fb9ac6b2d6c8b3d04b20f647b507e1f/server/src/main/scala/org/apache/livy/server/SessionServlet.scala#L130

Also, the calculation of the total number of child processes in {{tooManySessions()}} is not atomic operation too.

So, when I submit the jobs almost in the same moment (POST request takes only about tens of milliseconds) then I can spawn more spark-submit processes despite of maximum limitation. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 26 17:21:34 UTC 2018,,,,,,,,,,"0|s00rww:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Nov/18 14:30;aromanenko;https://github.com/apache/incubator-livy/pull/129;;;","26/Nov/18 17:21;vanzin;Issue resolved by pull request 129
[https://github.com/apache/incubator-livy/pull/129];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark 2.4.0, kubernetes client mode. TypeError: object of type 'NoneType' has no len() in authenticate_and_accum_updates of pyspark/accumulators.py",LIVY-534,13199283,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,,saivarunvishal,saivarunvishal,19/Nov/18 10:12,21/Nov/18 05:17,19/Dec/25 04:15,21/Nov/18 05:17,0.5.0,,,,,Core,,,,,,,,,,0,,,,,,"step 1) create dataframe.
{code:java}
df=spark_session.createDataFrame([{'a':1}])
{code}
step 2) do a count or collect
{code:java}
df.count(){code}
I get this output.
{code:java}
// Exception happened during processing of request from ('127.0.0.1', 38690)
Traceback (most recent call last):
File ""/usr/lib/python3.6/socketserver.py"", line 317, in _handle_request_noblock
self.process_request(request, client_address)
File ""/usr/lib/python3.6/socketserver.py"", line 348, in process_request
self.finish_request(request, client_address)
File ""/usr/lib/python3.6/socketserver.py"", line 361, in finish_request
self.RequestHandlerClass(request, client_address, self)
File ""/usr/lib/python3.6/socketserver.py"", line 721, in __init__
self.handle()
File ""/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/accumulators.py"", line 266, in handle
poll(authenticate_and_accum_updates)
File ""/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/accumulators.py"", line 241, in poll
if func():
File ""/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/accumulators.py"", line 254, in authenticate_and_accum_updates
received_token = self.rfile.read(len(auth_token))
TypeError: object of type 'NoneType' has no len()

{code}
Repeat the step 2, the error no longer comes.

Â ","kubernetes on minikube.
kubernetes version 1.10.0
Spark 2.4.0 client mode on kubernetes.
Apache livy 0.5.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 21 05:16:52 UTC 2018,,,,,,,,,,"0|s00n8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,"21/Nov/18 05:16;saivarunvishal;Building livy from source at [https://github.com/apache/incubator-livy/commit/4cfb6bcb8fb9ac6b2d6c8b3d04b20f647b507e1f]Â was the fix, some how this fixed the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Spark jobs submitted via programmatic API cannot always be canceled ,LIVY-533,13198174,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,bjoern.lohrmann,bjoern.lohrmann,bjoern.lohrmann,13/Nov/18 22:14,03/Dec/18 23:34,19/Dec/25 04:15,03/Dec/18 23:34,0.5.0,,,0.6.0,,RSC,,,,,,,,,,0,pull-request-available,,,,,"Running stages of Spark jobs submitted via Livy' programmatic API cannot (always) be successfully cancelled.

The current implementation of .JobWrapper.cancel() interrupts the worker thread on the Spark driver (via Future.cancel(true)):

[https://github.com/apache/incubator-livy/blob/4cfb6bcb8fb9ac6b2d6c8b3d04b20f647b507e1f/rsc/src/main/java/org/apache/livy/rsc/driver/JobWrapper.java#L84]

This does not always cancel all activity in Spark, e.g. long-running stages may remain unaffected.

The Spark-way of cancelling jobs seems to be via SparkContext.setJobGroup()/cancelJobGroup(), which is also being used in Livy's REPL Session:

[https://github.com/apache/incubator-livy/blob/4cfb6bcb8fb9ac6b2d6c8b3d04b20f647b507e1f/repl/src/main/scala/org/apache/livy/repl/Session.scala#L164]

I have opened a PR that invokes setJobGroup()/cancelJobGroup() in addition to interrupting the worker thread running on the driver:

[https://github.com/apache/incubator-livy/pull/128]

Â 

It would be great if the fix could make it into the 0.6 release.

Â ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Dec 03 23:34:19 UTC 2018,,,,,,,,,,"0|s00geg:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Dec/18 23:34;vanzin;Issue resolved by pull request 128
[https://github.com/apache/incubator-livy/pull/128];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update spark 2.3 default version to 2.3.2,LIVY-531,13194078,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,25/Oct/18 07:57,25/Oct/18 19:36,19/Dec/25 04:15,25/Oct/18 19:35,0.6.0,,,0.6.0,,Build,,,,,,,,,,0,,,,,,Due to spark 2.3.2 is highly recommended version to upgrade for all 2.3.x users which contains many stability bug fixes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 25 19:35:40 UTC 2018,,,,,,,,,,"0|i3zmfr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Oct/18 08:04;cltlfcjin;PR: https://github.com/apache/incubator-livy/pull/124;;;","25/Oct/18 19:35;vanzin;Issue resolved by pull request 124
[https://github.com/apache/incubator-livy/pull/124];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix supported Python version from 2.6+ to 2.7+ in documentation,LIVY-529,13191928,,Documentation,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,itholic,itholic,itholic,16/Oct/18 13:51,16/Oct/18 21:00,19/Dec/25 04:15,16/Oct/18 21:00,0.6.0,,,0.6.0,,Docs,,,,,,,,,,0,,,,,,"As of Spark 2.2, Python 2.6 is officially dropped

(https://issues.apache.org/jira/browse/SPARK-12661).

Since Livy support Spark 2.2+, looks this limitation is inheritted.

Currently, Livy doc (https://github.com/apache/incubator-livy/blob/master/README.md) notes Python 2.6+ is required.

This should be changed to 2.7+",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 16 21:00:04 UTC 2018,,,,,,,,,,"0|i3z987:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/Oct/18 21:00;vanzin;Issue resolved by pull request 122
[https://github.com/apache/incubator-livy/pull/122];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade jetty version,LIVY-526,13190429,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,arunmahadevan,arunmahadevan,arunmahadevan,09/Oct/18 17:14,13/Oct/18 07:50,19/Dec/25 04:15,13/Oct/18 07:47,,,,0.6.0,,,,,,,,,,,,0,,,,,,Upgrade Jetty version to a more recent one that has fixes for different CVEs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Oct 13 07:47:46 UTC 2018,,,,,,,,,,"0|i3z013:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Oct/18 07:47;jerryshao;Issue resolved by pull request 120
[https://github.com/apache/incubator-livy/pull/120];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Response Content-Type is application/json, but a simple string gets returned",LIVY-525,13190319,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,IngoSchuster,IngoSchuster,IngoSchuster,09/Oct/18 08:55,05/Nov/18 17:37,19/Dec/25 04:15,05/Nov/18 17:36,0.5.0,,,0.6.0,,Server,,,,,,,,,,0,,,,,,"When trying to execute a script that is not whitlisted, the error response has content type application/json, but the content is in fact just a plain string:

{{$> curl -v -X POST --data '\{""file"": ""/root/NoAccess.R""}' -H ""Content-Type: application/json"" <SERVER>:8998/batches}}
{{Note: Unnecessary use of -X or --request, POST is already inferred.}}
{{*Â Â  Trying <IP ADDRESS>...}}
{{* TCP_NODELAY set}}
{{* Connected to <SERVER> (<IP ADDRESS>) port 8998 (#0)}}
{{> POST /batches HTTP/1.1}}
{{> Host: <SERVER>:8998}}
{{> User-Agent: curl/7.55.1}}
{{> Accept: */*}}
{{> Content-Type: application/json}}
{{> Content-Length: 28}}
{{> }}
{{* upload completely sent off: 28 out of 28 bytes}}
{{< HTTP/1.1 400 Bad Request}}
{{< Date: Tue, 09 Oct 2018 08:20:12 GMT}}
{{< Content-Type: application/json; charset=UTF-8}}
{{< Content-Length: 83}}
{{< Server: Jetty(9.2.16.v20160414)}}
{{< }}
{{* Connection #0 to host <SERVER> left intact}}
{{""requirement failed: Local path /root/NoAccess.R cannot be added to user sessions.""}}

Â 

[https://github.com/apache/incubator-livy/blob/master/server/src/main/scala/org/apache/livy/sessions/Session.scala#L131]

Â ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 05 17:36:27 UTC 2018,,,,,,,,,,"0|i3yzcn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Nov/18 17:36;vanzin;Issue resolved by pull request 127
[https://github.com/apache/incubator-livy/pull/127];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Basic Auth support in curl requests to LIVY REST API,LIVY-524,13190306,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,toopt4,toopt4,09/Oct/18 07:36,09/Oct/18 15:55,19/Dec/25 04:15,09/Oct/18 15:55,,,,,,,,,,,,,,,,0,,,,,,#security as manyÂ teamsÂ would not be going down kerberos routeÂ ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-508,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 09 15:55:55 UTC 2018,,,,,,,,,,"0|i3yz9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Oct/18 15:55;vanzin;Please search JIRA before filing a new feature request or bug.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Spark 2.4,LIVY-523,13189838,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,vanzin,vanzin,05/Oct/18 23:15,05/Oct/18 23:16,19/Dec/25 04:15,05/Oct/18 23:16,0.6.0,,,,,Core,,,,,,,,,,0,,,,,,"The REPL API has changed in Spark 2.4, Livy will need changes to support it. Also the current hardcoded max version is 2.3.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-518,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 05 23:16:32 UTC 2018,,,,,,,,,,"0|i3ywgf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Oct/18 23:16;vanzin;Oops forgot I already filed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log when a session is deleted because of timeout,LIVY-520,13187858,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,mgaido,mgaido,mgaido,27/Sep/18 10:13,01/Oct/18 20:39,19/Dec/25 04:15,01/Oct/18 20:38,0.6.0,,,0.6.0,,Server,,,,,,,,,,0,,,,,,"When a session is idle for more than {{livy.server.session.timeout}} it is GCed, ie. it is closed. But there is no evidence that this is the reason why the session is closed, so it may be non-trivial to detect the reason why the session was closed.

This JIRA proposes to add a log information when a session is GCed in order to make it clearer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 01 20:38:58 UTC 2018,,,,,,,,,,"0|i3yka7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"01/Oct/18 20:38;vanzin;Issue resolved by pull request 116
[https://github.com/apache/incubator-livy/pull/116];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flaky test: SparkYarnApp ""should kill yarn app """,LIVY-519,13187687,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,runzhiwang,vanzin,vanzin,26/Sep/18 18:22,03/Sep/19 05:00,19/Dec/25 04:15,03/Sep/19 05:00,0.6.0,,,0.7.0,,Tests,,,,,,,,,,0,,,,,,"From a travis run:

{noformat}
[32mSparkYarnApp[0m
[32m- should poll YARN state and terminate (116 milliseconds)[0m
[31m- should kill yarn app *** FAILED *** (83 milliseconds)[0m
[31m  org.mockito.exceptions.verification.WantedButNotInvoked: Wanted but not invoked:[0m
[31myarnClient.killApplication([0m
[31m    application_1467912463905_0021[0m
[31m);[0m
[31m-> at org.apache.livy.utils.SparkYarnAppSpec$$anonfun$1$$anonfun$apply$mcV$sp$5$$anonfun$apply$mcV$sp$7$$anonfun$apply$mcV$sp$8.apply$mcV$sp(SparkYarnAppSpec.scala:156)[0m
[31m[0m
[31mHowever, there were other interactions with this mock:[0m
[31m-> at org.apache.livy.utils.SparkYarnApp$$anonfun$1.apply$mcV$sp(SparkYarnApp.scala:261)[0m
[31m-> at org.apache.livy.utils.SparkYarnApp$$anonfun$1.apply$mcV$sp(SparkYarnApp.scala:270)[0m
[31m  at org.apache.livy.utils.SparkYarnAppSpec$$anonfun$1$$anonfun$apply$mcV$sp$5$$anonfun$apply$mcV$sp$7$$anonfun$apply$mcV$sp$8.apply$mcV$sp(SparkYarnAppSpec.scala:156)[0m
[31m  at org.apache.livy.utils.SparkYarnAppSpec.org$apache$livy$utils$SparkYarnAppSpec$$cleanupThread(SparkYarnAppSpec.scala:43)[0m
[31m  at org.apache.livy.utils.SparkYarnAppSpec$$anonfun$1$$anonfun$apply$mcV$sp$5$$anonfun$apply$mcV$sp$7.apply$mcV$sp(SparkYarnAppSpec.scala:148)[0m
[31m  at org.apache.livy.utils.Clock$.withSleepMethod(Clock.scala:31)[0m
[31m  at org.apache.livy.utils.SparkYarnAppSpec$$anonfun$1$$anonfun$apply$mcV$sp$5.apply$mcV$sp(SparkYarnAppSpec.scala:126)[0m
[31m  at org.apache.livy.utils.SparkYarnAppSpec$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(SparkYarnAppSpec.scala:126)[0m
[31m  at org.apache.livy.utils.SparkYarnAppSpec$$anonfun$1$$anonfun$apply$mcV$sp$5.apply(SparkYarnAppSpec.scala:126)[0m
[31m  at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)[0m
[31m  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[0m
[31m  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[0m
[31m  ...[0m
{noformat}
",,"runzhiwang commented on pull request #221: [LIVY-519]fix travis failed on should kill yarn app
URL: https://github.com/apache/incubator-livy/pull/221
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""should kill yarn app""
   
   The cause of failed is as follows:
   1. When create SparkYarnApp, the yarnAppMonitorThread will be created, which changeState of app to Failed. Because before the commit https://github.com/apache/incubator-livy/commit/a90f4fac8be27a38cc961c24043a494a739ff188, the pair <RUNNING: getYarnApplicationState, SUCCEEDED: getFinalApplicationStatus> which was mocked in test, was not defined in mapYarnState, so the state of app will be changed to failed.
   
   2. Then the test kill app, which will call killApplication when the app is running. However the app has been changed to failed in step 1, and killApplication won't be called. So verify(mockYarnClient).killApplication(appId) failed.
   
   3. So if yarnAppMonitorThread change state of app before main thread kill app, the test will failed. If not, the test will succeed.
   
   4. Though the commit https://github.com/apache/incubator-livy/commit/a90f4fac8be27a38cc961c24043a494a739ff188 fixed the bug accidentally, it is necessary to ensure the app is running before kill app.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Sep/19 00:51;githubbot;600","runzhiwang commented on pull request #221: [LIVY-519]Fix travis failed on should kill yarn app
URL: https://github.com/apache/incubator-livy/pull/221
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 02:26;githubbot;600","runzhiwang commented on pull request #221: [LIVY-519]Fix travis failed on should kill yarn app
URL: https://github.com/apache/incubator-livy/pull/221
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""should kill yarn app""
   
   The cause of failed is as follows:
   1. When create SparkYarnApp, the yarnAppMonitorThread will be created, which change app stateto Failed. Because before the commit https://github.com/apache/incubator-livy/commit/a90f4fac8be27a38cc961c24043a494a739ff188, the pair <RUNNING: getYarnApplicationState, SUCCEEDED: getFinalApplicationStatus> which was mocked in test, was not defined in mapYarnState, so the state of app will be changed to failed.
   
   2. Then the test kills app, which will call killApplication when the app is running. However the app has been changed to failed in step 1, so killApplication won't be called, and verify(mockYarnClient).killApplication(appId) failed.
   
   3. So if yarnAppMonitorThread changes app state before main thread kills app, the test will failed. If not, the test will succeed.
   
   4. Though the commit https://github.com/apache/incubator-livy/commit/a90f4fac8be27a38cc961c24043a494a739ff188 fixed the bug accidentally, it is necessary to ensure the app is running before kill app.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 02:26;githubbot;600","jerryshao commented on pull request #221: [LIVY-519][TEST] Fix travis failed on should kill yarn app
URL: https://github.com/apache/incubator-livy/pull/221
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 06:30;githubbot;600","runzhiwang commented on pull request #221: [LIVY-519][TEST] Fix travis failed on should kill yarn app
URL: https://github.com/apache/incubator-livy/pull/221
 
 
   ## What changes were proposed in this pull request?
   
   Fix travis failed on ""should kill yarn app""
   
   The cause of failed is as follows:
   1. When create SparkYarnApp, the yarnAppMonitorThread will be created, which change app state to Failed. Because before recent commit https://github.com/apache/incubator-livy/commit/a90f4fac8be27a38cc961c24043a494a739ff188, the pair <RUNNING: getYarnApplicationState, SUCCEEDED: getFinalApplicationStatus> which was mocked in test, but was not defined in mapYarnState, so the state of app will be changed to failed.
   
   2. Then the test kills app, which will call killApplication when the app is running. However the app has been changed to failed in step 1, so killApplication won't be called, and verify(mockYarnClient).killApplication(appId) failed.
   
   3. So if yarnAppMonitorThread changes app state before main thread kills app, the test will failed. If not, the test will succeed.
   
   4. Though the recent commit https://github.com/apache/incubator-livy/commit/a90f4fac8be27a38cc961c24043a494a739ff188 fixed the bug accidentally, it is necessary to ensure the app is running before kill app.
   
   ## How was this patch tested?
   
   Existed UT and IT.
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 06:30;githubbot;600","jerryshao commented on pull request #221: [LIVY-519][TEST] Fix travis failed on should kill yarn app
URL: https://github.com/apache/incubator-livy/pull/221
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Sep/19 04:59;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 03 05:00:42 UTC 2019,,,,,,,,,,"0|i3yj87:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Aug/19 07:35;runzhiwang;Iâ€˜m workin on it.;;;","03/Sep/19 05:00;jerryshao;Issue resolved by pull request 221
https://github.com/apache/incubator-livy/pull/221;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for Spark 2.4,LIVY-518,13187661,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,gurwls223,vanzin,vanzin,26/Sep/18 16:36,12/Dec/22 17:43,19/Dec/25 04:15,07/Nov/18 17:16,0.6.0,,,0.6.0,,,,,,,,,,,,2,,,,,,"Spark 2.4 is currently explicitly disallowed by Livy, since it checks for the max version that is currently 2.3.

The internal REPL API in Spark has also changed in 2.4, so there will be changes needed in the interactive session code for things to work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-523,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 07 17:16:56 UTC 2018,,,,,,,,,,"0|i3yj2f:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Nov/18 17:16;vanzin;Issue resolved by pull request 121
[https://github.com/apache/incubator-livy/pull/121];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in session manager cleanup thread,LIVY-517,13186679,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,gyogal,vanzin,vanzin,21/Sep/18 17:59,27/Mar/19 00:50,19/Dec/25 04:15,27/Mar/19 00:50,0.5.0,,,0.7.0,,Server,,,,,,,,,,0,,,,,,"Saw this when running unit tests. Tests pass, but there is this exception in the output which may mean a more serious bug.

{noformat}
Exception in thread ""session gc thread"" java.lang.NullPointerException
        at org.apache.livy.sessions.SessionManager.delete(SessionManager.scala:111)
        at org.apache.livy.sessions.SessionManager$$anonfun$collectGarbage$2.apply(SessionManager.scala:152)
        at org.apache.livy.sessions.SessionManager$$anonfun$collectGarbage$2.apply(SessionManager.scala:152)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.immutable.List.foreach(List.scala:392)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.immutable.List.map(List.scala:296)
        at org.apache.livy.sessions.SessionManager.collectGarbage(SessionManager.scala:152)
        at org.apache.livy.sessions.SessionManager$GarbageCollector.run(SessionManager.scala:181)
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 27 00:50:40 UTC 2019,,,,,,,,,,"0|i3yd1b:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Mar/19 00:50;vanzin;Issue resolved by pull request 163
[https://github.com/apache/incubator-livy/pull/163];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secure Livy Rest API,LIVY-516,13186470,,Question,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Information Provided,,sdandey,sdandey,20/Sep/18 21:42,05/Oct/18 23:19,19/Dec/25 04:15,05/Oct/18 23:19,0.5.0,,,,,Server,,,,,,,,,,0,,,,,,HowÂ do we secure Livy Rest API by using Basic Authentication method.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 05 23:19:09 UTC 2018,,,,,,,,,,"0|i3ybqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Oct/18 23:19;vanzin;See LIVY-508.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No LivyClientFactory implementation was found,LIVY-514,13185424,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Cannot Reproduce,,ay2408,ay2408,17/Sep/18 07:25,05/Oct/18 23:18,19/Dec/25 04:15,05/Oct/18 23:18,,,,,,,,,,,,,,,,0,,,,,,"Â 

I am trying to submit jobs programmatically through Livy Scala API. I am able to do it through REST call but not through programmatic API. Here is the code:Â \{{ }}

<code>

import org.apache.livy._

import org.apache.livy.scalaapi._

Â 

import java.net.URI

import java.io.

{File, FileNotFoundException}

import scala.concurrent.duration._

import scala.concurrent.Await

object LivyClient {

Â  def main(args: Array[String]): Unit =

{ Â  Â  val livyurl = ""http://localhost:8998"" Â  Â  val javaClient = new LivyClientBuilder(false) Â  Â  Â  .setURI(new URI(livyurl)) Â  Â  Â  .build() Â  Â  val livyScalaClient = javaClient.asScalaClient Â  Â  val testJarPath = ""target/scala-2.11/SparkApp-assembly-0.1.jar"" Â  Â  livyScalaClient.uploadJar(new File(testJarPath)) Â  Â  var jobHandle = livyScalaClient.submit(JobContext => ""hello"") Â  Â  println(Await.result(jobHandle, 2000 millis)) Â  }

}

</code>

I am running this code on EMR cluster where livy server is running at port 8998. I am running this code through spark-submit.
 The code is giving error while creating javaClient. Error:Â 

Exception in thread ""main"" java.lang.IllegalStateException: No LivyClientFactory implementation was found.

at org.apache.livy.LivyClientBuilder.build(LivyClientBuilder.java:124)

at LivyClient$.main(LivyClient.scala:16)

at LivyClient.main(LivyClient.scala)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:498)

at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)

at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)

at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)

at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)

at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)

at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

Â 

Â ",on linux using spark 2.3.0 and scala 2.11 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,,Fri Oct 05 23:18:26 UTC 2018,,,,,,,,,,"0|i3y5bb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Sep/18 17:59;vanzin;Do you have {{livy-client-http}} in your dependencies?;;;","05/Oct/18 23:18;vanzin;Without an answer to the above I can only say it's a problem on your side.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: SessionHeartbeatWatchdog,LIVY-513,13185222,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,gyogal,vanzin,vanzin,14/Sep/18 18:35,27/Mar/19 00:51,19/Dec/25 04:15,27/Mar/19 00:50,,,,0.7.0,,Server,Tests,,,,,,,,,0,,,,,,"This test fails sometimes locally for me:

{noformat}
SessionHeartbeatWatchdog
- should delete only expired sessions *** FAILED *** (28 milliseconds)
  org.mockito.exceptions.verification.TooManyActualInvocations: testSession$1.stop();
Wanted 1 time:
-> at org.apache.livy.server.interactive.SessionHeartbeatSpec$$anonfun$2$$anonfun$apply$mcV$sp$5.apply$mcV$sp(SessionHeartbeatSpec.scala:83)
But was 2 times. Undesired invocation:
-> at org.apache.livy.sessions.SessionManager.delete(SessionManager.scala:111)
  at org.apache.livy.server.interactive.SessionHeartbeatSpec$$anonfun$2$$anonfun$apply$mcV$sp$5.apply$mcV$sp(SessionHeartbeatSpec.scala:83)
  at org.apache.livy.server.interactive.SessionHeartbeatSpec$$anonfun$2$$anonfun$apply$mcV$sp$5.apply(SessionHeartbeatSpec.scala:68)
  at org.apache.livy.server.interactive.SessionHeartbeatSpec$$anonfun$2$$anonfun$apply$mcV$sp$5.apply(SessionHeartbeatSpec.scala:68)
  at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
  at org.scalatest.Transformer.apply(Transformer.scala:22)
  at org.scalatest.Transformer.apply(Transformer.scala:20)
  at org.scalatest.FunSpecLike$$anon$1.apply(FunSpecLike.scala:422)
  at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
  ...
{noformat}
",,"gyogal commented on pull request #163: [LIVY-513][LIVY-517][TEST] Fix SessionHeartbeatSpec flakiness, occasional NPE
URL: https://github.com/apache/incubator-livy/pull/163
 
 
   ## What changes were proposed in this pull request?
   
   This PR aims to fix the following issues in the test suite `SessionHeartbeatSpec`:
   
    * Since there is no implementation provided for `stop()` in `mock[TestSession]`, Mockito returns `null` by default and this causes `SessionManager#delete` to throw
      `NullPointerException` when executing `map()` on the `Future[Unit]` object it expects.
      This can be observed in the unit test output (`server/target/unit-tests.log`):
   
   ```
   19/03/25 18:20:30.123 ScalaTest-main-running-SessionHeartbeatSpec INFO SessionHeartbeatSpec$$anonfun$2$TestWatchdog$1: Registering new session 0
   19/03/25 18:20:30.123 ScalaTest-main-running-SessionHeartbeatSpec INFO SessionHeartbeatSpec$$anonfun$2$TestWatchdog$1: Registering new session 1
   19/03/25 18:20:30.124 ScalaTest-main-running-SessionHeartbeatSpec INFO SessionHeartbeatSpec$$anonfun$2$TestWatchdog$1: Session 0 expired. Last heartbeat is at null.
   19/03/25 18:20:30.124 ScalaTest-main-running-SessionHeartbeatSpec WARN SessionHeartbeatSpec$$anonfun$2$TestWatchdog$1: Exception was thrown when deleting expired session 0
   java.lang.NullPointerException
           at org.apache.livy.sessions.SessionManager.delete(SessionManager.scala:123)
           at org.apache.livy.server.interactive.SessionHeartbeatWatchdog$$anonfun$deleteExpiredSessions$1.apply(SessionHeartbeat.scala:113)
           at org.apache.livy.server.interactive.SessionHeartbeatWatchdog$$anonfun$deleteExpiredSessions$1.apply(SessionHeartbeat.scala:110)
           at scala.collection.Iterator$class.foreach(Iterator.scala:891)
           at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
           at scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:206)
           at org.apache.livy.server.interactive.SessionHeartbeatWatchdog$class.deleteExpiredSessions(SessionHeartbeat.scala:110)
           at org.apache.livy.server.interactive.SessionHeartbeatSpec$$anonfun$2$TestWatchdog$1.deleteExpiredSessions(SessionHeartbeatSpec.scala:60)
   ```
        
    * In some rare cases, the SessionManager GC thread may run `collectGarbage()` after the test sessions are registered but before the test is finished. The GC thread will also attempt to delete the `mock[TestSession]` objects because the mocked session's `lastActivity()` function returns 0 by default which means`currentTime - session.lastActivity > sessionTimeout` will always be true. This causes a race condition that can have the following outcomes:
   
        1. The `stop()` method may be called twice (both by `SessionHeartbeatWatchdog#deleteExpiredSessions` and `SessionManager#collectGarbage`), resulting in a `TooManyActualInvocations` exception (LIVY-513):
   
   		```
   		  org.mockito.exceptions.verification.TooManyActualInvocations: testSession$1.stop();
   		Wanted 1 time:
   		-> at org.apache.livy.server.interactive.SessionHeartbeatSpec$$anonfun$2$$anonfun$apply$mcV$sp$5.apply$mcV$sp(SessionHeartbeatSpec.scala:93)
   		But was 2 times. Undesired invocation:
   		-> at org.apache.livy.sessions.SessionManager.delete(SessionManager.scala:124)
   		  at org.apache.livy.server.interactive.SessionHeartbeatSpec$$anonfun$2$$anonfun$apply$mcV$sp$5.apply$mcV$sp(SessionHeartbeatSpec.scala:93)
   		  at org.apache.livy.server.interactive.SessionHeartbeatSpec$$anonfun$2$$anonfun$apply$mcV$sp$5.apply(SessionHeartbeatSpec.scala:70)
   		  at org.apache.livy.server.interactive.SessionHeartbeatSpec$$anonfun$2$$anonfun$apply$mcV$sp$5.apply(SessionHeartbeatSpec.scala:70)
   		  at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
   		  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
   		  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
   		  at org.scalatest.Transformer.apply(Transformer.scala:22)
   		  at org.scalatest.Transformer.apply(Transformer.scala:20)
   		  at org.scalatest.FunSpecLike$$anon$1.apply(FunSpecLike.scala:422)
   		  at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
   		  ...
   		```
   
   	 2. The `stop()` method may be called by `collectGarbage()` after `deleteExpiredSessions()`, but before the test is finished, which causes the following error message to appear on the console (LIVY-517).
   
   		```
   		Deleting Mock for TestSession$1, hashCode: 1666638753 because it was inactive for more than 3600000.0 ms.
   		Exception in thread ""session gc thread"" java.lang.NullPointerException
   			at org.apache.livy.sessions.SessionManager.delete(SessionManager.scala:124)
   			at org.apache.livy.sessions.SessionManager$$anonfun$collectGarbage$2.apply(SessionManager.scala:171)
   			at org.apache.livy.sessions.SessionManager$$anonfun$collectGarbage$2.apply(SessionManager.scala:168)
   			at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
   			at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
   			at scala.collection.immutable.List.foreach(List.scala:392)
   			at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
   			at scala.collection.immutable.List.map(List.scala:296)
   			at org.apache.livy.sessions.SessionManager.collectGarbage(SessionManager.scala:168)
   			at org.apache.livy.sessions.SessionManager$GarbageCollector.run(SessionManager.scala:201)
   		```
   
   These race conditions can be triggered easily by inserting `Thread.sleep(x)` (where x is between 10 to 50) before the loop in `org.apache.livy.sessions.SessionManager.GarbageCollector#run` and rerunning the test.
   
   With this change applied, the SessionManager GC should not attempt to collect the `mock[TestSession]` objects created by the test, so the race condition should be eliminated and the flakiness of the test should be fixed.
   
   ## How was this patch tested?
   
   By running the existing UTs.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Mar/19 16:28;githubbot;600","asfgit commented on pull request #163: [LIVY-513][LIVY-517][TEST] Fix SessionHeartbeatSpec flakiness, occasional NPE
URL: https://github.com/apache/incubator-livy/pull/163
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Mar/19 00:50;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 27 00:50:33 UTC 2019,,,,,,,,,,"0|i3y42f:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Mar/19 00:50;vanzin;Issue resolved by pull request 163
[https://github.com/apache/incubator-livy/pull/163];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove support for Scala 2.10,LIVY-512,13185206,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,vanzin,14/Sep/18 17:49,27/Sep/18 17:19,19/Dec/25 04:15,27/Sep/18 17:19,,,,0.6.0,,,,,,,,,,,,0,release-notes,,,,,"As discussed on the mailing list, let's remove support for Scala 2.10. Since we're also dropping support for Spark 1.6 (and up to 2.1, the last version with Scala 2.10), there's no need to keep that around.

Since there will be future Scala versions, though, we should keep the infrastructure for building modules against different versions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2018-09-14 17:49:15.0,,,,,,,,,,"0|i3y3yv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump minimum Spark version to 2.2,LIVY-511,13185205,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,vanzin,14/Sep/18 17:47,27/Sep/18 17:19,19/Dec/25 04:15,27/Sep/18 17:19,,,,0.6.0,,,,,,,,,,,,0,release-notes,,,,,"As discussed in the mailing list, let's increase the minimum supported version of Spark to 2.2. 2.1 and earlier are basically EOL at this point, and anyone stuck with those versions can keep running the current Livy version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2018-09-14 17:47:18.0,,,,,,,,,,"0|i3y3yn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove support for jdk7,LIVY-510,13185204,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,vanzin,14/Sep/18 17:45,20/Sep/18 03:50,19/Dec/25 04:15,20/Sep/18 03:49,,,,0.6.0,,,,,,,,,,,,0,release-notes,,,,,"As discussed on the dev mailing list, let's remove support for building and running Livy with jdk7. This will simplify the code and the test matrix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Sep 20 03:49:46 UTC 2018,,,,,,,,,,"0|i3y3yf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Sep/18 03:49;jerryshao;Issue resolved by pull request 111
[https://github.com/apache/incubator-livy/pull/111];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
livy SQL drops leading zeros,LIVY-509,13184795,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Invalid,,gm310509,gm310509,13/Sep/18 06:25,07/Oct/18 00:30,19/Dec/25 04:15,07/Oct/18 00:30,0.4.0,0.5.0,,,,Interpreter,,,,,,,,,,0,,,,,,"The %sql drops leading zeros from columns withÂ *String* datatype.

Consider the following test data:

Â 
{code:java}
0123,zero one two three
1230,one two three zero
1010,one zero one zero

{code}
Created as an external table in Hive using:

Â 

Â 
{code:java}
create external table lz_test (
  id String,
  description String
) row format delimited
fields terminated by ','
location '/pathTo/leadingZero_test'
;
{code}
and accessed using the followingÂ scala (%livy) code:

Â 

Â 
{code:java}
val lzDF = sql(""select * from lz_Test"")
lzDF.createOrReplaceTempView(""LZT"")
lzDF.printSchema
lzDF.show(false)
{code}
and the following sql in the same notebook:

Â 

Â 
{code:java}
%sql
select *
from LZT
{code}
The result is the following (note the missing zero on the first record):

Â 

!Screen Shot 2018-09-13 at 4.21.37 pm.png!

The output of the scala code does, however, display the leading zero.

Also note the data types from the print schema: ID is a String.

Â 
{code:java}
lzDF: org.apache.spark.sql.DataFrame = [id: string, description: string]
root
|-- id: string (nullable = true)
|-- description: string (nullable = true)

+----+------------------+
|id |description |
+----+------------------+
|0123|zero one two three|
|1230|one two three zero|
|1010|one zero one zero |
+----+------------------+

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/18 06:22;gm310509;Screen Shot 2018-09-13 at 4.21.37 pm.png;https://issues.apache.org/jira/secure/attachment/12939505/Screen+Shot+2018-09-13+at+4.21.37+pm.png","13/Sep/18 06:21;gm310509;Screen Shot 2018-09-13 at 4.21.37 pm.png;https://issues.apache.org/jira/secure/attachment/12939506/Screen+Shot+2018-09-13+at+4.21.37+pm.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Oct 07 00:30:38 UTC 2018,,,,,,,,,,"0|i3y1gf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Oct/18 00:30;zjffdu;This is zeppelin issue which is fixed in ZEPPELIN-3701;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support custom auth filter for LivyServer,LIVY-508,13184312,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,jerryshao,11/Sep/18 12:04,26/Aug/21 07:56,19/Dec/25 04:15,11/Feb/19 18:01,0.5.0,,,0.6.0,,Server,,,,,,,,,,0,,,,,,"Current Livy only supports Spnego auth support, we may have different requirements for auth filter. So here propose to make Livy to support custom auth filter.",,"asfgit commented on pull request #110: [LIVY-508][Server] Support custom auth filter for livy server
URL: https://github.com/apache/incubator-livy/pull/110
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Feb/19 10:58;githubbot;600","durgaswaroop commented on pull request #110:
URL: https://github.com/apache/incubator-livy/pull/110#issuecomment-906177991


   > To be fair there isn't really any documentation about configuring the server. The config template file is currently the most complete source of information for that, so I'm fine with just the example that was added there.
   
   I know it's an old thread, but the given example configuration in the file is not very helpful. It doesn't mention, for example, which class the Filter should even extend. 
   
   I think a proper example with some sample code would be really helpful for new users. 
   
   I still don't understand what the params are for and how to use them. An example describing all of these would be quite useful. 


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/21 07:51;githubbot;600","durgaswaroop edited a comment on pull request #110:
URL: https://github.com/apache/incubator-livy/pull/110#issuecomment-906177991


   > To be fair there isn't really any documentation about configuring the server. The config template file is currently the most complete source of information for that, so I'm fine with just the example that was added there.
   
   I know it's an old thread, but the given example configuration in the file is not very helpful. It doesn't mention, for example, which class the Filter should even extend.  It hasn't been updated since then even after two years. 
   
   I think a proper example with some sample code would be really helpful for new users. 
   
   I still don't understand what the params are for and how to use them. An example describing all of these would be quite useful. 


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/21 07:52;githubbot;600","durgaswaroop commented on pull request #110:
URL: https://github.com/apache/incubator-livy/pull/110#issuecomment-906180869


   And with the following configuration, livy UI doesn't give me any option to enter a username or password. 
   
   ```
   livy.server.auth.type = dis
   livy.server.auth.dis.class = com.dis.DISAuthenticationFilter
   livy.server.auth.dis.param.username = user
   livy.server.auth.dis.param.password = password
   ```
   
   What other parameters need to be passed?


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: reviews-unsubscribe@livy.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Aug/21 07:56;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,LIVY-524,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 11 18:27:05 UTC 2019,,,,,,,,,,"0|i3xyhb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Sep/18 19:54;junping_du;+1 on having this improvement which provide flexibility of auth methods.;;;","11/Feb/19 11:01;mgaido;[~jerryshao] Sorry, seems like I don't have permissions to close the JIRA, may you please close this as ""Resolved""? Thanks.;;;","11/Feb/19 11:02;mgaido;Issue resolved by PR 100
https://github.com/apache/incubator-livy/pull/110;;;","11/Feb/19 18:02;vanzin;Done, also added you to the right jira group (forgot to do that earlier).;;;","11/Feb/19 18:27;mgaido;Thanks [~vanzin];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move RPC classes used in thrifserver in a separate module,LIVY-503,13181576,13177985,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,mgaido,mgaido,28/Aug/18 10:44,29/Nov/18 18:11,19/Dec/25 04:15,29/Nov/18 18:09,,,,0.6.0,,,,,,,,,,,,0,,,,,,"As suggested in the discussion for the original PR (https://github.com/apache/incubator-livy/pull/104#discussion_r212806490), we should move the RPC classes which need to be uploaded to the Spark session in a separate module, in order to upload as few classes as possible and avoid eventual interaction with the Spark session created.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 29 18:09:58 UTC 2018,,,,,,,,,,"0|i3xhv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"29/Nov/18 18:09;vanzin;Issue resolved by pull request 118
[https://github.com/apache/incubator-livy/pull/118];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup Hive dependencies,LIVY-502,13181575,13177985,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mgaido,mgaido,mgaido,28/Aug/18 10:42,23/Apr/19 17:58,19/Dec/25 04:15,30/Nov/18 23:57,,,,0.6.0,,,,,,,,,,,,0,,,,,,"In the starting implementation we are relying/delegating some of the work to the Hive classes used in the HiveServer2. This helped simplifying the creation of the first implementation, as it saved to write a lot of code. But this caused also a dependency on the {{hive-exec}} package, as well as compelled us to modify a bit some of the existing Hive classes.

The JIRA tracks removing these workarounds by re-implementing the same logic in Livy to get rid of all Hive dependencies, other than the rpc and service layers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 23 17:58:20 UTC 2019,,,,,,,,,,"0|i3xhuv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/Nov/18 23:57;vanzin;Issue resolved by pull request 117
[https://github.com/apache/incubator-livy/pull/117];;;","23/Apr/19 17:58;tanakahda;Hello. I'm seeing an error after this change, and created LIVY-590. I'd like to have a comment on this. Thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Spark 2.3 SparkR backend,LIVY-501,13181484,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Problem,,shanyu,shanyu,27/Aug/18 23:45,28/Aug/18 03:54,19/Dec/25 04:15,28/Aug/18 01:28,0.5.0,0.5.1,,,,Interpreter,,,,,,,,,,0,,,,,,"Spark 2.3 modified RBackend:init() method to return Tuple2[Int, RAuthHelper] instead of just an Int.

[https://github.com/apache/spark/commit/628c7b517969c4a7ccb26ea67ab3dd61266073ca]

Livy does not work with this version of RBackend because it still expecting a single Int in the return value from init().

Â ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/18 23:56;shanyu;LIVY-501.patch;https://issues.apache.org/jira/secure/attachment/12937350/LIVY-501.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 28 03:54:00 UTC 2018,,,,,,,,,,"0|i3xhan:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Aug/18 23:53;vanzin;[~jerryshao] I thought you committed the changes to support the new Spark APIs?;;;","28/Aug/18 00:01;shanyu;Spark 2.3 works except for SparkR. I just attached the patch on top of branch-0.5;;;","28/Aug/18 01:26;jerryshao;Yes, I've fixed this issue in master branch (https://github.com/apache/incubator-livy/blob/8027ca708fdc3df9a5b08d2d33d0436018154bcc/repl/src/main/scala/org/apache/livy/repl/SparkRInterpreter.scala#L93), not in branch-0.5.;;;","28/Aug/18 01:28;jerryshao;BTW, branch-0.5 doesn't claim to support Spark 2.3+.;;;","28/Aug/18 03:54;shanyu;Thanks [~jerryshao]. Where does Livy mention the compatibility with Spark versions? I checked release notes of livy 0.5 which didn't say which version of Spark does it support.

In [https://github.com/apache/incubator-livy,]Â it mentioned that:

""Livy requires at least Spark 1.6 and supports both Scala 2.10 and 2.11 builds of Spark, Livy will automatically pick repl dependencies through detecting the Scala version of Spark.

Livy also supports Spark 2.0+ for both interactive and batch submission, you could seamlessly switch to different versions of Spark throughÂ {{SPARK_HOME}}Â configuration, without needing to rebuild Livy."";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add client for testing thriftserver,LIVY-500,13181174,13177985,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mgaido,mgaido,mgaido,26/Aug/18 09:44,05/Oct/18 20:20,19/Dec/25 04:15,05/Oct/18 20:20,,,,0.6.0,,,,,,,,,,,,0,,,,,,"Other than running the UT for it, it can be useful, during dev work, to have a client for interacting with the thriftserver.

This JIRA tracks adding a beeline client for dev in order to be able to easily connect to the thriftserver.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 05 20:20:49 UTC 2018,,,,,,,,,,"0|i3xfdz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Oct/18 20:20;vanzin;Issue resolved by pull request 113
[https://github.com/apache/incubator-livy/pull/113];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add REST API for Updating Sentry Policies,LIVY-499,13180541,,Improvement,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Invalid,,schizoid,schizoid,22/Aug/18 19:00,22/Aug/18 19:03,19/Dec/25 04:15,22/Aug/18 19:03,,,,,,,,,,,,,,,,0,,,,,,We are trying to automate the {{Sentry}} policy creation/deletion and it would be ideal if {{Sentry}} offered a REST API similar to whatÂ {{Ranger}} offers.Â ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 22 19:03:40 UTC 2018,,,,,,,,,,"0|i3xbhr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Aug/18 19:01;vanzin;Wrong project?;;;","22/Aug/18 19:03;schizoid;Ooop sorry.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR interpreter fails to execute statement with Windows CRLF on *nix machine,LIVY-498,13179587,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,jerryshao,17/Aug/18 13:40,22/Aug/18 02:49,19/Dec/25 04:15,22/Aug/18 02:49,0.5.0,,,0.5.1,0.6.0,REPL,,,,,,,,,,0,,,,,,"If the issued R query contains Windows CRLF, it will be failed to run on *nix machine. This is happened when submitting queries from Windows machine via Zeppelin. So here we should convert statement to match system's EOL.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Aug 22 02:49:17 UTC 2018,,,,,,,,,,"0|i3x5mv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Aug/18 02:49;jerryshao;Issue resolved by pull request 105
[https://github.com/apache/incubator-livy/pull/105];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add basic UI for thriftserver,LIVY-495,13178584,13177985,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mgaido,mgaido,mgaido,13/Aug/18 12:50,25/Oct/18 19:44,19/Dec/25 04:15,25/Oct/18 19:44,,,,0.6.0,,,,,,,,,,,,0,,,,,,The issue tracks the implementation of a UI showing basic information about the status of the Livy thriftserver.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 25 19:44:33 UTC 2018,,,,,,,,,,"0|i3wzgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Oct/18 19:44;vanzin;Issue resolved by pull request 114
[https://github.com/apache/incubator-livy/pull/114];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add thriftserver to Livy server,LIVY-494,13178583,13177985,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mgaido,mgaido,mgaido,13/Aug/18 12:49,24/Sep/18 22:42,19/Dec/25 04:15,24/Sep/18 22:42,,,,0.6.0,,,,,,,,,,,,0,,,,,,Including the thriftserver in the Livy server. This means starting the Thriftserver at Livy server startup and adding the needed script in order to interact with it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2018-08-13 12:49:31.0,,,,,,,,,,"0|i3wzgf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add UTs to the thriftserver module,LIVY-493,13178582,13177985,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mgaido,mgaido,mgaido,13/Aug/18 12:46,10/Sep/18 03:45,19/Dec/25 04:15,10/Sep/18 03:43,,,,0.6.0,,,,,,,,,,,,0,,,,,,Tracks the implementation and addition of UT for the new Livy thriftserver.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 10 03:43:15 UTC 2018,,,,,,,,,,"0|i3wzg7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/Sep/18 03:43;jerryshao;Issue resolved by pull request 104
[https://github.com/apache/incubator-livy/pull/104];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Base implementation Livy thriftserver,LIVY-492,13178581,13177985,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mgaido,mgaido,mgaido,13/Aug/18 12:46,10/Sep/18 03:45,19/Dec/25 04:15,10/Sep/18 03:43,,,,0.6.0,,,,,,,,,,,,0,,,,,,The issue tracks the lading of the initial implementation of the Livy thriftserver,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 10 03:43:04 UTC 2018,,,,,,,,,,"0|i3wzfz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/Sep/18 03:43;jerryshao;Issue resolved by pull request 104
[https://github.com/apache/incubator-livy/pull/104];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add thriftserver module,LIVY-491,13178579,13177985,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mgaido,mgaido,mgaido,13/Aug/18 12:43,10/Sep/18 03:44,19/Dec/25 04:15,10/Sep/18 03:42,0.6.0,,,0.6.0,,Server,,,,,,,,,,0,,,,,,"Add a new module for the implementation of the Livy thriftserver.
This includes adding the base thriftserver implementation from Hive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 10 03:42:48 UTC 2018,,,,,,,,,,"0|i3wzfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/Sep/18 03:42;jerryshao;Issue resolved by pull request 104
[https://github.com/apache/incubator-livy/pull/104];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add thriftserver module,LIVY-490,13178578,13177985,Sub-task,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,mgaido,mgaido,13/Aug/18 12:41,13/Aug/18 12:44,19/Dec/25 04:15,13/Aug/18 12:44,,,,,,,,,,,,,,,,0,,,,,,Add a new module for the Thriftserver implementation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2018-08-13 12:41:43.0,,,,,,,,,,"0|i3wzfb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose a JDBC endpoint for Livy,LIVY-489,13177985,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Done,mgaido,mgaido,mgaido,09/Aug/18 12:58,29/Mar/21 18:04,19/Dec/25 04:15,03/Apr/19 16:49,0.6.0,,,0.6.0,,API,Server,,,,,,,,,4,,,,,,"Many users and BI tools use JDBC connections in order to retrieve data. As Livy exposes only a REST API, this is a limitation in its adoption. Hence, adding a JDBC endpoint may be a very useful feature, which could also make Livy a more attractive solution for end user to adopt.

Moreover, currently, Spark exposes a JDBC interface, but this has many limitations, including that all the queries are submitted to the same application, therefore there is no isolation/security, which can be offered by Livy, making a Livy JDBC API a better solution for companies/users who want to use Spark in order to run they queries through JDBC.

In order to make the transition from existing solutions to the new JDBC server seamless, the proposal is to use the Hive thrift-server and extend it as it was done by the STS.

[Here, you can find the design doc.|https://docs.google.com/document/d/18HAR_VnQLegbYyzGg8f4zwD4GtDP5q_t3K21eXecZC4/edit] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Mar 29 18:04:29 UTC 2021,,,,,,,,,,"0|i3wvrj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Aug/18 02:20;jerryshao;Hi [~mgaido] can you please also list out the tasks you're planning to do and create subtasks accordingly.;;;","13/Aug/18 09:11;mgaido;Hi [~jerryshao]. Thanks for your comment. Unfortunately I am not sure how to spit the implementation as most of the code is required for it to work. s of now, the only two tasks I have been able to split it into are:
 - Initial Thriftserver implementation;
 - Adding a Thriftserver UI.
I will keep on thinking on this anyway. Any suggestion is welcomed. Thanks.;;;","13/Aug/18 11:30;jerryshao;I see. But it is hard to review for such a big thing, it would be better to split to small pieces. You don't have to make them isolated, a series of dependent tasks is OK.;;;","13/Aug/18 12:25;jerryshao;BTW, [~mgaido] do you have a branch or a diff so that I can take a review.;;;","13/Aug/18 12:37;mgaido;Sure [~jerryshao], the branch is https://github.com/mgaido91/incubator-livy/tree/livy_thrift, and the diff is https://github.com/apache/incubator-livy/compare/master...mgaido91:livy_thrift.;;;","13/Aug/18 12:51;mgaido;[~jerryshao] I created 5 subtasks for this. Hope they are reasonable to you. Thanks.;;;","15/Aug/18 13:14;jerryshao;Hi [~mgaido], let's combine task 2, 3, 4 together, and 5, 6 separately. Also please submit a PR accordingly. I've already reviewed partly, can you please submit a PR against Livy master?;;;","15/Aug/18 13:41;mgaido;Sure [~jerryshao], thank you. I am submitting the first PR for 2, 3, 4. Thanks.;;;","04/Sep/19 08:49;vonatzki;Hi [~mgaido], I am interested in testing this feature but I do not know where to start. Does this assume that I can connect via JDBC in the same port of the UI? I tried using beeline but it seems it's not working. Would appreciate some guidance on this.

Â 

Thanks!;;;","04/Sep/19 10:25;mgaido;Hi [~vonatzki]. Please check the design doc attached here. Moreover, please check the configurations added in LivyConf. You'll see that the thriftserver must be enabled and configured properly and default values for the configurations are there.

As a suggestion, it is possible for you, I'd recommend to build Livy from master branch, even though the thriftserver is present also in the 0.6.0 version. Indeed, it was the first release for it, so might find issues which have been fixed on current master.

Thanks.;;;","04/Sep/19 23:53;vonatzki;Awesome, thanks [~mgaido]. Will check the livy.conf.;;;","07/Sep/19 09:51;vonatzki;Hi [~mgaido], sorry for being persistent. I can't figure it out even when browsing through the conf options. Which options pertain to the thriftserver? I tried connecting using beeline to both port 10000 and 8998 to no avail. I think I am missing a step that would boot the thriftserver.

Â 

I am really sorry for being such a newbie on this.;;;","07/Sep/19 10:19;mgaido;[~vonatzki] nothing to be sorry about. Please see https://github.com/apache/incubator-livy/blob/master/server/src/main/scala/org/apache/livy/LivyConf.scala#L101. {{livy.server.thrift.enabled}} must be set to true, the port is controlled by https://github.com/apache/incubator-livy/blob/master/server/src/main/scala/org/apache/livy/LivyConf.scala#L109. Then you should be able to connect using the beeline you can find in the repo or with any Hive 3 client. Thanks.;;;","07/Sep/19 10:33;vonatzki;I see now. All this time I've been looking in the wrong file! I thought you were referring to the {{livy.conf.template}} file under {{/conf}} folder.

Thank you so much!

Made me think: If I put {{livy.server.thrift.enabled}} and {{livy.server.thrift.port}} in the {{livy.conf}}, will that work?;;;","07/Sep/19 10:46;mgaido;Yes, sure. The point is: that template in not exhaustive and does not contain all the configs which can be set.;;;","07/Sep/19 12:58;vonatzki;Noted and thank you! If I have some more noob question, what is the appropriate thread for it?;;;","07/Sep/19 14:55;mgaido;The right place is the user mailing list, please see https://livy.apache.org/community/.;;;","23/Mar/21 21:50;akshay.kotechajain;HelloÂ [~mgaido] 
Is there a working example of this? I tried to run the beeline command and it does not seem to get connected to an already running LIVY session.;;;","29/Mar/21 17:53;mgaido;[~akshay.kotechajain]Â you can find an example in the tests ([https://github.com/apache/incubator-livy/blob/4d8a912699683b973eee76d4e91447d769a0cb0d/thriftserver/server/src/test/scala/org/apache/livy/thriftserver/ThriftServerSuites.scala#L448).]Â In general you need to specify {{livy.server.sessionId}} equal to the ID of the session you want to connect to while starting your connection with your client. Please notice that you can connect only to scala sessions.;;;","29/Mar/21 18:04;akshay.kotechajain;oh, just scala. that makes sense. thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,
livy 0.5 on cdh 5.14.0 throw java.io.FileNotFoundException,LIVY-486,13173092,,Question,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Problem,,xianhua111,xianhua111,19/Jul/18 03:16,25/Jul/18 12:46,19/Dec/25 04:15,25/Jul/18 12:46,0.5.0,,,,,,,,,,,,,,,0,,,,,,"hi:

my cluster is cdh 5.14.0Â  with spark2.2 .

livy 0.5.0 run with confï¼š

livy.conf:

livy.spark.master = yarn

livy.spark.deploy-mode = client

livy-env.sh:

export SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2
 export HADOOP_CONF_DIR=/etc/hadoop
 export LIVY_LOG_DIR=/var/log/livy

create session with curl command:

curl -X POST -d '\{""kind"": ""spark""}' -H ""Content-Type: application/json"" localhost:8998/sessions

then the log as follow:
{code:java}
// code placeholder
18/07/19 10:11:16 INFO yarn.Client: client token: N/A diagnostics: N/A ApplicationMaster host: N/A ApplicationMaster RPC port: -1 queue: root.users.hdfs start time: 1531966275080 final status: UNDEFINED tracking URL: http://scm.iwellmass.com:8088/proxy/application_1531876562801_0010/ user: hdfs 18/07/19 10:11:17 INFO yarn.Client: Application report for application_1531876562801_0010 (state: FAILED) 18/07/19 10:11:17 INFO yarn.Client: client token: N/A diagnostics: Application application_1531876562801_0010 failed 1 times due to AM Container for appattempt_1531876562801_0010_000001 exited with exitCode: -1000 For more detailed output, check application tracking page:http://scm.iwellmass.com:8088/proxy/application_1531876562801_0010/Then, click on links to logs of each attempt. Diagnostics: java.io.FileNotFoundException: File file:/opt/livy-0.5.0-incubating-bin/repl_2.11-jars/commons-codec-1.9.jar does not exist Failing this attempt. Failing the application. ApplicationMaster host: N/A ApplicationMaster RPC port: -1 queue: root.users.hdfs start time: 1531966275080 final status: FAILED tracking URL: http://scm.iwellmass.com:8088/cluster/app/application_1531876562801_0010 user: hdfs 18/07/19 10:11:17 INFO yarn.Client: Deleted staging directory file:/var/lib/hadoop-hdfs/.sparkStaging/application_1531876562801_0010 18/07/19 10:11:17 ERROR spark.SparkContext: Error initializing SparkContext. org.apache.spark.SparkException: Yarn application has already ended! It might have been killed or unable to launch application master. at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:85) at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62) at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:165) at org.apache.spark.SparkContext.<init>(SparkContext.scala:512) at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2511) at org.apache.spark.SparkContext.getOrCreate(SparkContext.scala) at org.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:51) at org.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:72) at org.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:69) at org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:95) at org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:70) at org.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:70) at org.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:340) at org.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:70) at org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:128) at org.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:122) at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 18/07/19 10:11:17 INFO server.AbstractConnector: Stopped Spark@2fa7233{HTTP/1.1,[http/1.1]} {0.0.0.0:4040} 18/07/19 10:11:17 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.10.234:4040
{code}
Â 

it said :Â Â 
{code:java}
file:/opt/livy-0.5.0-incubating-bin/repl_2.11-jars/commons-codec-1.9.jar does not exist{code}
butÂ  the jar file is actual exist

Â 

is there anyÂ solutions?

thanks

Â 

Â 

Â 

Â 

Â ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2018-07-19 03:16:40.0,,,,,,,,,,"0|i3w1vr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for hadoop.security.auth_to_local ,LIVY-481,13170402,,New Feature,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Invalid,,Tagar,Tagar,06/Jul/18 03:53,30/Jan/22 00:42,19/Dec/25 04:15,03/Jan/19 22:07,0.5.0,0.6.0,,,,Core,,,,,,,,,,0,auth_to_local,authentication,,,,"Would be great to have support for
{code:java}
hadoop.security.auth_to_local{code}
Â 

[https://hortonworks.com/blog/fine-tune-your-apache-hadoop-security-settings/]

[https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_sg_kerbprin_to_sn.html]Â 

[https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html]Â 

Â 

PS. I was thinkingÂ {code}livy.server.auth.kerberos.name-rules {code} could provide something similar based on config name, but can't find any confirmation to this in documentation, nor in code.. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jan 30 00:42:21 UTC 2022,,,,,,,,,,"0|i3vlkv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Jan/19 22:07;Tagar;livy.server.auth.kerberos.name_rules ;;;","30/Jan/22 00:36;jeff.xu.z@gmail.com;Just want to post a real example of how to set this property. I spent a few hours to figure this out, which is the last piece of a working setup of Kerberos cross-realm using Livy API using AWS EMR.

Â 
{noformat}
# /etc/livy/conf/livy.conf
livy.server.auth.kerberos.name_rules = RULE:[1:$1@$0](.*@EXAMPLE\.COM)s/@.*//g\nRULE:[1:$1@$0](.*@)s/@.*///L\nDEFAULT {noformat};;;","30/Jan/22 00:42;jeff.xu.z@gmail.com;A little bit additional info for the name_rules example above, here's my 2 kerberos realm setup in AWS.
 # I applied the livy.conf on Kerberized Livy server running in COMPUTE.INTERNAL realm.
 # I used a user principal from EXAMPLE.COM to connect to the Livy server to run a simple Spark query.

{noformat}
[realms]

Â  Â  COMPUTE.INTERNAL = {
Â  Â  Â  Â  kdc = <hostname>.compute.internal:88
Â  Â  Â  Â  admin_server = <hostname>.compute.internal:749
Â  Â  Â  Â  default_domain = us-west-2.compute.internal
Â  Â  }

Â  Â  EXAMPLE.COM = {
Â  Â  Â  Â  kdc = kdc.example.com:88
Â  Â  Â  Â  admin_server = kdc.example.com:749
Â  Â  }

[domain_realm]
Â  Â  .us-west-2.compute.internal = COMPUTE.INTERNAL
Â Â  Â  us-west-2.compute.internal = COMPUTE.INTERNAL
Â  Â  .example.com = EXAMPLE.COM
Â Â  Â  example.com = EXAMPLE.COM {noformat};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache livy and Ajax,LIVY-480,13170035,,Request,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Invalid,,Melchi,Melchi,04/Jul/18 10:07,05/Jul/18 03:26,19/Dec/25 04:15,05/Jul/18 03:26,0.5.0,,,,,API,Interpreter,REPL,Server,,,,,,,0,,,,,,"Good morning every one,

can someone help me with how to send a post request by ajax to the apache livy server? I tried my best but I don't see how to do that. I have a textarea in which user can put his spark code. I want to send that code to the apache livy by ajax and get the result. I don't if my question is clear. Thank you.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 05 03:26:07 UTC 2018,,,,,,,,,,"0|i3vjbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Jul/18 03:26;jerryshao;Please ask question in the mail list, to see if someone can answer your questions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable livy.rsc.launcher.address configuration,LIVY-479,13167454,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,taoli-hwx,taoli-hwx,taoli-hwx,21/Jun/18 18:00,19/Dec/18 22:31,19/Dec/25 04:15,19/Dec/18 22:30,0.5.0,,,0.6.0,,RSC,,,,,,,,,,0,,,,,,"The current behavior is that we are settingÂ livy.rsc.launcher.address to RPC server and ignoring whatever is specified in configs. However in some scenarios there is a need for the user to be able to explicitly configureÂ it.Â For example, the IP address for an active Livy server might change over the time, so rather than using the fixed IP address, weÂ need to specify this setting to a more generic host name. For this reason, we want to enable the capability of user side configurations.Â ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 19 22:30:15 UTC 2018,,,,,,,,,,"0|i3v3nj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"21/Jun/18 18:59;taoli-hwx;PR created:Â https://github.com/apache/incubator-livy/pull/101;;;","27/Jun/18 17:14;taoli-hwx;[~jerryshao] can you please take a look at this change? Thanks!;;;","28/Jun/18 00:39;jerryshao;Yes, I've left the comments on the PR.;;;","26/Jul/18 23:25;taoli-hwx;Thanks [~jerryshao] I have replied.;;;","19/Dec/18 22:30;vanzin;Issue resolved by pull request 101
[https://github.com/apache/incubator-livy/pull/101];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Livy Scala version to 2.11.12,LIVY-477,13165244,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,jerryshao,jerryshao,jerryshao,11/Jun/18 05:49,12/Jun/18 12:54,19/Dec/25 04:15,12/Jun/18 12:54,0.5.0,,,0.6.0,,Build,,,,,,,,,,0,,,,,,"Scala version below 2.11.12 has CVE ([https://scala-lang.org/news/security-update-nov17.html),]Â and Spark will also upgrade its supported version to 2.11.12.Â 

So here upgrading Livy's Scala version also.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 12 12:53:56 UTC 2018,,,,,,,,,,"0|i3upr3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Jun/18 12:53;jerryshao;Issue resolved by pull request 100
[https://github.com/apache/incubator-livy/pull/100|https://github.com/apache/incubator-livy/pull/100];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing log-level INFO,LIVY-476,13164412,,Question,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Problem,,AntoineLy,AntoineLy,06/Jun/18 09:50,07/Jun/18 12:56,19/Dec/25 04:15,07/Jun/18 12:56,0.4.0,,,,,API,,,,,,,,,,0,,,,,,"I am using the LIVY API with a web application. We use it to submit spark job then collect results to print them on the front page. To do so, we have to interpret the LIVY callback.

However, despite we changed the log level (log4j params changed to ERROR instead of INFO), we get INFO message that prevent the application to interpret properly the LIVY output. (see image below). It happens only with RSpark session.

We also tried to to change log4j log level directly in hdfs and spark but no success...

Could you please help on that to be able to remove the underlined lines below?

Â 

Thanks a lot.

!scrren.jpg!

Â 

Â 

Â 

Â 

Â ",Azure ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/18 09:49;AntoineLy;scrren.jpg;https://issues.apache.org/jira/secure/attachment/12926705/scrren.jpg",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 07 11:46:46 UTC 2018,,,,,,,,,,"0|i3ukmf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Jun/18 11:24;jerryshao;Not super sure about your issues, can you please explain more?;;;","07/Jun/18 11:46;AntoineLy;Actually, just found that it comes form spark log4j params and this cannot be managed from Livy. If someone has the same question, below is what you can do to remove log INFO:

Â 

Set spark.driver.extraJavaOptions to

-Dhdp.version=\{{hdp_full_version}} -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -*Dlog4jspark.root.logger=ERROR,console,RFA,ETW,Anonymizer* -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=45

Â 

Set spark.executor.extraJavaOptions to

-Dhdp.version=\{{hdp_full_version}} -Detwlogger.component=sparkexecutor -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -*Dlog4jspark.root.logger=ERROR,console,RFA,ETW,Anonymizer* -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkexecutor.log -Dlog4j.configuration=file:/usr/hdp/current/spark2-client/conf/log4j.properties -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=45

Â 

The tricky thing is that according to distribution you are using and components, you have several log4j options. Hard to find the correct one....Â ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support of Hadoop CredentialProvider API for livy.keystore.password,LIVY-475,13162958,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,idzikovsky,idzikovsky,idzikovsky,30/May/18 17:57,31/May/18 11:55,19/Dec/25 04:15,31/May/18 11:53,0.5.0,,,0.6.0,,Server,,,,,,,,,,0,,,,,,"It would be good to have an option to get ""livy.keystore.password"" and ""livy.key-password"" by usingÂ Hadoop CredentialProvider API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 31 11:53:57 UTC 2018,,,,,,,,,,"0|i3ubnr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/May/18 18:09;idzikovsky;PR:Â https://github.com/apache/incubator-livy/pull/99;;;","31/May/18 11:53;jerryshao;Issue resolved by pull request 99
[https://github.com/apache/incubator-livy/pull/99];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor refactor of integration test to remove some old codes,LIVY-473,13162336,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,jerryshao,jerryshao,jerryshao,28/May/18 07:12,29/May/18 05:51,19/Dec/25 04:15,29/May/18 05:51,,,,0.6.0,,,,,,,,,,,,0,,,,,,"Integration test has some legacy codes related to different cluster type (mini or real). Since now we already use real Spark package to do test and partially removed that code, we should refactor to remove all the unused legacy code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 29 05:51:42 UTC 2018,,,,,,,,,,"0|i3u7tr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"29/May/18 05:51;jerryshao;Issue resolved by pull request 97
[https://github.com/apache/incubator-livy/pull/97];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the logs for fail-to-create session,LIVY-472,13161300,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,jerryshao,jerryshao,jerryshao,23/May/18 05:13,25/May/18 01:36,19/Dec/25 04:15,25/May/18 01:36,0.5.0,,,0.5.1,0.6.0,Server,,,,,,,,,,0,,,,,,"Livy currently doesn't give a very clear log about the fail-to-create session, it only says that session related app tag cannot be found in RM, but doesn't tell user how to search and get the true root cause. So here change the logs to make it more clear.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 25 01:36:36 UTC 2018,,,,,,,,,,"0|i3u1gn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/May/18 01:36;jerryshao;Issue resolved by pull request 96
[https://github.com/apache/incubator-livy/pull/96];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reverse proxy support for webui,LIVY-468,13158431,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,m-wcislo,m-wcislo,m-wcislo,10/May/18 10:04,23/May/18 02:30,19/Dec/25 04:15,23/May/18 02:27,0.6.0,,,0.6.0,,Server,,,,,,,,,,0,,,,,,"Support for making Livy work behind reverse proxy (e.g.: Kong). The problem is that Livy webui is working on ""/"" path. The idea is to make it work on for example ""/my-livy/"" path.

It is not working by default because html/css/js is generating urls using ""/"". Code is executed in user browser which then make redirection/requests to non-existing sources.

Â 

Proposed solution is to add parameter in livy.conf to setup base path on livy startup.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 23 02:28:44 UTC 2018,,,,,,,,,,"0|i3tk87:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/May/18 10:15;m-wcislo;Hi,

I've already prepared working change on [https://github.com/nokia/incubator-livy/tree/LIVY-468_reverse_proxy_support] .

Â 

Waiting for your comments and some guidance on how to approach tests for this, integration tests?

Â 

BR,

MichaÅ‚.;;;","10/May/18 18:34;ajbozarth;Feel free to open a WIP PR with your branch, it will be easier to give feedback that way;;;","12/May/18 05:12;m-wcislo;Created PR at [https://github.com/apache/incubator-livy/pull/93] .;;;","23/May/18 02:27;ajbozarth;[~jerryshao] I can't seem to assign this to Michal, can you do it or do we need help from the sysadmin?;;;","23/May/18 02:28;jerryshao;Ok, I will do it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy kills spark streaming job after one hour,LIVY-467,13158417,,Improvement,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Problem,,Kevin Yao,Kevin Yao,10/May/18 09:28,10/May/18 18:41,19/Dec/25 04:15,10/May/18 18:41,,,,,,,,,,,,,,,,0,,,,,,"I submit a spark streaming job by Livy, but after one hour Livy kills the job. At that time, streaming job is running normally. I want spark-streaming job to run perpetually. I don't know why Livy kills a running job. And if Livy doesn't support streaming job runningÂ perpetually, what solution does Livy take ?

The attachment is Livy log.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/18 09:28;Kevin Yao;spark_streaming_livy_log.txt;https://issues.apache.org/jira/secure/attachment/12922810/spark_streaming_livy_log.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 10 18:41:06 UTC 2018,,,,,,,,,,"0|i3tk53:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/May/18 18:41;ajbozarth;This was fixed by LIVY-306Â in the 0.4 release, if you're still seeing this on master feel free to reopen this and ping me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RSCDriver throws exception during RPC shutdown,LIVY-466,13156524,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,jerryshao,02/May/18 07:38,03/May/18 06:37,19/Dec/25 04:15,03/May/18 06:37,0.5.0,,,0.5.1,0.6.0,RSC,,,,,,,,,,0,,,,,,"During RSCDriver's shutdown, it will first shutdown RPC server, and then all the RPC clients. When RPC client is closed, it will register a timeout to avoid orphaned RSCDriver, but this is not necessary during RSCDriver's shutdown, and will throw an exception as show below, so here fixing this issue.
{noformat}
18/05/02 14:03:53 INFO utils.LineBufferedStream: 18/05/02 14:03:53 WARN DefaultPromise: An exception was thrown by org.apache.livy.rsc.Utils$2.operationComplete()
18/05/02 14:03:53 INFO utils.LineBufferedStream: java.util.concurrent.RejectedExecutionException: event executor terminated
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:821)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:327)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:320)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:746)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.util.concurrent.AbstractScheduledEventExecutor.schedule(AbstractScheduledEventExecutor.java:195)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.util.concurrent.AbstractScheduledEventExecutor.schedule(AbstractScheduledEventExecutor.java:140)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.util.concurrent.AbstractEventExecutorGroup.schedule(AbstractEventExecutorGroup.java:50)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at org.apache.livy.rsc.driver.RSCDriver.setupIdleTimeout(RSCDriver.java:238)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at org.apache.livy.rsc.driver.RSCDriver.access$100(RSCDriver.java:70)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at org.apache.livy.rsc.driver.RSCDriver$2.onSuccess(RSCDriver.java:220)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at org.apache.livy.rsc.driver.RSCDriver$2.onSuccess(RSCDriver.java:216)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at org.apache.livy.rsc.Utils$2.operationComplete(Utils.java:108)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:82)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannel$CloseFuture.setClosed(AbstractChannel.java:1148)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannel$AbstractUnsafe.doClose0(AbstractChannel.java:764)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannel$AbstractUnsafe.close(AbstractChannel.java:740)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannel$AbstractUnsafe.close(AbstractChannel.java:611)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.DefaultChannelPipeline$HeadContext.close(DefaultChannelPipeline.java:1301)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.invokeClose(AbstractChannelHandlerContext.java:624)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.close(AbstractChannelHandlerContext.java:608)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.ChannelDuplexHandler.close(ChannelDuplexHandler.java:73)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.invokeClose(AbstractChannelHandlerContext.java:624)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.close(AbstractChannelHandlerContext.java:608)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.close(AbstractChannelHandlerContext.java:465)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.DefaultChannelPipeline.close(DefaultChannelPipeline.java:973)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannel.close(AbstractChannel.java:238)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at org.apache.livy.rsc.rpc.Rpc.close(Rpc.java:310)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at org.apache.livy.rsc.driver.RSCDriver.shutdownServer(RSCDriver.java:311)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at org.apache.livy.rsc.driver.RSCDriver.shutdown(RSCDriver.java:133)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at org.apache.livy.rsc.driver.RSCDriver.handle(RSCDriver.java:398)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at java.lang.reflect.Method.invoke(Method.java:497)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at org.apache.livy.rsc.rpc.RpcDispatcher.handleCall(RpcDispatcher.java:133)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at org.apache.livy.rsc.rpc.RpcDispatcher.channelRead0(RpcDispatcher.java:80)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.handler.codec.ByteToMessageCodec.channelRead(ByteToMessageCodec.java:103)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
18/05/02 14:03:53 INFO utils.LineBufferedStream: at java.lang.Thread.run(Thread.java:745){noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 03 06:37:46 UTC 2018,,,,,,,,,,"0|i3t8vb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/May/18 06:37;jerryshao;Issue resolved by pull request 90
[https://github.com/apache/incubator-livy/pull/90];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue starting Livy using ./bin/livy-server,LIVY-462,13155904,,Documentation,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,ajbozarth,appunni.m,appunni.m,28/Apr/18 15:45,02/May/18 19:50,19/Dec/25 04:15,02/May/18 19:50,,,,,,Docs,,,,,,,,,,0,,,,,,"In documentation, it was not directly clear to use ./bin/livy-server start. It's not trivial to assume people know what they are doing. As many times, those who just want to explore the project first before diving in, or a product manager. Please change the linesÂ 

Then start the server with:

{{./bin/livy-server}}

{{WithÂ }}

Then start the server with:

{{./bin/livy-server start}}

Â ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 30 00:44:18 UTC 2018,,,,,,,,,,"0|i3t51r:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/Apr/18 00:36;ajbozarth;That usage is a holder-over from earlier versions of Livy.Â The only difference between the two command you listed is that start will start Livy detached and setup logging whereas without runs Livy in the given terminal session and logs go to stdout/stderr. Given start is the more correct version for the current Livy I'll update the website as suggested.;;;","30/Apr/18 00:44;ajbozarth;Opened a PRÂ https://github.com/apache/incubator-livy-website/pull/22;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot upload Jar file using LivyClientBuilder in Scala,LIVY-461,13154808,,Question,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Cannot Reproduce,,LianaN,LianaN,24/Apr/18 13:43,02/May/18 06:05,19/Dec/25 04:15,02/May/18 06:05,,,,,,Batch,,,,,,,,,,0,,,,,,"I am using Livy on Docker and then I submit Spark job to Livy server from Scala:

{{scalaClient = new LivyClientBuilder()}}
{{ .setURI(""http://0.0.0.0:8998""). Â // checked localhost as well}}
{{ .build()}}

{{println(""> Uploading Jar file"")}}
{{scalaClient.uploadJar(new File(myLocalJarPath)).get()}}

Â 

The step ""Uploading Jar file"" takes forever.

How can I figure out what's happening?

Â 

I checked that [http://localhost:8998|http://localhost:8998/]Â outputs the following. So, Apache Livy seems to be up and running. Spark master and workers are up as well.
h1. Operational Menu
 * [Metrics|http://localhost:8998/metrics?pretty=true]
 * [Ping|http://localhost:8998/ping]
 * [Threads|http://localhost:8998/threads]
 * [Healthcheck|http://localhost:8998/healthcheck?pretty=true]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 02 06:04:26 UTC 2018,,,,,,,,,,"0|i3sydr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"02/May/18 06:04;jerryshao;I just verified with PiApp example included in Livy, it seems work fine.Â ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when using livy with spark2.3, something wrong with the stdout and stderr",LIVY-460,13154726,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,jerryshao,cq365423762,cq365423762,24/Apr/18 08:21,26/Apr/18 01:37,19/Dec/25 04:15,26/Apr/18 01:37,0.4.0,,,0.6.0,,,,,,,,,,,,0,,,,,,"When using livy with spark2.3, something wrong with the stdout and stderr.

The stderr log of spark is redirect to stdout in livy.

!image-2018-04-24-16-19-42-211.png!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/18 08:19;cq365423762;image-2018-04-24-16-19-42-211.png;https://issues.apache.org/jira/secure/attachment/12920419/image-2018-04-24-16-19-42-211.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 26 01:37:28 UTC 2018,,,,,,,,,,"0|i3sxvj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Apr/18 18:08;ajbozarth;This comes from line 41 in LineBufferedStream which was added inÂ [LIVY-359|https://issues.cloudera.org/browse/LIVY-359]Â 
{quote}{{info(s""stdout: $line"")}}
{quote}
[~kpraveen] you originally added the line, why is it add ""stdout"" to all string?

Â 

Also [~cq365423762] are you sure it's redirecting to stdout and not just adding the string stdout to a stderr output?;;;","25/Apr/18 02:10;cq365423762;[~ajbozarth]

>Â  AlsoÂ [~cq365423762]Â are you sure it's redirecting to stdout and not just adding the string stdout to a stderr output?

Actually, I am not sure.;;;","25/Apr/18 02:12;jerryshao;I think the word ""stdout"" is misleading, actually it still captured from stderr. I think we can improving wording things.;;;","26/Apr/18 01:37;jerryshao;Issue resolved by pull request 89
[https://github.com/apache/incubator-livy/pull/89];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade jackson version from 2.9.2 to 2.9.5,LIVY-458,13153219,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,jerryshao,18/Apr/18 02:56,18/Apr/18 08:52,19/Dec/25 04:15,18/Apr/18 08:52,0.5.0,0.6.0,,0.6.0,,Build,,,,,,,,,,0,,,,,,"Due to several security issues of jackson databind module (CVE-2018-5968, CVE-2017-17485, CVE-2018-7489), here propose to upgrade jackson version 2.9.5.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 18 08:52:10 UTC 2018,,,,,,,,,,"0|i3somv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Apr/18 08:52;jerryshao;Issue resolved by pull request 87
[https://github.com/apache/incubator-livy/pull/87];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark `sqlContext.sparkSession` incorrect on Spark 2.x,LIVY-457,13152770,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,danfike,danfike,16/Apr/18 16:51,18/Apr/18 02:30,19/Dec/25 04:15,18/Apr/18 02:30,0.6.0,,,0.5.1,0.6.0,,,,,,,,,,,0,,,,,,"It looks like the {{SQLContext}} we create in {{PySpark}} sessions isn't constructed correctly. Compare how the behavior has changed between Livy 0.4.0 and what is currently on {{master}} (0.6.0).

Livy 0.4.0
{code}
$ curl --silent -X POST --data '{""kind"": ""pyspark""}' -H ""Content-Type: application/json"" localhost:8998/sessions | python -m json.tool

$ curl --silent localhost:8998/sessions/1/statements -X POST -H 'Content-Type: application/json' -d '{""code"":""sqlContext.sparkSession""}' | python -m json.tool

$ curl --silent localhost:8998/sessions/1/statements/0 | python -m json.tool
{
    ""id"": 0,
    ""state"": ""available"",
    ""output"": {
        ""status"": ""ok"",
        ""execution_count"": 0,
        ""data"": {
            ""text/plain"": ""<pyspark.sql.session.SparkSession object at 0x15a26d0>""
        }
    },
    ""progress"": 1.0
}
{code}

Livy 0.6.0
{code}
$ curl --silent -X POST --data '{""kind"": ""pyspark""}' -H ""Content-Type: application/json"" localhost:8998/sessions | python -m json.tool

$ curl --silent localhost:8998/sessions/0/statements -X POST -H 'Content-Type: application/json' -d '{""code"":""sqlContext.sparkSession""}' | python -m json.tool

$ curl --silent localhost:8998/sessions/0/statements/0 | python -m json.tool
{
    ""id"": 0,
    ""code"": ""sqlContext.sparkSession"",
    ""state"": ""available"",
    ""output"": {
        ""status"": ""ok"",
        ""execution_count"": 0,
        ""data"": {
            ""text/plain"": ""JavaObject id=o4""
        }
    },
    ""progress"": 1.0
}

$ curl --silent localhost:8998/sessions/0/statements -X POST -H 'Content-Type: application/json' -d '{""code"":""sqlContext.sparkSession.toString()""}' | python -m json.tool

$ curl --silent localhost:8998/sessions/0/statements/1 | python -m json.tool
{
    ""id"": 1,
    ""code"": ""sqlContext.sparkSession.toString()"",
    ""state"": ""available"",
    ""output"": {
        ""status"": ""ok"",
        ""execution_count"": 1,
        ""data"": {
            ""text/plain"": ""'org.apache.spark.sql.hive.HiveContext@200334d0'""
        }
    },
    ""progress"": 1.0
}
{code}

Notice how the value of {{sqlContext.sparkSession}} went from a {{pyspark.sql.session.SparkSession}} to a {{org.apache.spark.sql.hive.HiveContext}}?

I suspect this is because of the change @ https://github.com/apache/incubator-livy/commit/c1aafeb6cb87f2bd7f4cb7cf538822b59fb34a9c#diff-c58e3946d3530f54014129c268988e01R563 passing {{jsqlc}} in as the second positional parameter to {{SQLContext}}, whereas the diff @ https://github.com/apache/spark/commit/89addd40abdacd65cc03ac8aa5f9cf3dd4a4c19b#diff-74ba016ef40c1cb268e14aee817d71bdR50 suggests it should be the _third_ positional parameter.

I'd wager the fix is simply to explicitly pass that parameter as a keyword argument instead.
{code}
sqlc = SQLContext(sc, jsqlContext=jsqlc)
{code}","RHEL6, Spark 2.1.2.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 18 02:30:22 UTC 2018,,,,,,,,,,"0|i3slvr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/Apr/18 18:30;danfike;CCÂ [~jerryshao2015], [~rickbernotas];;;","17/Apr/18 05:58;jerryshao;Yes, we also met this issue today. The signature of creating SQLContext is different in Spark2, and we're still using the Spark1's constructor, which will will throw an exception.;;;","17/Apr/18 06:18;jerryshao;I made a PR about this issue (https://github.com/apache/incubator-livy/pull/86).;;;","18/Apr/18 02:30;jerryshao;Issue resolved by pull request 86
[https://github.com/apache/incubator-livy/pull/86];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Livy Spark SQL ""not a primitive class java.math.BigDecimal"" when querying decimal columns",LIVY-455,13149867,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,cartershanklin,cartershanklin,04/Apr/18 01:50,11/Apr/18 02:25,19/Dec/25 04:15,11/Apr/18 02:25,,,,0.5.1,0.6.0,,,,,,,,,,,0,,,,,,"I was using the new SQL option introduced by LIVY-19 and ran into what appears to be a limitation with DECIMAL support.

The set of queries I run roughly looks like this:

{code}
drop table if exists sales_fact_1998;
create external table sales_fact_1998(
    product_id int,time_id int,customer_id int,promotion_id int,
    store_id int,store_sales decimal(10,4),store_cost decimal(10,4),
    unit_sales decimal(10,4))
stored as parquet
location '${root}/sales_fact_1998';

select
    customer_id,
    sum(store_sales) - sum(store_cost) gross_profit
from
    sales_fact_1998
where
    store_id=${store_id}
    and time_id=${time_id}
group by
    customer_id
order by
    gross_profit desc
limit 10;
{code}

The full response I get from Livy is:
{code}
{""id"":2,""code"":""\n\nselect\n    customer_id,\n    sum(store_sales) - sum(store_cost) gross_profit\nfrom\n    sales_fact_1998\nwhere\n    store_id=12\n    and time_id=1037\ngroup by\n    customer_id\norder by\n    gross_profit desc\nlimit 10"",""state"":""available"",""output"":{""status"":""error"",""execution_count"":2,""ename"":""Error"",""evalue"":""not a primitive class java.math.BigDecimal"",""traceback"":[""scala.sys.package$.error(package.scala:27)"",""org.apache.livy.shaded.json4s.Extraction$.writePrimitive(Extraction.scala:216)"",""org.apache.livy.shaded.json4s.Extraction$.internalDecomposeWithBuilder(Extraction.scala:127)"",""org.apache.livy.shaded.json4s.Extraction$.internalDecomposeWithBuilder(Extraction.scala:141)"",""org.apache.livy.shaded.json4s.Extraction$.internalDecomposeWithBuilder(Extraction.scala:151)"",""org.apache.livy.shaded.json4s.Extraction$.decomposeWithBuilder(Extraction.scala:67)"",""org.apache.livy.shaded.json4s.Extraction$.decompose(Extraction.scala:194)"",""org.apache.livy.repl.SQLInterpreter.execute(SQLInterpreter.scala:101)"",""org.apache.livy.repl.Session$$anonfun$7.apply(Session.scala:274)"",""org.apache.livy.repl.Session$$anonfun$7.apply(Session.scala:272)"",""scala.Option.map(Option.scala:146)"",""org.apache.livy.repl.Session.org$apache$livy$repl$Session$$executeCode(Session.scala:272)"",""org.apache.livy.repl.Session$$anonfun$execute$1.apply$mcV$sp(Session.scala:168)"",""org.apache.livy.repl.Session$$anonfun$execute$1.apply(Session.scala:163)"",""org.apache.livy.repl.Session$$anonfun$execute$1.apply(Session.scala:163)"",""scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)"",""scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)"",""java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)"",""java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)"",""java.lang.Thread.run(Thread.java:748)""]},""progress"":1.0}
{code}

My REST calls are:
{code}
POST {} to http://192.168.59.21:9999/sessions

POST {""code"": ""drop table if exists sales_fact_1998"", ""kind"": ""sql""} to http://192.168.59.21:9999/sessions/1/statements

POST {""code"": ""\ncreate external table sales_fact_1998(\n    product_id int,time_id int,customer_id int,promotion_id int,\n    store_id int,store_sales decimal(10,4),store_cost decimal(10,4),\n    unit_sales decimal(10,4))\nstored as parquet\nlocation '/apps/hive/warehouse/foodmart_parquet.db/sales_fact_1998'"", ""kind"": ""sql""} to http://192.168.59.21:9999/sessions/1/statements

POST {""code"": ""\n\nselect\n    customer_id,\n    sum(store_sales) - sum(store_cost) gross_profit\nfrom\n    sales_fact_1998\nwhere\n    store_id=12\n    and time_id=1037\ngroup by\n    customer_id\norder by\n    gross_profit desc\nlimit 10"", ""kind"": ""sql""} to http://192.168.59.21:9999/sessions/1/statements
{code}

If I change the last statement to say cast(sum(store_sales) - sum(store_cost) as double) everything works.

If anyone wants the Parquet file, find it at https://github.com/cartershanklin/structor/tree/master/modules/sample_hive_data/files/foodmart_parquet.db/sales_fact_1998",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 11 02:25:03 UTC 2018,,,,,,,,,,"0|i3s41r:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Apr/18 01:40;jerryshao;Thanks for the reporting, seems like there's some bug in JSON encoding, let me try to debug.;;;","11/Apr/18 02:25;jerryshao;Issue resolved by pull request 85
[https://github.com/apache/incubator-livy/pull/85];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating batch sessions seems to be synchronous. Is there any plan to make it asynchronous ?,LIVY-454,13148137,,Question,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Won't Fix,,kondziolka,kondziolka,27/Mar/18 06:24,02/Apr/18 03:08,19/Dec/25 04:15,02/Apr/18 03:08,0.4.0,0.5.0,,,,Batch,Server,,,,,,,,,0,,,,,,"Creating batch sessions seems to be synchronous,Â  below one line from _BatchSession.scala_ which is executed during creating batch session request.
{code:java}
sessionStore.save(BatchSession.RECOVERY_SESSION_TYPE, s.recoveryMetadata){code}
In case of session storageÂ  basing on HDFS, time of creating batch session (so waiting for answer from server) can be quite long, for example during failover of some node in Hadoop cluster. It can lead to some integration issues.

Therefore, the question is are you going to provide asynchronous approach to creating batch session ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 02 03:07:48 UTC 2018,,,,,,,,,,"0|i3rtev:",9223372036854775807,,,,,,,,,,,,,,,,,,,"02/Apr/18 03:07;jerryshao;No, I don't think your problem can be solvedÂ  with sync or async launch of Spark applications, or it is related to sync or async.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Differentiate FAILED and KILLED states,LIVY-452,13145735,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,aromanenko,aromanenko,aromanenko,16/Mar/18 14:40,21/May/18 01:28,19/Dec/25 04:15,21/May/18 01:27,,,,0.6.0,,,,,,,,,,,,1,,,,,,"Now it's not possible to distinguish between twoÂ states -Â _SparkApp.State.KILLED_ andÂ  _SparkApp.State.FAILED._ In both cases the session state will beÂ _SessionState.Dead()._

In our use caseÂ it's important toÂ distinguish whether job was failed or killed. So, I propose to add newÂ _SessionState.Killed()_ which will be used when job was actually killed by user.Â 

If this idea will be approved then I can submit PR about that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 21 01:28:08 UTC 2018,,,,,,,,,,"0|i3renj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/Apr/18 11:33;aromanenko;If there are no objections or any other comments on this, then I could submit PR for this.;;;","09/May/18 15:59;aromanenko;PR: [https://github.com/apache/incubator-livy/pull/92]

Please, could someone assign this Jira to me?;;;","21/May/18 01:28;jerryshao;Issue resolved by pull request 92
[https://github.com/apache/incubator-livy/pull/|https://github.com/apache/incubator-livy/pull/86]92;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Override config options by environment variables.,LIVY-450,13145098,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Do,,aromanenko,aromanenko,14/Mar/18 17:05,17/Jan/20 21:31,19/Dec/25 04:15,13/Aug/18 09:13,,,,,,,,,,,,,,,,0,,,,,,"Add a possibility to override all config options by environment variables.Â 

For example:
 {{LIVY_SPARK_MASTER}} automatically overrides {{livy.spark.master}} and {{LIVY_SPARK_DEPLOY_MODE}} overrides {{livy.spark.deploy-mode}}.

It can be quite useful to use the same Livy image in different modes (local and cluster).Â 

Any pros/cons for or against that?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 13 09:13:13 UTC 2018,,,,,,,,,,"0|i3raq7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"15/Mar/18 00:46;zjffdu;I don't think it make sense to do that. Some properties are in blacklist which don't allow user to change that. If you want to change that, you need to remove it from blacklist instead of using env to override it. And IIUC, spark prefer to use properties instead of env variable. Livy should also use this convention. ;;;","15/Mar/18 13:26;aromanenko;[~zjffdu]Â , thank you for your arguments, it makes sense for me but let me argue a little bit with that.

1) If config option is blacklisted then we will ignore corresponding environment variableÂ as well and override only if this option is allowed.

2) Regarding that Spark prefers to use properties - yes, I totally agree with that. On the other hand, why Livy can't support both ways of configuration? Is there anyÂ principled objection against that which I probably missed?;;;","15/Mar/18 13:32;zjffdu;Spark is trying to deprecate environment variable, I don't think it is necessary for livy to support that. Could you describe in what kind of scenario that you have to use environment variable instead of properties ?;;;","15/Mar/18 14:53;aromanenko;Our use case is the following - we have a docker image with Livy and other projects, and this image can be used in different environment. So, depending on where we run it, we have to set different config options. For now, it's mostly _livy.server.recovery.mode_,Â _livy.spark.deploy-mode_ andÂ _livy.spark.master._ So, it would be very useful to set these options by env variables without changing the image or mounting different config.;;;","27/Mar/18 14:37;rskraba;Hello!Â  Thanks for taking a look â€“ I just wanted to add my voice why this is a *good* feature for us.

Â 

In our system, we're launching the livy server in different ways,Â configured forÂ different types of user groups.Â  We've had quite a bit of successÂ deploying these different scenarios with docker (and kubernetes), and it's much easier for us to configure the *same*Â image *different* ways viaÂ environment variables.Â  To be clear â€“ it's the admin, not users submitting jobs, that will be setting these environment variables (via docker-compose or other) and they're only readÂ when theÂ livy server starts.

Â 

If Spark is moving to deprecate environment variables, it's to simplify the setup for the user submitting spark jobs â€“ this is the right thing to do!Â  Usually, they'll be able to get all of the default spark configuration directly from their cluster (or from the cluster admins) and it'll have reasonable default values.Â  Encapsulating the expertise from the admins in property files makes this simpler for everyone.

Â 

AÂ job server like Livy, serving many users, also shares this goal (simplifying job submission for the user, encapsulating the cluster config from the experts).Â  Only the livy admin will be touching the livy.conf â€“ or launching the livy server process, with or without environment variables.Â  (The blacklist is kind of irrelevant here, since it applies to the user submitting jobs, not the admin starting livy).

Â 

I hope this helps!Â  Â There's a simple and clear workaround for us: to mount a pre-configured livy.conf or modify the existing livy.confÂ during the image entrypoint. Both of those feel relatively inelegant in comparison toÂ [~aromanenko]'s proposed solution, and aÂ long-lived service like Livy, with a single point of launch, can very likely justify the use of configuration via environment.;;;","01/Apr/18 03:39;zjffdu;Sorry for late response, Thanks for the detail explanation [~rskraba] [~aromanenko]Â The docker scenario make sense to me, thoughts ? \cc [~jerryshao];;;","10/Apr/18 11:31;aromanenko;Since, there are no other thoughts on this, probably, it would make sense to go forward and submit a PR. [~zjffdu] what do you think?;;;","15/May/18 10:19;aromanenko;PR: [https://github.com/apache/incubator-livy/pull/94]Â ;;;","13/Aug/18 09:13;aromanenko;After discussion on github it was agreed to not continue with this approach.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assembly format is duplicated in pom.xml and assembly.xml,LIVY-448,13143122,,Improvement,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,meisam,meisam,meisam,07/Mar/18 06:59,22/Jan/19 22:27,19/Dec/25 04:15,08/Mar/18 00:59,0.5.0,,,0.6.0,,Build,,,,,,,,,,0,maven,,,,,"The archive format is available as a mvn property inÂ {{$\{assembly.format}}}.
 * It is used in maven-assembly-plugin {{<format>${assembly.format}</format>}}.
 * It also is hardcoded as {{<format>zip</format>}} in {{assembly/assembly.xml}}.

Only one of the two suffices. users can build assemble a zip artifact with
{code:java}
mvn -DskipTests -am -pl assembly package
{code}
or a tar.gz artifact with
{code:java}
mvn -Ptargz -DskipTests -am -pl assembly package
{code}",all build environments.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 22 22:27:08 UTC 2019,,,,,,,,,,"0|i3qyz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Jan/19 22:27;meisam;Fix is merged into master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Spark 2.3 for Livy,LIVY-446,13142431,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,jerryshao,05/Mar/18 06:13,12/Mar/18 02:38,19/Dec/25 04:15,12/Mar/18 02:38,0.5.0,,,0.6.0,,Build,,,,,,,,,,0,,,,,,"Apache Spark 2.3.0 is out, Livy should also upgrade to support Spark 2.3.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2018-03-05 06:13:20.0,,,,,,,,,,"0|i3qupj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Netty version to avoid security issue,LIVY-445,13141421,,Improvement,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,jerryshao,jerryshao,28/Feb/18 03:19,28/Feb/18 03:35,19/Dec/25 04:15,28/Feb/18 03:35,0.5.0,,,,,RSC,,,,,,,,,,0,,,,,,handler/ssl/OpenSslEngine.java in Netty 4.0.x before 4.0.37.Final and 4.1.x before 4.1.1.Final allows remote attackers to cause a denial of service (infinite loop).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-408,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2018-02-28 03:19:18.0,,,,,,,,,,"0|i3qohj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update NOTICE to mention third party dependancies,LIVY-443,13136684,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Won't Fix,,ajbozarth,ajbozarth,06/Feb/18 22:04,06/Feb/18 23:38,19/Dec/25 04:15,06/Feb/18 23:38,0.4.1,0.5.1,0.6.0,,,,,,,,,,,,,0,,,,,,"The NOTICE file could be improved to mention third party deps (see Spark's for example)

This was suggested in the vote to release 0.5.0-incubating: [https://www.mail-archive.com/dev@livy.incubator.apache.org/msg00301.html]

In this update we should make sure to include a reference toÂ glyphicons like Spark does.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-440,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 06 23:38:29 UTC 2018,,,,,,,,,,"0|i3pvb3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Feb/18 23:23;jmclean;Please don't. Dependancies and licensing information do not belong in the notice file. [1]Â Spark's NOTICE file is a poor example to copy from.

1.Â [http://www.apache.org/dev/licensing-howto.html#mod-notice];;;","06/Feb/18 23:38;ajbozarth;If you think this is a bad idea [~jmclean] then we won't, personally it felt odd to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Public download page must not promote unreleased code,LIVY-442,13136650,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,ajbozarth,sebb,sebb,06/Feb/18 19:24,14/Feb/18 22:02,19/Dec/25 04:15,14/Feb/18 22:02,,,,,,,,,,,,,,,,0,,,,,,"Thanks for fixing the http/s links.

Just noticed that the page has instructions for downloading from Git.
This is not allowed; only formal releases can be promoted to the general public.

Links to repos etc should only be published on pages intended for developers

http://www.apache.org/dev/release-distribution#unreleased",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 14 02:48:29 UTC 2018,,,,,,,,,,"0|i3pv3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Feb/18 20:01;ajbozarth;[~sebb@apache.org] I opened a PR ([https://github.com/apache/incubator-livy-website/pull/18)]Â to address this if you could take a look. I remove the link to the GitHub page, but the instructions to checkout the source code are specifically to check out the release tag and were originally taken from the Apache Bahir download page ([http://bahir.apache.org/downloads/spark/)]. I would assume those instructions are fine as long as they're not paired with a link to dev code.;;;","13/Feb/18 20:59;ajbozarth;[~sebb@apache.org] this has sat for a week, doÂ you want to take a quick look at the fix? Otherwise I'll just merge it as is and close this.;;;","13/Feb/18 21:26;sebb;The problem is that the instructions result in downloading the entire Git repo, including code that has not been formally released.

Such instructions are really only appropriate to developers, not people who just want the code for a release - which the page already provides.;;;","13/Feb/18 21:57;ajbozarth;I understand your reasoning and don't disagree, I just don't understand why it's ok for top level projects like Bahir but not here.;;;","13/Feb/18 22:16;sebb;I think it's also wrong for Bahir.;;;","14/Feb/18 02:48;ajbozarth;Ok, I'll update my pr to remove the whole section then;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Please use HTTPS for links to KEYS, sigs and hashes",LIVY-441,13136420,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,sebb,sebb,05/Feb/18 23:23,06/Feb/18 18:15,19/Dec/25 04:15,06/Feb/18 18:15,,,,,,,,,,,,,,,,0,,,,,,"Please use https for download links to KEYS, sigs and hashes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 06 00:06:35 UTC 2018,,,,,,,,,,"0|i3ptov:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Feb/18 23:53;ajbozarth;Nice catch, I'll address thisÂ by eod;;;","06/Feb/18 00:06;ajbozarth;pr:Â https://github.com/apache/incubator-livy-website/pull/17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Address comments from last release vote,LIVY-440,13136385,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,ajbozarth,05/Feb/18 20:44,06/Feb/18 23:39,19/Dec/25 04:15,06/Feb/18 23:39,0.4.1,0.5.1,0.6.0,,,,,,,,,,,,,0,,,,,,"Address the following comments raised in the 0.5.0-incubating vote:
 * NOTICE copyright period shouldÂ be extended to include 2018
 * The NOTICE file could be improved to mention third party deps (see Spark's for example)
 * LICENSE is OK but missing a license
 ** livy-0.5.0-incubating-src/docs/assets/themes/apache/bootstrap/fonts/glyphicons-halflings-regular/*
 ** livy-0.5.0-incubating-src/server/src/main/resources/org/apache/livy/server/ui/static/fonts/*

Â 

https://www.mail-archive.com/dev@livy.incubator.apache.org/msg00301.html

https://www.mail-archive.com/general@incubator.apache.org/msg62373.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-443,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 06 23:22:34 UTC 2018,,,,,,,,,,"0|i3pth3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Feb/18 00:13;ajbozarth;If someone wants to tackle this go ahead, otherwise I'll pick it up when I have time;;;","06/Feb/18 22:04;ajbozarth;Split off improving NOTICE into another JiraÂ task: https://issues.apache.org/jira/browse/LIVY-443Â 

Merged aÂ pr for updating NOTICE copyright year: [https://github.com/apache/incubator-livy/pull/79]

Â 

[~jmclean] in your vote you mentioned the missing license forÂ glyphicons. I double checked why it was missing and according to [http://glyphicons.com/license/]Â theÂ glyphicons in bootstrap are included in the bootstrap license, which is in the LICENSE file already. Is this satisfactory in following up your issues? I also included puttingÂ glyphicons in the dependancies list in the aboveÂ Jira similar to how Spark did so. (If you give the ok I'll close this);;;","06/Feb/18 23:22;jmclean;Re ""The NOTICE file could be improved to mention third party deps (see Spark's for example)"" please don't. Dependancies do not belong in the notice file. [1]Â Spark's NOTICE file is a poor example to copy from.

1.Â http://www.apache.org/dev/licensing-howto.html#mod-notice;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
livy-python-api fails to build because of some Python dependencies,LIVY-439,13136266,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Problem,,idzikovsky,idzikovsky,05/Feb/18 12:40,10/Nov/23 22:01,19/Dec/25 04:15,10/Nov/23 22:01,0.4.0,0.5.0,,0.9.0,,,,,,,,,,,,0,,,,,,"Hi all!

I'm trying to build Livy inside of clean ubuntu:16.04 Docker image (with maven, default-jdk and python-setuptools installed from default Ubuntu repo), and got exception on livy-python-api module:
{code:java}
root@32eb90f4dee4:~/incubator-livy# mvn clean package -DskipTests -pl python-api
[INFO] Scanning for projects...
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Building livy-python-api 0.6.0-incubating-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:3.0.0:clean (default-clean) @ livy-python-api ---
[INFO] Deleting /root/incubator-livy/python-api/target
[INFO] 
[INFO] --- maven-antrun-plugin:1.8:run (python-api clean) @ livy-python-api ---
[INFO] Executing tasks

main:
[INFO] Executed tasks
[INFO] 
[INFO] --- maven-enforcer-plugin:1.4.1:enforce (enforce-versions) @ livy-python-api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.7.201606060606:prepare-agent (default) @ livy-python-api ---
[INFO] Skipping JaCoCo execution because property jacoco.skip is set.
[INFO] argLine set to empty
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.5:process (process-resource-bundles) @ livy-python-api ---
[INFO] 
[INFO] --- scala-maven-plugin:3.2.2:compile (scala-compile-first) @ livy-python-api ---
[INFO] No sources to compile
[INFO] 
[INFO] --- exec-maven-plugin:1.2.1:exec (python-api install) @ livy-python-api ---
Traceback (most recent call last):
File ""setup.py"", line 57, in <module>
tests_require=['pytest']
File ""/usr/lib/python2.7/distutils/core.py"", line 111, in setup
_setup_distribution = dist = klass(attrs)
File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 269, in __init__
self.fetch_build_eggs(attrs['setup_requires'])
File ""/usr/lib/python2.7/dist-packages/setuptools/dist.py"", line 313, in fetch_build_eggs
replace_conflicting=True,
File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 826, in resolve
dist = best[req.key] = env.best_match(req, ws, installer)
File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 1085, in best_match
dist = working_set.find(req)
File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 695, in find
raise VersionConflict(dist, req)
pkg_resources.VersionConflict: (setuptools 20.7.0 (/usr/lib/python2.7/dist-packages), Requirement.parse('setuptools>=30'))
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2.054 s
[INFO] Finished at: 2018-02-05T12:38:13+00:00
[INFO] Final Memory: 33M/612M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:exec (python-api install) on project livy-python-api: Command execution failed. Process exited with an error: 1 (Exit value: 1) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{code}
Â 

I got this issue on LivyÂ from master branch and on Livy from branch-0.4.

Â 

I did some investigation on this issue before, and found that it's caused by some incompatibilities in Python dependencies of livy-python-api module, and solved it by explicitly setting version of flake8 Python package.",,"idzikovsky closed pull request #78: LIVY-439 livy-python-api fails to build because of some Python dependencies
URL: https://github.com/apache/incubator-livy/pull/78


;10/Nov/23 15:19;githubbot;600","idzikovsky commented on PR #78:
URL: https://github.com/apache/incubator-livy/pull/78#issuecomment-1805922049

   Out of date


;10/Nov/23 15:19;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Nov 12 22:10:08 UTC 2022,,,,,,,,,,"0|i3psqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Feb/18 12:43;idzikovsky;Created PR for this:
https://github.com/apache/incubator-livy/pull/78;;;","12/Nov/22 22:10;lmccay;Due to the pending 0.8.0 release and reduced scope to dependency upgrades and security fixes as the first release by a renewed community, this issue has been moved to the 0.9.0 release as part of a bulk update. If you feel this is moved out inappropriately, feel free to provide justification and reset the Fix Version to 0.8.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy 0.5-incubating Release,LIVY-434,13131374,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,ajbozarth,16/Jan/18 22:06,09/Feb/18 22:13,19/Dec/25 04:15,09/Feb/18 22:13,,,,0.5.0,,,,,,,,,,,,0,,,,,,A location to track the 0.5 release process.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 24 01:50:06 UTC 2018,,,,,,,,,,"0|i3oz7j:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/Jan/18 22:24;ajbozarth;[~jerryshao] Just to get the ball rolling I tracked down the list of new features to high-light in this release. Out of the Jira currently marked as 0.5 I believe the following should be high lighted in the release:

LIVY-397 LIVY-245 LIVY-19 LIVY-299 LIVY-7

I also think we could highlight LIVY-104 and LIVY-141 / LIVY-175 which haven't been finished yet, but those are smaller than the above ones.

As for some TODOs for the release:
 # Cut branch and RC1.
 # Document this process this time, I would be willing to runÂ the 0.6Â release if there were docs on the process.
 # Create PR to update website with release info

Â ;;;","16/Jan/18 22:29;ajbozarth;We also don't want to forget to build and release the Docs with the Website update;;;","17/Jan/18 01:06;jerryshao;Great, LGTM.:);;;","17/Jan/18 01:14;zjffdu;We may start the releasing after spark 2.3, so that can make livy 0.5 support spark 2.3;;;","17/Jan/18 21:27;ajbozarth;[~zjffdu] are we sure the update to Spark 2.3 will go smoothly? Because if it will take time to support it I'd rather release 0.5 now and include 2.3 support in 0.6 and turn out that release in April once we've worked out the kinks.;;;","19/Jan/18 01:11;jerryshao;IMO, I think we should have a timeline for 0.5 release, which cannot highly depend on Spark 2.3. If Spark 2.3 is release before the timeline, then we also support it, if not we should not defer our release.

What do you think? [~ajbozarth] [~zjffdu].;;;","19/Jan/18 01:18;zjffdu;Agree, but if possible, could we verify and make spark 2.3 works basically on livy 0.5 before release so that we can announce that spark 2.3 support in livy 0.5 is experimental, but not officially support. Because I believe spark 2.3 is in RC stage, so won't have big code change. 
;;;","19/Jan/18 01:21;jerryshao;Sure. We can test with Spark 2.3. But my concern is that 2.3 is a big release after 6 month, there're bunch of big changes need to stabilize, it may not be so smoothly to release.;;;","19/Jan/18 01:27;ajbozarth;I generally agree with what both of you are saying, I'm ok including 2.3 if it doesn't delay 0.5 and we mark it as experimental. On the timeline though I think we should cut the branch either tomorrow or monday and cut rc1 on monday (so the vote is during the work week). Other than my docs pr I don't see any other open prs making the 0.5 cut based on their current progress.;;;","19/Jan/18 01:37;jerryshao;LGTM.;;;","19/Jan/18 01:41;ajbozarth;Also I'm not sure (since I haven't tried) but as a committer (not ppmc) am I able to cut those branches/RCs. If so I'm willing to do it this time around if you want a break jerry (I'm equally ok with you doing it).;;;","19/Jan/18 01:45;jerryshao;From permission side I think it is OK to do it. But I'm not sure if Apache has restrictions on it. I think it should be fine.

AFAIK ""Sameer Agarwal"", Spark 2.3's release manager, is also not a PMC, so should be fine to do so.

Please don't forget to publish and add your PGP to the file.;;;","22/Jan/18 20:41;ajbozarth;Thanks [~jerryshao], I'll be doing the leg work to cut rc1 and start the dev list vote tomorrow. And thanks for the heads up about the PGP, I hadn't looked into that part of the release process yet.

Also [~jerryshao] and [~zjffdu] do the following JIRAs look like a good feature highlight list for this release? Should we add/remove any?Â 

LIVY-397 LIVY-245 LIVY-19 LIVY-299 LIVY-7 Â LIVY-104;;;","23/Jan/18 02:49;zjffdu; The highlight list look good to me, Thanks [~ajbozarth] for taking care of this release. ;;;","24/Jan/18 01:50;ajbozarth;Vote for RC1 is openÂ https://www.mail-archive.com/dev@livy.incubator.apache.org/msg00294.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy batch mode ignore proxyUser parameter,LIVY-431,13130126,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Problem,,benoit 2r,benoit 2r,11/Jan/18 09:22,15/May/18 03:35,19/Dec/25 04:15,15/May/18 03:35,0.4.0,,,,,API,Core,,,,,,,,,0,,,,,,"Hello,

We test Livy at work using it only in batch mode in order to have a spark gateway. We use the last tagged version 0.4.0-incubating. We use the REST API, as described here : [http://livy.incubator.apache.org/docs/latest/rest-api.html]

In the POST /batches request, we set the proxyUser parameter but Livy seems to ignore it.
I don't speak scala so I'm a little lost to see if there is a bug in the source code.",Cloudera,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 15 03:34:04 UTC 2018,,,,,,,,,,"0|i3osaf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Jan/18 20:38;ajbozarth;Questions like this would be better raised on the livy mailing list ( user@livy.incubating.apache.org ). As for you question, IIRC you must set livy.impersonation.enabled=true in livy.conf on the livy server for proxyUser to be respected in requests.;;;","15/May/18 03:34;jerryshao;I think you should enable impersonation first in Livy (livy.impersonation.enabled).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy skips url fragment identifier in spark.yarn.dist.archives setting,LIVY-429,13127195,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,broartem,broartem,broartem,26/Dec/17 10:00,04/Jan/18 02:25,19/Dec/25 04:15,04/Jan/18 02:24,0.3,,,0.5.0,,Server,,,,,,,,,,0,,,,,,"When submitting Spark session via Livy API with the following parameters:
{code}
{
	""kind"": ""pyspark"",
	""name"": ""my-app""
	""conf"": {
		""spark.yarn.appMasterEnv.PYSPARK_PYTHON"": ""./ENVS/custom-python/bin/python""
	},
	""archives"": [""/path/to/custom-python.zip#ENVS""]
}
{code}
Spark session fails with:
{code}
java.io.IOException: Cannot run program ""./ENVS/custom-python/bin/python"": error=2, No such file or directory
{code}
Because Livy uses java.net.URI#getPath method under the hood, which just skips URI fragment identifier part and Spark is being submitted with spark.yarn.dist.archives=/path/to/custom-python.zip and does not even extracts the archive in this case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 04 02:24:08 UTC 2018,,,,,,,,,,"0|i3oac7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Jan/18 02:24;jerryshao;Issue resolved by pull request 71
[https://github.com/apache/incubator-livy/pull/71];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Does livy support concurrency for centain spark session?,LIVY-428,13127091,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,suheng.cloud,suheng.cloud,25/Dec/17 09:26,03/Jan/18 02:06,19/Dec/25 04:15,03/Jan/18 02:06,0.4.0,,,,,API,,,,,,,,,,0,,,,,,"To use spark sql concurrency for internal fair schedule
in [post] /sessions/{id}/statements use param ""conf"":
{""spark.scheduler.mode"":""FAIR"",""spark.scheduler.allocation.file"":""/home/.../spark-schedule.xml""}

spark-schedule.xml is as follows:
<allocations>
  <pool name=""FAIR"">
    <schedulingMode>FAIR</schedulingMode>
  </pool>
</allocations>

Log shows tow jobs surely be added into fair pool:
INFO scheduler.FairSchedulableBuilder: Added task set TaskSet_10.0 tasks to pool FAIR
...
INFO scheduler.FairSchedulableBuilder: Added task set TaskSet_11.0 tasks to pool FAIR

But from spark web console the second job starts only when the first finished.
I also tried with zeppelin, which concurrency works well with spark but not work in livy.
Did I miss some configuration? Thanks.","livy 0.4
spark2.2.0-cloudera1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-421,,,,,,,,"25/Dec/17 09:20;suheng.cloud;spark-fair.png;https://issues.apache.org/jira/secure/attachment/12903625/spark-fair.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 03 02:04:51 UTC 2018,,,,,,,,,,"0|i3o9pb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Jan/18 02:04;jerryshao;This issue is similar to LIVY-421. This is the current design choice and we don't have a good solution to fix it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy stops session quietly without error message,LIVY-426,13123089,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,tjltjl888,tjltjl888,06/Dec/17 04:54,08/Dec/17 21:29,19/Dec/25 04:15,08/Dec/17 21:29,,,,,,,,,,,,,,,,0,,,,,,"Livy stops interactive sessions quietly without any error messages. In YARN log it only shows RECEIVED SIGNAL TERM which I think indicates Livy wants to terminate the driver in yarn. In Livy log it only gives Client RPC channel closed unexpectedly. No other information found.
Spark Version: 2.0.1
Scala Version: 2.11.8
Livy Version: 0.4.0-incubating
Zeppelin Version: 0.7.2
Hadoop Version: 2.7.3

YARN log:
{code:java}
17/12/06 00:39:23 INFO storage.BlockManagerMaster: Removed 3 successfully in removeExecutor
17/12/06 00:39:23 INFO spark.ExecutorAllocationManager: Existing executor 3 has been removed (new total is 0)
17/12/06 00:46:07 ERROR yarn.ApplicationMaster: RECEIVED SIGNAL TERM
17/12/06 00:46:07 INFO spark.SparkContext: Invoking stop() from shutdown hook
17/12/06 00:46:07 INFO server.ServerConnector: Stopped ServerConnector@690ca6ae{HTTP/1.1}{0.0.0.0:0}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5959d99{/stages/stage/kill,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5daa4dcf{/api,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1f30b9f2{/,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1a36836{/static,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@56c1c4df{/executors/threadDump/json,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5ea39f64{/executors/threadDump,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5be8acbc{/executors/json,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@17a813b2{/executors,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@48780245{/environment/json,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@66bc6e53{/environment,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7b06e14a{/storage/rdd/json,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@58f0ffff{/storage/rdd,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4cfc52cb{/storage/json,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@731dd75e{/storage,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1716986b{/stages/pool/json,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@54380417{/stages/pool,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5b75d33{/stages/stage/json,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@24b81ae5{/stages/stage,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@68356b10{/stages/json,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4422af95{/stages,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@39431dbc{/jobs/job/json,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@162b5d5e{/jobs/job,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@a84d7e6{/jobs/json,null,UNAVAILABLE}
17/12/06 00:46:07 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@76d179b8{/jobs,null,UNAVAILABLE}
17/12/06 00:46:07 INFO ui.SparkUI: Stopped Spark web UI at http://10.28.24.141:33403
{code}

And Livy log:
{code:java}
17/12/06 00:46:07 WARN rsc.RSCClient: Client RPC channel closed unexpectedly.
17/12/06 00:46:07 WARN rsc.RSCClient: Error stopping RPC.
io.netty.util.concurrent.BlockingOperationException: DefaultChannelPromise@74e40000(uncancellable)
        at io.netty.util.concurrent.DefaultPromise.checkDeadLock(DefaultPromise.java:390)
        at io.netty.channel.DefaultChannelPromise.checkDeadLock(DefaultChannelPromise.java:157)
        at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:251)
        at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:129)
        at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:28)
        at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:218)
        at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:117)
        at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:28)
        at org.apache.livy.rsc.rpc.Rpc.close(Rpc.java:307)
        at org.apache.livy.rsc.RSCClient.stop(RSCClient.java:232)
        at org.apache.livy.rsc.RSCClient$2$1.onSuccess(RSCClient.java:129)
        at org.apache.livy.rsc.RSCClient$2$1.onSuccess(RSCClient.java:123)
        at org.apache.livy.rsc.Utils$2.operationComplete(Utils.java:108)
        at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
        at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
        at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:406)
        at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:82)
        at io.netty.channel.AbstractChannel$CloseFuture.setClosed(AbstractChannel.java:956)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.doClose0(AbstractChannel.java:608)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.close(AbstractChannel.java:586)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.closeOnRead(AbstractNioByteChannel.java:71)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:158)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Unknown Source)
{code}

There is not enough debug information for this behavior and the behavior happens occasionally. Sometimes the session got stopped immediately upon startup. Other times the session can run properly for hours before it hit this. I have changed some configurations in Livy, not sure if that is related:
livy.conf:
livy.spark.master = yarn
livy.spark.deploy-mode = cluster
livy.server.session.timeout = 24h
livy.impersonation.enabled = true
livy.file.local-dir-whitelist = /usr/lib/sparkExternalLibs
livy.repl.enable-hive-context = true
livy.server.yarn.app-lookup-timeout = 600s

livy-client.conf:
livy.rsc.server.connect.timeout = 360s
livy.rsc.client.connect.timeout = 120s
(I initially suspect it might be due to some rpc timeout issue and hence increased the timeout, but nothing changed)

Not sure if anyone encountered similar issues and any workaround?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 08 21:29:59 UTC 2017,,,,,,,,,,"0|i3nl4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Dec/17 06:32;tjltjl888;Also I found out that when Livy has this problem on one user, it tends to kill other users sessions as well, and the same thing is no error message. ;;;","06/Dec/17 21:06;tjltjl888;Enabled debug log level on both ends but no useful information was found. 
I think somehow YARN killed the context. In Application Master log it says received term signal and shuts down the context, hence breaking the RPC channel. Livy sees it and yells out unexpected Client RPC channel closed unexpectedly, and failing the session.
If so, I don't know the reason why YARN kills the context. And this does not happen all the time. Instead this only happens sometimes. Any ideas?
;;;","07/Dec/17 05:05;jerryshao;I also encountered this exception before. Do you have a fix for this?;;;","07/Dec/17 05:41;shiva.sharma;I am also facing this issue, any one is having fix for this. Any support would be highly appreciable.;;;","07/Dec/17 07:12;jerryshao;Sorry I misunderstood your issue, what I mean is that I also saw this netty exception, but I don't see ""Livy session stops quietly"" issue.

It seems that AM is killed somehow, but I cannot figure out why, do you find some clues in AM log and Livy server log?;;;","08/Dec/17 21:28;tjltjl888;So at the end I found my issue was caused by YARN killing my Spark jobs due to ""queue capacity full"".
However this is a bug in YARN for false reporting queue usages in combination with partitions. I am submitting the jobs to our spark-queue in SPARK partition but YARN reports my jobs using resources in DEFAULT partition. spark-queue in DEFAULT partition is configured to have max-capacity of 0.1% and YARN thinks that someone is overusing resources in spark-queue in DEFAULT partition, hence killing my jobs. 
My issue is not caused by Livy. ;;;","08/Dec/17 21:29;tjltjl888;Not a Livy bug;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding Scala 2.12 support,LIVY-423,13122098,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tprelle-ubi,jerryshao,jerryshao,01/Dec/17 06:45,16/Aug/21 22:36,19/Dec/25 04:15,02/Jul/20 07:51,,,,0.8.0,,Build,Core,REPL,,,,,,,,3,,,,,,"Spark 2.3 already integrates with Scala 2.12 support, it will possibly release 2.12 artifacts. So in the Livy side we should support Scala 2.12 build and interpreter. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-562,LIVY-593,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 16 22:36:16 UTC 2021,,,,,,,,,,"0|i3nf33:",9223372036854775807,,,,,,,,,,,,,,,,,,,"01/Dec/17 22:45;ajbozarth;We should do this in conjunction with LIVY-104;;;","05/Dec/17 02:46;jerryshao;Based on the communication in Spark community, Scala 2.12 may not be shipped with Spark 2.3, it still has some pending issues. So this one can be postponed.;;;","20/Aug/18 19:09;antonkulaga;Spark 2.4 will be crossrelized for Scala 2.11 and 2.12. There are also some libraries that are Scala 2.12 only;;;","06/May/19 01:06;niknetniko;Any update on this issue? Meanwhile Spark has moved on to 2.12 as the default, and has deprecated 2.11:
{quote}Support for Scala 2.11 is deprecated as of Spark 2.4.1 and will be removed in Spark 3.0.
{quote}
Â (SeeÂ [https://spark.apache.org/docs/latest/]);;;","03/Jun/19 19:54;sdandey;Any update on this issue.Â  We have just upgraded our spark cluster from 2.3.1 to 2.4.2 to use delta features and now livy failed as it's running on Scala 2.11.Â Â ;;;","24/Feb/20 23:33;pengyuedyx@gmail.com;Any updates on this issue? Spark 3.0 has removed the support for Scala 2.11.;;;","25/Feb/20 01:09;jerryshao;No update on it. There's no one working on this feature, you can take it if you're interested.;;;","25/Feb/20 15:10;gtompa;Hi! I'm currently working on this issue, ETA 1 or 2 weeks for Scala 2.12 Support with Spark 2.4.5

[~jerryshao]Â can you add me to the contributor list, so I can assign this issue to myself?;;;","26/Feb/20 01:01;jerryshao;We don't typically assign JIRA to someone before it merges. I think you could leave message here in this JIRA that you're working on this, once the feature is merged, we will assign this to you.;;;","30/Mar/20 07:48;jimenefe;Hi [~gtompa], did you manage to make progress with this? I could really do with a 2.12 compatible Livy.

If you have stalled, could you push your branch perhaps?;;;","01/Apr/20 15:50;gtompa;[~jimenefe]Â It's very much work in progress, I'll update this thread if there is any ETA or any meaningful update.;;;","08/Apr/20 13:27;andrasbeni;I've been working with [~gtompa] on this and we just noticed https://github.com/apache/incubator-livy/pull/289/ . It has a huge overlap with what we've done and seems to have solved most of our blockers. 
It doesn't add explicit Scala 2.12 support for Spark 2.4 line, but I believe it's easy to do once that pull request is merged.;;;","02/Jul/20 07:51;jerryshao;Issue resolved by pull request 300
https://github.com/apache/incubator-livy/pull/300;;;","16/Aug/21 22:36;sergiimk;Hi, was this closed prematurely without adding new Scala version to the release?

Latest release (0.7.1 on Feb 2021) only has Scala 2.11 and is incompatible with Spark 3+;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change livy.spark.deploy-mode to livy.spark.deployMode in livy.conf.template,LIVY-422,13121242,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Not A Problem,,lucas.partridge,lucas.partridge,28/Nov/17 10:06,30/Nov/17 09:19,19/Dec/25 04:15,28/Nov/17 20:48,,,,,,,,,,,,,,,,0,,,,,,Shouldn't livy.spark.deploy-mode at https://github.com/apache/incubator-livy/blob/master/conf/livy.conf.template#L36 be livy.spark.deployMode? Because the latter is what's used in https://github.com/cloudera/livy/commit/7448c7c#diff-18b55407feae186248ae4f664d0fe47dR17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 30 09:19:14 UTC 2017,,,,,,,,,,"0|i3n9tb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Nov/17 20:48;ajbozarth;We updated our config naming in https://github.com/apache/incubator-livy/commit/221aa9cf8f33b48fd9c792f67c61b78708bfae99 which was included in 0.4
So this isn't an issue, just a difference between 0.3 and 0.4, sorry for any confusion or inconvenience;;;","29/Nov/17 09:16;lucas.partridge;Ok, thanks for clarifying Alex. Yes, it was a bit confusing! I'll bear that in mind for v0.4.

Why the change in naming convention? To bring it into line with Spark? Although, looking at https://spark.apache.org/docs/latest/running-on-yarn.html#spark-properties for example, it looks like they can't quite decide between hyphenated property names and camel-case ones! (E.g., spark.yarn.scheduler.initial-allocation.interval but spark.yarn.containerLauncherMaxThreads);;;","29/Nov/17 19:02;ajbozarth;Before the change we actually had three separate naming conventions and some of them were used in the same config. We decided if we were going to clean it up it would be better to choose just one and stick with it, given how young Livy is this was a minor change (especially since we introduced config deprecation with the change to allow for back compatibility), whereas Spark is a large established project where changes to configs would be hard to agree on and merge.;;;","30/Nov/17 09:19;lucas.partridge;Ok, thanks. That's good the config is backwardly compatible; that should reduce disruption when moving between versions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Respect spark.pyspark.python for PythonInterpreter,LIVY-418,13120121,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,zjffdu,22/Nov/17 02:24,23/Nov/17 02:19,19/Dec/25 04:15,23/Nov/17 02:19,,,,0.5.0,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-11-22 02:24:24.0,,,,,,,,,,"0|i3n2wn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not able to work with dataframes on livy,LIVY-417,13119474,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Problem,,partha_pg,partha_pg,19/Nov/17 10:17,29/Nov/17 18:49,19/Dec/25 04:15,29/Nov/17 18:49,,,,,,,,,,,,,,,,0,,,,,,"I am using livy's programmatic API. The requirement is to create multiple contexts through livy, pull dataframes and later persist them back. Through a job a DataFrame can be pulled into my application. However, when persistence is required, the dataframe is sent to another Job which receives a DataFrame and tries to persist it. Here DataFrame internals are null. 

So, what is the procedure to extract a DataFrame from spark context using livy to an application and later persisting it back to the same spark context through livy. At present not able to find any such route.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 29 18:49:18 UTC 2017,,,,,,,,,,"0|i3myxj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"21/Nov/17 07:05;jerryshao;Which version of Livy are you using? I think shared variables was just supported in master branch.;;;","23/Nov/17 02:47;partha_pg;Tried out the latest livy version. I am getting the same exception. My code is like - 

DataFrame df = client.submit(new SparkJob(sql)).get(); // SparkJob implements livy Job

getClient(clientContext).submit(new PersistJob(df, storageLevel)).get(); // StorageLevel has been set to MEMORY_AND_DISK

Exception - 
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.NullPointerException: null
org.apache.spark.sql.DataFrame.persist(DataFrame.scala:1640)
com.gridedge.framework.livyjob.PersistJob.call(PersistJob.java:43);;;","29/Nov/17 18:49;vanzin;{code}
DataFrame df = client.submit(new SparkJob(sql)).get(); // SparkJob implements livy Job
{code}

You can't do that. {{DataFrame}} only really works in the process where there is a valid {{SparkContext}}. If you serialize it, you lose the reference to the context, so your local process can't really call methods on the data frame.

You'll have to change your app to keep the {{DataFrame}} only in the Livy side, and submit a Livy job every time you need to interact with it, without sending the actual data frame over the Livy client.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
com.fasterxml.jackson.core.JsonGenerationException is sometimes thrown,LIVY-416,13119233,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,kjmrknsn,kjmrknsn,kjmrknsn,17/Nov/17 14:08,23/Nov/17 05:21,19/Dec/25 04:15,23/Nov/17 05:21,,,,0.5.0,,,,,,,,,,,,0,,,,,,"com.fasterxml.jackson.core.JsonGenerationException is sometimes thrown. The full stack trace is show on JsonGenerationException.txt which is attached to this issue.

This is because of the Jackson's bug (https://github.com/FasterXML/jackson-core/issues/307) which is fixed at Jackson 2.7.7.

To fix this issue, the version of Jackson should be updated from 2.4.4 to the latest one (2.9.2).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/17 14:06;kjmrknsn;JsonGenerationException.txt;https://issues.apache.org/jira/secure/attachment/12898206/JsonGenerationException.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-11-17 14:08:53.0,,,,,,,,,,"0|i3mxfz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use objects and abstract classes in for Kind and SessionState.,LIVY-415,13117981,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,aa8y,aa8y,aa8y,13/Nov/17 15:33,15/Nov/17 03:46,19/Dec/25 04:15,15/Nov/17 03:44,,,,0.5.0,,REPL,Server,,,,,,,,,0,,,,,,"Scala lets us define Singleton Objects (https://docs.scala-lang.org/tour/singleton-objects.html) rather than creating (case) classes and which just have a default constructor. Also, using abstract classes helps with brevity in this case as we can assign default definitions to the functions we need.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-11-13 15:33:54.0,,,,,,,,,,"0|i3mpq7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CreateSession should be reject when too many child process ""spark-submit"" is running",LIVY-412,13110247,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,shenhong,shenhong,shenhong,18/Oct/17 09:37,07/Nov/17 08:32,19/Dec/25 04:15,07/Nov/17 08:31,0.4.0,,,0.5.0,,Server,,,,,,,,,,0,,,,,,"In our cluster, livy server run with spark yarn cluster mode, when createSession request is too frequently, livyServer will start too more spark-submit child process, it will cause the machine oom. I think livy server should reject the create session request when there is too more spark-submit child process. I have fix it in our own cluster.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 07 08:31:01 UTC 2017,,,,,,,,,,"0|i3leif:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/Oct/17 03:18;shenhong;Add a patch: https://github.com/apache/incubator-livy/pull/58;;;","07/Nov/17 08:31;jerryshao;Issue resolved by pull request 58
[https://github.com/apache/incubator-livy/pull/58];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Session cannot be started when Python or R package is missing,LIVY-411,13110208,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,yeshavora,jerryshao,18/Oct/17 06:17,20/Oct/17 03:13,19/Dec/25 04:15,20/Oct/17 03:13,0.5.0,,,0.5.0,,REPL,,,,,,,,,,0,,,,,,"In Livy 0.5.0, we supported multiple languages in one session, but it requires that the packages should be available, such as R package and Python package, otherwise session will be failed to create. However, in some cases python or R package may be missing in Spark distro, this will make Livy fail to creation interactive session.

To fix this issue, we should not force such restriction on session creation, but delay the check until related interpreter is used. If such packaging is missing, we should make the related execution failure and return user the cause of issue, but don't affect other correctly started interpreters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 03:13:12 UTC 2017,,,,,,,,,,"0|i3le9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Oct/17 06:35;githubbot;GitHub user jerryshao opened a pull request:

    https://github.com/apache/incubator-livy/pull/56

    [LIVY-411][REPL] Fix session cannot start issue when python or r package is missing

    ## What changes were proposed in this pull request?
    
    https://issues.apache.org/jira/browse/LIVY-411
    
    In Livy 0.5.0, we supported multiple languages in one session, but it requires that all the packages should be available before session creation, such as R package and Python package, otherwise session will be failed to create. However, in some cases python or R package may be missing in Spark distro, this will make Livy fail to creation interactive session.
    
    To fix this issue, we should not force such restriction on session creation, but delay the check until related interpreter is used. If such packaging is missing, we should make the related execution failure and return user the cause of issue, but don't affect other correctly started interpreters.
    
    
    ## How was this patch tested?
    
    Existing test and manually verification on local cluster.


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-411

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/56.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #56
    
----
commit 6f808d282757f3f4d6d38bdeb4dda59b167d9de5
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-10-18T06:30:25Z

    Fix session cannot start issue when python or r package is missing

----
;;;","18/Oct/17 12:47;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/56
;;;","18/Oct/17 12:47;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/56

    [LIVY-411][REPL] Fix session cannot start issue when python or r package is missing

    ## What changes were proposed in this pull request?
    
    https://issues.apache.org/jira/browse/LIVY-411
    
    In Livy 0.5.0, we supported multiple languages in one session, but it requires that all the packages should be available before session creation, such as R package and Python package, otherwise session will be failed to create. However, in some cases python or R package may be missing in Spark distro, this will make Livy fail to creation interactive session.
    
    To fix this issue, we should not force such restriction on session creation, but delay the check until related interpreter is used. If such packaging is missing, we should make the related execution failure and return user the cause of issue, but don't affect other correctly started interpreters.
    
    
    ## How was this patch tested?
    
    Existing test and manually verification on local cluster.


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-411

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/56.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #56
    
----
commit 6f808d282757f3f4d6d38bdeb4dda59b167d9de5
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-10-18T06:30:25Z

    Fix session cannot start issue when python or r package is missing

----
;;;","18/Oct/17 13:26;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/56
;;;","18/Oct/17 13:26;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/56

    [LIVY-411][REPL] Fix session cannot start issue when python or r package is missing

    ## What changes were proposed in this pull request?
    
    https://issues.apache.org/jira/browse/LIVY-411
    
    In Livy 0.5.0, we supported multiple languages in one session, but it requires that all the packages should be available before session creation, such as R package and Python package, otherwise session will be failed to create. However, in some cases python or R package may be missing in Spark distro, this will make Livy fail to creation interactive session.
    
    To fix this issue, we should not force such restriction on session creation, but delay the check until related interpreter is used. If such packaging is missing, we should make the related execution failure and return user the cause of issue, but don't affect other correctly started interpreters.
    
    
    ## How was this patch tested?
    
    Existing test and manually verification on local cluster.


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-411

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/56.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #56
    
----
commit 6f808d282757f3f4d6d38bdeb4dda59b167d9de5
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-10-18T06:30:25Z

    Fix session cannot start issue when python or r package is missing

----
;;;","18/Oct/17 14:13;githubbot;Github user codecov-io commented on the issue:

    https://github.com/apache/incubator-livy/pull/56
  
    # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=h1) Report
    > Merging [#56](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=desc) into [master](https://codecov.io/gh/apache/incubator-livy/commit/8056e4682531a5a4b717a0b13d992a46637fd412?src=pr&el=desc) will **decrease** coverage by `0.01%`.
    > The diff coverage is `33.33%`.
    
    [![Impacted file tree graph](https://codecov.io/gh/apache/incubator-livy/pull/56/graphs/tree.svg?width=650&token=0MkVbiUFwE&height=150&src=pr)](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=tree)
    
    ```diff
    @@             Coverage Diff              @@
    ##             master      #56      +/-   ##
    ============================================
    - Coverage     70.82%   70.81%   -0.02%     
    - Complexity      789      791       +2     
    ============================================
      Files            97       97              
      Lines          5413     5431      +18     
      Branches        800      807       +7     
    ============================================
    + Hits           3834     3846      +12     
    - Misses         1049     1052       +3     
    - Partials        530      533       +3
    ```
    
    
    | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=tree) | Coverage Î” | Complexity Î” | |
    |---|---|---|---|
    | [...e/livy/server/interactive/InteractiveSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `60.37% <0%> (-1.34%)` | `43 <0> (Ã¸)` | |
    | [...c/main/scala/org/apache/livy/repl/ReplDriver.scala](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=tree#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9SZXBsRHJpdmVyLnNjYWxh) | `28.2% <0%> (Ã¸)` | `0 <0> (Ã¸)` | :arrow_down: |
    | [...scala/org/apache/livy/repl/PythonInterpreter.scala](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=tree#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9QeXRob25JbnRlcnByZXRlci5zY2FsYQ==) | `54.22% <0%> (Ã¸)` | `2 <0> (Ã¸)` | :arrow_down: |
    | [.../src/main/scala/org/apache/livy/repl/Session.scala](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=tree#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9TZXNzaW9uLnNjYWxh) | `68.75% <50%> (-3.08%)` | `1 <0> (Ã¸)` | |
    | [...cala/org/apache/livy/scalaapi/ScalaJobHandle.scala](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=tree#diff-c2NhbGEtYXBpL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zY2FsYWFwaS9TY2FsYUpvYkhhbmRsZS5zY2FsYQ==) | `55.88% <0%> (+2.94%)` | `0% <0%> (Ã¸)` | :arrow_down: |
    | [...main/java/org/apache/livy/rsc/ContextLauncher.java](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9Db250ZXh0TGF1bmNoZXIuamF2YQ==) | `84.15% <0%> (+2.97%)` | `18% <0%> (+1%)` | :arrow_up: |
    | [...in/java/org/apache/livy/rsc/rpc/RpcDispatcher.java](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9ycGMvUnBjRGlzcGF0Y2hlci5qYXZh) | `67% <0%> (+3%)` | `20% <0%> (+1%)` | :arrow_up: |
    
    ------
    
    [Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=continue).
    > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
    > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
    > Powered by [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=footer). Last update [8056e46...6f808d2](https://codecov.io/gh/apache/incubator-livy/pull/56?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

;;;","18/Oct/17 14:16;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/56
;;;","18/Oct/17 14:16;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/56

    [LIVY-411][REPL] Fix session cannot start issue when python or r package is missing

    ## What changes were proposed in this pull request?
    
    https://issues.apache.org/jira/browse/LIVY-411
    
    In Livy 0.5.0, we supported multiple languages in one session, but it requires that all the packages should be available before session creation, such as R package and Python package, otherwise session will be failed to create. However, in some cases python or R package may be missing in Spark distro, this will make Livy fail to creation interactive session.
    
    To fix this issue, we should not force such restriction on session creation, but delay the check until related interpreter is used. If such packaging is missing, we should make the related execution failure and return user the cause of issue, but don't affect other correctly started interpreters.
    
    
    ## How was this patch tested?
    
    Existing test and manually verification on local cluster.


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-411

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/56.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #56
    
----

----
;;;","18/Oct/17 19:39;githubbot;Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/56
  
    On the code side this LGTM, but I didn't test out the functionality
;;;","20/Oct/17 03:13;jerryshao;Issue resolved by pull request 56
[https://github.com/apache/incubator-livy/pull/56];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve User Experience in livy-shell,LIVY-409,13107304,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,ericjperry,ericjperry,ericjperry,05/Oct/17 17:57,20/Oct/17 02:06,19/Dec/25 04:15,20/Oct/17 02:04,,,,0.5.0,,,,,,,,,,,,0,,,,,,"The livy-shell is useful in testing and evaluation of the use of Livy, but has a few minor UX issues that could be fixed without many changes:

# The use of httplib and a single connection can cause problems in environments where network reliability is low.
# The shell prompt does not include any contextual information, which may be helpful when a user has multiple shells running.
# There isn't an easy way to cancel the current command other than deleting all the text on the prompt as SIGINT breaks out of the REPL and causes the session to be deleted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 02:04:38 UTC 2017,,,,,,,,,,"0|i3kxfz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Oct/17 18:06;githubbot;Github user ericjperry commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    @ajbozarth thank you for the quick response
    
    * Sorry about that, I have created https://issues.apache.org/jira/browse/LIVY-409 and updated the title of this PR to reflect the link.
    * I removed `livy-shell-requirements.txt` and exit with a message if that module cannot be imported. Happy to take suggestions if you'd like to see something different.
;;;","05/Oct/17 18:45;githubbot;Github user ericjperry commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    Running build again.
;;;","05/Oct/17 18:45;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","05/Oct/17 18:45;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

----
;;;","06/Oct/17 12:26;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","06/Oct/17 12:27;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

commit 494a0b32a3f61ff216a23a99e313f25cf8c59152
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T21:06:42Z

    Use raise_for_status in favor of raising HTTPException

----
;;;","06/Oct/17 13:18;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","06/Oct/17 13:18;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

commit 494a0b32a3f61ff216a23a99e313f25cf8c59152
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T21:06:42Z

    Use raise_for_status in favor of raising HTTPException

----
;;;","06/Oct/17 14:06;githubbot;Github user ericjperry commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    @ajbozarth I fixed another issue I missed and re-ran the build a few times to get it to pass. This should be ready to review again.
;;;","09/Oct/17 03:10;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    Thanks @ericjperry for your contribution, I think we seldom use Livy shell for testing and others, but it is better to keep this up-to-date, I think it is useful in local test. Will spend time to review this.
;;;","09/Oct/17 19:36;githubbot;Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    If @jerryshao signs off on this then LGTM
;;;","11/Oct/17 22:02;githubbot;Github user ericjperry commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    @ajbozarth @jerryshao apologies, but I found an issue with the SIGINT handling in my code, which was concatenating lines of input together into a single command. I updated the loop to continue and reissue the `raw_input` call if a user submitted the ctrl-C command. This works far better and doesn't involve the raw writes to stdout which is a little nicer.
;;;","12/Oct/17 12:26;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","12/Oct/17 12:26;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

commit 494a0b32a3f61ff216a23a99e313f25cf8c59152
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T21:06:42Z

    Use raise_for_status in favor of raising HTTPException

commit a8bf1f796d069df4a83d66432ac5a92aee4a6236
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-11T22:00:12Z

    Fixed issue with commands being concatenated.

----
;;;","13/Oct/17 01:40;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/55#discussion_r144449564
  
    --- Diff: dev/livy-shell ---
    @@ -56,33 +65,30 @@ class LiteralDict(dict):
         return name
     
     
    -def request(conn, method, uri, body):
    -  body = json.dumps(body) if body else None
    -  headers = { 'Content-Type' : 'application/json' }
    -  conn.request(method, uri, body=body, headers=headers)
    -
    -  resp = conn.getresponse()
    -  data = resp.read()
    -  if resp.status < 200 or resp.status >= 400:
    -    raise httplib.HTTPException, (resp.status, resp.reason, data)
    -  if resp.status < 300 and resp.status != httplib.NO_CONTENT:
    -    return json.loads(data)
    +def request(method, uri, body):
    +  kwargs = { 'headers': { 'Content-Type' : 'application/json' } }
    --- End diff --
    
    I would suggest you to add CSRF header also, you can refer to python-api module to know the details.
;;;","13/Oct/17 01:45;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/55#discussion_r144449993
  
    --- Diff: dev/livy-shell ---
    @@ -30,13 +30,22 @@
     # By default, a Spark (Scala) session is created.
     #
     
    -import httplib
     import json
     import readline
    +import signal
     import sys
     import time
     import urlparse
     
    +try:
    +  import requests
    +except ImportError:
    +  print ""Unable to import 'requests' module, which is required by livy-shell.""
    --- End diff --
    
    Can you please change the script to make it python3 compatible, I think this syntax can only be worked in python2.
;;;","13/Oct/17 13:52;githubbot;Github user ericjperry commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/55#discussion_r144559182
  
    --- Diff: dev/livy-shell ---
    @@ -56,33 +65,30 @@ class LiteralDict(dict):
         return name
     
     
    -def request(conn, method, uri, body):
    -  body = json.dumps(body) if body else None
    -  headers = { 'Content-Type' : 'application/json' }
    -  conn.request(method, uri, body=body, headers=headers)
    -
    -  resp = conn.getresponse()
    -  data = resp.read()
    -  if resp.status < 200 or resp.status >= 400:
    -    raise httplib.HTTPException, (resp.status, resp.reason, data)
    -  if resp.status < 300 and resp.status != httplib.NO_CONTENT:
    -    return json.loads(data)
    +def request(method, uri, body):
    +  kwargs = { 'headers': { 'Content-Type' : 'application/json' } }
    --- End diff --
    
    Added the `X-Requested-By` header to match the other clients.
;;;","13/Oct/17 13:52;githubbot;Github user ericjperry commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/55#discussion_r144559288
  
    --- Diff: dev/livy-shell ---
    @@ -30,13 +30,22 @@
     # By default, a Spark (Scala) session is created.
     #
     
    -import httplib
     import json
     import readline
    +import signal
     import sys
     import time
     import urlparse
     
    +try:
    +  import requests
    +except ImportError:
    +  print ""Unable to import 'requests' module, which is required by livy-shell.""
    --- End diff --
    
    I updated this line to reuse the `message` method and updated the `message` method to use a python3 compatible print call.
;;;","13/Oct/17 23:28;githubbot;Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    The latest update still look fine to me, but I was wondering why there are so many extra blank lines in the file? Is it a python thing? Cause I've alway thought multiple empty lines in a row was bad style no matter the language.
;;;","16/Oct/17 13:57;githubbot;Github user ericjperry commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    @ajbozarth I may have gone a bit overboard but PEP8's guidance on blank lines says to surround top level functions and class definitions with two blank lines: http://legacy.python.org/dev/peps/pep-0008/#blank-lines.
    
    Since there aren't any class members in here I split everything up with two blank lines.
;;;","16/Oct/17 13:58;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","16/Oct/17 13:58;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

commit 494a0b32a3f61ff216a23a99e313f25cf8c59152
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T21:06:42Z

    Use raise_for_status in favor of raising HTTPException

commit a8bf1f796d069df4a83d66432ac5a92aee4a6236
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-11T22:00:12Z

    Fixed issue with commands being concatenated.

commit e09ff965f602913cfb82bc5ddb3432105511666a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:32:11Z

    Add parens to print and use message for python 3 compatibility

commit c78150e2859c0094ef9e16304f8b15123dab764e
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:51:07Z

    Added CSRF token to requests

----
;;;","16/Oct/17 14:01;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    I think @ericjperry is right, python code follows two blank lines convention between class definitions and top level functions, you can refer to pyspark code @ajbozarth .
;;;","16/Oct/17 17:33;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","16/Oct/17 17:33;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

commit 494a0b32a3f61ff216a23a99e313f25cf8c59152
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T21:06:42Z

    Use raise_for_status in favor of raising HTTPException

commit a8bf1f796d069df4a83d66432ac5a92aee4a6236
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-11T22:00:12Z

    Fixed issue with commands being concatenated.

commit e09ff965f602913cfb82bc5ddb3432105511666a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:32:11Z

    Add parens to print and use message for python 3 compatibility

commit c78150e2859c0094ef9e16304f8b15123dab764e
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:51:07Z

    Added CSRF token to requests

----
;;;","16/Oct/17 18:23;githubbot;Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    Thanks, good to know more python style conventions :)
;;;","16/Oct/17 18:39;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

commit 494a0b32a3f61ff216a23a99e313f25cf8c59152
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T21:06:42Z

    Use raise_for_status in favor of raising HTTPException

commit a8bf1f796d069df4a83d66432ac5a92aee4a6236
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-11T22:00:12Z

    Fixed issue with commands being concatenated.

commit e09ff965f602913cfb82bc5ddb3432105511666a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:32:11Z

    Add parens to print and use message for python 3 compatibility

commit c78150e2859c0094ef9e16304f8b15123dab764e
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:51:07Z

    Added CSRF token to requests

----
;;;","16/Oct/17 18:39;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","16/Oct/17 19:50;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","16/Oct/17 19:50;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

commit 494a0b32a3f61ff216a23a99e313f25cf8c59152
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T21:06:42Z

    Use raise_for_status in favor of raising HTTPException

commit a8bf1f796d069df4a83d66432ac5a92aee4a6236
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-11T22:00:12Z

    Fixed issue with commands being concatenated.

commit e09ff965f602913cfb82bc5ddb3432105511666a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:32:11Z

    Add parens to print and use message for python 3 compatibility

commit c78150e2859c0094ef9e16304f8b15123dab764e
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:51:07Z

    Added CSRF token to requests

----
;;;","16/Oct/17 22:06;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","16/Oct/17 22:06;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

commit 494a0b32a3f61ff216a23a99e313f25cf8c59152
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T21:06:42Z

    Use raise_for_status in favor of raising HTTPException

commit a8bf1f796d069df4a83d66432ac5a92aee4a6236
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-11T22:00:12Z

    Fixed issue with commands being concatenated.

commit e09ff965f602913cfb82bc5ddb3432105511666a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:32:11Z

    Add parens to print and use message for python 3 compatibility

commit c78150e2859c0094ef9e16304f8b15123dab764e
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:51:07Z

    Added CSRF token to requests

----
;;;","17/Oct/17 17:49;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","17/Oct/17 17:49;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

commit 494a0b32a3f61ff216a23a99e313f25cf8c59152
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T21:06:42Z

    Use raise_for_status in favor of raising HTTPException

commit a8bf1f796d069df4a83d66432ac5a92aee4a6236
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-11T22:00:12Z

    Fixed issue with commands being concatenated.

commit e09ff965f602913cfb82bc5ddb3432105511666a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:32:11Z

    Add parens to print and use message for python 3 compatibility

commit c78150e2859c0094ef9e16304f8b15123dab764e
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:51:07Z

    Added CSRF token to requests

----
;;;","17/Oct/17 18:50;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","17/Oct/17 18:50;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

commit 494a0b32a3f61ff216a23a99e313f25cf8c59152
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T21:06:42Z

    Use raise_for_status in favor of raising HTTPException

commit a8bf1f796d069df4a83d66432ac5a92aee4a6236
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-11T22:00:12Z

    Fixed issue with commands being concatenated.

commit e09ff965f602913cfb82bc5ddb3432105511666a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:32:11Z

    Add parens to print and use message for python 3 compatibility

commit c78150e2859c0094ef9e16304f8b15123dab764e
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:51:07Z

    Added CSRF token to requests

----
;;;","17/Oct/17 20:45;githubbot;Github user ericjperry commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    @ajbozarth @jerryshao is there any trick here to getting the tests to pass?
;;;","17/Oct/17 20:51;githubbot;Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    Usually just trying it again, I've never seen it fail this much before
;;;","18/Oct/17 00:40;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","18/Oct/17 00:40;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

commit 494a0b32a3f61ff216a23a99e313f25cf8c59152
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T21:06:42Z

    Use raise_for_status in favor of raising HTTPException

commit a8bf1f796d069df4a83d66432ac5a92aee4a6236
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-11T22:00:12Z

    Fixed issue with commands being concatenated.

commit e09ff965f602913cfb82bc5ddb3432105511666a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:32:11Z

    Add parens to print and use message for python 3 compatibility

commit c78150e2859c0094ef9e16304f8b15123dab764e
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:51:07Z

    Added CSRF token to requests

----
;;;","18/Oct/17 12:29;githubbot;Github user ericjperry closed the pull request at:

    https://github.com/apache/incubator-livy/pull/55
;;;","18/Oct/17 12:29;githubbot;GitHub user ericjperry reopened a pull request:

    https://github.com/apache/incubator-livy/pull/55

    [LIVY-409] Livy shell UX improvements

    I'm not sure how large of an audience the livy-shell has, but I've come to use it quite a bit in my testing/use of Livy and added a few small improvements that have made it a bit easier to use for me (and may help others):
    
    * Replaced the use of httplib with requests to fix dropped connection issues (and a few other robustness issues).
    * Added some context to the REPL prompt, by including the session `kind` and ID.
    * Ignore `SIGINT` signal so that users can type `CTRL-C` to cancel out a command.
    
    I know the use of `requests` is pretty ubiquitous but I included `livy-shell-requirements.txt` just in case 1) someone doesn't have it installed or 2) more dependencies are added down the road.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ericjperry/incubator-livy feature/livy-shell-improvements

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/55.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #55
    
----
commit f45110c2f24d9b0d1e8645de09e7eda94b959487
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:48:37Z

    Minor livy-shell UX improvements

commit da809a902423856b5c828d8c5a65ab4dcc33e95a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:50:02Z

    Add requirements file for livy-shell

commit 3b282afad13e3550387aba11850fbdc61ee01496
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T15:54:15Z

    Remove commented code

commit 64a5abf423b9ae70eebaf04f6c78ef3f4c4862c2
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T18:05:46Z

    Remove livy-shell-requirements.txt, check for requests at launch.

commit 494a0b32a3f61ff216a23a99e313f25cf8c59152
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-05T21:06:42Z

    Use raise_for_status in favor of raising HTTPException

commit a8bf1f796d069df4a83d66432ac5a92aee4a6236
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-11T22:00:12Z

    Fixed issue with commands being concatenated.

commit e09ff965f602913cfb82bc5ddb3432105511666a
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:32:11Z

    Add parens to print and use message for python 3 compatibility

commit c78150e2859c0094ef9e16304f8b15123dab764e
Author: Eric Perry <eric@ericjperry.com>
Date:   2017-10-13T13:51:07Z

    Added CSRF token to requests

----
;;;","18/Oct/17 13:13;githubbot;Github user ericjperry commented on the issue:

    https://github.com/apache/incubator-livy/pull/55
  
    @ajbozarth @jerryshao Tests passed!
;;;","20/Oct/17 02:04;jerryshao;Issue resolved by pull request 55
[https://github.com/apache/incubator-livy/pull/55];;;"
Upgrade Netty version to avoid some security issues,LIVY-408,13105634,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,jerryshao,jerryshao,jerryshao,28/Sep/17 02:44,28/Feb/18 03:35,19/Dec/25 04:15,29/Sep/17 02:07,0.4.1,0.5.0,,0.5.0,,RSC,,,,,,,,,,0,,,,,,Netty version below 4.0.37.Final has some potential security issue (https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-4970) which is fixed in this version. So we should upgrade Livy to avoid such issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-445,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 29 02:07:58 UTC 2017,,,,,,,,,,"0|i3kn7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Sep/17 04:05;githubbot;GitHub user jerryshao opened a pull request:

    https://github.com/apache/incubator-livy/pull/53

    [LIVY-408][RSC] Update Netty version to 4.0.37.Final

    Netty version below 4.0.37.Final has some potential security issue (https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-4970), it is fixed in this version. So propose to upgrade Netty to avoid such issue.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-408

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/53.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #53
    
----
commit 4e4b30b7104a7502eda936038c4c68f0a8ab7e4a
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-28T04:03:24Z

    Update Netty version to 4.0.37.Final

----
;;;","28/Sep/17 05:11;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/53
;;;","28/Sep/17 05:11;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/53

    [LIVY-408][RSC] Update Netty version to 4.0.37.Final

    Netty version below 4.0.37.Final has some potential security issue (https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-4970), it is fixed in this version. So propose to upgrade Netty to avoid such issue.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-408

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/53.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #53
    
----
commit 4e4b30b7104a7502eda936038c4c68f0a8ab7e4a
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-28T04:03:24Z

    Update Netty version to 4.0.37.Final

----
;;;","28/Sep/17 05:38;githubbot;Github user codecov-io commented on the issue:

    https://github.com/apache/incubator-livy/pull/53
  
    # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=h1) Report
    > Merging [#53](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=desc) into [master](https://codecov.io/gh/apache/incubator-livy/commit/0ca16d5d2fb9bed6c66a6a787afb5a7f025c577f?src=pr&el=desc) will **decrease** coverage by `3.86%`.
    > The diff coverage is `n/a`.
    
    [![Impacted file tree graph](https://codecov.io/gh/apache/incubator-livy/pull/53/graphs/tree.svg?width=650&height=150&src=pr&token=0MkVbiUFwE)](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree)
    
    ```diff
    @@             Coverage Diff              @@
    ##             master      #53      +/-   ##
    ============================================
    - Coverage     70.69%   66.82%   -3.87%     
    + Complexity      789      757      -32     
    ============================================
      Files            97       97              
      Lines          5384     5287      -97     
      Branches        798      793       -5     
    ============================================
    - Hits           3806     3533     -273     
    - Misses         1040     1249     +209     
    + Partials        538      505      -33
    ```
    
    
    | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree) | Coverage Î” | Complexity Î” | |
    |---|---|---|---|
    | [...main/scala/org/apache/livy/server/LivyServer.scala](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvTGl2eVNlcnZlci5zY2FsYQ==) | `1.76% <0%> (-35.3%)` | `2% <0%> (-8%)` | |
    | [...rc/main/scala/org/apache/livy/utils/SparkApp.scala](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya0FwcC5zY2FsYQ==) | `61.53% <0%> (-19.24%)` | `1% <0%> (Ã¸)` | |
    | [.../java/org/apache/livy/rsc/driver/SparkEntries.java](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9kcml2ZXIvU3BhcmtFbnRyaWVzLmphdmE=) | `41.53% <0%> (-18.74%)` | `8% <0%> (-2%)` | |
    | [core/src/main/scala/org/apache/livy/Logging.scala](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvTG9nZ2luZy5zY2FsYQ==) | `54.54% <0%> (-18.19%)` | `0% <0%> (Ã¸)` | |
    | [...main/java/org/apache/livy/rsc/ContextLauncher.java](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9Db250ZXh0TGF1bmNoZXIuamF2YQ==) | `67.67% <0%> (-13.52%)` | `13% <0%> (-4%)` | |
    | [...ain/scala/org/apache/livy/utils/SparkYarnApp.scala](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya1lhcm5BcHAuc2NhbGE=) | `54.34% <0%> (-10.15%)` | `29% <0%> (-3%)` | |
    | [.../scala/org/apache/livy/sessions/SessionState.scala](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvc2Vzc2lvbnMvU2Vzc2lvblN0YXRlLnNjYWxh) | `35.55% <0%> (-8.89%)` | `0% <0%> (Ã¸)` | |
    | [...rg/apache/livy/repl/AbstractSparkInterpreter.scala](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9BYnN0cmFjdFNwYXJrSW50ZXJwcmV0ZXIuc2NhbGE=) | `51.04% <0%> (-7.7%)` | `1% <0%> (Ã¸)` | |
    | [...la/org/apache/livy/utils/LineBufferedProcess.scala](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9MaW5lQnVmZmVyZWRQcm9jZXNzLnNjYWxh) | `71.42% <0%> (-7.15%)` | `6% <0%> (-1%)` | |
    | [...la/org/apache/livy/utils/SparkProcessBuilder.scala](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya1Byb2Nlc3NCdWlsZGVyLnNjYWxh) | `50.54% <0%> (-6.6%)` | `11% <0%> (-2%)` | |
    | ... and [31 more](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=tree-more) | |
    
    ------
    
    [Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=continue).
    > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
    > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
    > Powered by [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=footer). Last update [0ca16d5...4e4b30b](https://codecov.io/gh/apache/incubator-livy/pull/53?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

;;;","29/Sep/17 02:03;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/53
  
    Merging to master.
;;;","29/Sep/17 02:06;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-livy/pull/53
;;;","29/Sep/17 02:07;jerryshao;Issue resolved by pull request 53
[https://github.com/apache/incubator-livy/pull/53];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy fail to get app id and related information when using yarn client mode,LIVY-406,13104147,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,jerryshao,22/Sep/17 03:01,26/Sep/17 13:08,19/Dec/25 04:15,26/Sep/17 05:28,0.4.1,0.5.0,,0.4.1,0.5.0,Server,,,,,,,,,,0,,,,,,"In yarn client mode, spark-submit process will never be exited, but in our SparkYarnApp logic, we have to wait for process to exit before to query app id. Since the process is never exited, so it blocks the follow-up logics to get app id and other yarn application information.",,"GitHub user jerryshao opened a pull request:

    https://github.com/apache/incubator-livy/pull/50

    [LIVY-406][SERVER] Fix Livy cannot app id in yarn client mode issue

    In our `SparkYarnApp` logic, we have to wait for process to exit before to query app id. Since the process is never exited in yarn client mode, so it will block follow-up logic to get app id and other yarn application information. 
    
    So here propose to remove this logic, also because now we will query app id ever since app is launched, so we should increase the look-up time to avoid timeout.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-406

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/50.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #50
    
----
commit 0a7f1c868f0fc06849c9b00a1617693718681187
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-22T03:15:51Z

    Fix Livy cannot app id in yarn client mode

----
;22/Sep/17 03:27;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/50
  
    CC @zjffdu to help to review, thanks!
;22/Sep/17 03:28;githubbot;600","Github user codecov-io commented on the issue:

    https://github.com/apache/incubator-livy/pull/50
  
    # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=h1) Report
    > Merging [#50](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=desc) into [master](https://codecov.io/gh/apache/incubator-livy/commit/219fdac5cfea1780cfb4e52f29777d3b21f8a55e?src=pr&el=desc) will **decrease** coverage by `<.01%`.
    > The diff coverage is `100%`.
    
    [![Impacted file tree graph](https://codecov.io/gh/apache/incubator-livy/pull/50/graphs/tree.svg?width=650&src=pr&token=0MkVbiUFwE&height=150)](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=tree)
    
    ```diff
    @@             Coverage Diff             @@
    ##             master     #50      +/-   ##
    ===========================================
    - Coverage     70.71%   70.7%   -0.01%     
    + Complexity      791     790       -1     
    ===========================================
      Files            97      97              
      Lines          5389    5384       -5     
      Branches        800     798       -2     
    ===========================================
    - Hits           3811    3807       -4     
      Misses         1040    1040              
    + Partials        538     537       -1
    ```
    
    
    | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=tree) | Coverage Î” | Complexity Î” | |
    |---|---|---|---|
    | [...ain/scala/org/apache/livy/utils/SparkYarnApp.scala](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya1lhcm5BcHAuc2NhbGE=) | `64.49% <Ã¸> (+1.55%)` | `32 <0> (+1)` | :arrow_up: |
    | [...rver/src/main/scala/org/apache/livy/LivyConf.scala](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9MaXZ5Q29uZi5zY2FsYQ==) | `95.93% <100%> (Ã¸)` | `15 <0> (Ã¸)` | :arrow_down: |
    | [...in/java/org/apache/livy/rsc/rpc/RpcDispatcher.java](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9ycGMvUnBjRGlzcGF0Y2hlci5qYXZh) | `64% <0%> (-3%)` | `19% <0%> (-1%)` | |
    | [...ain/java/org/apache/livy/rsc/driver/RSCDriver.java](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9kcml2ZXIvUlNDRHJpdmVyLmphdmE=) | `77.87% <0%> (-1.28%)` | `40% <0%> (-2%)` | |
    | [...main/java/org/apache/livy/rsc/ContextLauncher.java](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9Db250ZXh0TGF1bmNoZXIuamF2YQ==) | `81.68% <0%> (+0.99%)` | `18% <0%> (+1%)` | :arrow_up: |
    | [...cala/org/apache/livy/scalaapi/ScalaJobHandle.scala](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=tree#diff-c2NhbGEtYXBpL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zY2FsYWFwaS9TY2FsYUpvYkhhbmRsZS5zY2FsYQ==) | `55.88% <0%> (+2.94%)` | `0% <0%> (Ã¸)` | :arrow_down: |
    
    ------
    
    [Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=continue).
    > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
    > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
    > Powered by [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=footer). Last update [219fdac...0a7f1c8](https://codecov.io/gh/apache/incubator-livy/pull/50?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

;22/Sep/17 04:09;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Sep 26 13:08:00 UTC 2017,,,,,,,,,,"0|i3ke3b:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Sep/17 18:15;githubbot;Github user bikassaha commented on the issue:

    https://github.com/apache/incubator-livy/pull/50
  
    No tests seem to have been added or modified?
;;;","26/Sep/17 05:23;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/50
  
    Merge to Master and branch 0.4.
;;;","26/Sep/17 05:25;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-livy/pull/50
;;;","26/Sep/17 07:26;githubbot;GitHub user jerryshao opened a pull request:

    https://github.com/apache/incubator-livy/pull/52

    [LIVY-406][FOLLOWUP][SERVER] Fix LIVY-406 UT failure in branch 0.4 and apply to Master branch for consistency

    Because we changed the AppState logic in #39 , so the UT which passed in master branch will be failed in branch 0.4, here to fix this issue.
    
    Also apply this PR to master branch for the consistency, it should also be worked in master branch.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-406-followup

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/52.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #52
    
----
commit 860b1ec897127b04b324879b88afba31d9c7c778
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-26T07:22:09Z

    Fix LIVY-406 UT failure in branch 0.4, also apply to master for consistency

----
;;;","26/Sep/17 09:06;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/52
;;;","26/Sep/17 09:06;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/52

    [LIVY-406][FOLLOWUP][SERVER] Fix LIVY-406 UT failure in branch 0.4 and apply to Master branch for consistency

    Because we changed the AppState logic in #39 , so the UT which passed in master branch will be failed in branch 0.4, here to fix this issue.
    
    Also apply this PR to master branch for the consistency, it should also be worked in master branch.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-406-followup

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/52.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #52
    
----
commit 860b1ec897127b04b324879b88afba31d9c7c778
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-26T07:22:09Z

    Fix LIVY-406 UT failure in branch 0.4, also apply to master for consistency

----
;;;","26/Sep/17 09:44;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/52
;;;","26/Sep/17 09:44;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/52

    [LIVY-406][FOLLOWUP][SERVER] Fix LIVY-406 UT failure in branch 0.4 and apply to Master branch for consistency

    Because we changed the AppState logic in #39 , so the UT which passed in master branch will be failed in branch 0.4, here to fix this issue.
    
    Also apply this PR to master branch for the consistency, it should also be worked in master branch.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-406-followup

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/52.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #52
    
----
commit 860b1ec897127b04b324879b88afba31d9c7c778
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-26T07:22:09Z

    Fix LIVY-406 UT failure in branch 0.4, also apply to master for consistency

----
;;;","26/Sep/17 10:21;githubbot;Github user codecov-io commented on the issue:

    https://github.com/apache/incubator-livy/pull/52
  
    # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/52?src=pr&el=h1) Report
    > Merging [#52](https://codecov.io/gh/apache/incubator-livy/pull/52?src=pr&el=desc) into [master](https://codecov.io/gh/apache/incubator-livy/commit/4a537e24d605766f901232760f39b44adae817a2?src=pr&el=desc) will **increase** coverage by `0.11%`.
    > The diff coverage is `n/a`.
    
    [![Impacted file tree graph](https://codecov.io/gh/apache/incubator-livy/pull/52/graphs/tree.svg?token=0MkVbiUFwE&src=pr&height=150&width=650)](https://codecov.io/gh/apache/incubator-livy/pull/52?src=pr&el=tree)
    
    ```diff
    @@             Coverage Diff              @@
    ##             master      #52      +/-   ##
    ============================================
    + Coverage     70.82%   70.93%   +0.11%     
    - Complexity      791      793       +2     
    ============================================
      Files            97       97              
      Lines          5384     5384              
      Branches        798      798              
    ============================================
    + Hits           3813     3819       +6     
    + Misses         1036     1033       -3     
    + Partials        535      532       -3
    ```
    
    
    | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/52?src=pr&el=tree) | Coverage Î” | Complexity Î” | |
    |---|---|---|---|
    | [...main/java/org/apache/livy/rsc/ContextLauncher.java](https://codecov.io/gh/apache/incubator-livy/pull/52?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9Db250ZXh0TGF1bmNoZXIuamF2YQ==) | `84.15% <0%> (+0.49%)` | `18% <0%> (+1%)` | :arrow_up: |
    | [...ain/java/org/apache/livy/rsc/driver/RSCDriver.java](https://codecov.io/gh/apache/incubator-livy/pull/52?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9kcml2ZXIvUlNDRHJpdmVyLmphdmE=) | `80% <0%> (+0.85%)` | `42% <0%> (Ã¸)` | :arrow_down: |
    | [...cala/org/apache/livy/scalaapi/ScalaJobHandle.scala](https://codecov.io/gh/apache/incubator-livy/pull/52?src=pr&el=tree#diff-c2NhbGEtYXBpL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zY2FsYWFwaS9TY2FsYUpvYkhhbmRsZS5zY2FsYQ==) | `55.88% <0%> (+2.94%)` | `0% <0%> (Ã¸)` | :arrow_down: |
    | [...java/org/apache/livy/rsc/rpc/KryoMessageCodec.java](https://codecov.io/gh/apache/incubator-livy/pull/52?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9ycGMvS3J5b01lc3NhZ2VDb2RlYy5qYXZh) | `98.21% <0%> (+3.57%)` | `19% <0%> (+1%)` | :arrow_up: |
    
    ------
    
    [Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-livy/pull/52?src=pr&el=continue).
    > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
    > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
    > Powered by [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/52?src=pr&el=footer). Last update [4a537e2...860b1ec](https://codecov.io/gh/apache/incubator-livy/pull/52?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

;;;","26/Sep/17 13:05;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/52
  
    Merging to master and branch 0.4.
;;;","26/Sep/17 13:08;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-livy/pull/52
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add getCompletion for Interpreter,LIVY-403,13101890,,New Feature,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,zjffdu,zjffdu,13/Sep/17 08:46,25/Sep/17 01:14,19/Dec/25 04:15,25/Sep/17 01:14,0.5.0,,,,,REPL,,,,,,,,,,0,,,,,,"This is for code completion feature of interpreter. Also need to add rest api.
\cc [~jerryshao]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-7,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Sep 15 20:14:56 UTC 2017,,,,,,,,,,"0|i3k0a7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Sep/17 18:10;ajbozarth;Could you flesh out the description a bit more? I have no idea what this is for based on the title/description, only that its related to something else you're working on.;;;","15/Sep/17 07:56;jerryshao;[~ajbozarth], the purpose of this JIRA is to support code completion for interpreters in Livy interactive session. Zeppelin Spark interpreter, ipython notebook has similar features to do code completion via tab, so we should also have such feature in Livy to make Zeppelin Livy interpreter supports code completion.;;;","15/Sep/17 20:14;ajbozarth;Thanks, sounds cool;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load external package from Hue,LIVY-402,13101257,,Question,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Invalid,,maziyar,maziyar,11/Sep/17 13:21,15/Oct/17 15:15,19/Dec/25 04:15,15/Oct/17 15:15,0.4.0,,,,,,,,,,,,,,,0,,,,,,"Hi,

Is there anyway to load external packages (ex: --packages) from Hue interface? There is SparkConf but neither of these key:value work:
* packages:io.spray:spray-json_2.10:1.3.1
* spark.jars.packages:io.spray:spray-json_2.10:1.3.1
* livy.spark.jars.packages:io.spray:spray-json_2.10:1.3.1

I am aware of loading jars before starting Livy, but it would be nice if users can have their packages without asking the admin.

Many thanks,

","Cloudera Manager 5.12
CDH 5.12
Hue 3.0
Hue 4.0
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/17 13:30;maziyar;Screenshot 2017-09-11 15.21.35.png;https://issues.apache.org/jira/secure/attachment/12886418/Screenshot+2017-09-11+15.21.35.png","11/Sep/17 13:30;maziyar;Screenshot 2017-09-11 15.21.44.png;https://issues.apache.org/jira/secure/attachment/12886417/Screenshot+2017-09-11+15.21.44.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 13 14:13:07 UTC 2017,,,,,,,,,,"0|i3jwfb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Oct/17 12:48;maziyar;I found a way to address my jars in HDFS from Livy and also in Zeppelin there is a package config (globally). But, still not possible by each user individually from Hue.;;;","13/Oct/17 14:13;zjffdu;[~maziyar] This is hue issue, please ask it in hue project. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create release script to handle Livy release process,LIVY-400,13099170,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,jerryshao,01/Sep/17 08:42,04/Sep/17 01:30,19/Dec/25 04:15,04/Sep/17 01:30,0.4.1,0.5.0,,0.4.1,0.5.0,Build,,,,,,,,,,0,,,,,,Add a script to automate package and release thing for Livy.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 04 01:30:56 UTC 2017,,,,,,,,,,"0|i3jjiv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Sep/17 01:30;jerryshao;Issue resolved by pull request 42
[https://github.com/apache/incubator-livy/pull/42];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable real test for PySpark and SparkR interpreters,LIVY-399,13098819,13098817,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,jerryshao,31/Aug/17 09:02,27/Nov/17 08:26,19/Dec/25 04:15,27/Nov/17 08:26,0.4.0,,,0.5.0,,REPL,,,,,,,,,,0,,,,,,"Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration test, also the UT for PySpark and SparkR doesn't involve Spark things, only test the plain python and R REPL, so we should figure out a way to support real test for PySpark and SparkR interpreters.",,"GitHub user jerryshao opened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

----
;21/Sep/17 01:55;githubbot;600","Github user codecov-io commented on the issue:

    https://github.com/apache/incubator-livy/pull/49
  
    # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=h1) Report
    > Merging [#49](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=desc) into [master](https://codecov.io/gh/apache/incubator-livy/commit/219fdac5cfea1780cfb4e52f29777d3b21f8a55e?src=pr&el=desc) will **increase** coverage by `0.66%`.
    > The diff coverage is `n/a`.
    
    [![Impacted file tree graph](https://codecov.io/gh/apache/incubator-livy/pull/49/graphs/tree.svg?height=150&width=650&token=0MkVbiUFwE&src=pr)](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=tree)
    
    ```diff
    @@             Coverage Diff              @@
    ##             master      #49      +/-   ##
    ============================================
    + Coverage     70.71%   71.38%   +0.66%     
    + Complexity      791      789       -2     
    ============================================
      Files            97       97              
      Lines          5389     5389              
      Branches        800      800              
    ============================================
    + Hits           3811     3847      +36     
    + Misses         1040     1007      -33     
    + Partials        538      535       -3
    ```
    
    
    | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=tree) | Coverage Î” | Complexity Î” | |
    |---|---|---|---|
    | [...in/java/org/apache/livy/rsc/rpc/RpcDispatcher.java](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9ycGMvUnBjRGlzcGF0Y2hlci5qYXZh) | `64% <0%> (-3%)` | `19% <0%> (-1%)` | |
    | [...ain/java/org/apache/livy/rsc/driver/RSCDriver.java](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9kcml2ZXIvUlNDRHJpdmVyLmphdmE=) | `77.87% <0%> (-1.28%)` | `41% <0%> (-1%)` | |
    | [...in/java/org/apache/livy/rsc/driver/JobWrapper.java](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9kcml2ZXIvSm9iV3JhcHBlci5qYXZh) | `80.64% <0%> (Ã¸)` | `7% <0%> (-1%)` | :arrow_down: |
    | [...server/interactive/InteractiveSessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `65.83% <0%> (+2.5%)` | `5% <0%> (Ã¸)` | :arrow_down: |
    | [.../apache/livy/server/batch/CreateBatchRequest.scala](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvYmF0Y2gvQ3JlYXRlQmF0Y2hSZXF1ZXN0LnNjYWxh) | `65.62% <0%> (+3.12%)` | `19% <0%> (Ã¸)` | :arrow_down: |
    | [...main/java/org/apache/livy/rsc/ContextLauncher.java](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9Db250ZXh0TGF1bmNoZXIuamF2YQ==) | `84.15% <0%> (+3.46%)` | `18% <0%> (+1%)` | :arrow_up: |
    | [...scala/org/apache/livy/repl/PythonInterpreter.scala](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=tree#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9QeXRob25JbnRlcnByZXRlci5zY2FsYQ==) | `58.86% <0%> (+4.25%)` | `2% <0%> (Ã¸)` | :arrow_down: |
    | [...e/livy/server/interactive/InteractiveSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `69.13% <0%> (+8.03%)` | `42% <0%> (Ã¸)` | :arrow_down: |
    
    ------
    
    [Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=continue).
    > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
    > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
    > Powered by [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=footer). Last update [219fdac...15f84a9](https://codecov.io/gh/apache/incubator-livy/pull/49?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

;21/Sep/17 04:30;githubbot;600","Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/49#discussion_r140151729
  
    --- Diff: .travis.yml ---
    @@ -57,17 +57,17 @@ before_install:
       - sudo apt-get -y install python3-pip python-dev
       - sudo apt-get -y install libkrb5-dev
       - sudo apt-get -y remove python-setuptools
    -  - pip2 install --user --upgrade pip ""setuptools < 36""
    -  - pip3 install --user --upgrade pip ""setuptools < 36""
    -  - pip2 install --user codecov cloudpickle
    -  - pip3 install --user cloudpickle
    +  - sudo pip2 install --upgrade pip ""setuptools < 36""
    +  - sudo pip3 install --upgrade pip ""setuptools < 36""
    +  - sudo pip2 install codecov cloudpickle
    --- End diff --
    
    It is weird that pyspark on yarn doesn't honor cloudpickle installed under user's directory, and will throw `no module named cloudpickle.cloudpickle`, and after changing to install these packages to system path, the issue is gone.
;21/Sep/17 05:28;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/49
  
    @zjffdu @ajbozarth please help to review, thanks!
;21/Sep/17 05:38;githubbot;600","Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/49
  
    I don't see any glaring issues so if @zjffdu is ok with this then I am too.
;21/Sep/17 22:11;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/49
  
    close and reopen to test again.
;22/Sep/17 00:48;githubbot;600","Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;22/Sep/17 00:48;githubbot;600","GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

----
;22/Sep/17 00:48;githubbot;600","Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;22/Sep/17 01:29;githubbot;600","GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

----
;22/Sep/17 01:29;githubbot;600","Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;22/Sep/17 06:27;githubbot;600","GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

----
;22/Sep/17 06:28;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,7200,,,0,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 13 02:44:43 UTC 2017,,,,,,,,,,"0|i3jhvj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Sep/17 03:16;githubbot;Github user zjffdu commented on the issue:

    https://github.com/apache/incubator-livy/pull/49
  
    LGTM
;;;","25/Sep/17 03:36;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/49
  
    Some pyspark test looks quite flaky, I'm still digging on it.
;;;","25/Sep/17 08:08;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;;;","25/Sep/17 08:08;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

commit e8ec20f87cbefa6898cda8cf7d56d63c33908331
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:11:58Z

    Print out log if travis test is failed

commit d96ff77701e27ab63e46ec2e4c0be22d8220057d
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:16:34Z

    Style fix

commit 5f223d4ee2d5cdea7f2f3200f1a587dc027f8440
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T05:38:23Z

    Increase the timeout to reduce flakiness

commit f033fa9185907dbf3a3bc80dccefebc059718410
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T07:33:33Z

    Add some test log

----
;;;","26/Sep/17 06:52;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;;;","26/Sep/17 06:52;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

commit e8ec20f87cbefa6898cda8cf7d56d63c33908331
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:11:58Z

    Print out log if travis test is failed

commit d96ff77701e27ab63e46ec2e4c0be22d8220057d
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:16:34Z

    Style fix

commit 5f223d4ee2d5cdea7f2f3200f1a587dc027f8440
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T05:38:23Z

    Increase the timeout to reduce flakiness

commit f033fa9185907dbf3a3bc80dccefebc059718410
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T07:33:33Z

    Add some test log

commit 3f3decbcb1664ec143e61325516c2cf87440fee4
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-26T06:09:24Z

    Add more log to identify the issue

----
;;;","26/Sep/17 07:26;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;;;","26/Sep/17 07:26;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

commit e8ec20f87cbefa6898cda8cf7d56d63c33908331
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:11:58Z

    Print out log if travis test is failed

commit d96ff77701e27ab63e46ec2e4c0be22d8220057d
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:16:34Z

    Style fix

commit 5f223d4ee2d5cdea7f2f3200f1a587dc027f8440
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T05:38:23Z

    Increase the timeout to reduce flakiness

commit f033fa9185907dbf3a3bc80dccefebc059718410
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T07:33:33Z

    Add some test log

commit 3f3decbcb1664ec143e61325516c2cf87440fee4
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-26T06:09:24Z

    Add more log to identify the issue

----
;;;","26/Sep/17 09:07;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;;;","26/Sep/17 09:07;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

commit e8ec20f87cbefa6898cda8cf7d56d63c33908331
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:11:58Z

    Print out log if travis test is failed

commit d96ff77701e27ab63e46ec2e4c0be22d8220057d
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:16:34Z

    Style fix

commit 5f223d4ee2d5cdea7f2f3200f1a587dc027f8440
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T05:38:23Z

    Increase the timeout to reduce flakiness

commit f033fa9185907dbf3a3bc80dccefebc059718410
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T07:33:33Z

    Add some test log

commit 3f3decbcb1664ec143e61325516c2cf87440fee4
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-26T06:09:24Z

    Add more log to identify the issue

----
;;;","27/Sep/17 01:44;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;;;","27/Sep/17 01:44;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

commit e8ec20f87cbefa6898cda8cf7d56d63c33908331
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:11:58Z

    Print out log if travis test is failed

commit d96ff77701e27ab63e46ec2e4c0be22d8220057d
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:16:34Z

    Style fix

commit 5f223d4ee2d5cdea7f2f3200f1a587dc027f8440
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T05:38:23Z

    Increase the timeout to reduce flakiness

commit f033fa9185907dbf3a3bc80dccefebc059718410
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T07:33:33Z

    Add some test log

commit 3f3decbcb1664ec143e61325516c2cf87440fee4
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-26T06:09:24Z

    Add more log to identify the issue

----
;;;","28/Sep/17 03:34;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;;;","28/Sep/17 03:34;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

commit e8ec20f87cbefa6898cda8cf7d56d63c33908331
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:11:58Z

    Print out log if travis test is failed

commit d96ff77701e27ab63e46ec2e4c0be22d8220057d
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:16:34Z

    Style fix

commit 5f223d4ee2d5cdea7f2f3200f1a587dc027f8440
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T05:38:23Z

    Increase the timeout to reduce flakiness

commit f033fa9185907dbf3a3bc80dccefebc059718410
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T07:33:33Z

    Add some test log

commit 3f3decbcb1664ec143e61325516c2cf87440fee4
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-26T06:09:24Z

    Add more log to identify the issue

----
;;;","28/Sep/17 05:14;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;;;","28/Sep/17 05:14;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

commit e8ec20f87cbefa6898cda8cf7d56d63c33908331
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:11:58Z

    Print out log if travis test is failed

commit d96ff77701e27ab63e46ec2e4c0be22d8220057d
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:16:34Z

    Style fix

commit 5f223d4ee2d5cdea7f2f3200f1a587dc027f8440
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T05:38:23Z

    Increase the timeout to reduce flakiness

commit f033fa9185907dbf3a3bc80dccefebc059718410
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T07:33:33Z

    Add some test log

commit 3f3decbcb1664ec143e61325516c2cf87440fee4
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-26T06:09:24Z

    Add more log to identify the issue

----
;;;","09/Oct/17 05:41;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;;;","09/Oct/17 05:41;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

commit e8ec20f87cbefa6898cda8cf7d56d63c33908331
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:11:58Z

    Print out log if travis test is failed

commit d96ff77701e27ab63e46ec2e4c0be22d8220057d
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:16:34Z

    Style fix

commit 5f223d4ee2d5cdea7f2f3200f1a587dc027f8440
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T05:38:23Z

    Increase the timeout to reduce flakiness

commit f033fa9185907dbf3a3bc80dccefebc059718410
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T07:33:33Z

    Add some test log

commit 3f3decbcb1664ec143e61325516c2cf87440fee4
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-26T06:09:24Z

    Add more log to identify the issue

----
;;;","09/Oct/17 07:12;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;;;","09/Oct/17 07:12;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

commit e8ec20f87cbefa6898cda8cf7d56d63c33908331
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:11:58Z

    Print out log if travis test is failed

commit d96ff77701e27ab63e46ec2e4c0be22d8220057d
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:16:34Z

    Style fix

commit 5f223d4ee2d5cdea7f2f3200f1a587dc027f8440
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T05:38:23Z

    Increase the timeout to reduce flakiness

commit f033fa9185907dbf3a3bc80dccefebc059718410
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T07:33:33Z

    Add some test log

commit 3f3decbcb1664ec143e61325516c2cf87440fee4
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-26T06:09:24Z

    Add more log to identify the issue

----
;;;","13/Oct/17 02:44;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/49
;;;","13/Oct/17 02:44;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/49

    [LIVY-399][TEST] Enable real test for PySpark and SparkR interpreters

    Currently because we lack pyspark and sparkr dependencies in our environment, so we neglect the pyspark and sparkr integration tests, here propose a way to enable these integration tests. The solution is mainly borrowed from Zeppelin - to download Spark binary before integration tests, so that we will have a complete Spark environment to do the test. Because of this several modules like minicluster-dependencies are not required, so here removed such modules. 
    
    Besides, we proposed to remove real test implementations, since we never used real test before and it mainly did what scripts should do with Scala code, which makes it hard to maintain. Also for now mini-cluster test is quite close to real test, so it is not so useful to still keep real test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-399

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit f91c57f348f9c7098c92cd25ff60df989eb419ad
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T01:46:57Z

    Enable real test for PySpark and SparkR interpreters
    
    Change-Id: I860e30fdeded40a35479ec1823e00fb302fe8238

commit e9e70237cd0de200ff59a470a0859e6bb5f22171
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:14:01Z

    temporary changes to travis.yml to show failure log

commit 15f84a9a915d2ca06c6cf742c40f0f2ba03873dc
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T03:59:39Z

    Travis changes

commit d4cc3d517d8628ad4abba9fc053ccbfefda6bc44
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-21T05:35:55Z

    Revert travis changes

commit e8ec20f87cbefa6898cda8cf7d56d63c33908331
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:11:58Z

    Print out log if travis test is failed

commit d96ff77701e27ab63e46ec2e4c0be22d8220057d
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T01:16:34Z

    Style fix

commit 5f223d4ee2d5cdea7f2f3200f1a587dc027f8440
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T05:38:23Z

    Increase the timeout to reduce flakiness

commit f033fa9185907dbf3a3bc80dccefebc059718410
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-25T07:33:33Z

    Add some test log

commit 3f3decbcb1664ec143e61325516c2cf87440fee4
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-26T06:09:24Z

    Add more log to identify the issue

----
;;;",,,,,,,,,,,,,,,,,,,,,,
Change docs to describe how to use shared language session,LIVY-398,13098818,13098817,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,jerryshao,31/Aug/17 08:59,22/Sep/17 05:32,19/Dec/25 04:15,22/Sep/17 05:32,,,,0.5.0,,Docs,,,,,,,,,,0,,,,,,,,"GitHub user jerryshao opened a pull request:

    https://github.com/apache/incubator-livy/pull/47

    [LIVY-398][DOC] Update rest API to reflect the changes of shared context session

    LIVY-194 changes the semantics of session in Livy and add supports of shared context for Livy session, so here change the docs in REST API to reflect the changes of LIVY-194.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-398

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/47.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #47
    
----
commit 0632265cfd213ba2bf9e4951b56e9fa2d4710b8b
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-09-15T07:34:19Z

    Update docs
    
    Change-Id: I1063a26ac4eb857f79816a91ff9b352943352c74

----
;15/Sep/17 07:38;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/47
  
    @ajbozarth would you please help to review when you have time, thanks!
;15/Sep/17 07:40;githubbot;600","Github user codecov-io commented on the issue:

    https://github.com/apache/incubator-livy/pull/47
  
    # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/47?src=pr&el=h1) Report
    > Merging [#47](https://codecov.io/gh/apache/incubator-livy/pull/47?src=pr&el=desc) into [master](https://codecov.io/gh/apache/incubator-livy/commit/52c89a9c4a8d2d58bf51ecd9b0e049191ed7a0b6?src=pr&el=desc) will **increase** coverage by `0.11%`.
    > The diff coverage is `n/a`.
    
    [![Impacted file tree graph](https://codecov.io/gh/apache/incubator-livy/pull/47/graphs/tree.svg?token=0MkVbiUFwE&width=650&src=pr&height=150)](https://codecov.io/gh/apache/incubator-livy/pull/47?src=pr&el=tree)
    
    ```diff
    @@             Coverage Diff              @@
    ##             master      #47      +/-   ##
    ============================================
    + Coverage      70.6%   70.71%   +0.11%     
    - Complexity      786      787       +1     
    ============================================
      Files            97       97              
      Lines          5388     5388              
      Branches        802      802              
    ============================================
    + Hits           3804     3810       +6     
    + Misses         1045     1041       -4     
    + Partials        539      537       -2
    ```
    
    
    | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/47?src=pr&el=tree) | Coverage Î” | Complexity Î” | |
    |---|---|---|---|
    | [...cala/org/apache/livy/scalaapi/ScalaJobHandle.scala](https://codecov.io/gh/apache/incubator-livy/pull/47?src=pr&el=tree#diff-c2NhbGEtYXBpL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zY2FsYWFwaS9TY2FsYUpvYkhhbmRsZS5zY2FsYQ==) | `52.94% <0%> (-2.95%)` | `0% <0%> (Ã¸)` | |
    | [...main/java/org/apache/livy/rsc/ContextLauncher.java](https://codecov.io/gh/apache/incubator-livy/pull/47?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9Db250ZXh0TGF1bmNoZXIuamF2YQ==) | `84.15% <0%> (+3.46%)` | `18% <0%> (+1%)` | :arrow_up: |
    
    ------
    
    [Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-livy/pull/47?src=pr&el=continue).
    > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
    > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
    > Powered by [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/47?src=pr&el=footer). Last update [52c89a9...0632265](https://codecov.io/gh/apache/incubator-livy/pull/47?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

;15/Sep/17 08:15;githubbot;600","Github user maziyarpanahi commented on the issue:

    https://github.com/apache/incubator-livy/pull/47
  
    Hi @jerryshao 
    Just a quick note:
    
    > Starting from Livy 0.5.0, each session could support **both** Scala, Python and R interpreters
    
    I think it's best to replace ""**both**"" with ""**all three**"".
    
    Thanks and cheers.
;15/Sep/17 10:39;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139249901
  
    --- Diff: docs/rest-api.md ---
    @@ -610,18 +624,25 @@ A session represents an interactive shell.
       </tr>
       <tr>
         <td><a href=""#pyspark"">pyspark</a></td>
    -    <td>Interactive Python 2 Spark session</td>
    -  </tr>
    -  <tr>
    -    <td><a href=""#pyspark3"">pyspark3</a></td>
    -    <td>Interactive Python 3 Spark session</td>
    +    <td>Interactive Python Spark session</td>
       </tr>
       <tr>
         <td>sparkr</td>
         <td>Interactive R Spark session</td>
       </tr>
     </table>
     
    +Starting from Livy 0.5.0, each session could support both Scala, Python and R interpreters. The
    --- End diff --
    
    - `0.5.0-incubating`
    - `could` -> `can`
    - `both` -> all three`
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139251278
  
    --- Diff: docs/rest-api.md ---
    @@ -610,18 +624,25 @@ A session represents an interactive shell.
       </tr>
       <tr>
         <td><a href=""#pyspark"">pyspark</a></td>
    -    <td>Interactive Python 2 Spark session</td>
    -  </tr>
    -  <tr>
    -    <td><a href=""#pyspark3"">pyspark3</a></td>
    -    <td>Interactive Python 3 Spark session</td>
    +    <td>Interactive Python Spark session</td>
       </tr>
       <tr>
         <td>sparkr</td>
         <td>Interactive R Spark session</td>
       </tr>
     </table>
     
    +Starting from Livy 0.5.0, each session could support both Scala, Python and R interpreters. The
    +``kind`` field in session creation is not required anymore, instead user needs to specify code kind
    +(spark, pyspark or sparkr) during statement submission.
    +
    +To be compatible with old protocol, user could still specify ``kind`` field in session creation
    +as a hint, while ignoring the ``kind`` field in statement submission, Livy will use this session
    +``kind`` as the default kind for all the submitted statements.
    --- End diff --
    
    ~~`the`~~
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139249455
  
    --- Diff: docs/rest-api.md ---
    @@ -266,6 +270,16 @@ Runs a statement in a session.
         <td>The code to execute</td>
         <td>string</td>
       </tr>
    +  <tr>
    +    <td>kind</td>
    +    <td>
    +      The kind of code to execute (optional). If session kind is not specified or the submitted
    --- End diff --
    
    Similar to above everything after the first sentence might be better as a footnote.
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139251642
  
    --- Diff: docs/rest-api.md ---
    @@ -610,18 +624,25 @@ A session represents an interactive shell.
       </tr>
       <tr>
         <td><a href=""#pyspark"">pyspark</a></td>
    -    <td>Interactive Python 2 Spark session</td>
    -  </tr>
    -  <tr>
    -    <td><a href=""#pyspark3"">pyspark3</a></td>
    -    <td>Interactive Python 3 Spark session</td>
    +    <td>Interactive Python Spark session</td>
       </tr>
       <tr>
         <td>sparkr</td>
         <td>Interactive R Spark session</td>
       </tr>
     </table>
     
    +Starting from Livy 0.5.0, each session could support both Scala, Python and R interpreters. The
    +``kind`` field in session creation is not required anymore, instead user needs to specify code kind
    +(spark, pyspark or sparkr) during statement submission.
    +
    +To be compatible with old protocol, user could still specify ``kind`` field in session creation
    +as a hint, while ignoring the ``kind`` field in statement submission, Livy will use this session
    +``kind`` as the default kind for all the submitted statements.
    +
    +If user want to submit code other than the default ``kind`` specified in session creation, user
    +needs to specify code kind (spark, pyspark or sparkr) during statement submission.
    +
    --- End diff --
    
    maybe include a note that `pyspark3` was deprecated and combined into `pyspark`?
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139248419
  
    --- Diff: docs/rest-api.md ---
    @@ -78,7 +78,11 @@ Creates a new interactive Scala, Python, or R shell in the cluster.
       <tr><th>Name</th><th>Description</th><th>Type</th></tr>
       <tr>
         <td>kind</td>
    -    <td>The session kind (required)</td>
    +    <td>
    +      The session kind. Starting from 0.5, this field is not required, to be compatible
    +      with old protocol, user could still specify this with spark, pyspark or sparkr, which
    +      implies that the submitted code snippet is one of these kinds.
    --- End diff --
    
    Grammar:
    - `one of these kinds` -> `the corresponding kind`
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139251423
  
    --- Diff: docs/rest-api.md ---
    @@ -610,18 +624,25 @@ A session represents an interactive shell.
       </tr>
       <tr>
         <td><a href=""#pyspark"">pyspark</a></td>
    -    <td>Interactive Python 2 Spark session</td>
    -  </tr>
    -  <tr>
    -    <td><a href=""#pyspark3"">pyspark3</a></td>
    -    <td>Interactive Python 3 Spark session</td>
    +    <td>Interactive Python Spark session</td>
       </tr>
       <tr>
         <td>sparkr</td>
         <td>Interactive R Spark session</td>
       </tr>
     </table>
     
    +Starting from Livy 0.5.0, each session could support both Scala, Python and R interpreters. The
    +``kind`` field in session creation is not required anymore, instead user needs to specify code kind
    +(spark, pyspark or sparkr) during statement submission.
    +
    +To be compatible with old protocol, user could still specify ``kind`` field in session creation
    +as a hint, while ignoring the ``kind`` field in statement submission, Livy will use this session
    +``kind`` as the default kind for all the submitted statements.
    +
    +If user want to submit code other than the default ``kind`` specified in session creation, user
    --- End diff --
    
    Grammar: plural `users` sounds better
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139248269
  
    --- Diff: docs/rest-api.md ---
    @@ -78,7 +78,11 @@ Creates a new interactive Scala, Python, or R shell in the cluster.
       <tr><th>Name</th><th>Description</th><th>Type</th></tr>
       <tr>
         <td>kind</td>
    -    <td>The session kind (required)</td>
    +    <td>
    +      The session kind. Starting from 0.5, this field is not required, to be compatible
    +      with old protocol, user could still specify this with spark, pyspark or sparkr, which
    --- End diff --
    
    Grammar:
     - `old protocol` -> `previous versions` (and no comma after)
     - `user` -> `users`
     - `could` -> `can`
     - `this` -> `kind`
    - `which implies that` ->` implying`
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139247248
  
    --- Diff: docs/rest-api.md ---
    @@ -78,7 +78,11 @@ Creates a new interactive Scala, Python, or R shell in the cluster.
       <tr><th>Name</th><th>Description</th><th>Type</th></tr>
       <tr>
         <td>kind</td>
    -    <td>The session kind (required)</td>
    +    <td>
    +      The session kind. Starting from 0.5, this field is not required, to be compatible
    --- End diff --
    
    Grammar: 
     - `from` -> `with version` 
     - use full version number (`0.5.0-incubating`)
     - remove first comma and replace second comma with period and start new sentence
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139248683
  
    --- Diff: docs/rest-api.md ---
    @@ -266,6 +270,16 @@ Runs a statement in a session.
         <td>The code to execute</td>
         <td>string</td>
       </tr>
    +  <tr>
    +    <td>kind</td>
    +    <td>
    +      The kind of code to execute (optional). If session kind is not specified or the submitted
    +      code is not the kind specified in session creation, this field should be filled with
    +      correct kind; Otherwise Livy will use kind specified in session creation as the default code
    --- End diff --
    
    Grammar: 
    - period instead of semi-colon
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139249328
  
    --- Diff: docs/rest-api.md ---
    @@ -78,7 +78,11 @@ Creates a new interactive Scala, Python, or R shell in the cluster.
       <tr><th>Name</th><th>Description</th><th>Type</th></tr>
       <tr>
         <td>kind</td>
    -    <td>The session kind (required)</td>
    +    <td>
    +      The session kind. Starting from 0.5, this field is not required, to be compatible
    --- End diff --
    
    Also it may be cleaner to move everything after `The session kind.` into a footnote below the table.
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139250822
  
    --- Diff: docs/rest-api.md ---
    @@ -610,18 +624,25 @@ A session represents an interactive shell.
       </tr>
       <tr>
         <td><a href=""#pyspark"">pyspark</a></td>
    -    <td>Interactive Python 2 Spark session</td>
    -  </tr>
    -  <tr>
    -    <td><a href=""#pyspark3"">pyspark3</a></td>
    -    <td>Interactive Python 3 Spark session</td>
    +    <td>Interactive Python Spark session</td>
       </tr>
       <tr>
         <td>sparkr</td>
         <td>Interactive R Spark session</td>
       </tr>
     </table>
     
    +Starting from Livy 0.5.0, each session could support both Scala, Python and R interpreters. The
    +``kind`` field in session creation is not required anymore, instead user needs to specify code kind
    +(spark, pyspark or sparkr) during statement submission.
    +
    +To be compatible with old protocol, user could still specify ``kind`` field in session creation
    --- End diff --
    
    Grammar:
    - `old protocol` -> `previous versions`
    - `could` -> `can`
    - ~~`field`~~
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139251178
  
    --- Diff: docs/rest-api.md ---
    @@ -610,18 +624,25 @@ A session represents an interactive shell.
       </tr>
       <tr>
         <td><a href=""#pyspark"">pyspark</a></td>
    -    <td>Interactive Python 2 Spark session</td>
    -  </tr>
    -  <tr>
    -    <td><a href=""#pyspark3"">pyspark3</a></td>
    -    <td>Interactive Python 3 Spark session</td>
    +    <td>Interactive Python Spark session</td>
       </tr>
       <tr>
         <td>sparkr</td>
         <td>Interactive R Spark session</td>
       </tr>
     </table>
     
    +Starting from Livy 0.5.0, each session could support both Scala, Python and R interpreters. The
    +``kind`` field in session creation is not required anymore, instead user needs to specify code kind
    +(spark, pyspark or sparkr) during statement submission.
    +
    +To be compatible with old protocol, user could still specify ``kind`` field in session creation
    +as a hint, while ignoring the ``kind`` field in statement submission, Livy will use this session
    --- End diff --
    
    - ~~`as a hint`~~
    - end sentence after submission
    - `Livy will then use`
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139250453
  
    --- Diff: docs/rest-api.md ---
    @@ -610,18 +624,25 @@ A session represents an interactive shell.
       </tr>
       <tr>
         <td><a href=""#pyspark"">pyspark</a></td>
    -    <td>Interactive Python 2 Spark session</td>
    -  </tr>
    -  <tr>
    -    <td><a href=""#pyspark3"">pyspark3</a></td>
    -    <td>Interactive Python 3 Spark session</td>
    +    <td>Interactive Python Spark session</td>
       </tr>
       <tr>
         <td>sparkr</td>
         <td>Interactive R Spark session</td>
       </tr>
     </table>
     
    +Starting from Livy 0.5.0, each session could support both Scala, Python and R interpreters. The
    +``kind`` field in session creation is not required anymore, instead user needs to specify code kind
    --- End diff --
    
    Grammar:
    - `not required anymore` -> `no longer required`
    - `user needs to` -> `users must`
;15/Sep/17 21:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139248881
  
    --- Diff: docs/rest-api.md ---
    @@ -266,6 +270,16 @@ Runs a statement in a session.
         <td>The code to execute</td>
         <td>string</td>
       </tr>
    +  <tr>
    +    <td>kind</td>
    +    <td>
    +      The kind of code to execute (optional). If session kind is not specified or the submitted
    --- End diff --
    
    I don't think `(optional)` is necessary here based on the following description.
;15/Sep/17 21:15;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/47
  
    Sorry about my bad grammar, I will update it soon.
;18/Sep/17 07:15;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139507855
  
    --- Diff: docs/rest-api.md ---
    @@ -157,6 +153,10 @@ Creates a new interactive Scala, Python, or R shell in the cluster.
       </tr>
     </table>
     
    +<a name=""footnote1"">1</a>: Starting with version 0.5.0-incubating this field is not required. To be
    +compatible with previous versions users could still specify this with spark, pyspark or sparkr,
    --- End diff --
    
    `could` -> `can`
;18/Sep/17 18:59;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139510231
  
    --- Diff: docs/rest-api.md ---
    @@ -153,6 +153,10 @@ Creates a new interactive Scala, Python, or R shell in the cluster.
       </tr>
     </table>
     
    +<a name=""footnote1"">1</a>: Starting with version 0.5.0-incubating this field is not required. To be
    --- End diff --
    
    to work with the href like I mentioned above this should be `id` instead of `name`
;18/Sep/17 18:59;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139508796
  
    --- Diff: docs/rest-api.md ---
    @@ -632,21 +631,25 @@ A session represents an interactive shell.
       </tr>
     </table>
     
    -Starting from Livy 0.5.0, each session could support both Scala, Python and R interpreters. The
    -``kind`` field in session creation is not required anymore, instead user needs to specify code kind
    -(spark, pyspark or sparkr) during statement submission.
    +Starting with version 0.5.0-incubating, each session can support all three Scala, Python and R
    +interpreters. The ``kind`` field in session creation is no longer required, instead users must
    --- End diff --
    
    `must` -> `should`
    On a second read, `should` makes more sense since it can still work the original way.
;18/Sep/17 18:59;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/47#discussion_r139509864
  
    --- Diff: docs/rest-api.md ---
    @@ -78,7 +78,7 @@ Creates a new interactive Scala, Python, or R shell in the cluster.
       <tr><th>Name</th><th>Description</th><th>Type</th></tr>
       <tr>
         <td>kind</td>
    -    <td>The session kind (required)</td>
    +    <td>The session kind<sup>[1](#footnote1)</sup></td>
    --- End diff --
    
    This syntax won't work with how we generate the Docs, this will need to be an href like other links in this doc
;18/Sep/17 18:59;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/47
  
    @ajbozarth would you please review again to see if there's html issues. Thanks!
;21/Sep/17 03:23;githubbot;600","Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/47
  
    None that I can see from reading the code, did you build the Docs using the Docs README to check if they render fine with the Jekyll build?
;21/Sep/17 21:16;githubbot;600","Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/47
  
    If you can confirm the Docs build works with a screen shot then this LGTM
;21/Sep/17 21:17;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/47
  
    OK, I will try to build the Docs to check it (though I haven't done it before).
;22/Sep/17 01:56;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/47
  
    Thanks for your review, merging to master branch.
;22/Sep/17 05:30;githubbot;600","Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-livy/pull/47
;22/Sep/17 05:31;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,13200,,,0,13200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-08-31 08:59:26.0,,,,,,,,,,"0|i3jhvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support multiple languages in one session,LIVY-397,13098817,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,jerryshao,31/Aug/17 08:53,27/Nov/17 08:27,19/Dec/25 04:15,27/Nov/17 08:27,0.4.0,,,0.5.0,,REPL,,,,,,,,,,2,,,,,,"This JIRA aims to support multiple languages in one session.

Currently in the Livy one session can only have one interpreter, which means user can only choose one language in one session, this limits the use case of using different languages under one Spark application. So here propose to support this feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,20400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-08-31 08:53:40.0,,,,,,,,,,"0|i3jhv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy does not map YARN app states correctly,LIVY-396,13098664,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,meisam,meisam,meisam,30/Aug/17 21:28,22/Jan/19 22:27,19/Dec/25 04:15,18/Sep/17 08:36,0.4.0,,,0.5.0,,Server,,,,,,,,,,0,,,,,,"If the status of a YARN app is {{KILLED}}, the final status should be {{KILLED}} too, but there is a test case in [{{SparkYarnAppSpec.scala}}|https://github.com/meisam/incubator-livy/blob/master/server/src/test/scala/org/apache/livy/utils/SparkYarnAppSpec.scala#L217] that expect the final status to be {{UNDEFINED}}.

{code}
 assert(app.mapYarnState(appId, KILLED, UNDEFINED) == State.KILLED)
{code}

 The case that app state is {{KILLED}} and the final status is {{UNDEFINED}} should never happen.",all enironments,"Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/39
  
    @meisam , can you please fix the UT, thanks.
;08/Sep/17 08:19;githubbot;600","Github user meisam commented on the issue:

    https://github.com/apache/incubator-livy/pull/39
  
    The fix is coming with LIVY-336: #44 
;08/Sep/17 19:35;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/39
  
    I don't think we can merge this PR with UT failure, please fix this here.
;11/Sep/17 01:04;githubbot;600","Github user meisam commented on the issue:

    https://github.com/apache/incubator-livy/pull/39
  
    @jerryshao I updated the PR with fix to UTs.
    Sorry for not being clear. What I meant was, I may close this PR without merging it because LIVY-336:#44 updates many unit tests, including the tests that fail here.
    But on a second though, I see that getting LIVY-336 merged takes a while, so I went ahead and fixed tests here.
;11/Sep/17 20:04;githubbot;600","Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/39
  
    @meisam just an fyi when you amend then force push your updates it makes it hard to track changes made from feedback. In the future you can just make another commit to your branch and push it, then we can see what changes you made.
;11/Sep/17 20:56;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/39#discussion_r138189999
  
    --- Diff: server/src/test/scala/org/apache/livy/utils/SparkYarnAppSpec.scala ---
    @@ -58,21 +59,50 @@ class SparkYarnAppSpec extends FunSpec with LivyBaseUnitTestSuite {
             val mockYarnClient = mock[YarnClient]
             val mockAppListener = mock[SparkAppListener]
     
    +
    --- End diff --
    
    unnecessary extra blank line
;11/Sep/17 21:03;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/39#discussion_r138190166
  
    --- Diff: server/src/test/scala/org/apache/livy/utils/SparkYarnAppSpec.scala ---
    @@ -58,21 +59,50 @@ class SparkYarnAppSpec extends FunSpec with LivyBaseUnitTestSuite {
             val mockYarnClient = mock[YarnClient]
             val mockAppListener = mock[SparkAppListener]
     
    +
             val mockAppReport = mock[ApplicationReport]
             when(mockAppReport.getApplicationId).thenReturn(appId)
    -        when(mockAppReport.getFinalApplicationStatus).thenReturn(FinalApplicationStatus.SUCCEEDED)
    -        // Simulate YARN app state progression.
    -        when(mockAppReport.getYarnApplicationState).thenAnswer(new Answer[YarnApplicationState]() {
    -          private var stateSeq = List(ACCEPTED, RUNNING, FINISHED)
    +//        when(mockAppReport.getFinalApplicationStatus)
    --- End diff --
    
    Why is this commented out line here?
;11/Sep/17 21:03;githubbot;600","Github user meisam commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/39#discussion_r138196284
  
    --- Diff: server/src/test/scala/org/apache/livy/utils/SparkYarnAppSpec.scala ---
    @@ -58,21 +59,50 @@ class SparkYarnAppSpec extends FunSpec with LivyBaseUnitTestSuite {
             val mockYarnClient = mock[YarnClient]
             val mockAppListener = mock[SparkAppListener]
     
    +
             val mockAppReport = mock[ApplicationReport]
             when(mockAppReport.getApplicationId).thenReturn(appId)
    -        when(mockAppReport.getFinalApplicationStatus).thenReturn(FinalApplicationStatus.SUCCEEDED)
    -        // Simulate YARN app state progression.
    -        when(mockAppReport.getYarnApplicationState).thenAnswer(new Answer[YarnApplicationState]() {
    -          private var stateSeq = List(ACCEPTED, RUNNING, FINISHED)
    +//        when(mockAppReport.getFinalApplicationStatus)
    --- End diff --
    
    I replaced it a with `when(mockAppReport.getFinalApplicationStatus).thenAnswer( ...` a few lines down.
;11/Sep/17 21:26;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/39#discussion_r138196781
  
    --- Diff: server/src/test/scala/org/apache/livy/utils/SparkYarnAppSpec.scala ---
    @@ -58,21 +59,50 @@ class SparkYarnAppSpec extends FunSpec with LivyBaseUnitTestSuite {
             val mockYarnClient = mock[YarnClient]
             val mockAppListener = mock[SparkAppListener]
     
    +
             val mockAppReport = mock[ApplicationReport]
             when(mockAppReport.getApplicationId).thenReturn(appId)
    -        when(mockAppReport.getFinalApplicationStatus).thenReturn(FinalApplicationStatus.SUCCEEDED)
    -        // Simulate YARN app state progression.
    -        when(mockAppReport.getYarnApplicationState).thenAnswer(new Answer[YarnApplicationState]() {
    -          private var stateSeq = List(ACCEPTED, RUNNING, FINISHED)
    +//        when(mockAppReport.getFinalApplicationStatus)
    --- End diff --
    
    Then should it be removed instead of commented out?
;11/Sep/17 21:28;githubbot;600","Github user meisam commented on the issue:

    https://github.com/apache/incubator-livy/pull/39
  
    > fyi when you amend then force push your updates it makes it hard to track changes made from feedback. In the future you can just make another commit to your branch and push it, then we can see what changes you made.
    Thanks for the feedback. On feature/fix branches that are not merged, I rebase my own commits to keep the git commit history clean, but this is a personal preference.
;11/Sep/17 22:19;githubbot;600","Github user meisam commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/39#discussion_r138216506
  
    --- Diff: server/src/test/scala/org/apache/livy/utils/SparkYarnAppSpec.scala ---
    @@ -58,21 +59,50 @@ class SparkYarnAppSpec extends FunSpec with LivyBaseUnitTestSuite {
             val mockYarnClient = mock[YarnClient]
             val mockAppListener = mock[SparkAppListener]
     
    +
             val mockAppReport = mock[ApplicationReport]
             when(mockAppReport.getApplicationId).thenReturn(appId)
    -        when(mockAppReport.getFinalApplicationStatus).thenReturn(FinalApplicationStatus.SUCCEEDED)
    -        // Simulate YARN app state progression.
    -        when(mockAppReport.getYarnApplicationState).thenAnswer(new Answer[YarnApplicationState]() {
    -          private var stateSeq = List(ACCEPTED, RUNNING, FINISHED)
    +//        when(mockAppReport.getFinalApplicationStatus)
    --- End diff --
    
    I removed it.
;11/Sep/17 23:14;githubbot;600","Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/39
  
    > Thanks for the feedback. On feature/fix branches that are not merged, I rebase my own commits to keep the git commit history clean, but this is a personal preference.
    I can completely understand that for projects were you commit to master directly, but in the case of Spark the merge script will squash all your commits and use the PR title and description for the commit message. So it's better in Spark work to not do rebases unless you need to merge master in multiple times (in which case that can get messy). This of course is all from a PR review perspective
;11/Sep/17 23:15;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/39
  
    Looks like travis environment is changed, which makes python-api build fail, let me fix this issue first.
;15/Sep/17 03:00;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/39
  
    @meisam would you please trigger the test again. I just fixed the travis issue.
;15/Sep/17 06:53;githubbot;600","Github user meisam commented on the issue:

    https://github.com/apache/incubator-livy/pull/39
  
    Closing and reopening the PR to trigger travis.
;15/Sep/17 13:33;githubbot;600","Github user meisam closed the pull request at:

    https://github.com/apache/incubator-livy/pull/39
;15/Sep/17 13:33;githubbot;600","GitHub user meisam reopened a pull request:

    https://github.com/apache/incubator-livy/pull/39

    [LIVY-396] Livy does not map YARN app states correctly

    Make sure that the invalid combinations of `state` and `finalStatus` do
    not occur, and if they do, put the application in the `FAILED` state.
    
    Task-Url: https://issues.apache.org/jira/browse/LIVY-396

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/meisam/incubator-livy LIVY-396

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/39.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #39
    
----
commit 2e05c7275c58be108f95ac3216537d678955d8e5
Author: Fathi Salmi, Meisam(mfathisalmi) <mfathisalmi@paypal.com>
Date:   2017-08-30T23:03:05Z

    [LIVY-396] Livy does not map YARN app states correctly
    
    Make sure that the invalid combinations of `state` and `finalStatus` do
    not occur, and if they do, put the application in the `FAILED` state.
    
    Task-Url: https://issues.apache.org/jira/browse/LIVY-396

commit 700af441247f800a66fe2557dc22a991ea026340
Author: Fathi Salmi, Meisam(mfathisalmi) <mfathisalmi@paypal.com>
Date:   2017-09-11T21:27:51Z

    [LIVY-396] [Minor] Code clean up
    
    Cleaning up the code it `SparkYarnAppSpec`
    
    Task-Url: https://issues.apache.org/jira/browse/LIVY-396

----
;15/Sep/17 13:33;githubbot;600","Github user codecov-io commented on the issue:

    https://github.com/apache/incubator-livy/pull/39
  
    # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/39?src=pr&el=h1) Report
    > Merging [#39](https://codecov.io/gh/apache/incubator-livy/pull/39?src=pr&el=desc) into [master](https://codecov.io/gh/apache/incubator-livy/commit/05bfa15ef3a1df1cd4b7dabdb5c7ce47d9d26d16?src=pr&el=desc) will **decrease** coverage by `0.01%`.
    > The diff coverage is `41.17%`.
    
    [![Impacted file tree graph](https://codecov.io/gh/apache/incubator-livy/pull/39/graphs/tree.svg?width=650&height=150&src=pr&token=0MkVbiUFwE)](https://codecov.io/gh/apache/incubator-livy/pull/39?src=pr&el=tree)
    
    ```diff
    @@             Coverage Diff              @@
    ##             master      #39      +/-   ##
    ============================================
    - Coverage     70.73%   70.71%   -0.02%     
    + Complexity      788      787       -1     
    ============================================
      Files            97       97              
      Lines          5388     5389       +1     
      Branches        802      800       -2     
    ============================================
      Hits           3811     3811              
    - Misses         1037     1042       +5     
    + Partials        540      536       -4
    ```
    
    
    | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/39?src=pr&el=tree) | Coverage Î” | Complexity Î” | |
    |---|---|---|---|
    | [...ain/scala/org/apache/livy/utils/SparkYarnApp.scala](https://codecov.io/gh/apache/incubator-livy/pull/39?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS91dGlscy9TcGFya1lhcm5BcHAuc2NhbGE=) | `62.93% <41.17%> (+0.96%)` | `31 <14> (+2)` | :arrow_up: |
    | [...in/java/org/apache/livy/rsc/rpc/RpcDispatcher.java](https://codecov.io/gh/apache/incubator-livy/pull/39?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9ycGMvUnBjRGlzcGF0Y2hlci5qYXZh) | `64% <0%> (-4%)` | `19% <0%> (-1%)` | |
    | [...ain/java/org/apache/livy/rsc/driver/RSCDriver.java](https://codecov.io/gh/apache/incubator-livy/pull/39?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9kcml2ZXIvUlNDRHJpdmVyLmphdmE=) | `76.59% <0%> (-1.28%)` | `39% <0%> (-1%)` | |
    | [...in/java/org/apache/livy/rsc/driver/JobWrapper.java](https://codecov.io/gh/apache/incubator-livy/pull/39?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9kcml2ZXIvSm9iV3JhcHBlci5qYXZh) | `80.64% <0%> (Ã¸)` | `7% <0%> (-1%)` | :arrow_down: |
    | [...main/java/org/apache/livy/rsc/ContextLauncher.java](https://codecov.io/gh/apache/incubator-livy/pull/39?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9Db250ZXh0TGF1bmNoZXIuamF2YQ==) | `83.66% <0%> (+2.47%)` | `17% <0%> (Ã¸)` | :arrow_down: |
    
    ------
    
    [Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-livy/pull/39?src=pr&el=continue).
    > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
    > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
    > Powered by [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/39?src=pr&el=footer). Last update [05bfa15...700af44](https://codecov.io/gh/apache/incubator-livy/pull/39?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

;15/Sep/17 14:15;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/39
  
    LGTM, merging to master.
;18/Sep/17 08:35;githubbot;600","Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-livy/pull/39
;18/Sep/17 08:36;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1200,0,12000,"1,000%",1200,0,12000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Sep 18 08:36:53 UTC 2017,,,,,,,,,,"0|i3jgx3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/Aug/17 21:37;ajbozarth;If I'm understanding that test correctly, if the YARN State is UNDEFINED then the Livy State should be KILLED, that is if YARN enters an undefined state the Livy App is killed in response. I'd have to look through the code in detail to make sure that's whats actually occurring though.;;;","30/Aug/17 21:54;kpraveen;I am afiraid not. If the final status is ""UNDEFINED"", it means the application is still running (or it is just submitted). So the state should be ""RUNNING"", ""NEW"", ""NEW_SAVING"", ""SUBMITTED"", or ""ACCEPTED"".;;;","31/Aug/17 02:34;meisam;I also noticed that [YARN-4207|https://issues.apache.org/jira/browse/YARN-4207] added a new final status, i.e. {{ENDED}}, as well, which is not taken into account in Livy yet ([Link to the commit|https://github.com/apache/hadoop/commit/0f708d465fbc4260f2c36e8067e27cd8b285fde7]).;;;","18/Sep/17 08:36;jerryshao;Issue resolved by pull request 39
[https://github.com/apache/incubator-livy/pull/39];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update version of Livy code base,LIVY-394,13097239,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,jerryshao,24/Aug/17 07:21,01/Sep/17 06:26,19/Dec/25 04:15,01/Sep/17 06:26,0.4.1,,,0.4.1,0.5.0,Build,,,,,,,,,,0,,,,,,"Update the version of master branch to 0.5.0-SNAPSHOT, also branch-0.4 should be updated to 0.4.1-SNAPSHOT.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 24 07:22:01 UTC 2017,,,,,,,,,,"0|i3j87r:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Aug/17 07:22;jerryshao;This could be done after 0.4.0-incubating is out.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy depends on two versions of apachecommons:httpclient,LIVY-393,13097101,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,ajbozarth,meisam,meisam,23/Aug/17 18:24,22/Jan/19 22:27,19/Dec/25 04:15,24/Aug/17 06:46,0.4.0,,,0.4.1,0.5.0,API,,,,,,,,,,0,,,,,,"The version of org.apache.httpcomponents:httpclient is different in /pom.xml from the version in /client-http/pom.xml

{code}
pom.xml             --->  ${httpclient.version} ---> 4.5.2
client-http/pom.xml --->                             4.5.1
{code}
",all environments,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1200,1200,,0%,1200,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 24 06:44:12 UTC 2017,,,,,,,,,,"0|i3j7d3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/Aug/17 20:06;ajbozarth;[~meisam] were you going to open a PR or should I?;;;","23/Aug/17 22:26;githubbot;GitHub user ajbozarth opened a pull request:

    https://github.com/apache/incubator-livy/pull/37

    [LIVY-393] [Minor] Update httpclient to 4.5.3

    Currently the httpclient version in the root pom is 4.5.2 and in the client-http module it is 4.5.1
    
    Now both use a common version and it's been updated to the latest version 4.5.3

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ajbozarth/incubator-livy hc-version

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/37.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #37
    
----
commit bb6e15cf46d8a688eb2cc40c1d2e93eaa0bef2ea
Author: Alex Bozarth <ajbozart@us.ibm.com>
Date:   2017-08-23T20:45:26Z

    [LIVY-393] [Minor] Update httpclient to 4.5.3
    
    Currently the httpclient version in the root pom is 4.5.2 and in the client-http module it is 4.5.1
    
    Now both use a common version and it's been updated to the latest version 4.5.3

----
;;;","23/Aug/17 22:27;ajbozarth;I opened up a minor pr for this https://github.com/apache/incubator-livy/pull/37;;;","23/Aug/17 22:37;meisam;Thanks Alex (y);;;","23/Aug/17 23:07;githubbot;Github user codecov-io commented on the issue:

    https://github.com/apache/incubator-livy/pull/37
  
    # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/37?src=pr&el=h1) Report
    > Merging [#37](https://codecov.io/gh/apache/incubator-livy/pull/37?src=pr&el=desc) into [master](https://codecov.io/gh/apache/incubator-livy/commit/ca5b1be777de1310d7a4ece96a8d77852af4796f?src=pr&el=desc) will **increase** coverage by `0.13%`.
    > The diff coverage is `n/a`.
    
    [![Impacted file tree graph](https://codecov.io/gh/apache/incubator-livy/pull/37/graphs/tree.svg?height=150&width=650&token=0MkVbiUFwE&src=pr)](https://codecov.io/gh/apache/incubator-livy/pull/37?src=pr&el=tree)
    
    ```diff
    @@             Coverage Diff              @@
    ##             master      #37      +/-   ##
    ============================================
    + Coverage     70.48%   70.61%   +0.13%     
      Complexity      777      777              
    ============================================
      Files            97       97              
      Lines          5370     5370              
      Branches        806      806              
    ============================================
    + Hits           3785     3792       +7     
    + Misses         1050     1041       -9     
    - Partials        535      537       +2
    ```
    
    
    | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/37?src=pr&el=tree) | Coverage Î” | Complexity Î” | |
    |---|---|---|---|
    | [...java/org/apache/livy/rsc/rpc/KryoMessageCodec.java](https://codecov.io/gh/apache/incubator-livy/pull/37?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9ycGMvS3J5b01lc3NhZ2VDb2RlYy5qYXZh) | `94.64% <0%> (-3.58%)` | `18% <0%> (-1%)` | |
    | [...main/java/org/apache/livy/rsc/ContextLauncher.java](https://codecov.io/gh/apache/incubator-livy/pull/37?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9Db250ZXh0TGF1bmNoZXIuamF2YQ==) | `82.92% <0%> (+2.43%)` | `18% <0%> (Ã¸)` | :arrow_down: |
    | [...in/java/org/apache/livy/rsc/rpc/RpcDispatcher.java](https://codecov.io/gh/apache/incubator-livy/pull/37?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9ycGMvUnBjRGlzcGF0Y2hlci5qYXZh) | `68% <0%> (+4%)` | `20% <0%> (+1%)` | :arrow_up: |
    
    ------
    
    [Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-livy/pull/37?src=pr&el=continue).
    > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
    > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
    > Powered by [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/37?src=pr&el=footer). Last update [ca5b1be...bb6e15c](https://codecov.io/gh/apache/incubator-livy/pull/37?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

;;;","24/Aug/17 06:34;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/37
  
    Looks good to me, merge to master and branch 0.4.
;;;","24/Aug/17 06:44;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-livy/pull/37
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check the license of all the files,LIVY-391,13096029,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,ajbozarth,jerryshao,gmcdonald,03/Aug/17 02:57,04/Aug/17 00:55,19/Dec/25 04:15,03/Aug/17 23:56,0.4.0,,,,,Docs,,,,,,,,,,0,,,,,,"As a part of release work, license/NOTICE should be checked if there's no license issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 04 00:55:12 UTC 2017,,,,,,,,,,"0|i3j0v3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Aug/17 21:47;ajbozarth;Is there a reason you opened this then immediately closed it as a duplicate? I think going through our code and checking the licenses for external libraries should be tracked here. I've already gone through the license issues in the Web UI and Docs in LIVY-384 but the rest of the project should be checked. I'll start taking a look at this in my free moments today.;;;","03/Aug/17 23:32;ajbozarth;I'm currently running http://maven.apache.org/plugins/maven-project-info-reports-plugin/ on the project to check for licenses and will follow up with a manual look through the code;;;","03/Aug/17 23:56;ajbozarth;Based on the report and my manual look through of the code I believe this will be covered by https://github.com/apache/incubator-livy/pull/26;;;","04/Aug/17 00:49;jerryshao;I was thinking to make this as a subtask of LIVY-389, then I changed the mind to not create subtasks in LIVY-389, instead tracking everything directly in LIVY-389, what do you think? If you think it is worthwhile to create subtasks then you can reopen it and add some more subtasks under LIVY-389.;;;","04/Aug/17 00:55;ajbozarth;I agree, any PR related to LIVY-389 will be MINOR by definition, in fact the pr I opened used [LIVY-389][MINOR] for that reasoning.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update travis to support JDK8 when building Livy against Spark 2.2,LIVY-390,13096028,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,03/Aug/17 02:30,03/Aug/17 03:24,19/Dec/25 04:15,03/Aug/17 03:24,0.4.0,,,0.4.0,,Tests,,,,,,,,,,0,,,,,,"Because Spark 2.2 removes JDK7 support but Livy still supports JDK7, so updating travis file to use JDK8 to build against Spark 2.2, whereas still using JDK7 to build with Spark 2.2-.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-08-03 02:30:26.0,,,,,,,,,,"0|i3j0uv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
First Apache Release,LIVY-389,13096027,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,02/Aug/17 03:27,04/Sep/17 01:16,19/Dec/25 04:15,04/Sep/17 01:16,0.4.0,,,0.4.0,,API,Batch,Core,Docs,Interpreter,REPL,RSC,Server,Tests,,0,,,,,,This JIRA is to track the works must done to prepare first release. The release version will be targeted to 0.4.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Aug 04 03:15:00 UTC 2017,,,,,,,,,,"0|i3j0un:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Aug/17 03:12;jerryshao;Add some tasks here:

* Check the license for all the files in incubator-livy and repo.
* Create the branch 0.4 and test on branch 0.4 with different Spark versions.;;;","03/Aug/17 03:14;jerryshao;[~ajbozarth] would you please add more items you could think out, thanks!;;;","03/Aug/17 21:44;ajbozarth;A few points for reference:
* I've addressed the license issues for the Web UI and Docs in the currently open Docs PR, but I have not looked into license issues elsewhere in the code.
* Once the release is passed and the distributions are ready we will need to update the website accordingly, I have already started preparing for this in a local branch on my computer.
* Separately I have an open PR to update the Docs on the website once the release has passed.
* The release version should be 0.4.0-incubating (not 0.4.0) for the tag name, documentation, and distributions. 0.4.0 is fine for reference in places like the jira and other discussion.
* From what I can tell our release features for 0.4 are only the Web UI and Improved ACLs.
* I believe once LIVY-384 is merged we should cut an rc1 for a vote unless there are other open PRs we think should be included.;;;","04/Aug/17 00:21;ajbozarth;There are still references to Cloudera repos in the root pom.xml, I'll open a pr to address pom updates;;;","04/Aug/17 00:36;ajbozarth;opened https://github.com/apache/incubator-livy/pull/31

I'm not sure if it is necessary for 0.4 or if we should let it go for now and include it in a larger update in the next release;;;","04/Aug/17 01:55;jerryshao;What kind of larger update are you referring to?;;;","04/Aug/17 03:15;ajbozarth;By larger update I meant taking a look at cleaning up and updating our all pom files to match any Apache standards.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy should expose server.connect.timeout in Rest Api,LIVY-388,13096026,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,,prabhujoseph,gmcdonald,01/Aug/17 09:39,03/Aug/17 02:26,19/Dec/25 04:15,03/Aug/17 02:26,0.3,,,,,Core,,,,,,,,,,0,,,,,,"Consider a Node which has NN1 (NameNode) and HiveMetaStore is down but we have HA for both services. Running livy script will create a new session and will wait for ipc.client.connect.timeout (20s) for each jar upload into hdfs 

{code}
17/07/31 13:59:29 INFO ContextLauncher: 17/07/31 13:59:39 INFO Client: Source and destination file systems are the same. Not copying hdfs://prabhu/hdp/apps/2.6.1.0-129/spark/spark-hdp-assembly.jar
17/07/31 13:59:49 INFO ContextLauncher: 17/07/31 13:59:49 INFO Client: Uploading resource file:/usr/hdp/current/livy-server/rsc-jars/livy-rsc-0.3.0.2.6.1.0-129.jar -> hdfs://prabhu/user/diasmi/.sparkStaging/application_1501501991083_0001/livy-rsc-0.3.0.2.6.1.0-129.jar
{code}

and 5 seconds (hive.metastore.client.socket.timeout)

{code}
17/07/26 09:09:46 INFO ContextLauncher: 17/07/26 09:09:46 INFO metastore: Trying to connect to metastore with URI thrift://prabhu01:9083
17/07/26 09:09:51 INFO ContextLauncher: 17/07/26 09:09:51 WARN metastore: Failed to connect to the MetaStore Server...
17/07/26 09:09:51 INFO ContextLauncher: 17/07/26 09:09:51 INFO metastore: Trying to connect to metastore with URI thrift://prabhu02:9083
17/07/26 09:09:51 INFO ContextLauncher: 17/07/26 09:09:51 INFO metastore: Connected to metastore.
{code}

and finally will fail with timeout with Livy Server Connect Timeout. 90 Seconds is too low for this case. RPC_CLIENT_HANDSHAKE_TIMEOUT(""server.connect.timeout"", ""90s""). This should be exposed Via Rest API for other components like Zeppelin to Override it.

{code}
17/07/31 14:00:51 ERROR RSCClient: Failed to connect to context.
java.util.concurrent.TimeoutException: Timed out waiting for context to start.
        at com.cloudera.livy.rsc.ContextLauncher.connectTimeout(ContextLauncher.java:133)
        at com.cloudera.livy.rsc.ContextLauncher.access$200(ContextLauncher.java:62)
        at com.cloudera.livy.rsc.ContextLauncher$2.run(ContextLauncher.java:121)
        at io.netty.util.concurrent.PromiseTask$RunnableAdapter.call(PromiseTask.java:38)
        at io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:120)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 03 02:25:53 UTC 2017,,,,,,,,,,"0|i3j0uf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Aug/17 02:25;jerryshao;I think Livy already expose this configuration as an RSC configuration, you can set ""livy.rsc.server.connect.timeout"" in session creation's json protocol {{conf}} field, it depends on how Zeppelin leverage this. So it should not be a problem of Livy.

If you cannot change this configuration, then the problem should be in Zeppelin's LivyInterpreter.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LIVY-344 Followup: Update Log page to split apart different logs,LIVY-387,13096025,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,gmcdonald,28/Jul/17 20:50,18/Aug/17 18:18,19/Dec/25 04:15,02/Aug/17 08:49,0.4.0,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"The first version of the Log Page introduced in LIVY-344 displays the entire log as one block. Usually (but not always) the log consists of a stdout, stderr, and YARN Diagnostics sections. We should display these separately.

Some initial options:
1. Split the three into separate <pre> tags and have anchor links to each at the top of the page
2. Split them up into separate ""tabs""
3. Either 1 or 2, but with resizable displays and scrolling.

My current plan would be 1 & 3, given the potential difficulty of 2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-87,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 01 22:02:34 UTC 2017,,,,,,,,,,"0|i3j0u7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"01/Aug/17 03:47;ajbozarth;During review of LIVY-344 I found out that LIVY-359 was recently merged. With this JIRA we should also add some sort of tooltip that mentions the logs are trimmed by that config;;;","01/Aug/17 22:02;ajbozarth;https://github.com/apache/incubator-livy/pull/27;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ability to build the Docs page and include the latest build in the website,LIVY-384,13096022,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,gmcdonald,26/Jul/17 19:37,04/Aug/17 06:21,19/Dec/25 04:15,04/Aug/17 06:21,0.4.0,,,0.4.0,,Docs,,,,,,,,,,0,,,,,,In LIVY-376 we moved the Docs and Examples out of the README into their own files. The Examples were moved to the website and the Docs were placed in the docs directory. We need to create the Jekyll setup necessary to build the Docs and add a way to include them in the website.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 28 23:18:47 UTC 2017,,,,,,,,,,"0|i3j0tj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Jul/17 23:18;ajbozarth;opened prs: 
https://github.com/apache/incubator-livy/pull/26
https://github.com/apache/incubator-livy-website/pull/7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Spark 2.2,LIVY-381,13096019,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,WeiqingYang,WeiqingYang,gmcdonald,20/Jul/17 23:50,01/Sep/17 01:42,19/Dec/25 04:15,26/Jul/17 06:27,0.3,0.4.0,,0.4.0,,Core,,,,,,,,,,0,,,,,,Spark 2.2 has been released. It's better for us to update the max version supported.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-07-20 23:50:23.0,,,,,,,,,,"0|i3j0sv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy should overwrite spark.sql.catalogImplementation if Hive classes are not present on classpath,LIVY-380,13096018,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mshen,mshen,gmcdonald,17/Jul/17 19:09,01/Sep/17 01:41,19/Dec/25 04:15,03/Aug/17 00:49,0.3,,,0.4.0,,REPL,,,,,,,,,,0,,,,,,"In {{SparkContextInitializer.spark2CreateContext}}, livy checks for whether catalog implementation is ""hive"" or ""in-memory"" to properly initialize the SparkSession.
If the config is set to ""hive"" (this is the case if {{livy.repl.enable-hive-context}} is set to ""true""), livy will further check if hive classes are present or not.
It only invokes the {{enableHiveSupport}} method of {{SparkSession.Builder}} if hive classes are found.
If not, it proceeds to initializing a SparkSession without Hive support enabled.
However, in Spark's code base, when creating SparkSession, it checks for {{spark.sql.catalogImplementation}} to determine which session state to initialize.
If {{spark.sql.catalogImplementation}} is set to ""hive"", it will initialize a {{HiveSessionState}}.
However, since Livy has already checked that hive classes are not present, it should not go ahead invoking {{SparkSession.builder.getOrCreate}} without overwriting {{spark.sql.catalogImplementation}} to ""in-memory"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 03 00:50:17 UTC 2017,,,,,,,,,,"0|i3j0sn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Jul/17 18:23;jerryshao;Hi Min, thanks for reporting, would you please submit a PR if you think it is a bug need to fix, thanks!;;;","19/Jul/17 01:34;mshen;Thanks for looking at this ticket.
I'm working on one patch to fix this issue, will submit a PR later this week.;;;","25/Jul/17 22:21;mshen;Patch submitted for this issue: https://github.com/apache/incubator-livy/pull/24;;;","03/Aug/17 00:50;jerryshao;Due to the permission limit of this JIRA, assignee cannot be specified.

The right assignee should be: Min Shen.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move wiki to website,LIVY-378,13096016,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,gmcdonald,30/Jun/17 22:11,31/Jul/17 23:32,19/Dec/25 04:15,20/Jul/17 02:10,0.4.0,,,0.4.0,,Docs,,,,,,,,,,0,,,,,,The Apache repo doesn't allow GitHub wiki pages so we need to move the wiki content to a page on the website. This should probably happen after the move in LIVY-377,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 20 02:10:12 UTC 2017,,,,,,,,,,"0|i3j0s7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Jul/17 02:10;ajbozarth;Completed as part of the initial website commit https://github.com/apache/incubator-livy-website;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move Website from gh-pages branch to apache/incubator-livy-website,LIVY-377,13096015,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,gmcdonald,30/Jun/17 22:09,31/Jul/17 23:32,19/Dec/25 04:15,20/Jul/17 02:08,0.4.0,,,0.4.0,,Docs,,,,,,,,,,0,,,,,,The website (livy.io) currently resides in the branch gh-pages in cloudera/livy. This should be moved to the new project apache/incubator-livy-website and at the same time we should remove non-website code. Currently the branch contains an old branch on the repo along with the website code.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 20 02:55:55 UTC 2017,,,,,,,,,,"0|i3j0rz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Jul/17 05:32;jerryshao;I'm wondering if we can put docs in Livy code base, whereas putting website frameworks and others to incubator-livy-website repo, like what Spark did.

What do you think?;;;","05/Jul/17 17:51;ajbozarth;I would agree with that, but what docs are there? The README contains all the Docs I know of, unless you're referring to the wiki. The website is really barebones on it's own;;;","06/Jul/17 01:02;jerryshao;I'm considering simplify README file, and move the specific usages to separate docs, also adding more to help users to transit from vanilla Spark to Livy, current README file is too large, I would keep it simple.;;;","08/Jul/17 22:46;ajbozarth;Ok, I agree the current README is too large, splitting it up into separate docs and linking to them from the webpage sounds like a good idea;;;","20/Jul/17 02:08;ajbozarth;website successfully moved to https://github.com/apache/incubator-livy-website will follow up on updates in LIVY-376;;;","20/Jul/17 02:50;jerryshao;Hi Alex, I checked the website https://livy.incubator.apache.org/, it turns out to be 404, can you please check again.

Also in the readme of https://github.com/apache/incubator-livy-website, url (https://livy.apache.org/) should be changed to https://livy.incubator.apache.org/;;;","20/Jul/17 02:55;ajbozarth;I'll update the REAME in my next PR, but both links will work. I'm not sure why it's not working yet, I'll email Luciano on the dev list and see if we missed something or if we just have to wait.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Readme and Docs with new naming and links,LIVY-376,13096014,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,gmcdonald,30/Jun/17 21:47,31/Jul/17 23:30,19/Dec/25 04:15,31/Jul/17 23:30,0.4.0,,,0.4.0,,Docs,,,,,,,,,,0,,,,,,The README and any other docs should be updated with any new links or names (e.g. Apache Livy or Travis CI build link),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 25 00:20:52 UTC 2017,,,,,,,,,,"0|i3j0rr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/Jun/17 22:06;ajbozarth;Note: this should probably happen after we move over things like the wiki and website since we need those new links;;;","20/Jul/17 02:09;ajbozarth;I will also move Docs content out of the README into the website as part of this cleanup;;;","24/Jul/17 06:46;jerryshao;Hi Alex, I think we could start working on this issue, at least we could update some links to point to new url.;;;","24/Jul/17 06:55;ajbozarth;My plan was to start this in the morning, should have a pr open by EOD tomorrow;;;","25/Jul/17 00:20;ajbozarth;Opened PRs on the main and website repos:
https://github.com/apache/incubator-livy/pull/21
https://github.com/apache/incubator-livy-website/pull/5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change Livy code package name to apache,LIVY-375,13096013,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,30/Jun/17 08:44,05/Jul/17 04:22,19/Dec/25 04:15,05/Jul/17 04:22,0.4.0,,,0.4.0,,API,Batch,Core,Interpreter,REPL,RSC,Server,Tests,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-06-30 08:44:10.0,,,,,,,,,,"0|i3j0rj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change and unify Livy code license header to Apache,LIVY-374,13096012,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,30/Jun/17 08:43,04/Jul/17 09:30,19/Dec/25 04:15,04/Jul/17 09:30,0.4.0,,,0.4.0,,API,Batch,Core,Docs,Interpreter,REPL,RSC,Server,Tests,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-06-30 08:43:03.0,,,,,,,,,,"0|i3j0rb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy to move to Apache,LIVY-373,13096011,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,30/Jun/17 08:37,01/Aug/17 03:34,19/Dec/25 04:15,01/Aug/17 03:34,0.4.0,,,0.4.0,,API,Batch,Core,Docs,Interpreter,REPL,RSC,Server,,,0,,,,,,Livy code should be changed to meet Apache criteria.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 01 03:34:38 UTC 2017,,,,,,,,,,"0|i3j0r3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/Jun/17 21:44;ajbozarth;I'm willing to help on these JIRAs but I'll be busy/on vacation most of next week. I'm willing to pick up anything that's still pending work when I get back;;;","03/Jul/17 01:12;jerryshao;Thanks, greatly appreciated!;;;","01/Aug/17 03:34;ajbozarth;Closing this since all the subtasks are complete. Assigning it to Jerry since the subtask he completed we larger than those I did.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
livy-python-api build fail,LIVY-370,13096008,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Problem,,anirudh985,gmcdonald,15/Jun/17 22:54,23/Jan/18 23:26,19/Dec/25 04:15,23/Jan/18 23:26,0.4.0,,,,,Tests,,,,,,,,,,0,broken-build,,,,,"Building the master branch by invoking ""mvn package"" results in a BUILD FAILURE for livy-python-api
Below is the detailed error:
============================= test session starts ==============================
platform darwin -- Python 2.7.10, pytest-3.1.2, py-1.4.34, pluggy-0.4.0 -- /usr/bin/python
cachedir: .cache
rootdir: /Users/${USER}/dev/workspace/livy/python-api, inifile: setup.cfg
collecting ... collected 12 items

src/test/python/livy-tests/client_test.py::test_create_new_session_without_default_config FAILED
src/test/python/livy-tests/client_test.py::test_create_new_session_with_default_config FAILED
src/test/python/livy-tests/client_test.py::test_connect_to_existing_session FAILED
src/test/python/livy-tests/client_test.py::test_add_jar <- <string> FAILED
src/test/python/livy-tests/client_test.py::test_submit_job_verify_running_state <- <string> FAILED
src/test/python/livy-tests/client_test.py::test_submit_job_verify_failed_state <- <string> FAILED
src/test/python/livy-tests/client_test.py::test_add_pyfile <- <string> FAILED
src/test/python/livy-tests/client_test.py::test_submit_job_verify_queued_state <- <string> FAILED
src/test/python/livy-tests/client_test.py::test_add_file <- <string> FAILED
src/test/python/livy-tests/client_test.py::test_upload_file <- <string> FAILED
src/test/python/livy-tests/client_test.py::test_upload_pyfile <- <string> FAILED
src/test/python/livy-tests/client_test.py::test_submit_job_verify_succeeded_state <- <string> FAILED

=================================== FAILURES ===================================
________________ test_create_new_session_without_default_config ________________

    def test_create_new_session_without_default_config():
>       mock_and_validate_create_new_session(False)

src/test/python/livy-tests/client_test.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<string>:3: in wrapper
    ???
src/test/python/livy-tests/client_test.py:47: in mock_and_validate_create_new_session
    load_defaults=defaults)
src/main/python/livy/client.py:88: in __init__
    session_conf_dict).json()['id']
src/main/python/livy/client.py:388: in _create_new_session
    headers=self._conn._JSON_HEADERS, data=data)
src/main/python/livy/client.py:499: in send_request
    json=data, auth=self._spnego_auth())
.eggs/requests-2.18.1-py2.7.egg/requests/api.py:58: in request
    return session.request(method=method, url=url, **kwargs)
.eggs/requests-2.18.1-py2.7.egg/requests/sessions.py:502: in request
    resp = self.send(prep, **send_kwargs)
.eggs/requests-2.18.1-py2.7.egg/requests/sessions.py:612: in send
    r = adapter.send(request, **kwargs)
.eggs/responses-0.5.1-py2.7.egg/responses.py:294: in unbound_on_send
    return self._on_request(adapter, request, *a, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <responses.RequestsMock object at 0x102c75790>
adapter = <requests.adapters.HTTPAdapter object at 0x102ea5bd0>
request = <PreparedRequest [POST]>
kwargs = {'cert': None, 'proxies': OrderedDict(), 'stream': False, 'timeout': 10, ...}
match = None
error_msg = 'Connection refused: POST http://c02jw0tgdkq2.local:8998/sessions/'
response = ConnectionError(u'Connection refused: POST http://c02jw0tgdkq2.local:8998/sessions/',)

    def _on_request(self, adapter, request, **kwargs):
        match = self._find_match(request)
        # TODO(dcramer): find the correct class for this
        if match is None:
            error_msg = 'Connection refused: {0} {1}'.format(request.method,
                                                             request.url)
            response = ConnectionError(error_msg)
            response.request = request
    
            self._calls.add(request, response)
>           raise response
E           ConnectionError: Connection refused: POST http://c02jw0tgdkq2.local:8998/sessions/

.eggs/responses-0.5.1-py2.7.egg/responses.py:239: ConnectionError
_________________ test_create_new_session_with_default_config __________________

    def test_create_new_session_with_default_config():
        os.environ[""LIVY_CLIENT_CONF_DIR""] = \
            os.path.dirname(os.path.abspath(__file__)) + ""/resources""
>       mock_and_validate_create_new_session(True)

src/test/python/livy-tests/client_test.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
<string>:3: in wrapper
    ???
src/test/python/livy-tests/client_test.py:47: in mock_and_validate_create_new_session
    load_defaults=defaults)
src/main/python/livy/client.py:88: in __init__
    session_conf_dict).json()['id']
src/main/python/livy/client.py:388: in _create_new_session
    headers=self._conn._JSON_HEADERS, data=data)
src/main/python/livy/client.py:499: in send_request
    json=data, auth=self._spnego_auth())
.eggs/requests-2.18.1-py2.7.egg/requests/api.py:58: in request
    return session.request(method=method, url=url, **kwargs)
.eggs/requests-2.18.1-py2.7.egg/requests/sessions.py:502: in request
    resp = self.send(prep, **send_kwargs)
.eggs/requests-2.18.1-py2.7.egg/requests/sessions.py:612: in send
    r = adapter.send(request, **kwargs)
.eggs/responses-0.5.1-py2.7.egg/responses.py:294: in unbound_on_send
    return self._on_request(adapter, request, *a, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <responses.RequestsMock object at 0x102c75790>
adapter = <requests.adapters.HTTPAdapter object at 0x102fa46d0>
request = <PreparedRequest [POST]>
kwargs = {'cert': None, 'proxies': OrderedDict(), 'stream': False, 'timeout': 10, ...}
match = None
error_msg = 'Connection refused: POST http://c02jw0tgdkq2.local:8998/sessions/'
response = ConnectionError(u'Connection refused: POST http://c02jw0tgdkq2.local:8998/sessions/',)

    def _on_request(self, adapter, request, **kwargs):
        match = self._find_match(request)
        # TODO(dcramer): find the correct class for this
        if match is None:
            error_msg = 'Connection refused: {0} {1}'.format(request.method,
                                                             request.url)
            response = ConnectionError(error_msg)
            response.request = request
    
            self._calls.add(request, response)
>           raise response
E           ConnectionError: Connection refused: POST http://c02jw0tgdkq2.local:8998/sessions/

.eggs/responses-0.5.1-py2.7.egg/responses.py:239: ConnectionError
_______________________ test_connect_to_existing_session _______________________

    def test_connect_to_existing_session():
        reconnect_mock_request_uri = base_uri + ""/sessions/"" + str(session_id) + \
            ""/connect""
        reconnect_session_uri = base_uri + ""/sessions/"" + str(session_id)
        json_data = {
            u'kind': u'pyspark', u'log': [], u'proxyUser': None,
            u'state': u'starting', u'owner': None, u'id': session_id
        }
        with responses.RequestsMock() as rsps:
            rsps.add(responses.POST, reconnect_mock_request_uri, json=json_data,
                status=201, content_type='application/json')
    
            client_reconnect = HttpClient(reconnect_session_uri,
>               load_defaults=False)

src/test/python/livy-tests/client_test.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.eggs/responses-0.5.1-py2.7.egg/responses.py:181: in __exit__
    self.stop()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <responses.RequestsMock object at 0x102f4a110>

    def stop(self):
        self._patcher.stop()
        if self.assert_all_requests_are_fired and self._urls:
            raise AssertionError(
                'Not all requests have been executed {0!r}'.format(
>                   [(url['method'], url['url']) for url in self._urls]))
E           AssertionError: Not all requests have been executed [(u'POST', 'http://C02JW0TGDKQ2.local:8998/sessions/0/connect')]

.eggs/responses-0.5.1-py2.7.egg/responses.py:304: AssertionError
_________________________________ test_add_jar _________________________________

>   ???

<string>:3: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @responses.activate
    def test_add_jar():
        file_uri = ""file://"" + os.path.dirname(os.path.abspath(__file__)) + \
            ""/resources/jar_file.jar""
>       add_file_future = mock_file_apis('add-jar', client_test.add_jar, file_uri)
E       AttributeError: 'NoneType' object has no attribute 'add_jar'

src/test/python/livy-tests/client_test.py:225: AttributeError
_____________________ test_submit_job_verify_running_state _____________________

>   ???

<string>:3: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/test/python/livy-tests/client_test.py:137: in test_submit_job_verify_running_state
    u'STARTED')
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

job = <function simple_spark_job at 0x102dbda28>, job_state = 'STARTED'
result = None, error = None

    def mock_submit_job_and_poll_result(
        job,
        job_state,
        result=None,
        error=None
    ):
        submit_request_mock_uri = base_uri + ""/sessions/"" + str(session_id) \
            + ""/submit-job""
        poll_request_mock_uri = base_uri + ""/sessions/"" + str(session_id) \
            + ""/jobs/"" + str(job_id)
    
        post_json_data = {
            u'state': u'SENT', u'error': None, u'id': job_id, u'result': None
        }
        responses.add(responses.POST, submit_request_mock_uri, status=201,
            json=post_json_data, content_type='application/json')
    
        get_json_data = {
            u'state': job_state, u'error': error, u'id': job_id, u'result': result
        }
        responses.add(responses.GET, poll_request_mock_uri, status=200,
            json=get_json_data, content_type='application/json')
    
>       submit_job_future = client_test.submit(job)
E       AttributeError: 'NoneType' object has no attribute 'submit'

src/test/python/livy-tests/client_test.py:80: AttributeError
_____________________ test_submit_job_verify_failed_state ______________________

>   ???

<string>:3: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/test/python/livy-tests/client_test.py:176: in test_submit_job_verify_failed_state
    error='Error job')
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

job = <function failure_job at 0x102dc00c8>, job_state = 'FAILED', result = None
error = 'Error job'

    def mock_submit_job_and_poll_result(
        job,
        job_state,
        result=None,
        error=None
    ):
        submit_request_mock_uri = base_uri + ""/sessions/"" + str(session_id) \
            + ""/submit-job""
        poll_request_mock_uri = base_uri + ""/sessions/"" + str(session_id) \
            + ""/jobs/"" + str(job_id)
    
        post_json_data = {
            u'state': u'SENT', u'error': None, u'id': job_id, u'result': None
        }
        responses.add(responses.POST, submit_request_mock_uri, status=201,
            json=post_json_data, content_type='application/json')
    
        get_json_data = {
            u'state': job_state, u'error': error, u'id': job_id, u'result': result
        }
        responses.add(responses.GET, poll_request_mock_uri, status=200,
            json=get_json_data, content_type='application/json')
    
>       submit_job_future = client_test.submit(job)
E       AttributeError: 'NoneType' object has no attribute 'submit'

src/test/python/livy-tests/client_test.py:80: AttributeError
_______________________________ test_add_pyfile ________________________________

>   ???

<string>:3: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @responses.activate
    def test_add_pyfile():
        file_uri = ""file://"" + os.path.dirname(os.path.abspath(__file__)) + \
            ""/resources/zip_file.zip""
>       add_file_future = mock_file_apis('add-pyfile', client_test.add_pyfile,
             file_uri)
E       AttributeError: 'NoneType' object has no attribute 'add_pyfile'

src/test/python/livy-tests/client_test.py:205: AttributeError
_____________________ test_submit_job_verify_queued_state ______________________

>   ???

<string>:3: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/test/python/livy-tests/client_test.py:152: in test_submit_job_verify_queued_state
    u'QUEUED')
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

job = <function simple_spark_job at 0x102dbda28>, job_state = 'QUEUED'
result = None, error = None

    def mock_submit_job_and_poll_result(
        job,
        job_state,
        result=None,
        error=None
    ):
        submit_request_mock_uri = base_uri + ""/sessions/"" + str(session_id) \
            + ""/submit-job""
        poll_request_mock_uri = base_uri + ""/sessions/"" + str(session_id) \
            + ""/jobs/"" + str(job_id)
    
        post_json_data = {
            u'state': u'SENT', u'error': None, u'id': job_id, u'result': None
        }
        responses.add(responses.POST, submit_request_mock_uri, status=201,
            json=post_json_data, content_type='application/json')
    
        get_json_data = {
            u'state': job_state, u'error': error, u'id': job_id, u'result': result
        }
        responses.add(responses.GET, poll_request_mock_uri, status=200,
            json=get_json_data, content_type='application/json')
    
>       submit_job_future = client_test.submit(job)
E       AttributeError: 'NoneType' object has no attribute 'submit'

src/test/python/livy-tests/client_test.py:80: AttributeError
________________________________ test_add_file _________________________________

>   ???

<string>:3: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @responses.activate
    def test_add_file():
        file_uri = ""file://"" + os.path.dirname(os.path.abspath(__file__)) + \
            ""/resources/text_file.txt""
>       add_file_future = mock_file_apis('add-file', client_test.add_file,
             file_uri)
E       AttributeError: 'NoneType' object has no attribute 'add_file'

src/test/python/livy-tests/client_test.py:185: AttributeError
_______________________________ test_upload_file _______________________________

>   ???

<string>:3: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @responses.activate
    def test_upload_file():
        file_path = os.path.dirname(os.path.abspath(__file__)) + \
            ""/resources/text_file.txt""
>       upload_file_future = mock_file_apis('upload-file', client_test.upload_file,
            file_path)
E       AttributeError: 'NoneType' object has no attribute 'upload_file'

src/test/python/livy-tests/client_test.py:195: AttributeError
______________________________ test_upload_pyfile ______________________________

>   ???

<string>:3: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @responses.activate
    def test_upload_pyfile():
        file_path = os.path.dirname(os.path.abspath(__file__)) + \
            ""/resources/zip_file.zip""
>       pyfile_future = mock_file_apis('upload-pyfile', client_test.upload_pyfile,
            file_path)
E       AttributeError: 'NoneType' object has no attribute 'upload_pyfile'

src/test/python/livy-tests/client_test.py:215: AttributeError
____________________ test_submit_job_verify_succeeded_state ____________________

>   ???

<string>:3: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/test/python/livy-tests/client_test.py:168: in test_submit_job_verify_succeeded_state
    result='Z0FKVkZGc3hNREFzSURJd01Dd2dNekF3TENBME1EQmRjUUF1')
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

job = <function simple_spark_job at 0x102dbda28>, job_state = 'SUCCEEDED'
result = 'Z0FKVkZGc3hNREFzSURJd01Dd2dNekF3TENBME1EQmRjUUF1', error = None

    def mock_submit_job_and_poll_result(
        job,
        job_state,
        result=None,
        error=None
    ):
        submit_request_mock_uri = base_uri + ""/sessions/"" + str(session_id) \
            + ""/submit-job""
        poll_request_mock_uri = base_uri + ""/sessions/"" + str(session_id) \
            + ""/jobs/"" + str(job_id)
    
        post_json_data = {
            u'state': u'SENT', u'error': None, u'id': job_id, u'result': None
        }
        responses.add(responses.POST, submit_request_mock_uri, status=201,
            json=post_json_data, content_type='application/json')
    
        get_json_data = {
            u'state': job_state, u'error': error, u'id': job_id, u'result': result
        }
        responses.add(responses.GET, poll_request_mock_uri, status=200,
            json=get_json_data, content_type='application/json')
    
>       submit_job_future = client_test.submit(job)
E       AttributeError: 'NoneType' object has no attribute 'submit'

src/test/python/livy-tests/client_test.py:80: AttributeError
========================== 12 failed in 0.76 seconds ===========================
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 4.908 s
[INFO] Finished at: 2017-06-15T13:59:36-07:00
[INFO] Final Memory: 24M/299M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:exec (python-api test) on project livy-python-api: Command execution failed.: Process exited with an error: 1 (Exit value: 1) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 23 23:26:40 UTC 2018,,,,,,,,,,"0|i3j0qf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/Jan/18 23:26;ajbozarth;I came across this error while doing tests for 0.5.0 rc1 and figured out the issue. This is caused by an issue with your computer's DNS. There's a line in client_test.py that gets your machine's hostname for connecting to the livy server during tests. This error is thrown when the DNS can't resolve your hostname. In my case I booted my laptop while at home, setting my hostname to something.local, then when I gotÂ into the office my work's DNS couldn't resolve that local hostname because while on their network I am given a new hostname (my-machine.company.com).Â 

Therefore myÂ ""solution"" to this issue was to reboot my laptop, though updating your hostname without rebooting should also work.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Authentication to UI,LIVY-366,13096004,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Problem,ajbozarth,ajbozarth,gmcdonald,23/May/17 19:01,16/Jan/18 20:15,19/Dec/25 04:15,16/Jan/18 20:15,0.4.0,,,,,Server,,,,,,,,,,0,,,,,,"Once LIVY-348 update the way the REST API authentication works, add the ability for users to authenticate in the UI to access restricted API calls, such as statements, logs, or the cancel/kill functionality.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-87,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 16 20:15:27 UTC 2018,,,,,,,,,,"0|i3j0pj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/Jan/18 20:15;ajbozarth;Closing this since as of 0.4 all forms of supported authentication work without a authentication portal on the UI. If this changes in the future a Jira can be opened to address it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include executed statement in /sessions/:id/statements return,LIVY-365,13096003,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,gmcdonald,23/May/17 18:57,03/Jul/17 06:32,19/Dec/25 04:15,03/Jul/17 06:32,0.3,,,0.4.0,,RSC,Server,,,,,,,,,0,,,,,,Based on this user list chain (https://groups.google.com/a/cloudera.org/forum/#!topic/livy-user/UNEUp1RAiSE) there is interest in having the original executed statement returned with its output. With the addition of the Web UI this is a good potential addition to the REST API and UI. From a quick look at plumbing this seems a non-trivial task since the original statement is not currently saved.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 24 20:44:56 UTC 2017,,,,,,,,,,"0|i3j0pb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/May/17 18:58;ajbozarth;If this is something we are interested in adding I'm willing to take this myself.;;;","24/May/17 20:44;ajbozarth;opened a pr since it was actually easier to implement than expected: https://github.com/cloudera/livy/pull/339;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
livy-server writes one log file when set LIVY_MAX_LOG_FILES to 0,LIVY-364,13096002,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,davidxu,davidxu,gmcdonald,23/May/17 03:16,01/Sep/17 01:42,19/Dec/25 04:15,12/Jul/17 16:42,0.3,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"When set LIVY_MAX_LOG_FILES to 0, livy-server renames old log file to livy-root-server.out.0  while startup. So there has one log file which is Inconsistent with LIVY_MAX_LOG_FILES value 0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 14 00:58:49 UTC 2017,,,,,,,,,,"0|i3j0p3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Jul/17 00:58;davidxu; pr available here: [https://github.com/cloudera/livy/pull/337|https://github.com/cloudera/livy/pull/337];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Method toURL in Utils.scala is deprecated,LIVY-363,13096001,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,davidxu,davidxu,gmcdonald,23/May/17 02:47,01/Sep/17 01:49,19/Dec/25 04:15,03/Jul/17 07:00,0.3,,,0.4.0,,Core,,,,,,,,,,0,,,,,,Method toURL in Utils.scala is deprecated. The toURL() method of File does not properly escape characters that aren't valid in a URL.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 03 07:00:47 UTC 2017,,,,,,,,,,"0|i3j0ov:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Jun/17 18:49;ajbozarth;Note: pr available here: https://github.com/cloudera/livy/pull/338;;;","03/Jul/17 07:00;jerryshao;Please assign the JIRA to yourself.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If LIVY_PID_DIR contains space, stop livy-server failed.",LIVY-361,13095999,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,davidxu,davidxu,gmcdonald,19/May/17 09:05,01/Sep/17 01:49,19/Dec/25 04:15,03/Jul/17 06:52,0.3,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"There is an error in cheking pid file existence while stop livy-server. If LIVY_PID_DIR contains space, It is unable to get the pid file within it, so stop livy-server failed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 03 06:52:49 UTC 2017,,,,,,,,,,"0|i3j0of:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/May/17 01:46;davidxu;The issue has been solved on PR [https://github.com/cloudera/livy/pull/336|https://github.com/cloudera/livy/pull/336]. 
Please review it. Thanks a lot.;;;","03/Jul/17 06:52;jerryshao;Please assign the JIRA to yourself, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The usage of livy-server description is not correct,LIVY-360,13095998,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,davidxu,davidxu,gmcdonald,19/May/17 07:24,01/Sep/17 01:50,19/Dec/25 04:15,03/Jul/17 06:45,0.3,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"The description of the usage of livy-server contains the start and stop parameters, but missing status parameter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 03 06:46:36 UTC 2017,,,,,,,,,,"0|i3j0o7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"19/May/17 07:52;davidxu;The issue has been solved on PR [https://github.com/cloudera/livy/pull/335|https://github.com/cloudera/livy/pull/335]. 
Please review it. Thanks a lot.;;;","03/Jul/17 06:46;jerryshao;[~davidxu] Can you please assign this JIRA to yourself, I don't have the permission to do it. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache livy logs as config driven,LIVY-359,13095997,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,kpraveen,kpraveen,gmcdonald,16/May/17 17:43,01/Sep/17 01:39,19/Dec/25 04:15,12/Jul/17 16:43,0.4.0,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"Currently we are cacheing logs in livy. In local mode and client mode it might cache too many logs and bring down livy.
So it's better to cache logs as config driven by specifying the no of log lines livy can cache.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 16 21:31:18 UTC 2017,,,,,,,,,,"0|i3j0nz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/May/17 21:31;zjffdu;How about deleting log from cache after a while ? I think we only need the log when the yarn app fails to launch and it is for diagnosing purpose, we could delete the log after a while if the client has got the log. Let me know if I miss anything other scenarios. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Increase the Jetty Http request and response header size,LIVY-358,13095996,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,12/May/17 03:48,12/May/17 16:46,19/Dec/25 04:15,12/May/17 16:46,0.4.0,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"When spnego is enabled, Jetty requires large header size for http request and response, current header size is not enough and will lead to error 403.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-05-12 03:48:35.0,,,,,,,,,,"0|i3j0nr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy UI throw MIME type exception,LIVY-357,13095995,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,jerryshao,gmcdonald,10/May/17 03:04,30/Jun/17 07:27,19/Dec/25 04:15,30/Jun/17 07:27,0.4.0,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"{code}
17/05/10 11:01:37 WARN util.MimeTypes$: There was an error detecting the mime type.
eu.medsea.mimeutil.MimeException: InputStream must support the mark() and reset() methods.
	at eu.medsea.mimeutil.MimeUtil2.getMimeTypes(MimeUtil2.java:495)
	at org.scalatra.util.Mimes$$anonfun$inputStreamMime$1.apply(Mimes.scala:54)
	at org.scalatra.util.Mimes$$anonfun$inputStreamMime$1.apply(Mimes.scala:54)
	at scala.util.control.Exception$Catch.apply(Exception.scala:102)
	at org.scalatra.util.Mimes$class.detectMime(Mimes.scala:87)
	at org.scalatra.util.Mimes$class.inputStreamMime(Mimes.scala:53)
	at org.scalatra.util.MimeTypes$.inputStreamMime(Mimes.scala:102)
	at org.scalatra.util.Mimes$class.apply(Mimes.scala:96)
	at org.scalatra.util.MimeTypes$.apply(Mimes.scala:102)
	at org.scalatra.ScalatraBase$$anonfun$contentTypeInferrer$1.applyOrElse(ScalatraBase.scala:381)
	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:218)
	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:214)
	at org.scalatra.ScalatraBase$class.renderResponse(ScalatraBase.scala:363)
	at org.scalatra.ScalatraServlet.renderResponse(ScalatraServlet.scala:49)
	at org.scalatra.ScalatraBase$class.executeRoutes(ScalatraBase.scala:189)
	at org.scalatra.ScalatraServlet.executeRoutes(ScalatraServlet.scala:49)
	at org.scalatra.ScalatraBase$$anonfun$handle$1.apply$mcV$sp(ScalatraBase.scala:113)
	at org.scalatra.ScalatraBase$$anonfun$handle$1.apply(ScalatraBase.scala:113)
	at org.scalatra.ScalatraBase$$anonfun$handle$1.apply(ScalatraBase.scala:113)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.scalatra.DynamicScope$class.withResponse(DynamicScope.scala:80)
	at org.scalatra.ScalatraServlet.withResponse(ScalatraServlet.scala:49)
	at org.scalatra.DynamicScope$$anonfun$withRequestResponse$1.apply(DynamicScope.scala:60)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.scalatra.DynamicScope$class.withRequest(DynamicScope.scala:71)
	at org.scalatra.ScalatraServlet.withRequest(ScalatraServlet.scala:49)
	at org.scalatra.DynamicScope$class.withRequestResponse(DynamicScope.scala:59)
	at org.scalatra.ScalatraServlet.withRequestResponse(ScalatraServlet.scala:49)
	at org.scalatra.ScalatraBase$class.handle(ScalatraBase.scala:111)
	at org.scalatra.ScalatraServlet.org$scalatra$servlet$ServletBase$$super$handle(ScalatraServlet.scala:49)
	at org.scalatra.servlet.ServletBase$class.handle(ServletBase.scala:43)
	at org.scalatra.ScalatraServlet.handle(ScalatraServlet.scala:49)
	at org.scalatra.ScalatraServlet.service(ScalatraServlet.scala:54)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:812)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:587)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1127)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1061)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:110)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
	at org.eclipse.jetty.server.Server.handle(Server.java:499)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)
	at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
	at java.lang.Thread.run(Thread.java:745)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 12 19:32:52 UTC 2017,,,,,,,,,,"0|i3j0nj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/May/17 18:47;ajbozarth;I'll take a look at this;;;","10/May/17 18:56;ajbozarth;[~jerryshao] how did you get this? I'll need repo steps if I'm going to figure it out. ;;;","12/May/17 01:09;jerryshao;Cross the reproduce steps here:

* build livy with ""mvn package -DskipTests""
* Configure Spark home to use spark 2.1.0
* Create a interactive session and open a Livy UI.

Then you will see the issue.;;;","12/May/17 19:32;ajbozarth;pr: https://github.com/cloudera/livy/pull/330
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for LDAP authentication in Livy Server,LIVY-356,13095994,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,micahzhao,januakhani.ce@gmail.com,gmcdonald,04/May/17 10:14,03/Aug/23 04:29,19/Dec/25 04:15,13/Oct/19 18:25,0.4.0,,,0.7.0,,Server,,,,,,,,,,0,,,,,,"Currently, Livy doesn't support LDAP Authentication from client(sparkmagic) to server(livy). We need to add LDAP authentication as that's preferable method due to security reasons. We won't be able to use Knox for this purpose. That is why I am raising this PR which contains LDAP authentication. I have upgraded hadoop.version in livy-main to 2.8.0 as this version contains LivyAuthenticationHandler. Below I have mentioned link for the same:
https://insight.io/github.com/apache/hadoop/blob/HEAD/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/server/LdapAuthenticationHandler.java",,"captainzmc commented on pull request #231: [LIVY-356][SERVER]Add LDAP authentication for livy-server.
URL: https://github.com/apache/incubator-livy/pull/231
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Sep/19 01:49;githubbot;600","captainzmc commented on pull request #231: [LIVY-356][SERVER]Add LDAP authentication for livy-server.
URL: https://github.com/apache/incubator-livy/pull/231
 
 
   ## What changes were proposed in this pull request?
   Currently, livy-server doesn't support LDAP Authentication from client to server(livy). We need to add LDAP authentication as that's preferable method due to security reasons.
   Here we reimplement LdapAuthenticationHandle, which is new in hadoop2.8+.
   ## How was this patch tested?
   
   UTs tests for this part have been added. We can test in UTs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Sep/19 01:49;githubbot;600","captainzmc commented on pull request #231: [LIVY-356][SERVER]Add LDAP authentication for livy-server.
URL: https://github.com/apache/incubator-livy/pull/231
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Oct/19 14:56;githubbot;600","captainzmc commented on pull request #231: [LIVY-356][SERVER]Add LDAP authentication for livy-server.
URL: https://github.com/apache/incubator-livy/pull/231
 
 
   ## What changes were proposed in this pull request?
   Currently, livy-server doesn't support LDAP Authentication from client to server(livy). We need to add LDAP authentication as that's preferable method due to security reasons.
   Here we reimplement LdapAuthenticationHandle, which is new in hadoop2.8+.
   ## How was this patch tested?
   
   UTs tests for this part have been added. We can test in UTs
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Oct/19 14:56;githubbot;600","mgaido91 commented on pull request #231: [LIVY-356][SERVER]Add LDAP authentication for livy-server.
URL: https://github.com/apache/incubator-livy/pull/231
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Oct/19 07:42;githubbot;600","yuanzhaoYZ commented on pull request #20:
URL: https://github.com/apache/incubator-livy/pull/20#issuecomment-674655356


   Thanks for adding this feature. But I couldn't find documentations on this topic. Posting my LDAP configs here. Hopefully it helps.
   
   **Livy config**
   ```bash
   # LDAP
   livy.server.auth.type = ldap
   livy.server.auth.ldap.url = ldap://localhost:389
   livy.server.auth.ldap.base-dn = ou=people,dc=intellinum,dc=co
   #livy.server.auth.ldap.username-domain = 
   livy.server.auth.ldap.enable-start-tls = false
   livy.server.auth.ldap.security-authentication = simple
   
   ```
   
   
   **LDAP Config**
   ```bash
   dn: uid=livy,ou=people,dc=intellinum,dc=co
   objectclass: inetOrgPerson
   cn: livy
   sn: livy
   uid: livy
   userpassword: XXXXXX
   ou: IT
   
   ````
   
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Aug/20 04:58;githubbot;600","wanggc2019 commented on PR #20:
URL: https://github.com/apache/incubator-livy/pull/20#issuecomment-1663273442

   > Thanks for adding this feature. But I couldn't find documentations on this topic. Posting my LDAP configs here. Hopefully it helps.
   > 
   > **Livy config**
   > 
   > ```shell
   > # LDAP
   > livy.server.auth.type = ldap
   > livy.server.auth.ldap.url = ldap://localhost:389
   > livy.server.auth.ldap.base-dn = ou=people,dc=intellinum,dc=co
   > #livy.server.auth.ldap.username-domain = 
   > livy.server.auth.ldap.enable-start-tls = false
   > livy.server.auth.ldap.security-authentication = simple
   > ```
   > 
   > **LDAP Config**
   > 
   > ```shell
   > dn: uid=livy,ou=people,dc=intellinum,dc=co
   > objectclass: inetOrgPerson
   > cn: livy
   > sn: livy
   > uid: livy
   > userpassword: XXXXXX
   > ou: IT
   > ```
   
   Thank you very much and may ask another question? My question is after enabling ldap, how to submit tasks with ldap authentication?
   


;03/Aug/23 04:29;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4200,,,0,4200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Oct 13 18:25:14 UTC 2019,,,,,,,,,,"0|i3j0nb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Jul/17 21:30;januakhani.ce@gmail.com;New PR for this ticket: https://github.com/apache/incubator-livy/pull/20;;;","13/Oct/19 18:25;mgaido;Issue resolved by https://github.com/apache/incubator-livy/pull/231.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor StatementProgressListener to fix binary compatibility broken issue,LIVY-355,13095993,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,02/May/17 03:27,09/May/17 21:18,19/Dec/25 04:15,09/May/17 21:18,0.4.0,,,0.4.0,,REPL,,,,,,,,,,0,,,,,,"Current Livy's {{StatementProgressListener}} is an implementation based on {{SparkListener}}, whereas the signature of {{SparkListener}} was changed in Spark2.0+, which means if Livy code is built against Spark 1.6, then it cannot be run on Spark2, vice versa. 

So we have to avoid directly using SparkListener, here propose to use a new way to calculate the progress of statement.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-05-02 03:27:36.0,,,,,,,,,,"0|i3j0n3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LIVY-342 Followup: Link AppId to Yarn UI,LIVY-353,13095991,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,gmcdonald,01/May/17 20:10,18/Aug/17 18:18,19/Dec/25 04:15,30/Jun/17 07:38,0.4.0,,,0.4.0,,Core,,,,,,,,,,0,,,,,,For Spark applications using Yarn link the AppId to the Yarn UI. This will require plumbing a way to surface the Yarn App UI.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-87,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 09 21:31:41 UTC 2017,,,,,,,,,,"0|i3j0mn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/May/17 21:31;ajbozarth;PR: https://github.com/cloudera/livy/pull/327;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LIVY-342 Followup: Add Pagination,LIVY-352,13095990,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,gmcdonald,01/May/17 20:08,18/Aug/17 18:18,19/Dec/25 04:15,03/Jul/17 06:28,0.4.0,,,0.4.0,,Server,,,,,,,,,,0,,,,,,Switch All Sessions page to use jQuery DataTables for search and pagination,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-87,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 09 21:23:09 UTC 2017,,,,,,,,,,"0|i3j0mf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/May/17 21:23;ajbozarth;PR: https://github.com/cloudera/livy/pull/326;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using Guava API in client mode will throw ClassNotFound issue,LIVY-351,13095989,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,28/Apr/17 03:01,28/Apr/17 03:49,19/Dec/25 04:15,28/Apr/17 03:49,0.4.0,,,0.4.0,,REPL,,,,,,,,,,0,,,,,,"Guava is not a dependency in Spark, so running in client mode driver will throw exception when using Guava api:

{code}
java.lang.ClassNotFoundException: com.google.common.base.Preconditions
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-04-28 03:01:54.0,,,,,,,,,,"0|i3j0m7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test - should report an error if accessing an unknown variable,LIVY-349,13095987,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,26/Apr/17 07:49,28/Apr/17 07:10,19/Dec/25 04:15,28/Apr/17 07:10,0.4.0,,,0.4.0,,Server,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-04-26 07:49:12.0,,,,,,,,,,"0|i3j0lr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Livy's ACL mechanism,LIVY-348,13095986,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,25/Apr/17 07:44,31/Jul/17 23:36,19/Dec/25 04:15,24/Jul/17 06:25,0.4.0,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"Here is the current status of Livy's ACLs:

1. Livy has ACLs, which is controlled by *livy.server.access-control.enabled*, if it is set to true, then only the allowed users (users configured in *livy.server.access-control.users*) can submit any REST requests. For example, if ACLs is enabled, and user ""A"", ""B"" and ""C"" are the allowed users, then user ""A"", ""B"" and ""C"" could create sessions, submit statements and others. But user ""D"" (which is not in the allowed list) cannot submit any REST queries, all of them will be responded with 403.

2. For the existing sessions (sessions already created), only the user who created this session or livy super user (*livy.superusers*) could access this session, including submitting statements, querying results. For example, if session ""1"" is created by user ""A"", then only user ""A"" and livy superuser could access this session. other user cannot POST or GET anything.

This is the current status of Livy's ACLs, I think it has basic functionalities of ACLs (access control), but it lacks fine-grained controls, like admin/modify/view users in Spark.

So I think it would be better to improve the current Livy's ACLs to have fine-grained controls like Spark.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 23 19:05:11 UTC 2017,,,,,,,,,,"0|i3j0lj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/May/17 19:05;ajbozarth;FYI: open pr by jerry here: https://github.com/cloudera/livy/pull/320;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add POST functionality to Web UI,LIVY-346,13095984,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Won't Do,ajbozarth,ajbozarth,gmcdonald,18/Apr/17 21:24,18/Aug/17 18:18,19/Dec/25 04:15,01/May/17 20:05,0.4.0,,,,,Server,,,,,,,,,,0,,,,,,"Create a form, either popup or page, for submitting new sessions with a creation link for each table (interactive and batch) on the All Session Page.

Similarly add a way to submit new statements to a Interactive Session via the Session Page

These features should require proper security

[This may be split up into two PRs if needed]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-87,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 01 20:05:48 UTC 2017,,,,,,,,,,"0|i3j0l3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Apr/17 07:48;jerryshao;Hi Alex, is it a good idea to add POST functionality to web UI? My original thought is to add some basic status check functionalities on the UI.

I guess lots of users who have such requirements will customize this UI themselves and integrates with their facilities.;;;","25/Apr/17 18:35;ajbozarth;Thanks for the feedback, this JIRA was actually a optional stretch goal for my proposed UI. I agree adding POST options might not be what we want so unless [~zjffdu] as the original PR creator wants this, I'll close this as 'Wont Fix' 
As a followup though, what are your thoughts on LIVY-345 since it would also use DELETE and POST;;;","01/May/17 20:05;ajbozarth;Closing this as Won't Do since including POST functionality is probably not the best idea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Session Log Page,LIVY-344,13095982,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,gmcdonald,18/Apr/17 21:18,18/Aug/17 18:18,19/Dec/25 04:15,01/Aug/17 13:41,0.4.0,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"Add link to Log Page on the All Sessions Page tables and the Session Page
Create Log Page
ï¿½ï¿½ï¿½	A log viewer similar to the Spark Web UI LogPage
ï¿½ï¿½ï¿½	Possible implement infinite scrolling?
-Access to the logs should require proper security- will be addressed in LIVY-366",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-87,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 26 21:17:04 UTC 2017,,,,,,,,,,"0|i3j0kn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/May/17 20:46;ajbozarth;I'm currently working on this as I wait for blockers to submit my pr for LIVY-343 and my code depends on it as well;;;","26/Jul/17 21:17;ajbozarth;Open PR: https://github.com/apache/incubator-livy/pull/25;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Session Summary Page,LIVY-343,13095981,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,gmcdonald,18/Apr/17 21:17,18/Aug/17 18:18,19/Dec/25 04:15,18/Jul/17 04:22,0.4.0,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"For each session in the All Sessions Page, add links to its Session Page.
Create Session Page
ï¿½ï¿½ï¿½	List of Session json data at top of page
ï¿½ï¿½ï¿½	Table of statements for the session
ï¿½ï¿½ï¿½	Table of appInfo for the session
-Access to the statements list should require proper security- will be addressed in LIVY-366",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-87,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 13 01:16:10 UTC 2017,,,,,,,,,,"0|i3j0kf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/May/17 20:00;ajbozarth;I have this ready once the blocking tasks are merged;;;","13/Jul/17 01:16;ajbozarth;https://github.com/apache/incubator-livy/pull/16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Web UI Servlet and All Sessions Page,LIVY-342,13095980,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,ajbozarth,gmcdonald,18/Apr/17 21:14,18/Aug/17 18:18,19/Dec/25 04:15,09/May/17 21:12,0.4.0,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"Add a WebUi Servlet to the current Livy Server (REST API) and setup static file access for html, js, css and img files.

Create All Sessions Page (default ui landing page)
ï¿½ï¿½ï¿½	Tables listing interactive and batch sessions (filled by GET /sessions and GET /batches)
ï¿½ï¿½ï¿½	Tables include all data returned in a Session json (except logs and appInfo)
ï¿½ï¿½ï¿½	Tooltips explain state and kind values",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-87,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 26 00:33:51 UTC 2017,,,,,,,,,,"0|i3j0k7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Apr/17 00:33;ajbozarth;Opened a pr: https://github.com/cloudera/livy/pull/319;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Queue is ignored for spark interactive sessions,LIVY-340,13095978,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,kpraveen,kpraveen,gmcdonald,13/Apr/17 03:27,01/Sep/17 01:45,19/Dec/25 04:15,18/Apr/17 20:13,0.3,,,0.4.0,,RSC,Server,,,,,,,,,0,correctness,,,,,For interactive sessions queue is ignored and all the applications are created under default queue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 18 00:16:40 UTC 2017,,,,,,,,,,"0|i3j0jr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Apr/17 00:16;purechoc;did you try this?

#spark-default.conf
spark.yarn.queue livy
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RPCServer bind to random port ,which make livy not work inside docker",LIVY-337,13095975,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,pralabhkumar,pralabhkumar,gmcdonald,31/Mar/17 07:01,01/Sep/17 01:44,19/Dec/25 04:15,08/Jun/17 08:40,0.3,,,0.4.0,,RSC,,,,,,,,,,0,LIVY,,,,,"Hi 

I am using Livy 0.3 in Docker in yarn cluster mode. So my livy is running on container and RSC will run on my Yarn Cluster . Now When Livy start , it bind 
RPCServer to random port 

.bind(0)

Now t I cannot open this port  from my docker ,since its random port . 

Now due to this the RSC cannot  be able to communicate back with RPC Server and Livy Server fails .

Now in order to fix above issue 
1 I started my docker ,opened with one specific port ,for e.g 31111
2 Create livy-client.conf and added  livy.rsc.launcher.port=31111
3 Change in RPCServer code to bind  to 

.bind(config.getInt(LAUNCHER_PORT))

This make my livy runs fine. 

Can we incorporate these change in main source.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 08 08:42:40 UTC 2017,,,,,,,,,,"0|i3j0j3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"31/Mar/17 07:33;zjffdu;I think we discussed it somewhere else. It is better to specify port range which is more general. ;;;","03/Apr/17 05:35;pralabhkumar;So is it planned to include the same in livy 0.4 ?;;;","03/Apr/17 09:18;zjffdu;If this is fixed before 0.4 release, it would definitely be included. ;;;","06/Apr/17 09:07;pralabhkumar;is somebody working on it , or we can contribute to the same;;;","06/Apr/17 09:42;zjffdu;@pralabhkumar No one is working on that, welcome to contribute on this. And please sign CCLA before contributing.

https://github.com/cloudera/livy/wiki/Contributing-to-Livy;;;","07/Apr/17 06:22;tc0312;Please contribute!! This's a very nice feature to have!;;;","07/Apr/17 09:49;pralabhkumar;ok thanks , I'll start working on it . Please assign it to me.;;;","07/Apr/17 10:42;zjffdu;You are not in the assignee list, I guess you need to sign CCLA first. But you can work on the PR at the same time. ;;;","16/May/17 05:38;pralabhkumar;Hi

Sorry for being late , I have send CCLA .  Once its done ,please assign the jira to me .;;;","18/May/17 13:51;pralabhkumar;Can u please assign this to me ,as I am working on this 
;;;","21/May/17 10:10;pralabhkumar;Hi  @zjffdu @alex-the-man 

I have created the pull request with initial code changes . https://github.com/cloudera/livy/pull/334

Please review the approach. 
;;;","31/May/17 04:59;pralabhkumar;@zjffdu 
Please review the pull request;;;","08/Jun/17 08:08;pralabhkumar;@zjffdu 

Thanks for review the pull request and merging . Please close the jira ;;;","08/Jun/17 08:42;zjffdu;@Pralabh, I have closed this ticket, but unfortunately, I could not find your name in the assignee list. But don't worry, your credit is in the git log history. Livy are in the middle of donating to ASF. livy will have its own jira tracking system later. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
livy-python-api failed build,LIVY-335,13095973,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Not A Bug,,stexxe,gmcdonald,23/Mar/17 15:29,07/Apr/17 05:50,19/Dec/25 04:15,07/Apr/17 05:50,0.3,,,,,Tests,,,,,,,,,,0,,,,,,"I followed all the instructions on https://github.com/cloudera/livy but after executing 
mvn package
I receive the following error:
[...]
ImportError: No module named setuptools_scm.integration
[...]
[INFO] livy-main ......................................... SUCCESS [3.656s]
[INFO] livy-api .......................................... SUCCESS [5.537s]
[INFO] livy-client-common ................................ SUCCESS [4.575s]
[INFO] livy-test-lib ..................................... SUCCESS [0.352s]
[INFO] livy-rsc .......................................... SUCCESS [49.099s]
[INFO] multi-scala-project-root .......................... SUCCESS [2.350s]
[INFO] livy-core-parent .................................. SUCCESS [0.044s]
[INFO] livy-core_2.10 .................................... SUCCESS [2.447s]
[INFO] livy-repl-parent .................................. SUCCESS [3.060s]
[INFO] livy-repl_2.10 .................................... SUCCESS [1:31.014s]
[INFO] livy-core_2.11 .................................... SUCCESS [2.019s]
[INFO] livy-repl_2.11 .................................... SUCCESS [1:44.948s]
[INFO] livy-server ....................................... SUCCESS [1:01.304s]
[INFO] livy-assembly ..................................... SUCCESS [3.478s]
[INFO] livy-client-http .................................. SUCCESS [9.757s]
[INFO] livy-scala-api-parent ............................. SUCCESS [0.081s]
[INFO] livy-scala-api_2.10 ............................... SUCCESS [20.952s]
[INFO] livy-scala-api_2.11 ............................... SUCCESS [21.472s]
[INFO] minicluster-dependencies-parent ................... SUCCESS [0.151s]
[INFO] minicluster-dependencies_2.10 ..................... SUCCESS [2.605s]
[INFO] minicluster-dependencies_2.11 ..................... SUCCESS [2.241s]
[INFO] livy-integration-test ............................. SUCCESS [0.293s]
[INFO] livy-coverage-report .............................. SUCCESS [2.114s]
[INFO] livy-examples ..................................... SUCCESS [0.164s]
[INFO] livy-python-api ................................... FAILURE [12.834s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 6:47.016s
[INFO] Finished at: Thu Mar 23 08:11:58 PDT 2017
[INFO] Final Memory: 50M/481M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:exec (python-api install) on project livy-python-api: Command execution failed. Process exited with an error: 1 (Exit value: 1) -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :livy-python-api

Searched on internet but seems that this kind of error is not well documented.
Can you please help?

Thanks in advance",Linux quickstart.cloudera 2.6.32-573.el6.x86_64 #1 SMP Thu Jul 23 15:44:03 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 07 05:25:37 UTC 2017,,,,,,,,,,"0|i3j0in:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Apr/17 05:25;tc0312;ImportError: No module named setuptools_scm.integration
Can you reinstall setuptools?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception on building livy client,LIVY-333,13095971,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,namitkabra,gmcdonald,21/Mar/17 09:29,05/Apr/17 20:09,19/Dec/25 04:15,05/Apr/17 20:09,0.3,,,,,Core,,,,,,,,,,0,,,,,,"While creating a LivyClient (not on main thread of program) the following code throws exception:
                  {{ try {
				LivyClient client = new LivyClientBuilder()
				        .setURI(new URI(""http://127.0.0.1:8998"")) 
				        .build();
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (URISyntaxException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}}}
Here is the exception that I get:
{{java.lang.RuntimeException: java.lang.RuntimeException: com.cloudera.livy.shaded.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field ""appId"" (class com.cloudera.livy.client.common.HttpMessages$SessionInfo), not marked as ignorable (6 known properties: ""state"", ""owner"", ""kind"", ""log"", ""proxyUser"", ""id""]) at [Source: com.cloudera.livy.shaded.apache.http.client.entity.LazyDecompressingInputStream@237e9dc; line: 1, column: 21] (through reference chain: com.cloudera.livy.client.common.SessionInfo[""appId""]) at com.cloudera.livy.client.http.HttpClient.propagate(HttpClient.java:185) at com.cloudera.livy.client.http.HttpClient.<init>(HttpClient.java:85) at com.cloudera.livy.client.http.HttpClientFactory.createClient(HttpClientFactory.java:38) at com.cloudera.livy.LivyClientBuilder.build(LivyClientBuilder.java:124) at 
}}",Mac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 22 05:58:07 UTC 2017,,,,,,,,,,"0|i3j0i7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Mar/17 05:58;namitkabra;It turns out that my Livy server was the latest (0.4.0-SNAPSHOT) and the code I was using an earlier version (0.2.0). ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix small bugs when enabling SSL,LIVY-329,13095967,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,jerryshao,jerryshao,gmcdonald,13/Mar/17 06:52,14/Mar/17 08:45,19/Dec/25 04:15,14/Mar/17 08:45,0.4.0,,,,,Server,,,,,,,,,,0,,,,,,"1. Livy server url is exposed as HTTP URL even Https is enabled, so we should handle this.
2. Livy server SSL keystore password and key password currently set to same configurations, which should be separated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-03-13 06:52:20.0,,,,,,,,,,"0|i3j0hb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for --properties-file,LIVY-328,13095966,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Do,,kpraveen,gmcdonald,09/Mar/17 20:03,01/Aug/17 05:15,19/Dec/25 04:15,16/Mar/17 19:15,0.3,,,,,Core,REPL,RSC,Server,,,,,,,0,,,,,,"spark-submit supports --properties-file to loads extra spark properties.
I am thinking to have similar option through livy.
The file can be in hdfs and user can send the file path with http request.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 16 19:03:02 UTC 2017,,,,,,,,,,"0|i3j0h3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Mar/17 20:06;kpraveen;[~tc0312] can you give me permission to modify tickets which I created. I cloned this ticket but not able to edit the content.;;;","09/Mar/17 20:08;kpraveen;spark-submit supports --properties-file to loads extra spark properties.
I am thinking to have similar option through livy.

The file can be in hdfs and user can send the file path with http request.;;;","09/Mar/17 21:39;ajbozarth;[~kpraveen] you can email [~vanzin] via the dev mailing list and he should be able to get you permissions, for now I'll copy your comment into the jira description;;;","09/Mar/17 21:44;ajbozarth;Also if you aren't planning on implementing this yourself I'd be willing to look into after I finish my work on LIVY-287 since I'm already working on that configs area of code;;;","10/Mar/17 00:43;zjffdu;I am not sure whether this is a good idea to specify properties-file, because properties-files is supposed to be on the livy server host. While client user should not know what's on the livy server. It is better for him to specify '--conf' in rest api;;;","16/Mar/17 00:07;ajbozarth;[~kpraveen] Based on what [~zjffdu] said, is there any reason using the conf param in the api doesn't fulfill your requirements? Otherwise I say we close this as ""Not a Problem"";;;","16/Mar/17 06:01;kpraveen;If there are many configurations we would like to send and are shared by multiple applications, it might be really useful. ;;;","16/Mar/17 06:08;zjffdu;Then I would suggest you to put them info spark-defaults.conf which would be shared by all the livy sessions. ;;;","16/Mar/17 07:27;kpraveen;But it may effect other jobs. Moreover there can be multiple teams (with multiple applications) having the same use case. Is there any alternative approach for this use case?;;;","16/Mar/17 08:21;zjffdu;What do you mean other jobs ? other jobs launched by spark-submit ? In that case I would suggest you to create another folder as SPARK_CONF_DIR for livy. 
I have no better solution if you want to have multiple shared properties file for one livy instance. And spark doesn't support properties file in hdfs, livy should not add other extra functions that spark doesn't support.;;;","16/Mar/17 19:03;kpraveen;Ok. Then we can close this ticket. Thanks for the info.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change Hadoop dependencies to Apache Hadoop,LIVY-326,13095964,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,09/Mar/17 06:21,13/Mar/17 06:13,19/Dec/25 04:15,13/Mar/17 06:13,0.4.0,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"Currently we depend on CDH Hadoop version (2.6.0-cdh5.5), it would be better to use Apache Hadoop instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-03-09 06:21:16.0,,,,,,,,,,"0|i3j0gn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark session dies after creation,LIVY-323,13095961,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,shmuma,gmcdonald,28/Feb/17 10:56,12/Apr/17 21:55,19/Dec/25 04:15,11/Apr/17 00:42,0.3,,,0.4.0,,Core,,,,,,,,,,0,,,,,,"I'm trying to create spark session using curl: 
curl -X POST --data '{""kind"": ""spark""}' -H ""Content-Type: application/json"" localhost:8998/sessions

Output of curl is about session starting:

{noformat}
{""id"":0,""appId"":null,""owner"":null,""proxyUser"":null,""state"":""starting"",""kind"":""spark"",""appInfo"":{""driverLogUrl"":null,""sparkUiUrl"":null},""log"":[]}
{noformat}

In livy's log (attached) I see messages about spark session creation, but after a while I see a message about spark session successfully created, and immediately it starts shutdown process.

If I check session status using curl command, it's in DEAD state:

{noformat}
$ curl http://localhost:8998/sessions | jq .
{
  ""from"": 0,
  ""total"": 1,
  ""sessions"": [
    {
      ""id"": 0,
      ""appId"": ""application_1482343367445_0210"",
      ""owner"": null,
      ""proxyUser"": null,
      ""state"": ""dead"",
      ""kind"": ""spark"",
      ""appInfo"": {
        ""driverLogUrl"": null,
        ""sparkUiUrl"": ""http://ip-10-200-139-129.ec2.internal:20888/proxy/application_1482343367445_0210/""
      },
      ""log"": []
    }
  ]
}
{noformat}

In yarn tasks list I can see this session in Finished state and there are no errors or exceptions in driver's log or executor's logs (attached).

With the same installation, I can successfully start and use pyspack sessions using the same livy server.","I'm using spark 2.0.1 running on EMR in yarn mode. 

Livy is 0.3 built with ""mvn clean package -DskipTests -Dspark-2.0 -Dscala-2.11""

In conf/livy.conf I've set ""livy.spark.master = yarn""

Server was started using this shell script:

{noformat}
#!/bin/sh
. /etc/spark/conf/spark-env.sh
livy/bin/livy-server
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 12 21:55:38 UTC 2017,,,,,,,,,,"0|i3j0fz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Feb/17 14:06;zjffdu;Seems you are using yarn-client mode and somehow the driver is killed according the log. Can you try yarn-cluster to see what happens in yarn-cluster mode ?

{code}
17/02/28 10:48:05 INFO ContextLauncher: 17/02/28 10:48:05 INFO SparkInterpreter: Created Spark session.
17/02/28 10:48:05 INFO ContextLauncher: 17/02/28 10:48:05 INFO SparkUI: Stopped Spark web UI at http://10.200.139.129:4040
17/02/28 10:48:05 INFO ContextLauncher: 17/02/28 10:48:05 INFO YarnClientSchedulerBackend: Interrupting monitor thread
17/02/28 10:48:05 INFO ContextLauncher: 17/02/28 10:48:05 INFO YarnClientSchedulerBackend: Shutting down all executors
17/02/28 10:48:05 INFO ContextLauncher: 17/02/28 10:48:05 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
17/02/28 10:48:05 INFO ContextLauncher: 17/02/28 10:48:05 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
{code};;;","28/Feb/17 16:17;shmuma;In yarn-cluster mode (when livy.spark.master = yarn-cluster), I have this exception in driver's log: http://pastebin.com/raw/3HXTaK12

In fact, I've had the same message in yarn-client mode, but it was in livy log file and it was after session shutdown message, so I though it was unrelated.;;;","28/Feb/17 16:38;shmuma;I have this error on both master branch and branch-0.3.;;;","01/Mar/17 07:54;jerryshao;Looks like this is the issue:

{noformat}
scala.reflect.internal.FatalError: object Predef does not have a member classOf
	at scala.reflect.internal.Definitions$DefinitionsClass.scala$reflect$internal$Definitions$DefinitionsClass$$fatalMissingSymbol(Definitions.scala:1186)
	at scala.reflect.internal.Definitions$DefinitionsClass.getMember(Definitions.scala:1203)
	at scala.reflect.internal.Definitions$DefinitionsClass.getMemberMethod(Definitions.scala:1238)
	at scala.reflect.internal.Definitions$DefinitionsClass$RunDefinitions.Predef_classOf$lzycompute(Definitions.scala:1469)
	at scala.reflect.internal.Definitions$DefinitionsClass$RunDefinitions.Predef_classOf(Definitions.scala:1469)
	at scala.reflect.internal.Definitions$DefinitionsClass$RunDefinitions.isPredefClassOf(Definitions.scala:1459)
	at scala.tools.nsc.typechecker.Typers$Typer.typedIdent$2(Typers.scala:4885)
	at scala.tools.nsc.typechecker.Typers$Typer.typedIdentOrWildcard$1(Typers.scala:4908)
	at scala.tools.nsc.typechecker.Typers$Typer.typedInAnyMode$1(Typers.scala:5340)
	at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5360)
	at scala.tools.nsc.typechecker.Typers$Typer.runTyper$1(Typers.scala:5396)
	at scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Typers$Typer$$typedInternal(Typers.scala:5423)
	at scala.tools.nsc.typechecker.Typers$Typer.body$2(Typers.scala:5370)
	at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5374)
	at scala.tools.nsc.interpreter.ReplGlobal$$anon$1$$anon$2.typed(ReplGlobal.scala:36)
	at scala.tools.nsc.typechecker.Typers$Typer.typedQualifier(Typers.scala:5472)
	at scala.tools.nsc.typechecker.Typers$Typer.typedQualifier(Typers.scala:5480)
	at scala.tools.nsc.typechecker.Typers$Typer.typedPackageDef$1(Typers.scala:5012)
	at scala.tools.nsc.typechecker.Typers$Typer.typedMemberDef$1(Typers.scala:5312)
	at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5359)
	at scala.tools.nsc.typechecker.Typers$Typer.runTyper$1(Typers.scala:5396)
	at scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Typers$Typer$$typedInternal(Typers.scala:5423)
	at scala.tools.nsc.typechecker.Typers$Typer.body$2(Typers.scala:5370)
	at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5374)
	at scala.tools.nsc.interpreter.ReplGlobal$$anon$1$$anon$2.typed(ReplGlobal.scala:36)
	at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5448)
	at scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.apply(Analyzer.scala:102)
	at scala.tools.nsc.Global$GlobalPhase$$anonfun$applyPhase$1.apply$mcV$sp(Global.scala:440)
	at scala.tools.nsc.Global$GlobalPhase.withCurrentUnit(Global.scala:431)
	at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:440)
	at scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.apply(Analyzer.scala:94)
	at scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.apply(Analyzer.scala:93)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.run(Analyzer.scala:93)
	at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1501)
	at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1486)
	at scala.tools.nsc.Global$Run.compileSources(Global.scala:1481)
	at scala.tools.nsc.interpreter.IMain.compileSourcesKeepingRun(IMain.scala:435)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.compileAndSaveRun(IMain.scala:855)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.compile(IMain.scala:813)
	at scala.tools.nsc.interpreter.IMain.bind(IMain.scala:675)
	at com.cloudera.livy.repl.SparkInterpreter$$anonfun$bind$1.apply(SparkInterpreter.scala:129)
	at com.cloudera.livy.repl.SparkInterpreter$$anonfun$bind$1.apply(SparkInterpreter.scala:129)
	at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:214)
	at com.cloudera.livy.repl.SparkInterpreter.bind(SparkInterpreter.scala:128)
	at com.cloudera.livy.repl.SparkContextInitializer$class.spark2CreateContext(SparkContextInitializer.scala:109)
	at com.cloudera.livy.repl.SparkContextInitializer$class.createSparkContext(SparkContextInitializer.scala:34)
	at com.cloudera.livy.repl.SparkInterpreter.createSparkContext(SparkInterpreter.scala:36)
	at com.cloudera.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:89)
	at com.cloudera.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:68)
	at com.cloudera.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:68)
	at com.cloudera.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:256)
	at com.cloudera.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:68)
	at com.cloudera.livy.repl.Session$$anonfun$1.apply(Session.scala:76)
	at com.cloudera.livy.repl.Session$$anonfun$1.apply(Session.scala:74)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat};;;","01/Mar/17 09:39;shmuma;Yes, it fails during executing line 109 in SparkContextInitializer.scala:

{code:java}
bind(""sc"", ""org.apache.spark.SparkContext"", sparkContext, List(""""""@transient""""""))
{code}

Any suggestions why it may happen?
;;;","01/Mar/17 09:41;jerryshao;Not very sure about this issue. Can you build livy with {{mvn clean package -DskipTests}}?

Scala version is not required, and my wild guessing is related to scala.;;;","01/Mar/17 10:42;shmuma;No, exception with 'mvn clean package -DskipTests' build is the same: http://pastebin.com/FLKqWHXf

My build log: [^build-log.txt] 
;;;","01/Mar/17 12:41;jerryshao;I tried in my local environment, but unfortunately cannot produce this issue.

Can you please elaborate your environment, like Spark version, Spark and Livy configurations, also how do you start Livy session? We have travis integration test run against Spark 2.0, 2.1, but we never met this issue before.;;;","01/Mar/17 12:55;shmuma;Spark is 2.0.1, with default yarn configuration from EMR, livy settings were also weren't changed, except livy.spark.master option.

Livy session is started using curl (command was given in original message).

In this thread, exception stack trace is the same as mine: https://groups.google.com/a/cloudera.org/forum/#!msg/livy-user/bBZMo9LC0S4/KlBjdoxuAAAJ

So, looks like it can be reproduced in EMR environment with spark > 2.0.;;;","02/Mar/17 00:42;zjffdu;Then I suspect it is due to EMR spark issue, Did you try apache spark ?;;;","04/Apr/17 14:38;cnero;Any update on this problem? I'm facing the same issue.
Thank you in advance.;;;","04/Apr/17 23:20;julienlaurenceau;I am also interested in any solution;;;","04/Apr/17 23:29;zjffdu;Could you describe your enviroment to reproduce it ? Currently we suspect it is due to EMR issue, as we can not reproduce it in other environments. ;;;","04/Apr/17 23:55;julienlaurenceau;I am working on Azure HDinsight cluster and the issues appeared when swithching from HDinsight 3.5 spark 1.6.3 livy 0.3 to HDinsight 3.5 spark 2..02. livy 0.3

I am wondering if it is not related to livy configuration.

In Azure doc there is this remarks that I am testing:
{quote}Using Livy on HDInsight 3.5 Spark clusters

HDInsight 3.5 cluster, by default, disables use of local file paths to access sample data files or jars. We encourage you to use the wasb:// path instead to access jars or sample data files from the cluster. If you do want to use local path, you must update the Ambari configuration accordingly. To do so:

    Go to the Ambari portal for the cluster. The Ambari Web UI is available on your HDInsight cluster at https://CLUSTERNAME.azurehdidnsight.net, where CLUSTERNAME is the name of your cluster.

    From the left navigation, click Livy, and then click Configs.

    Under livy-default add the property name livy.file.local-dir-whitelist and set it's value to ""/"" if you want to allow full access to file system. If you want to allow access only to a specific directory, provide the path to that directory as the value.
{quote};;;","05/Apr/17 06:06;julienlaurenceau;No the issue is still there unfortunately;;;","06/Apr/17 09:23;julienlaurenceau;indeed my problem may be due to some transitive deps not functionning correctly.

On yarn logs I see
17/04/06 09:16:13 INFO SparkContext: Successfully stopped SparkContext
17/04/06 09:16:13 ERROR ApplicationMaster: User class threw exception: scala.reflect.internal.FatalError: object Predef does not have a member classOf
scala.reflect.internal.FatalError: object Predef does not have a member classOf
	at scala.reflect.internal.Definitions$DefinitionsClass.scala$reflect$internal$Definitions$DefinitionsClass$$fatalMissingSymbol(Definitions.scala:1186)

My cluster use spark 2.0.2 and scala 2.11.
I did not put any settings in livy conf for 
""livy.spark.scala-version""
""livy.spark.version""

still investigating;;;","06/Apr/17 12:19;julienlaurenceau;Still not working with updated livy conf.

yarn logs attached
	at scala.tools.nsc.interpreter.IMain.bind(IMain.scala:675)
	at com.cloudera.livy.repl.SparkInterpreter$$anonfun$bind$1.apply(SparkInterpreter.scala:129)
	at com.cloudera.livy.repl.SparkInterpreter$$anonfun$bind$1.apply(SparkInterpreter.scala:129)
	at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:214)
 [^yarn_application_1491290444239_0073.log] ;;;","06/Apr/17 14:30;julienlaurenceau;working after deleting on my azure HDinsight cluster the file /home/livy/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar

with ansible it gives:
    - name: run hack livy HDinsight
      command: cp /usr/bin/livy2/jars/unused-1.0.0.jar org.scala-lang_scala-reflect-2.11.8.jar
      args:
        chdir: ""/home/livy/.ivy2/jars""
      ignore_errors: yes
;;;","07/Apr/17 06:18;tc0312;These packages shouldn't depend on scala-reflect. I think it must be a transitive dependency got pulled in unexpectedly.
Anyway I've a workaround and am testing it.;;;","11/Apr/17 00:42;tc0312;https://github.com/cloudera/livy/commit/07f6072f9d90ec970a1f9fbaccf0452f3a783538;;;","12/Apr/17 21:55;julienlaurenceau;Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,
Livy python-api client test failing,LIVY-320,13095958,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,akchin,akchin,gmcdonald,21/Feb/17 23:39,22/Feb/17 18:25,19/Dec/25 04:15,22/Feb/17 18:25,0.4.0,,,0.4.0,,Tests,,,,,,,,,,0,,,,,,"When building Livy master, the livy-python-api module client_test fails with the following:
__________ ERROR collecting src/test/python/livy-tests/client_test.py __________
src/test/python/livy-tests/client_test.py:29: in <module>
    base_uri = 'http://{}:{}'.format(socket.gethostname(), 8998)
E   ValueError: zero length field name in format

Prior to python 2.7 the field number needs to be specified.","RHEL6.8, Python 2.6.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 22 18:25:56 UTC 2017,,,,,,,,,,"0|i3j0fb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Feb/17 01:18;akchin;PR for fix - https://github.com/cloudera/livy/pull/298;;;","22/Feb/17 18:25;akchin;Merged -  https://github.com/cloudera/livy/commit/6bfe17749c25da2145aac9eedbc6bedb1a4d33eb

Thx!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unify the indent for all POM file,LIVY-319,13095957,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,ajbozarth,jerryshao,gmcdonald,20/Feb/17 09:31,04/Mar/17 01:00,19/Dec/25 04:15,04/Mar/17 01:00,0.4.0,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"Some pom files has 4 spaces indent, we should unify all to use 2 space indent.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 02 02:39:15 UTC 2017,,,,,,,,,,"0|i3j0f3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"02/Mar/17 02:39;ajbozarth;Opened a PR: [#302|https://github.com/cloudera/livy/pull/302];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when starting kinit thread,LIVY-316,13095954,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,15/Feb/17 07:36,16/Feb/17 00:16,19/Dec/25 04:15,16/Feb/17 00:16,0.3,,,0.4.0,,Core,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-02-15 07:36:21.0,,,,,,,,,,"0|i3j0ef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable hive in SparkRInterpreter,LIVY-315,13095953,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,15/Feb/17 05:50,19/Feb/17 08:19,19/Dec/25 04:15,19/Feb/17 08:19,0.3,,,0.4.0,,Interpreter,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 15 09:42:06 UTC 2017,,,,,,,,,,"0|i3j0e7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"15/Feb/17 09:42;purechoc;as I know, this is already support.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkRInterpreter always return SUCCESS,LIVY-313,13095951,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jonalter,zjffdu,gmcdonald,09/Feb/17 03:06,23/Mar/17 06:57,19/Dec/25 04:15,23/Mar/17 06:57,0.3,,,0.4.0,,Interpreter,,,,,,,,,,0,,,,,,"{noformat}
{
  ""total_statements"": 1,
  ""statements"": [
    {
      ""id"": 0,
      ""state"": ""available"",
      ""output"": {
        ""status"": ""ok"",
        ""execution_count"": 0,
        ""data"": {
          ""text/plain"": ""Error in cat(a) : object 'a' not found""
        }
      }
    }
  ]
}
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 09 18:45:17 UTC 2017,,,,,,,,,,"0|i3j0dr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Mar/17 18:45;jonalter;PR: [#307|https://github.com/cloudera/livy/pull/307];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restructure the projects for scala-2.10 and scala-2.11,LIVY-312,13095950,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,zjffdu,gmcdonald,08/Feb/17 05:34,20/Feb/17 08:37,19/Dec/25 04:15,20/Feb/17 08:37,0.3,,,0.4.0,,Core,,,,,,,,,,0,,,,,,"I found currently livy-core-parent, livy-repl-parent, livy-scala-api-parent are built individually, instead they are built in their child project. This leads 2 issues.
* Third party can not use livy-core_2.11 as maven dependency because it can not find its parent pom
* Code style check skip the code in parent project.  e.g. The following import order is not correct, but we didn't detect it.

SparkRInterpreter.scala
{noformat}
import org.apache.commons.codec.binary.Base64
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.util.{ChildFirstURLClassLoader, MutableURLClassLoader, Utils}
import org.apache.commons.lang.StringEscapeUtils
import org.json4s._
import org.json4s.JsonDSL._
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-02-08 05:34:33.0,,,,,,,,,,"0|i3j0dj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set classpath in MiniCluster only when it is not in real cluster,LIVY-311,13095949,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,08/Feb/17 05:23,16/Feb/17 00:19,19/Dec/25 04:15,16/Feb/17 00:19,0.3,,,0.4.0,,Tests,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-02-08 05:23:57.0,,,,,,,,,,"0|i3j0db:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kinit should be done before checking recovery from filesystem,LIVY-310,13095948,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,03/Feb/17 06:46,16/Feb/17 00:20,19/Dec/25 04:15,16/Feb/17 00:20,0.3,,,0.4.0,,Server,,,,,,,,,,0,,,,,,Otherwise HDFS operations will be failed with Kerberos exception.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-02-03 06:46:50.0,,,,,,,,,,"0|i3j0d3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.NoClassDefFoundError: scala/runtime/AbstractPartialFunction$mcVL$sp,LIVY-309,13095947,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,sub,gmcdonald,01/Feb/17 17:30,22/Feb/17 22:20,19/Dec/25 04:15,22/Feb/17 22:20,0.2,,,,,Core,,,,,,,,,,0,,,,,,"While running  job from a livy server  in a existing spark/mesos environment. I am using livy version 0.2.0

7/01/31 19:45:54 INFO ContextLauncher: Exception in thread ""main"" java.lang.NoClassDefFoundError: scala/runtime/AbstractPartialFunction$mcVL$sp
17/01/31 19:45:54 INFO ContextLauncher:         at java.lang.ClassLoader.defineClass1(Native Method)
17/01/31 19:45:54 INFO ContextLauncher:         at java.lang.ClassLoader.defineClass(ClassLoader.java:763)
17/01/31 19:45:54 INFO ContextLauncher:         at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
17/01/31 19:45:54 INFO ContextLauncher:         at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 03 06:30:23 UTC 2017,,,,,,,,,,"0|i3j0cv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Feb/17 06:30;jerryshao;This is a wrong Scala version problem. Livy 0.2.0 only supports Scala 2.10. Please try 0.3.0. If you met the issue please elaborate the problem in detail.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Livy run with Spark 2.1,LIVY-308,13095946,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,linchan,gmcdonald,30/Jan/17 22:22,16/Feb/17 00:20,19/Dec/25 04:15,16/Feb/17 00:20,0.3,,,0.4.0,,Tests,,,,,,,,,,0,,,,,,Make Livy run with Spark 2.1. Including tests as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-01-30 22:22:39.0,,,,,,,,,,"0|i3j0cn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't enforce the minimum session timeout as 1 hour,LIVY-306,13095944,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,zjffdu,gmcdonald,22/Jan/17 01:43,22/Feb/17 01:36,19/Dec/25 04:15,22/Feb/17 01:36,0.3,,,0.4.0,,Interpreter,,,,,,,,,,0,,,,,,1 hour is not feasible for testing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 14 23:28:22 UTC 2017,,,,,,,,,,"0|i3j0c7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"08/Feb/17 18:46;jonalter;Session timeout duration can be changed by uncommenting and changing the value for *livy.server.session.timeout* in {{conf/livy.conf}};;;","08/Feb/17 23:36;zjffdu;It can be changed, but whatever value you set, the minimum is 1 hour which is not feasible for testing. ;;;","10/Feb/17 23:26;ajbozarth;I was thinking the same as Jon, but I just found the code bit that sets the minimum. Is there a reason it was set to 1hr previously? I'm willing to take this but should we change the minimum to another number, make it a var so tests can change it or just get rid of it? I found the commit when it was added but there no explanation for why.;;;","14/Feb/17 23:28;ajbozarth;opened a pr: https://github.com/cloudera/livy/pull/290;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support removing old statements in RSC,LIVY-303,13095941,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,19/Jan/17 08:42,03/Mar/17 00:53,19/Dec/25 04:15,03/Mar/17 00:53,0.3,,,0.4.0,,Interpreter,RSC,,,,,,,,,0,,,,,,"In the current implementation of Livy, statements history will kept in Spark driver's memory forever, which will potentially lead to OOM issue if the sessions runs for days. So here propose to add statements removing mechanism.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 03 00:53:26 UTC 2017,,,,,,,,,,"0|i3j0bj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Mar/17 00:53;ajbozarth;Resolved by pr [279|https://github.com/cloudera/livy/pull/279];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fail to parse Spark version when including ""-SNAPSHOT""",LIVY-302,13095940,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,purechoc,purechoc,gmcdonald,18/Jan/17 05:40,21/Jan/17 00:22,19/Dec/25 04:15,21/Jan/17 00:22,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,"Fail to parse Spark version when including ""-SNAPSHOT""

when spark version is ""2.1.1-SNAPSHOT"", livy cannot parsed that string",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-01-18 05:40:59.0,,,,,,,,,,"0|i3j0bb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in rsc's RPC channel,LIVY-300,13095938,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,11/Jan/17 22:17,21/Jan/17 00:23,19/Dec/25 04:15,21/Jan/17 00:23,0.3,,,0.3,,RSC,,,,,,,,,,0,,,,,,"RPC calls are enqueued to the send buffer of the Channel as they are handled out of the event loop.
While replies are sent to the socket immediately because they are handled in the event loop.
Message reply might preempt the on going RPC call and causes the RPC channel to go to an unknown state:

Send call header
Send reply header
Send reply payload
Send call payload
To fix this, make RPC calls in the event loop.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-01-11 22:17:26.0,,,,,,,,,,"0|i3j0av:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Only the output of last line is returned,LIVY-299,13095937,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,zjffdu,gmcdonald,11/Jan/17 12:05,23/Jan/18 23:37,19/Dec/25 04:15,24/Nov/17 05:32,0.3,,,0.5.0,,Interpreter,,,,,,,,,,0,,,,,,"Request:
{code}
{""code"": ""print(1);\nprint(1)""}
{code}

Response:
{code}
{
  ""total_statements"": 1,
  ""statements"": [
    {
      ""id"": 0,
      ""state"": ""available"",
      ""output"": {
        ""status"": ""ok"",
        ""execution_count"": 0,
        ""data"": {
          ""text/plain"": ""1""
        }
      }
    }
  ]
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 23 23:37:57 UTC 2018,,,,,,,,,,"0|i3j0an:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Jan/17 13:02;purechoc;if user using magic, (as i know, magic need to be place last line)
Should not the previous output be ignored?

;;;","11/Jan/17 13:05;zjffdu;In that case, we can return a composite statement result rather than only the last line which might not be what user expect. ;;;","17/Nov/17 13:58;zjffdu;Reopen it as I still think we need to fix it, welcome any comment ;;;","23/Jan/18 23:37;ajbozarth;was fixed byÂ https://github.com/apache/incubator-livy/pull/66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cannot find hive-site.xml,LIVY-298,13095936,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,,purechoc,gmcdonald,11/Jan/17 07:30,11/Jan/17 08:40,19/Dec/25 04:15,11/Jan/17 08:40,0.3,,,,,REPL,Server,,,,,,,,,0,,,,,,"cannot find hive-site.xml when hive-site.xml in livy classpath (/home/livy/livy-master/hive-conf/hive-site.xml)

in executor node
{code}
java.io.FileNotFoundException: Added file file:/home/livy/livy-master/hive-conf/hive-site.xml does not exist.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 11 08:40:58 UTC 2017,,,,,,,,,,"0|i3j0af:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Jan/17 08:11;zjffdu;This might due to https://issues.apache.org/jira/browse/SPARK-18160

Can you try spark 2.1.0 ?;;;","11/Jan/17 08:40;zjffdu;This is due to SPARK bug SPARK-18160;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkRInterpreter doesn't work with statement with quotation mark,LIVY-297,13095935,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,zjffdu,gmcdonald,11/Jan/17 06:56,12/Jan/17 06:31,19/Dec/25 04:15,12/Jan/17 06:31,0.3,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"It looks like it is caused by LIVY-269. 

The following simple R command can not be executed.
{code}
{""code"": ""print(\""a\"")""}
{code}

Response:
{code}
{
  ""total_statements"": 1,
  ""statements"": [
    {
      ""id"": 0,
      ""state"": ""available"",
      ""output"": {
        ""status"": ""ok"",
        ""execution_count"": 0,
        ""data"": {
          ""text/plain"": ""Error: unexpected symbol in \""try(eval(parse(text=\""print(\""a\""""
        }
      }
    }
  ]
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-01-11 06:56:23.0,,,,,,,,,,"0|i3j0a7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make livy.rsc.jars a Livy configuration,LIVY-295,13095933,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,10/Jan/17 09:26,12/Jan/17 06:25,19/Dec/25 04:15,12/Jan/17 06:25,0.3,,,0.3,,RSC,,,,,,,,,,0,,,,,,"Currently {{livy.rsc.jars}} is a RSC configuration, which means user should specify this configuration each time when creating a session, this is semantically incorrect and inconvenient, also equivalent to {{livy.repl.jars}}, we should change this to Livy configuration.

Besides current doc uses {{livy.jars}} as a configuration name, this should also be updated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-01-10 09:26:34.0,,,,,,,,,,"0|i3j09r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveContext is always created instead of SQLContext for pyspark,LIVY-294,13095932,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,10/Jan/17 03:07,12/Jan/17 06:25,19/Dec/25 04:15,12/Jan/17 06:25,0.3,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"HiveContext is always created no matter when we enable hiveContext through spark.repl.enableHiveContext.

The root cause is that we depends on shell.py of spark. As the following codes shows, as long as we build hive with spark, we would always created HiveContext, and unformatenlly HiveContext would initialize itself when any method is called. So the following line 'sqlContext=HiveContext' would throw any exception. 

{code}
try:
    # Try to access HiveConf, it will raise exception if Hive is not added
    sc._jvm.org.apache.hadoop.hive.conf.HiveConf()
    sqlContext = HiveContext(sc)
except py4j.protocol.Py4JError:
    sqlContext = SQLContext(sc)
except TypeError:
    sqlContext = SQLContext(sc)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-01-10 03:07:21.0,,,,,,,,,,"0|i3j09j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redirect Spark Log to REST Response Log field instead of redirecting to livi-server.out log file,LIVY-293,13095931,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,prabhu1984,prabhu1984,gmcdonald,09/Jan/17 19:47,01/Sep/17 01:47,19/Dec/25 04:15,09/Mar/17 05:45,0.3,,,0.4.0,,Core,REPL,RSC,Server,,,,,,,0,,,,,,"Currently, Livy redirects the spark log to $LIVY_HOME//loglivy-livy-server.out log file for Interactive Sessions.

So, when a session fails before initiating the SparkContext, then user doesn't have a way to know why his job got failed unless he is going to $LIVY_HOME//loglivy-livy-server.out to debug. But, all users are not going to have access to that log file.

Hence, it would be better if we redirect the spark log lines to REST response log field for debugging (as how we are doing for Batch session).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 09 07:19:27 UTC 2017,,,,,,,,,,"0|i3j09b:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Jan/17 22:49;kpraveen;1. Starting Spark Scala Interactive Session through Livy
-bash-4.1$ curl --silent --negotiate -u: livyhost.example.com:8998/sessions -X POST -H 'Content-Type: application/json' -d '{
  ""kind"":""spark"",
  ""proxyUser"":""livyuser"",
  ""name"":""testing""
}'

{code}
{
    ""id"": 31,
    ""appId"": null,
    ""owner"": ""livyuser"",
    ""proxyUser"": ""livyuser"",
    ""state"": ""starting"",
    ""kind"": ""spark"",
    ""appInfo"": {
        ""driverLogUrl"": null,
        ""sparkUiUrl"": null
    },
    ""log"": []
}
{code}

2. Retrieving Spark Log 
-bash-4.1$ curl --silent --negotiate -u: livyhost.example.com:8998/sessions/31/log 
{code}
{
    ""id"": 31,
    ""from"": 0,
    ""total"": 213,
    ""log"": [
        ""2017-01-09 11:26:48,713 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"",
        ""2017-01-09 11:26:49,305 INFO  [main] impl.TimelineClientImpl: Timeline service address: http://resource-manager.example.com:8188/ws/v1/timeline/"",
        ""2017-01-09 11:26:49,968 WARN  [main] shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded."",
        ""2017-01-09 11:26:50,455 INFO  [main] client.ConfiguredRMFailoverProxyProvider: Failing over to rm2"",
        ""2017-01-09 11:26:50,475 INFO  [main] yarn.Client: Requesting a new application from cluster with 8 NodeManagers"",
        ""2017-01-09 11:26:50,487 INFO  [main] yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (307200 MB per container)"",
        ""2017-01-09 11:26:50,488 INFO  [main] yarn.Client: Will allocate AM container, with 4505 MB memory including 409 MB overhead"",
        ""2017-01-09 11:26:50,488 INFO  [main] yarn.Client: Setting up container launch context for our AM"",
        ""2017-01-09 11:26:50,488 INFO  [main] yarn.Client: Setting up the launch environment for our AM container"",
        ""2017-01-09 11:26:50,513 INFO  [main] yarn.Client: Preparing resources for our AM container"",
        ""2017-01-09 11:26:50,530 INFO  [main] yarn.YarnSparkHadoopUtil: getting token for namenode: hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002"",
        ""2017-01-09 11:26:50,562 INFO  [main] hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 281599 for livyuser on ha-hdfs:cluster"",
        ""2017-01-09 11:26:51,152 INFO  [main] hive.metastore: Trying to connect to metastore with URI thrift://zk2.example.com:9083"",
        ""2017-01-09 11:26:51,212 INFO  [main] hive.metastore: Connected to metastore."",
        ""2017-01-09 11:26:51,429 INFO  [main] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0xd1d8e1a connecting to ZooKeeper ensemble=zk1.example.com:2181,zk2.example.com:2181,livyhost.example.com:2181,resource-manager.example.com:2181,zk3.example.com:2181"",
        ""2017-01-09 11:26:51,433 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-258--1, built on 04/25/2016 05:22 GMT"",
        ""2017-01-09 11:26:51,433 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=livyhost.example.com"",
        ""2017-01-09 11:26:51,433 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.8.0_73"",
        ""2017-01-09 11:26:51,433 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation"",
        ""2017-01-09 11:26:51,433 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home=/x/java/jdk1.8.0_73/jre"",
        ""2017-01-09 11:26:51,433 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path=/usr/hdp/current/hbase-client/lib/microsoft-windowsazure-storage-sdk-0.6.0.jar:/usr/hdp/current/hbase-client/lib/curator-client-2.7.1.jar:/usr/hdp/current/hbase-client/lib/ranger-hbase-plugin-impl/ranger_solrj-0.5.0.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/ranger-hbase-plugin-impl/ranger-plugins-cred-0.5.0.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/ranger-hbase-plugin-impl/javax.persistence-2.1.0.jar:/usr/hdp/current/hbase-client/lib/ranger-hbase-plugin-impl/gson-2.2.4.jar:/usr/hdp/current/hbase-client/lib/ranger-hbase-plugin-impl/ranger-plugins-common-0.5.0.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/ranger-hbase-plugin-impl/ranger-plugins-audit-0.5.0.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/ranger-hbase-plugin-impl/eclipselink-2.5.2-M1.jar:/usr/hdp/current/hbase-client/lib/ranger-hbase-plugin-impl/noggit-0.6.jar:/usr/hdp/current/hbase-client/lib/ranger-hbase-plugin-impl/httpmime-4.2.5.jar:/usr/hdp/current/hbase-client/lib/ranger-hbase-plugin-impl/ranger-hbase-plugin-0.5.0.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/servlet-api-2.5.jar:/usr/hdp/current/hbase-client/lib/commons-cli-1.2.jar:/usr/hdp/current/hbase-client/lib/log4j-1.2.17.jar:/usr/hdp/current/hbase-client/lib/ranger-hbase-plugin-shim-0.5.0.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/javax.inject-1.jar:/usr/hdp/current/hbase-client/lib/jersey-server-1.9.jar:/usr/hdp/current/hbase-client/lib/hbase-shell-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/libthrift-0.9.0.jar:/usr/hdp/current/hbase-client/lib/jaxb-api-2.2.2.jar:/usr/hdp/current/hbase-client/lib/jersey-guice-1.9.jar:/usr/hdp/current/hbase-client/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/hdp/current/hbase-client/lib/hbase-it-1.1.2.2.4.2.0-258-tests.jar:/usr/hdp/current/hbase-client/lib/jettison-1.3.3.jar:/usr/hdp/current/hbase-client/lib/asm-3.1.jar:/usr/hdp/current/hbase-client/lib/jetty-util-6.1.26.hwx.jar:/usr/hdp/current/hbase-client/lib/commons-httpclient-3.1.jar:/usr/hdp/current/hbase-client/lib/hbase-server-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/jsp-api-2.1-6.1.14.jar:/usr/hdp/current/hbase-client/lib/slf4j-api-1.7.7.jar:/usr/hdp/current/hbase-client/lib/hbase-hadoop2-compat-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/commons-math3-3.1.1.jar:/usr/hdp/current/hbase-client/lib/okhttp-2.4.0.jar:/usr/hdp/current/hbase-client/lib/findbugs-annotations-1.3.9-1.jar:/usr/hdp/current/hbase-client/lib/hbase-client-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/jsr305-1.3.9.jar:/usr/hdp/current/hbase-client/lib/junit-4.11.jar:/usr/hdp/current/hbase-client/lib/hbase-it-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/paranamer-2.3.jar:/usr/hdp/current/hbase-client/lib/jasper-runtime-5.5.23.jar:/usr/hdp/current/hbase-client/lib/commons-daemon-1.0.13.jar:/usr/hdp/current/hbase-client/lib/jersey-client-1.9.jar:/usr/hdp/current/hbase-client/lib/okio-1.4.0.jar:/usr/hdp/current/hbase-client/lib/commons-configuration-1.6.jar:/usr/hdp/current/hbase-client/lib/commons-collections-3.2.2.jar:/usr/hdp/current/hbase-client/lib/apacheds-i18n-2.0.0-M15.jar:/usr/hdp/current/hbase-client/lib/guice-3.0.jar:/usr/hdp/current/hbase-client/lib/hbase-protocol-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/xml-apis-1.3.04.jar:/usr/hdp/current/hbase-client/lib/hbase-common-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/guava-12.0.1.jar:/usr/hdp/current/hbase-client/lib/protobuf-java-2.5.0.jar:/usr/hdp/current/hbase-client/lib/jersey-core-1.9.jar:/usr/hdp/current/hbase-client/lib/aopalliance-1.0.jar:/usr/hdp/current/hbase-client/lib/guice-servlet-3.0.jar:/usr/hdp/current/hbase-client/lib/commons-lang-2.6.jar:/usr/hdp/current/hbase-client/lib/avro-1.7.4.jar:/usr/hdp/current/hbase-client/lib/commons-beanutils-core-1.8.0.jar:/usr/hdp/current/hbase-client/lib/commons-compress-1.4.1.jar:/usr/hdp/current/hbase-client/lib/hbase-annotations-1.1.2.2.4.2.0-258-tests.jar:/usr/hdp/current/hbase-client/lib/jamon-runtime-2.3.1.jar:/usr/hdp/current/hbase-client/lib/xz-1.0.jar:/usr/hdp/current/hbase-client/lib/hbase-procedure-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/hbase-server-1.1.2.2.4.2.0-258-tests.jar:/usr/hdp/current/hbase-client/lib/activation-1.1.jar:/usr/hdp/current/hbase-client/lib/httpclient-4.2.5.jar:/usr/hdp/current/hbase-client/lib/commons-net-3.1.jar:/usr/hdp/current/hbase-client/lib/gson-2.2.4.jar:/usr/hdp/current/hbase-client/lib/joni-2.1.2.jar:/usr/hdp/current/hbase-client/lib/jsp-2.1-6.1.14.jar:/usr/hdp/current/hbase-client/lib/java-xmlbuilder-0.4.jar:/usr/hdp/current/hbase-client/lib/hbase-resource-bundle-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/curator-framework-2.7.1.jar:/usr/hdp/current/hbase-client/lib/xercesImpl-2.9.1.jar:/usr/hdp/current/hbase-client/lib/jsch-0.1.42.jar:/usr/hdp/current/hbase-client/lib/hbase-rest-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/hbase-hadoop-compat-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/servlet-api-2.5-6.1.14.jar:/usr/hdp/current/hbase-client/lib/disruptor-3.3.0.jar:/usr/hdp/current/hbase-client/lib/commons-io-2.4.jar:/usr/hdp/current/hbase-client/lib/commons-logging-1.2.jar:/usr/hdp/current/hbase-client/lib/jcodings-1.0.8.jar:/usr/hdp/current/hbase-client/lib/jaxb-impl-2.2.3-1.jar:/usr/hdp/current/hbase-client/lib/jetty-sslengine-6.1.26.hwx.jar:/usr/hdp/current/hbase-client/lib/hbase-annotations-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/jersey-json-1.9.jar:/usr/hdp/current/hbase-client/lib/commons-codec-1.9.jar:/usr/hdp/current/hbase-client/lib/jets3t-0.9.0.jar:/usr/hdp/current/hbase-client/lib/commons-digester-1.8.jar:/usr/hdp/current/hbase-client/lib/metrics-core-2.2.0.jar:/usr/hdp/current/hbase-client/lib/hbase-common-1.1.2.2.4.2.0-258-tests.jar:/usr/hdp/current/hbase-client/lib/hbase-thrift-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/api-asn1-api-1.0.0-M20.jar:/usr/hdp/current/hbase-client/lib/jruby-complete-1.6.8.jar:/usr/hdp/current/hbase-client/lib/spymemcached-2.11.6.jar:/usr/hdp/current/hbase-client/lib/hbase-prefix-tree-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/leveldbjni-all-1.8.jar:/usr/hdp/current/hbase-client/lib/jasper-compiler-5.5.23.jar:/usr/hdp/current/hbase-client/lib/commons-el-1.0.jar:/usr/hdp/current/hbase-client/lib/ranger-plugin-classloader-0.5.0.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/xmlenc-0.52.jar:/usr/hdp/current/hbase-client/lib/commons-beanutils-1.7.0.jar:/usr/hdp/current/hbase-client/lib/jetty-6.1.26.hwx.jar:/usr/hdp/current/hbase-client/lib/curator-recipes-2.7.1.jar:/usr/hdp/current/hbase-client/lib/hbase-examples-1.1.2.2.4.2.0-258.jar:/usr/hdp/current/hbase-client/lib/netty-all-4.0.23.Final.jar:/usr/hdp/current/hbase-client/lib/snappy-java-1.0.4.1.jar:/usr/hdp/current/hbase-client/lib/api-util-1.0.0-M20.jar:/usr/hdp/current/hbase-client/lib/commons-math-2.2.jar:/usr/hdp/current/hbase-client/lib/httpcore-4.2.5.jar:/usr/hdp/current/hbase-client/lib/htrace-core-3.1.0-incubating.jar:/etc/hbase/conf/:/etc/hbase/conf/hbase-site.xml:/etc/spark/conf/hive-site.xml:/home/livy/scaas/spark/conf/:/home/livy/scaas/spark/lib/spark-assembly-1.6.1.2.4.2.0-258-hadoop2.7.1.2.4.2.0-258.jar:/home/livy/scaas/spark/lib/datanucleus-api-jdo-3.2.6.jar:/home/livy/scaas/spark/lib/datanucleus-core-3.2.10.jar:/home/livy/scaas/spark/lib/datanucleus-rdbms-3.2.9.jar:/etc/hadoop/conf/"",
        ""2017-01-09 11:26:51,433 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib"",
        ""2017-01-09 11:26:51,433 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp"",
        ""2017-01-09 11:26:51,433 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>"",
        ""2017-01-09 11:26:51,433 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name=Linux"",
        ""2017-01-09 11:26:51,433 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch=amd64"",
        ""2017-01-09 11:26:51,434 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version=2.6.32-504.30.3.el6.x86_64"",
        ""2017-01-09 11:26:51,434 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name=livy"",
        ""2017-01-09 11:26:51,434 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home=/home/livy"",
        ""2017-01-09 11:26:51,434 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir=/home/livy/scaas/livy_interactive/bin"",
        ""2017-01-09 11:26:51,434 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString=zk1.example.com:2181,zk2.example.com:2181,livyhost.example.com:2181,resource-manager.example.com:2181,zk3.example.com:2181 sessionTimeout=90000 watcher=hconnection-0xd1d8e1a0x0, quorum=zk1.example.com:2181,zk2.example.com:2181,livyhost.example.com:2181,resource-manager.example.com:2181,zk3.example.com:2181, baseZNode=/hbase-secure"",
        ""2017-01-09 11:26:51,446 INFO  [main-SendThread(livyhost.example.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server livyhost.example.com/10.196.187.47:2181. Will not attempt to authenticate using SASL (unknown error)"",
        ""2017-01-09 11:26:51,447 INFO  [main-SendThread(livyhost.example.com:2181)] zookeeper.ClientCnxn: Socket connection established to livyhost.example.com/10.196.187.47:2181, initiating session"",
        ""2017-01-09 11:26:51,450 INFO  [main-SendThread(livyhost.example.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server livyhost.example.com/10.196.187.47:2181, sessionid = 0x458f6d8aa78264b, negotiated timeout = 40000"",
        ""2017-01-09 11:26:51,644 INFO  [main] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x458f6d8aa78264b"",
        ""2017-01-09 11:26:51,645 INFO  [main] zookeeper.ZooKeeper: Session: 0x458f6d8aa78264b closed"",
        ""2017-01-09 11:26:51,645 INFO  [main-EventThread] zookeeper.ClientCnxn: EventThread shut down"",
        ""2017-01-09 11:26:51,656 INFO  [main] yarn.YarnSparkHadoopUtil: Added HBase security token to credentials."",
        ""2017-01-09 11:26:51,669 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/hdp/spark-hdp-assembly-1.6.1.jar"",
        ""2017-01-09 11:26:51,717 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/livy-audit-log-0.3.0-SNAPSHOT.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/livy-audit-log-0.3.0-SNAPSHOT.jar"",
        ""2017-01-09 11:26:52,099 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/livy-rsc-0.3.0-SNAPSHOT.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/livy-rsc-0.3.0-SNAPSHOT.jar"",
        ""2017-01-09 11:26:52,124 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/terajdbc4-14.10.00.39.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/terajdbc4-14.10.00.39.jar"",
        ""2017-01-09 11:26:52,149 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/mysql-connector-java-5.1.24.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/mysql-connector-java-5.1.24.jar"",
        ""2017-01-09 11:26:52,171 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/datanucleus-api-jdo-3.2.6.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/datanucleus-api-jdo-3.2.6.jar"",
        ""2017-01-09 11:26:52,192 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/teradmhelper-1.07.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/teradmhelper-1.07.jar"",
        ""2017-01-09 11:26:52,217 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/datanucleus-core-3.2.10.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/datanucleus-core-3.2.10.jar"",
        ""2017-01-09 11:26:52,242 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/datanucleus-rdbms-3.2.9.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/datanucleus-rdbms-3.2.9.jar"",
        ""2017-01-09 11:26:52,264 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/logging-0.1.0.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/logging-0.1.0.jar"",
        ""2017-01-09 11:26:52,342 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/netty-all-4.0.29.Final.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/netty-all-4.0.29.Final.jar"",
        ""2017-01-09 11:26:52,364 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/livy-api-0.3.0-SNAPSHOT.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/livy-api-0.3.0-SNAPSHOT.jar"",
        ""2017-01-09 11:26:52,382 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/tdgssconfig-14.10.00.39.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/tdgssconfig-14.10.00.39.jar"",
        ""2017-01-09 11:26:52,401 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/depend-jars/datalakeudf_2.10-0.1.jar"",
        ""2017-01-09 11:26:52,405 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/depend-jars/datanucleus-api-jdo-3.2.6.jar"",
        ""2017-01-09 11:26:52,411 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/depend-jars/datanucleus-core-3.2.10.jar"",
        ""2017-01-09 11:26:52,415 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/depend-jars/datanucleus-rdbms-3.2.9.jar"",
        ""2017-01-09 11:26:52,418 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/depend-jars/scaas-logger-0.1.0-jar-with-dependencies.jar"",
        ""2017-01-09 11:26:52,421 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/depend-jars/scaas-etl_2.10-0.1.0.jar"",
        ""2017-01-09 11:26:52,424 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/ST4-4.0.4.jar"",
        ""2017-01-09 11:26:52,474 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/calcite-avatica-1.2.0-incubating.jar"",
        ""2017-01-09 11:26:52,477 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/calcite-core-1.2.0-incubating.jar"",
        ""2017-01-09 11:26:52,480 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/calcite-linq4j-1.2.0-incubating.jar"",
        ""2017-01-09 11:26:52,483 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/commons-beanutils-1.7.0.jar"",
        ""2017-01-09 11:26:52,486 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/commons-beanutils-core-1.8.0.jar"",
        ""2017-01-09 11:26:52,489 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/commons-cli-1.2.jar"",
        ""2017-01-09 11:26:52,529 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/commons-pool-1.5.4.jar"",
        ""2017-01-09 11:26:52,532 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/curator-client-2.6.0.jar"",
        ""2017-01-09 11:26:52,535 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/curator-framework-2.6.0.jar"",
        ""2017-01-09 11:26:52,538 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/curator-recipes-2.4.0.jar"",
        ""2017-01-09 11:26:52,561 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/gson-2.2.4.jar"",
        ""2017-01-09 11:26:52,563 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/guava-15.0.jar"",
        ""2017-01-09 11:26:52,594 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-ant-2.0.1.jar"",
        ""2017-01-09 11:26:52,596 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-common-2.0.1.jar"",
        ""2017-01-09 11:26:52,599 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-exec-2.0.1.jar"",
        ""2017-01-09 11:26:52,601 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-hplsql-2.0.1.jar"",
        ""2017-01-09 11:26:52,603 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-llap-client-2.0.1.jar"",
        ""2017-01-09 11:26:52,606 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-llap-common-2.0.1.jar"",
        ""2017-01-09 11:26:52,608 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-llap-tez-2.0.1.jar"",
        ""2017-01-09 11:26:52,611 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-orc-2.0.1.jar"",
        ""2017-01-09 11:26:52,613 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-shims-0.23-2.0.1.jar"",
        ""2017-01-09 11:26:52,615 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-shims-2.0.1.jar"",
        ""2017-01-09 11:26:52,618 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-shims-common-2.0.1.jar"",
        ""2017-01-09 11:26:52,620 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-shims-scheduler-2.0.1.jar"",
        ""2017-01-09 11:26:52,622 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/hive-storage-api-2.0.1.jar"",
        ""2017-01-09 11:26:52,625 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/htrace-core4-4.0.1-incubating.jar"",
        ""2017-01-09 11:26:52,646 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/javax.inject-1.jar"",
        ""2017-01-09 11:26:52,648 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/javax.servlet-3.0.0.v201112011016.jar"",
        ""2017-01-09 11:26:52,650 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/jaxb-api-2.2.2.jar"",
        ""2017-01-09 11:26:52,652 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/jaxb-core-2.2.7.jar"",
        ""2017-01-09 11:26:52,655 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/jaxb-impl-2.2.7.jar"",
        ""2017-01-09 11:26:52,676 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/jline-2.12.jar"",
        ""2017-01-09 11:26:52,679 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/joda-time-2.9.jar"",
        ""2017-01-09 11:26:52,681 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/json-20090211.jar"",
        ""2017-01-09 11:26:52,684 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/jsr305-2.0.1.jar"",
        ""2017-01-09 11:26:52,686 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/leveldbjni-all-1.8.jar"",
        ""2017-01-09 11:26:52,688 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/libthrift-0.9.2.jar"",
        ""2017-01-09 11:26:52,691 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/livy-audit-log-0.3.0-SNAPSHOT.jar"",
        ""2017-01-09 11:26:52,694 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/livy-core_2.10-0.3.0-SNAPSHOT.jar"",
        ""2017-01-09 11:26:52,696 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/livy-repl_2.10-0.3.0-SNAPSHOT.jar"",
        ""2017-01-09 11:26:52,710 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/logging-0.1.0.jar"",
        ""2017-01-09 11:26:52,712 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/mail-1.4.1.jar"",
        ""2017-01-09 11:26:52,714 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/metrics-core-3.1.0.jar"",
        ""2017-01-09 11:26:52,717 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/metrics-json-3.1.2.jar"",
        ""2017-01-09 11:26:52,719 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/metrics-jvm-3.1.2.jar"",
        ""2017-01-09 11:26:52,721 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/mysql-connector-java-5.1.24.jar"",
        ""2017-01-09 11:26:52,723 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/netty-3.8.0.Final.jar"",
        ""2017-01-09 11:26:52,725 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/oro-2.0.8.jar"",
        ""2017-01-09 11:26:52,727 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/protobuf-java-2.5.0.jar"",
        ""2017-01-09 11:26:52,730 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/slf4j-log4j12-1.7.10.jar"",
        ""2017-01-09 11:26:52,732 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/snappy-0.2.jar"",
        ""2017-01-09 11:26:52,734 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/snappy-java-1.1.2.jar"",
        ""2017-01-09 11:26:52,736 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/stax-api-1.0-2.jar"",
        ""2017-01-09 11:26:52,738 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/stax-api-1.0.1.jar"",
        ""2017-01-09 11:26:52,741 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/stringtemplate-3.2.1.jar"",
        ""2017-01-09 11:26:52,743 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/tdgssconfig-14.10.00.39.jar"",
        ""2017-01-09 11:26:52,745 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/teradmhelper-1.07.jar"",
        ""2017-01-09 11:26:52,747 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/terajdbc4-14.10.00.39.jar"",
        ""2017-01-09 11:26:52,749 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/velocity-1.5.jar"",
        ""2017-01-09 11:26:52,750 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/xmlenc-0.52.jar"",
        ""2017-01-09 11:26:52,752 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/repl-jars/zookeeper-3.4.6.jar"",
        ""2017-01-09 11:26:52,755 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/spark/lib/datanucleus-api-jdo-3.2.6.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/datanucleus-api-jdo-3.2.6.jar"",
        ""2017-01-09 11:26:52,775 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/spark/lib/datanucleus-core-3.2.10.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/datanucleus-core-3.2.10.jar"",
        ""2017-01-09 11:26:52,798 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/spark/lib/datanucleus-rdbms-3.2.9.jar -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/datanucleus-rdbms-3.2.9.jar"",
        ""2017-01-09 11:26:52,820 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/spark/conf/hive-site.xml -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/hive-site.xml"",
        ""2017-01-09 11:26:52,837 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/spark/conf/hbase-site.xml -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/hbase-site.xml"",
        ""2017-01-09 11:26:52,883 INFO  [main] yarn.Client: Uploading resource file:/tmp/spark-6c1630dd-ced1-4a19-856a-b88acd81a433/__spark_conf__1646313007251990503.zip -> hdfs://cluster/user/livyuser/.sparkStaging/application_1483653328024_3002/__spark_conf__1646313007251990503.zip"",
        ""2017-01-09 11:26:52,922 WARN  [main] yarn.Client: spark.yarn.am.extraJavaOptions will not take effect in cluster mode"",
        ""2017-01-09 11:26:52,932 INFO  [main] spark.SecurityManager: Changing view acls to: livy,livyuser"",
        ""2017-01-09 11:26:52,933 INFO  [main] spark.SecurityManager: Changing modify acls to: livy,livyuser"",
        ""2017-01-09 11:26:52,933 INFO  [main] spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(livy, livyuser); users with modify permissions: Set(livy, livyuser)"",
        ""2017-01-09 11:26:52,993 INFO  [main] yarn.Client: Submitting application 3002 to ResourceManager"",
        ""2017-01-09 11:26:53,322 INFO  [main] impl.YarnClientImpl: Submitted application application_1483653328024_3002"",
        ""2017-01-09 11:26:53,323 INFO  [main] yarn.Client: Application report for application_1483653328024_3002 (state: ACCEPTED)"",
        ""2017-01-09 11:26:53,325 INFO  [main] yarn.Client: "",
        ""\t client token: N/A"",
        ""\t diagnostics: N/A"",
        ""\t ApplicationMaster host: N/A"",
        ""\t ApplicationMaster RPC port: -1"",
        ""\t queue: default"",
        ""\t start time: 1483990013108"",
        ""\t final status: UNDEFINED"",
        ""\t tracking URL: http://zk2.example.com:8088/proxy/application_1483653328024_3002/"",
        ""\t user: livyuser"",
        ""2017-01-09 11:26:53,328 INFO  [Thread-5] util.ShutdownHookManager: Shutdown hook called"",
        ""2017-01-09 11:26:53,329 INFO  [Thread-5] util.ShutdownHookManager: Deleting directory /tmp/spark-6c1630dd-ced1-4a19-856a-b88acd81a433""
    ]
}
{code};;;","09/Jan/17 22:57;kpraveen;*Design Proposal:*

We have [driveProcess|https://github.com/cloudera/livy/blob/f5dea605bc53ed6ee420e0cc66a3e17457ef6613/rsc/src/main/java/com/cloudera/livy/rsc/ContextLauncher.java#L154] maintained in [ContextLauncher|https://github.com/cloudera/livy/blob/f5dea605bc53ed6ee420e0cc66a3e17457ef6613/rsc/src/main/java/com/cloudera/livy/rsc/ContextLauncher.java]. How about passing it to [SparkApp|https://github.com/cloudera/livy/blob/8b546dc95ba215b6b8f3f9c7d4106e8ce564a006/server/src/main/scala/com/cloudera/livy/utils/SparkApp.scala#L86] and it will be handled like Batch?

If this approach looks good, I can make a PR.;;;","10/Jan/17 02:37;prabhu1984;Dev Team,

Let me know your thoughts on this Jira ticket. We are getting request from user community that sometime they were getting dead state, but it was not returning any log to know why their interactive session didn't start.

Here, is the illustration with production (open source version) vs development (fix version) livy, when interactive session fail to start. 

*+_Scenario:_+* User starts an interactive session with external jar ""file-not-found.jar"" (this jar is not present on hdfs and hence this session fails). On production version, it just say dead and hence user has to go to livy server log file to find the reason. On fixed version, we are returning complete log file. Let us know your thoughts on this fix to send PR.

*+_Production (Open Source Version):_+*

{code}
curl --silent --negotiate -u:$USER prod-livy.company.com:8998/sessions -X POST -H 'Content-Type: application/json' -d '{
  ""kind"":""spark"",
  ""proxyUser"":""prabhu"",
  ""name"":""testingProduction"",
  ""jars"":[""hdfs:///user/livy/depend-jars/file-not-found.jar""]
}' | python -m json.tool
{
    ""id"": 6319,
    ""appId"": null,
    ""owner"": ""prabhu"",
    ""proxyUser"": ""prabhu"",
    ""state"": ""starting"",
    ""kind"": ""spark"",
    ""appInfo"": {
        ""driverLogUrl"": null,
        ""sparkUiUrl"": null
    },
    ""log"": []
}
{code}

{code}
curl --silent --negotiate -u:$USER prod-livy.company.com:8998/sessions/6319 | python -m json.tool
{
    ""id"": 6319,
    ""appId"": null,
    ""owner"": ""prabhu"",
    ""proxyUser"": ""prabhu"",
    ""state"": ""dead"",
    ""kind"": ""spark"",
    ""appInfo"": {
        ""driverLogUrl"": null,
        ""sparkUiUrl"": null
    },
    ""log"": []
}
{code}

{code}
curl --silent --negotiate -u:$USER prod-livy.company.com:8998/sessions/6319/log | python -m json.tool
{
    ""id"": 6319,
    ""from"": 0,
    ""total"": 0,
    ""log"": []
}
{code}

*+_Development (Fixed Version):_+*

{code}
curl --silent --negotiate -u:$USER dev-livy.company.com/sessions -X POST -H 'Content-Type: application/json' -d '{
  ""kind"":""spark"",
  ""proxyUser"":""prabhu"",
  ""name"":""testingDev"",
  ""jars"":[""hdfs:///user/livy/depend-jars/file-not-found.jar""]
}' | python -m json.tool
{
    ""id"": 36,
    ""appId"": null,
    ""owner"": ""prabhu"",
    ""proxyUser"": ""prabhu"",
    ""state"": ""starting"",
    ""kind"": ""spark"",
    ""appInfo"": {
        ""driverLogUrl"": null,
        ""sparkUiUrl"": null
    },
    ""log"": []
}
{code}

{code}
curl --silent --negotiate -u:$USER dev-livy.company.com/sessions/36 | python -m json.tool
{
    ""id"": 36,
    ""appId"": null,
    ""owner"": ""prabhu"",
    ""proxyUser"": ""prabhu"",
    ""state"": ""dead"",
    ""kind"": ""spark"",
    ""appInfo"": {
        ""driverLogUrl"": null,
        ""sparkUiUrl"": null
    },
    ""log"": [
        ""\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)"",
        ""\tat org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:163)"",
        ""\tat org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:161)"",
        ""\tat java.security.AccessController.doPrivileged(Native Method)"",
        ""\tat javax.security.auth.Subject.doAs(Subject.java:422)"",
        ""\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)"",
        ""\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:161)"",
        ""\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)"",
        ""\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)"",
        ""\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)""
    ]
}
{code}

{code}
curl --silent --negotiate -u:$USER dev-livy.company.com/sessions/36/log | python -m json.tool
{
    ""id"": 36,
    ""from"": 0,
    ""total"": 92,
    ""log"": [
        ""2017-01-09 18:15:16,923 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"",
        ""2017-01-09 18:15:17,497 INFO  [main] impl.TimelineClientImpl: Timeline service address: http://host0005.lvs.company.com:8188/ws/v1/timeline/"",
        ""2017-01-09 18:15:18,163 WARN  [main] shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded."",
        ""2017-01-09 18:15:18,626 INFO  [main] client.ConfiguredRMFailoverProxyProvider: Failing over to rm2"",
        ""2017-01-09 18:15:18,648 INFO  [main] yarn.Client: Requesting a new application from cluster with 8 NodeManagers"",
        ""2017-01-09 18:15:18,660 INFO  [main] yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (307200 MB per container)"",
        ""2017-01-09 18:15:18,661 INFO  [main] yarn.Client: Will allocate AM container, with 4505 MB memory including 409 MB overhead"",
        ""2017-01-09 18:15:18,661 INFO  [main] yarn.Client: Setting up container launch context for our AM"",
        ""2017-01-09 18:15:18,662 INFO  [main] yarn.Client: Setting up the launch environment for our AM container"",
        ""2017-01-09 18:15:18,688 INFO  [main] yarn.Client: Preparing resources for our AM container"",
        ""2017-01-09 18:15:18,704 INFO  [main] yarn.YarnSparkHadoopUtil: getting token for namenode: hdfs://cluster/user/prabhu/.sparkStaging/application_1483653328024_3036"",
        ""2017-01-09 18:15:18,709 INFO  [main] hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 281633 for prabhu on ha-hdfs:cluster"",
        ""2017-01-09 18:15:19,317 INFO  [main] hive.metastore: Trying to connect to metastore with URI thrift://host0003.lvs.company.com:9083"",
        ""2017-01-09 18:15:19,348 INFO  [main] hive.metastore: Connected to metastore."",
        ""2017-01-09 18:15:19,820 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/hdp/spark-hdp-assembly-1.6.1.jar"",
        ""2017-01-09 18:15:19,865 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/livy-audit-log-0.3.0-SNAPSHOT.jar -> hdfs://cluster/user/prabhu/.sparkStaging/application_1483653328024_3036/livy-audit-log-0.3.0-SNAPSHOT.jar"",
        ""2017-01-09 18:15:20,002 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/livy-rsc-0.3.0-SNAPSHOT.jar -> hdfs://cluster/user/prabhu/.sparkStaging/application_1483653328024_3036/livy-rsc-0.3.0-SNAPSHOT.jar"",
        ""2017-01-09 18:15:20,074 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/datanucleus-api-jdo-3.2.6.jar -> hdfs://cluster/user/prabhu/.sparkStaging/application_1483653328024_3036/datanucleus-api-jdo-3.2.6.jar"",
        ""2017-01-09 18:15:20,116 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/datanucleus-core-3.2.10.jar -> hdfs://cluster/user/prabhu/.sparkStaging/application_1483653328024_3036/datanucleus-core-3.2.10.jar"",
        ""2017-01-09 18:15:20,140 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/datanucleus-rdbms-3.2.9.jar -> hdfs://cluster/user/prabhu/.sparkStaging/application_1483653328024_3036/datanucleus-rdbms-3.2.9.jar"",
        ""2017-01-09 18:15:20,165 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/logging-0.1.0.jar -> hdfs://cluster/user/prabhu/.sparkStaging/application_1483653328024_3036/logging-0.1.0.jar"",
        ""2017-01-09 18:15:20,230 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/netty-all-4.0.29.Final.jar -> hdfs://cluster/user/prabhu/.sparkStaging/application_1483653328024_3036/netty-all-4.0.29.Final.jar"",
        ""2017-01-09 18:15:20,334 INFO  [main] yarn.Client: Uploading resource file:/home/livy/scaas/livy_interactive/rsc-jars/livy-api-0.3.0-SNAPSHOT.jar -> hdfs://cluster/user/prabhu/.sparkStaging/application_1483653328024_3036/livy-api-0.3.0-SNAPSHOT.jar"",
        ""2017-01-09 18:15:20,370 INFO  [main] yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/livy/depend-jars/file-not-found.jar"",
        ""2017-01-09 18:15:20,373 INFO  [main] yarn.Client: Deleting staging directory .sparkStaging/application_1483653328024_3036"",
        ""Exception in thread \""main\"" java.io.FileNotFoundException: File does not exist: hdfs://cluster/user/livy/depend-jars/file-not-found.jar"",
        ""\tat org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:134)"",
        ""\tat org.apache.hadoop.fs.AbstractFileSystem.resolvePath(AbstractFileSystem.java:467)"",
        ...........................
        ""\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)""
    ]
}
-bash-4.1$
{code}

Thanks!;;;","21/Jan/17 01:05;tc0312;This's great.. We will appreciate if you send a PR.;;;","22/Feb/17 00:31;kpraveen;Made a PR [#297|https://github.com/cloudera/livy/pull/297]

Please give your comments;;;","22/Feb/17 22:16;tc0312;Thank you for your contribution :).;;;","09/Mar/17 05:48;jerryshao;[~kpraveen] due to the limited access permission of this JIRA, I could find your name on the Assignee list, could you please assign this JIRA to yourself?;;;","09/Mar/17 07:09;kpraveen;[~jerryshao] I am not able to assign the issue to myself. Can you please check the permissions once.;;;","09/Mar/17 07:19;jerryshao;Unfortunately I don't have the right to check permission. You'd better ask Cloudera guys to give you some JIRA permissions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Python Job API bug and and Python / Java Job API examples,LIVY-292,13095930,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,06/Jan/17 07:02,16/Jan/17 07:32,19/Dec/25 04:15,16/Jan/17 07:32,0.3,,,0.3,,API,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-01-06 07:02:47.0,,,,,,,,,,"0|i3j093:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy interactive session statement status is ok when there is an error,LIVY-290,13095928,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,jeffersonezra,gmcdonald,05/Jan/17 22:20,22/Feb/17 22:23,19/Dec/25 04:15,22/Feb/17 22:22,0.3,,,,,REPL,,,,,,,,,,1,,,,,,"When there is an error in statement execution, the status of the statement is set to ok instead of error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-01-05 22:20:14.0,,,,,,,,,,"0|i3j08n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NPE in ContextLauncher when Driver is not successfully started,LIVY-289,13095927,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,04/Jan/17 03:27,12/Jan/17 06:26,19/Dec/25 04:15,12/Jan/17 06:26,0.3,,,0.3,,RSC,,,,,,,,,,0,,,,,,"In the current code, if {{ContextLauncher#startDriver}} doesn't start correctly, it will trigger a listener with {{child}} be empty, this will introduce NPE. So here we should check whether {{child}} is null or not.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-01-04 03:27:14.0,,,,,,,,,,"0|i3j08f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Travis build keep timing out,LIVY-288,13095926,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,tc0312,gmcdonald,03/Jan/17 23:00,04/Jan/17 03:30,19/Dec/25 04:15,04/Jan/17 03:30,0.3,,,,,Tests,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jan 04 03:30:18 UTC 2017,,,,,,,,,,"0|i3j087:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Jan/17 03:29;jerryshao;Hi Alex, I've already created a JIRA to track this (LIVY-285). Let's close this as duplicated. Thanks.;;;","04/Jan/17 03:30;jerryshao;Dup with LIVY-285.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make LivyConf consistent before cutting 0.3,LIVY-287,13095925,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,tc0312,gmcdonald,03/Jan/17 22:00,05/Apr/17 23:37,19/Dec/25 04:15,05/Apr/17 23:37,0.3,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"Names of LivyConfs are inconsistent. e.g.:
livy.spark.scalaVersion
staging-dir
csrf_protection.enabled",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 24 00:43:04 UTC 2017,,,,,,,,,,"0|i3j07z:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/Feb/17 23:54;ajbozarth;[~tc0312] I can take this, Is there a preference for which naming convention to use? CamelCase, dash or underscore? I'm preferential to camelCase myself.;;;","22/Feb/17 22:15;tc0312;Me 2 and agree. I think in general we try to keep convention the same as Spark's. Thanks for taking this. Kudos.;;;","24/Feb/17 00:43;ajbozarth;PR: https://github.com/cloudera/livy/pull/300;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the docs to align with Livy 0.3,LIVY-286,13095924,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,03/Jan/17 09:37,12/Jan/17 06:26,19/Dec/25 04:15,12/Jan/17 06:26,0.3,,,0.3,,Docs,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2017-01-03 09:37:15.0,,,,,,,,,,"0|i3j07r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consider handling travis unit test timeout,LIVY-285,13095923,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,26/Dec/16 09:40,12/Jan/17 06:27,19/Dec/25 04:15,12/Jan/17 06:27,0.3,,,0.3,,Tests,,,,,,,,,,0,,,,,,"With the increase of project size, more and more tests are added, now the unit test is failed frequently due to travis job timeout (50 mins by default). We should consider improving the speed of unit test - mainly integration test.

https://docs.travis-ci.com/user/customizing-the-build/#Build-Timeouts",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-26 09:40:17.0,,,,,,,,,,"0|i3j07j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Livy web server request log retained days configurable,LIVY-284,13095922,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,jerryshao,jerryshao,gmcdonald,26/Dec/16 07:32,27/Dec/16 03:20,19/Dec/25 04:15,27/Dec/16 03:20,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,"Current Livy server will log the Http request in a daily manner, by default these log's retained days is 31 days, we'd better make this retained days configurable to limit the log size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-26 07:32:01.0,,,,,,,,,,"0|i3j07b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix PySpark Session cannot start issue,LIVY-282,13095920,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,22/Dec/16 02:54,23/Dec/16 06:01,19/Dec/25 04:15,23/Dec/16 06:01,0.3,,,0.3,,REPL,,,,,,,,,,0,,,,,,"Due to the bug in LIVY-233 Python Job API, if Livy is running with Spark version which doesn't have {{SparkSession}}, pyspark session will be failed to start.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-22 02:54:56.0,,,,,,,,,,"0|i3j06v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
com.cloudera.livy:livy-core_2.10:jar:0.3.0-SNAPSHOT is missing,LIVY-281,13095919,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,hsareen,gmcdonald,20/Dec/16 08:35,22/Dec/16 02:47,19/Dec/25 04:15,22/Dec/16 02:47,0.1,0.3,,,,REPL,,,,,,,,,,0,livy,Repl,repl_2.10,,,"Hey,

While trying to build livy on amaozn EMR box facing following issue : 

[INFO] ------------------------------------------------------------------------
[INFO] Building livy-repl_2.10 0.3.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
Downloading: https://repository.cloudera.com/artifactory/libs-snapshot-local/com/cloudera/livy/livy-rsc/0.3.0-SNAPSHOT/maven-metadata.xml
Downloaded: https://repository.cloudera.com/artifactory/libs-snapshot-local/com/cloudera/livy/livy-rsc/0.3.0-SNAPSHOT/maven-metadata.xml (754 B at 0.4 KB/sec)
Downloading: https://repository.cloudera.com/artifactory/libs-snapshot-local/com/cloudera/livy/livy-rsc/0.3.0-SNAPSHOT/livy-rsc-0.3.0-20160531.184046-1.pom
Downloaded: https://repository.cloudera.com/artifactory/libs-snapshot-local/com/cloudera/livy/livy-rsc/0.3.0-SNAPSHOT/livy-rsc-0.3.0-20160531.184046-1.pom (22 KB at 164.7 KB/sec)
Downloading: https://repository.cloudera.com/artifactory/libs-snapshot-local/com/cloudera/livy/livy-main/0.3.0-SNAPSHOT/maven-metadata.xml
Downloading: https://repository.cloudera.com/artifactory/libs-snapshot-local/com/cloudera/livy/livy-main/0.3.0-SNAPSHOT/livy-main-0.3.0-SNAPSHOT.pom
Downloading: https://repository.cloudera.com/artifactory/libs-snapshot-local/com/cloudera/livy/livy-core_2.10/0.3.0-SNAPSHOT/livy-core_2.10-0.3.0-SNAPSHOT.pom
[WARNING] The POM for com.cloudera.livy:livy-core_2.10:jar:0.3.0-SNAPSHOT is missing, no dependency information available
[WARNING] The POM for com.cloudera.livy:livy-core_2.10:jar:tests:0.3.0-SNAPSHOT is missing, no dependency information available
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] livy-repl_2.10 ..................................... FAILURE [ 18.785 s]


Following link doesn't have 0.3.0 snapshot.
https://repository.cloudera.com/artifactory/libs-snapshot-local/com/cloudera/livy/livy-repl_2.10/

Regards,
Himanshu Sareen",Redhat/CentOS - Amazon EMR,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 22 02:47:01 UTC 2016,,,,,,,,,,"0|i3j06n:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Dec/16 08:52;zjffdu;Please use this repository for livy 0.2. And livy currently don't publish snapshot jars.  

https://oss.sonatype.org/content/repositories/releases
;;;","22/Dec/16 02:47;jerryshao;I think you have to build from Livy project root. Currently there's no snapshot artifacts in the repo.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy session leakage issue when app submission timeout,LIVY-280,13095918,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,17/Dec/16 03:13,12/Jan/17 06:27,19/Dec/25 04:15,12/Jan/17 06:27,0.3,,,0.3,,Core,,,,,,,,,,0,,,,,,"Sometimes we see no yarn app tagged exception. This is due to it takes a little long time to launch the yarn app (cluster is not busy). But the yarn app finally is still launched. That means the livy session is leaked. 

{code}
java.lang.Exception: No YARN application is tagged with livy-session-1-sfqqmw85.
com.cloudera.livy.utils.SparkYarnApp.com$cloudera$livy$utils$SparkYarnApp$$getAppIdFromTag(SparkYarnApp.scala:131) com.cloudera.livy.utils.SparkYarnApp$$anonfun$1$$anonfun$4.apply(SparkYarnApp.scala:195) com.cloudera.livy.utils.SparkYarnApp$$anonfun$1$$anonfun$4.apply(SparkYarnApp.scala:192) scala.Option.getOrElse(Option.scala:120) com.cloudera.livy.utils.SparkYarnApp$$anonfun$1.apply$mcV$sp(SparkYarnApp.scala:192) com.cloudera.livy.Utils$$anon$1.run(Utils.scala:95)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-17 03:13:09.0,,,,,,,,,,"0|i3j06f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Spark 2.1,LIVY-279,13095917,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,zjffdu,gmcdonald,16/Dec/16 05:11,21/Dec/16 04:32,19/Dec/25 04:15,21/Dec/16 04:32,0.3,,,0.3,,Core,,,,,,,,,,0,,,,,,Spark 2.1 will be released soon. we need the update the max version supported,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-16 05:11:45.0,,,,,,,,,,"0|i3j067:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTTP Auth support for Python LivyClient,LIVY-277,13095915,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Done,,tc0312,gmcdonald,14/Dec/16 02:46,20/Feb/17 08:12,19/Dec/25 04:15,20/Feb/17 08:12,0.3,,,,,RSC,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 20 08:10:53 UTC 2017,,,,,,,,,,"0|i3j05r:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Feb/17 08:10;jerryshao;Since Python already support this, I will close this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InteractiveIT.recover interactive session occasionally gets stuck,LIVY-276,13095914,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,14/Dec/16 01:54,12/Jan/17 06:28,19/Dec/25 04:15,12/Jan/17 06:28,0.3,,,0.3,,Tests,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-14 01:54:07.0,,,,,,,,,,"0|i3j05j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add rest api to detect livy version,LIVY-275,13095913,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,zjffdu,gmcdonald,13/Dec/16 09:05,26/Dec/16 00:26,19/Dec/25 04:15,26/Dec/16 00:26,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,We may has rest api changes from version to version. It is helpful for the client to detect which version of the livy they are using.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-13 09:05:39.0,,,,,,,,,,"0|i3j05b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add travis CI status in README,LIVY-274,13095912,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jonalter,zjffdu,gmcdonald,13/Dec/16 06:39,20/Feb/17 08:10,19/Dec/25 04:15,20/Feb/17 08:10,0.3,,,0.4.0,,Docs,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 14 20:02:11 UTC 2017,,,,,,,,,,"0|i3j053:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Feb/17 20:02;jonalter;PR: [#288|https://github.com/cloudera/livy/pull/288];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add spnego support for Livy Java/Scala and Python client,LIVY-273,13095911,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,12/Dec/16 12:55,11/Jan/17 09:41,19/Dec/25 04:15,11/Jan/17 09:41,0.3,,,0.3,,API,,,,,,,,,,0,,,,,,"Livy server supports spnego authentication of accessing requests, we should also add supports in the client side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-12 12:55:34.0,,,,,,,,,,"0|i3j04v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add progressInfo to Statement,LIVY-272,13095910,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,zjffdu,gmcdonald,12/Dec/16 06:42,12/May/17 02:19,19/Dec/25 04:15,12/May/17 02:19,0.3,,,0.4.0,,REPL,,,,,,,,,,0,,,,,,"After LIVY-215, we would have jobGroupId for each statement. So that we can get jobProgressInfo using SparkJobListener. It is helpful to add progressInfo to Statement, so that client can see the progress of the job execution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Dec 15 12:24:41 UTC 2016,,,,,,,,,,"0|i3j04n:",9223372036854775807,,,,,,,,,,,,,,,,,,,"15/Dec/16 00:49;tc0312;+1;;;","15/Dec/16 12:24;jerryshao;I'm going to take a crack at this, I'm thinking using SparkListener could also improve the current implementation of statement cancellation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
traceback could be json list or json object which is misleading,LIVY-271,13095909,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,12/Dec/16 03:20,13/Dec/16 05:31,19/Dec/25 04:15,13/Dec/16 05:31,0.3,,,0.3,,REPL,,,,,,,,,,0,,,,,,"This is the 2 statement output I get. You can see that the traceback of the first statement is a json object while the second is a json list. This is misleading to users especially user want to deserialize these json string to java plain object. 
{noformat}
    {
      ""id"": 1,
      ""state"": ""available"",
      ""output"": {
        ""status"": ""error"",
        ""execution_count"": 1,
        ""ename"": ""Error"",
        ""evalue"": ""incomplete statement"",
        ""traceback"": {}
      }
    },
    {
      ""id"": 2,
      ""state"": ""available"",
      ""output"": {
        ""status"": ""error"",
        ""execution_count"": 2,
        ""ename"": ""Error"",
        ""evalue"": ""<console>:26: error: value sm is not a member of org.apache.spark.rdd.RDD[Int]"",
        ""traceback"": [
          ""       sc.parallelize(0 to 100).sm()"",
          ""\n                                ^""
        ]
      }
    }

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-12 03:20:59.0,,,,,,,,,,"0|i3j04f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark %table magic return error when include NoneType value,LIVY-270,13095908,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,purechoc,purechoc,gmcdonald,09/Dec/16 05:36,15/Dec/16 02:27,19/Dec/25 04:15,15/Dec/16 02:27,0.3,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"{code}
x = [{""name"":""a"", ""value"":None}, {""name"":""b"", ""value"":2}]
%table x
{code}
return table rows have different types

if input value is dict type, need to be allow None value.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 09 05:42:31 UTC 2016,,,,,,,,,,"0|i3j047:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Dec/16 05:42;purechoc;description fix : if input value is ""dict"" or ""tuple"" or ""list"" type, it need to be allow None value.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkRInterpreter hangs forever on executing `x[`,LIVY-269,13095907,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,09/Dec/16 02:23,22/Dec/16 08:35,19/Dec/25 04:15,22/Dec/16 08:35,0.3,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"I also noticed that since the R interpreter is not using eval(), R interpreter will hang forever if it executes an incomplete statement. e.g. `x[`.

hyunwoo cho commented that we can fix it using 
{code:none}
tryCatch({eval(parse(text=""yyy""))},error = function(e) {print(""----LIVY_ERROR----"")})
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-09 02:23:18.0,,,,,,,,,,"0|i3j03z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test fail: SparkYarnAppSpec.can kill spark-submit while it's running,LIVY-268,13095906,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,tc0312,tc0312,gmcdonald,09/Dec/16 00:52,09/Dec/16 01:46,19/Dec/25 04:15,09/Dec/16 01:46,0.3,,,0.3,,Tests,,,,,,,,,,0,,,,,,Unit test is not mocking the right thing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-09 00:52:00.0,,,,,,,,,,"0|i3j03r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy sessions/batches are not secured. Any user can stop another user session/batch,LIVY-266,13095904,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Works for Me,,prabhu1984,gmcdonald,07/Dec/16 21:51,30/Nov/18 06:12,19/Dec/25 04:15,22/Feb/17 22:24,0.3,,,,,Core,,,,,,,,,,0,,,,,,"Dev,

Livy session or batches are not currently secured. i.e. User A can start a session or batch and User B can submit code to session started by User A or even stop that session. This is critical issue on secured cluster, when User A is having sensitive data access, there may be a chance User B can access those sensitive datasets through User-A Session.

Here, is an example from our secured cluster.

# Starting session from user ""prabhu""

curl --silent --negotiate -u:prabhu localhost:8998/sessions -X POST -H 'Content-Type: application/json' -d '{
  ""kind"":""scala"",
  ""proxyUser"":""prabhu"",
  ""name"":""Testing""
}' | python -m json.tool

{
    ""id"": 371,
    ""appId"": null,
    ""owner"": ""prabhu"",
    ""proxyUser"": ""prabhu"",
    ""state"": ""starting"",
    ""kind"": ""spark"",
    ""appInfo"": {
        ""driverLogUrl"": null,
        ""sparkUiUrl"": null
    },
    ""log"": []
}

# Executing code to above session by some other user ""don""

curl --silent --negotiate -u:don localhost:8998/sessions/371/statements -X POST -H 'Content-Type: application/json' -d '{
  ""code"":""sc.applicationId""
}' | python -m json.tool
{
    ""id"": 0,
    ""state"": ""available"",
    ""output"": {
        ""status"": ""ok"",
        ""execution_count"": 0,
        ""data"": {
            ""text/plain"": ""res0: String = application_1476926173701_398436""
        }
    }
}

# Stopping above session by different user ""john"" this time

curl --silent --negotiate -u:john localhost:8998/sessions/371 -X DELETE | python -m json.tool
{
    ""msg"": ""deleted""
}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 30 06:12:55 UTC 2018,,,,,,,,,,"0|i3j03b:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Dec/16 20:51;tc0312;Are you using spnego?;;;","12/Dec/16 08:58;jerryshao;Downgrade the priority of this issue. There're several problems related to multi-user access to same session. Like access control, file and data permission, user impersonation, etc. It may not be easy to fix with simple patch. We should carefully think about it.

I think Spnego maybe cannot prevent authenticated users to access same session. Kerberos only authenticate users, doesn't differentiate users, all the authenticated users are equal from Kerberos point. May be a valid solution is to use ACL.
;;;","12/Dec/16 09:42;zjffdu;Do you use the same principal for these 2 users ?;;;","22/Feb/17 22:24;tc0312;Resolving because it works in our test environment. If it isn't working for you, please reopen and post your config. Thanks.;;;","30/Nov/18 06:12;shanyu;[~tc0312] are you saying that all the requests are actually using SPNEGO with identity ""prabhu"", therefore it all works fine. If a different user tries to post statements or kill sessions owned by user ""prabhu"", livy will deny that request?

If knox is used to access livy in a keberized cluster, and knox user is configured as ""livy.superusers"", then proxyUser field is enforced for post /sessions request, However, post statements request to any sessions from knox server to livy server will always be successful because the caller identity is knox user not the end user. How does livy find out who is making the request to it?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.matser can not be override,LIVY-265,13095903,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,zjffdu,zjffdu,gmcdonald,07/Dec/16 03:36,07/Dec/16 04:57,19/Dec/25 04:15,07/Dec/16 04:57,0.3,,,,,Core,,,,,,,,,,0,,,,,,livy.spark.master is always used as the spark.master even when spark.master is excluded in blacklist.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 07 04:57:15 UTC 2016,,,,,,,,,,"0|i3j033:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Dec/16 04:57;zjffdu;HA require spark.master not to be overridden. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support DataFrame when using table magic,LIVY-264,13095902,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,purechoc,purechoc,gmcdonald,06/Dec/16 06:28,09/Dec/16 01:48,19/Dec/25 04:15,09/Dec/16 01:48,0.3,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"table magic in pyspark interpreter

{code}
%table dataframe # is not working.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Dec 06 07:57:10 UTC 2016,,,,,,,,,,"0|i3j02v:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Dec/16 07:57;purechoc;if first row's column is null (age=None)  that magic return 'table rows have different types'
and when using dataframe, cant not setting column header

I want to support type this. (df.collect())
{code}
[Row(age=None, name=u'Michael'), Row(age=30, name=u'Andy'), Row(age=19, name=u'Justin')]
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repl failed to start in Travis build 936.1,LIVY-263,13095901,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,06/Dec/16 06:26,15/Dec/16 00:45,19/Dec/25 04:15,15/Dec/16 00:45,0.3,,,,,REPL,,,,,,,,,,0,,,,,,"livy-server failed session because of some RPC issues in [build 936.1|https://travis-ci.org/cloudera/livy/jobs/180001092]. [log|https://livy.blob.core.windows.net/buildlogs/936.1.zip]

{noformat}
16/11/30 07:08:55.676 RSCClient-4-3: Received result for 31dfa59d-975a-49c1-890d-5b30288fad06
16/11/30 07:08:55.677 RSCClient-4-3: InteractiveSession 3 session state change from starting to running
16/11/30 07:08:58.807 RSCClient-4-3: [ClientProtocol] Failed to find handler for msg 'com.cloudera.livy.rsc.rpc.Rpc$MessageHeader'.
16/11/30 07:08:58.813 RSCClient-4-3: [ClientProtocol] Closing channel due to exception in pipeline (null).
16/11/30 07:08:58.813 RSCClient-4-3: Client RPC channel closed unexpectedly.
16/11/30 07:08:58.813 RSCClient-4-3: Error stopping RPC.
io.netty.util.concurrent.BlockingOperationException: DefaultChannelPromise@47fae43(uncancellable)
	at io.netty.util.concurrent.DefaultPromise.checkDeadLock(DefaultPromise.java:390)
	at io.netty.channel.DefaultChannelPromise.checkDeadLock(DefaultChannelPromise.java:157)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:251)
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:129)
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:28)
	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:218)
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:117)
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:28)
	at com.cloudera.livy.rsc.rpc.Rpc.close(Rpc.java:305)
	at com.cloudera.livy.rsc.RSCClient.stop(RSCClient.java:221)
	at com.cloudera.livy.rsc.RSCClient$2$1.onSuccess(RSCClient.java:118)
	at com.cloudera.livy.rsc.RSCClient$2$1.onSuccess(RSCClient.java:112)
	at com.cloudera.livy.rsc.Utils$2.operationComplete(Utils.java:108)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:406)
	at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:82)
	at io.netty.channel.AbstractChannel$CloseFuture.setClosed(AbstractChannel.java:956)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.doClose0(AbstractChannel.java:608)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.close(AbstractChannel.java:586)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.close(DefaultChannelPipeline.java:1107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeClose(AbstractChannelHandlerContext.java:543)
	at io.netty.channel.AbstractChannelHandlerContext.close(AbstractChannelHandlerContext.java:528)
	at io.netty.channel.ChannelDuplexHandler.close(ChannelDuplexHandler.java:73)
	at io.netty.channel.AbstractChannelHandlerContext.invokeClose(AbstractChannelHandlerContext.java:543)
	at io.netty.channel.AbstractChannelHandlerContext.close(AbstractChannelHandlerContext.java:528)
	at io.netty.channel.AbstractChannelHandlerContext.close(AbstractChannelHandlerContext.java:394)
	at com.cloudera.livy.rsc.rpc.RpcDispatcher.exceptionCaught(RpcDispatcher.java:182)
	at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:246)
	at io.netty.channel.AbstractChannelHandlerContext.notifyHandlerException(AbstractChannelHandlerContext.java:737)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:310)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.handler.codec.ByteToMessageCodec.channelRead(ByteToMessageCodec.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
16/11/30 07:08:58.817 RSCClient-4-3: [ClientProtocol] Expected RPC header, got com.cloudera.livy.rsc.ReplJobResults instead.
16/11/30 07:08:58.817 RSCClient-4-3: [ClientProtocol] Closing channel due to exception in pipeline (null).
16/11/30 07:08:58.817 RSCClient-4-3: [ClientProtocol] Closing RPC channel with 1 outstanding RPCs.
16/11/30 07:08:58.821 qtp1554249927-29: internal error
java.util.concurrent.ExecutionException: java.util.concurrent.CancellationException
	at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37)
	at com.cloudera.livy.server.interactive.InteractiveSession.executeStatement(InteractiveSession.scala:463)
	at com.cloudera.livy.server.interactive.InteractiveSessionServlet$$anonfun$11$$anonfun$apply$6.apply(InteractiveSessionServlet.scala:124)
	at com.cloudera.livy.server.interactive.InteractiveSessionServlet$$anonfun$11$$anonfun$apply$6.apply(InteractiveSessionServlet.scala:123)
	at com.cloudera.livy.server.SessionServlet.doWithSession(SessionServlet.scala:194)
	at com.cloudera.livy.server.SessionServlet.withSession(SessionServlet.scala:187)
	at com.cloudera.livy.server.interactive.InteractiveSessionServlet$$anonfun$11.apply(InteractiveSessionServlet.scala:123)
	at com.cloudera.livy.server.interactive.InteractiveSessionServlet$$anonfun$11.apply(InteractiveSessionServlet.scala:122)
	at com.cloudera.livy.server.JsonServlet.com$cloudera$livy$server$JsonServlet$$doAction(JsonServlet.scala:114)
	at com.cloudera.livy.server.JsonServlet$$anonfun$jpost$1.apply(JsonServlet.scala:76)
	at org.scalatra.ScalatraBase$class.org$scalatra$ScalatraBase$$liftAction(ScalatraBase.scala:270)
	at org.scalatra.ScalatraBase$$anonfun$invoke$1.apply(ScalatraBase.scala:265)
	at org.scalatra.ScalatraBase$$anonfun$invoke$1.apply(ScalatraBase.scala:265)
	at org.scalatra.ApiFormats$class.withRouteMultiParams(ApiFormats.scala:178)
	at com.cloudera.livy.server.JsonServlet.withRouteMultiParams(JsonServlet.scala:40)
	at org.scalatra.ScalatraBase$class.invoke(ScalatraBase.scala:264)
	at org.scalatra.ScalatraServlet.invoke(ScalatraServlet.scala:49)
	at org.scalatra.ScalatraBase$$anonfun$runRoutes$1$$anonfun$apply$8.apply(ScalatraBase.scala:240)
	at org.scalatra.ScalatraBase$$anonfun$runRoutes$1$$anonfun$apply$8.apply(ScalatraBase.scala:238)
	at scala.Option.flatMap(Option.scala:170)
	at org.scalatra.ScalatraBase$$anonfun$runRoutes$1.apply(ScalatraBase.scala:238)
	at org.scalatra.ScalatraBase$$anonfun$runRoutes$1.apply(ScalatraBase.scala:237)
	at scala.collection.immutable.Stream.flatMap(Stream.scala:446)
	at org.scalatra.ScalatraBase$class.runRoutes(ScalatraBase.scala:237)
	at org.scalatra.ScalatraServlet.runRoutes(ScalatraServlet.scala:49)
	at org.scalatra.ScalatraBase$class.runActions$1(ScalatraBase.scala:163)
	at org.scalatra.ScalatraBase$$anonfun$executeRoutes$1.apply$mcV$sp(ScalatraBase.scala:175)
	at org.scalatra.ScalatraBase$$anonfun$executeRoutes$1.apply(ScalatraBase.scala:175)
	at org.scalatra.ScalatraBase$$anonfun$executeRoutes$1.apply(ScalatraBase.scala:175)
	at org.scalatra.ScalatraBase$class.org$scalatra$ScalatraBase$$cradleHalt(ScalatraBase.scala:193)
	at org.scalatra.ScalatraBase$class.executeRoutes(ScalatraBase.scala:175)
	at org.scalatra.ScalatraServlet.executeRoutes(ScalatraServlet.scala:49)
	at org.scalatra.ScalatraBase$$anonfun$handle$1.apply$mcV$sp(ScalatraBase.scala:113)
	at org.scalatra.ScalatraBase$$anonfun$handle$1.apply(ScalatraBase.scala:113)
	at org.scalatra.ScalatraBase$$anonfun$handle$1.apply(ScalatraBase.scala:113)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.scalatra.DynamicScope$class.withResponse(DynamicScope.scala:80)
	at org.scalatra.ScalatraServlet.withResponse(ScalatraServlet.scala:49)
	at org.scalatra.DynamicScope$$anonfun$withRequestResponse$1.apply(DynamicScope.scala:60)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.scalatra.DynamicScope$class.withRequest(DynamicScope.scala:71)
	at org.scalatra.ScalatraServlet.withRequest(ScalatraServlet.scala:49)
	at org.scalatra.DynamicScope$class.withRequestResponse(DynamicScope.scala:59)
	at org.scalatra.ScalatraServlet.withRequestResponse(ScalatraServlet.scala:49)
	at org.scalatra.ScalatraBase$class.handle(ScalatraBase.scala:111)
	at org.scalatra.ScalatraServlet.org$scalatra$servlet$ServletBase$$super$handle(ScalatraServlet.scala:49)
	at org.scalatra.servlet.ServletBase$class.handle(ServletBase.scala:43)
	at com.cloudera.livy.server.SessionServlet.org$scalatra$MethodOverride$$super$handle(SessionServlet.scala:40)
	at org.scalatra.MethodOverride$class.handle(MethodOverride.scala:28)
	at com.cloudera.livy.server.interactive.InteractiveSessionServlet.org$scalatra$servlet$FileUploadSupport$$super$handle(InteractiveSessionServlet.scala:42)
	at org.scalatra.servlet.FileUploadSupport$class.handle(FileUploadSupport.scala:93)
	at com.cloudera.livy.server.interactive.InteractiveSessionServlet.handle(InteractiveSessionServlet.scala:42)
	at org.scalatra.ScalatraServlet.service(ScalatraServlet.scala:54)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:812)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:587)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1127)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1061)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:110)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
	at org.eclipse.jetty.server.Server.handle(Server.java:499)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)
	at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.CancellationException
16/11/30 07:08:58.867 ForkJoinPool-1-worker-3: Stopping InteractiveSession 3...
16/11/30 07:08:58.868 ForkJoinPool-1-worker-3: InteractiveSession 3 session state change from running to shutting_down
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Dec 06 19:55:05 UTC 2016,,,,,,,,,,"0|i3j02n:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Dec/16 19:55;tc0312;Can't repo with a simple stress test that creates and deletes session in a loop.
The stress test was ran overnight and sucessfully created and deleted 1237 sessions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sharing variables across Job in RSC in Python,LIVY-262,13095900,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Do,,tc0312,gmcdonald,06/Dec/16 06:20,09/Dec/16 03:02,19/Dec/25 04:15,09/Dec/16 03:02,0.3,,,,,RSC,,,,,,,,,,0,,,,,,Users are asking for somthing similar to NamedRDD & NamedObject in JobServer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 09 03:02:28 UTC 2016,,,,,,,,,,"0|i3j02f:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Dec/16 03:02;tc0312;Users can use the global dict. Closing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test fail: SparkRSessionSpec.should report an error if accessing an unknown variable,LIVY-261,13095899,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,05/Dec/16 23:00,15/Dec/16 00:45,19/Dec/25 04:15,15/Dec/16 00:45,0.3,,,0.3,,Interpreter,Tests,,,,,,,,,0,,,,,,"SparkRSessionSpec.should report an error if accessing an unknown variable fails randomly:


{noformat}
- should report an error if accessing an unknown variable *** FAILED *** (170 milliseconds)
  JObject(List((status,JString(ok)), (execution_count,JInt(0)), (data,JObject(List((text/plain,JString())))))) did not equal JObject(List((status,JString(ok)), (execution_count,JInt(0)), (data,JObject(List((text/plain,JString(Error: object 'x' not found))))))) (SparkRSessionSpec.scala:140)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Dec 09 02:21:45 UTC 2016,,,,,,,,,,"0|i3j027:",9223372036854775807,,,,,,,,,,,,,,,,,,,"08/Dec/16 00:47;tc0312;This's a real bug in R interpreter.
It's a synchronization issue between R writing to stdout and stderr.

When the evaluation of an expression fails, R writes the output first to stdout then to stderr. 
Livy relies on reading output marker {{\-\-\-\-LIVY_END_OF_COMMAND\-\-\-\-}} from stdout to determine has evaluation completed.
Since R doesn't synchronize writing to stdout and stderr, it's possible that {{\-\-\-\-LIVY_END_OF_COMMAND\-\-\-\-}} has been written to stdout but the error of the evaluation hasn't been written to/flushed to stderr. Because of the timing issue, Livy will not see the error in stderr.

There a few ways to fix this:
# Redirect stderr to stdout. Using a single output stream will avoid the synchronization issue entirely.
# Execute and capture statements in R using `try(eval(parse(text=<...>)))`. Capture its output to a variable. Print that variable along with the output marker at once.

I'm not that familiar with R. Can someone help? @zjffdu;;;","08/Dec/16 01:06;tc0312;I also noticed that since the R interpreter is not using eval(), R interpreter will hung forever if it executes an incomplete statement. e.g. `x[`.

We need fix R interpreter to call eval() in any case.;;;","08/Dec/16 11:32;purechoc;using eval() in any case, can fixed incomplete statement.

R Interpreter is check error rely on ""takeErrorLines()"". timing issue in here.
takeErrorLines() called before R stderr flush.

so, how about like this?
{code}
tryCatch({eval(parse(text=""yyy""))},error = function(e) {print(""----LIVY_ERROR----"")})
{code}
;;;","08/Dec/16 12:39;zjffdu;tryCatch should work I believe;;;","09/Dec/16 02:21;tc0312;This's a good idea. However, I'm fixing them separately.
For this bug I will change R interpreter to use stdout only.

I created LIVY-269 for the eval issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
livy-repl is stuck in busy state if Scala interpreter returns a specific error,LIVY-260,13095898,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,05/Dec/16 22:44,22/Dec/16 08:35,19/Dec/25 04:15,22/Dec/16 08:35,0.3,,,0.3,,REPL,,,,,,,,,,0,,,,,,"If Scala returns an error that has 2 stacktraces, livy-repl will stuck in busy state.

Example:
{noformat}
scala> sc.setJobGroup(groupName, groupName, true)
<console>:27: error: type mismatch;
 found   : Int
 required: String
       sc.setJobGroup(groupName, groupName, true)
                      ^
<console>:27: error: type mismatch;
 found   : Int
 required: String
       sc.setJobGroup(groupName, groupName, true)
	                         ^
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-05 22:44:49.0,,,,,,,,,,"0|i3j01z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot overwrite applciation name in session,LIVY-259,13095897,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,linchan,linchan,gmcdonald,03/Dec/16 00:04,06/Dec/16 06:19,19/Dec/25 04:15,06/Dec/16 06:19,0.3,,,0.3,,REPL,,,,,,,,,,0,,,,,,"In 0.3, the application name is currently hardcoded to ""livy-session-ID"". We need to let user specify it in ""name"" parameter or ""conf"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-12-03 00:04:38.0,,,,,,,,,,"0|i3j01r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local mode doesn't work in security cluster,LIVY-258,13095896,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,zjffdu,zjffdu,gmcdonald,30/Nov/16 05:24,30/Nov/16 06:04,19/Dec/25 04:15,30/Nov/16 06:04,0.3,,,,,REPL,,,,,,,,,,0,,,,,,"{code}
16/11/29 07:53:24 INFO ContextLauncher: 16/11/29 07:53:24 INFO SecurityManager: Changing view acls to: root,admin
16/11/29 07:53:24 INFO ContextLauncher: 16/11/29 07:53:24 INFO SecurityManager: Changing modify acls to: root,admin
16/11/29 07:53:24 WARN RSCClient: Client RPC channel closed unexpectedly.
16/11/29 07:53:24 INFO ContextLauncher: Exception in thread ""main"" java.lang.IllegalArgumentException: Error: a secret key must be specified via the spark.authenticate.secret config
16/11/29 07:53:24 INFO ContextLauncher:         at org.apache.spark.SecurityManager.generateSecretKey(SecurityManager.scala:397)
16/11/29 07:53:24 INFO ContextLauncher:         at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:219)
16/11/29 07:53:24 INFO ContextLauncher:         at org.apache.spark.repl.SparkIMain.<init>(SparkIMain.scala:118)
16/11/29 07:53:24 INFO ContextLauncher:         at com.cloudera.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:49)
16/11/29 07:53:24 INFO ContextLauncher:         at com.cloudera.livy.repl.Session$$anonfun$1.apply(Session.scala:66)
16/11/29 07:53:24 INFO ContextLauncher:         at com.cloudera.livy.repl.Session$$anonfun$1.apply(Session.scala:64)
16/11/29 07:53:24 INFO ContextLauncher:         at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
16/11/29 07:53:24 INFO ContextLauncher:         at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
16/11/29 07:53:24 WARN RSCClient: Error stopping RPC.
io.netty.util.concurrent.BlockingOperationException: DefaultChannelPromise@544e6f7(uncancellable)
        at io.netty.util.concurrent.DefaultPromise.checkDeadLock(DefaultPromise.java:390)
        at io.netty.channel.DefaultChannelPromise.checkDeadLock(DefaultChannelPromise.java:157)
        at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:251)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Nov 30 06:04:37 UTC 2016,,,,,,,,,,"0|i3j01j:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/Nov/16 06:04;zjffdu;Close it as won't fix. Because it is spark issue, in local mode, we should specify spark.authenticate.secret ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
access log is not available,LIVY-257,13095895,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,29/Nov/16 09:23,06/Dec/16 00:55,19/Dec/25 04:15,06/Dec/16 00:55,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,The livy server access log is not available,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-11-29 09:23:05.0,,,,,,,,,,"0|i3j01b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Gzip compression support for Http response,LIVY-256,13095894,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,jerryshao,jerryshao,gmcdonald,29/Nov/16 02:45,09/Dec/16 01:48,19/Dec/25 04:15,09/Dec/16 01:48,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,"Sometimes when queried data is big, it is efficient to transfer compressed json data rather than raw format. So here propose to add compress support for REST API response.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-11-29 02:45:17.0,,,,,,,,,,"0|i3j013:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document update for spark2 support,LIVY-255,13095893,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,zjffdu,gmcdonald,28/Nov/16 05:39,30/Nov/16 04:28,19/Dec/25 04:15,30/Nov/16 04:28,0.3,,,0.3,,Docs,,,,,,,,,,0,,,,,,"@Saisai Shao, I assign it to you since you did most of work to support spark2.

README.rst is the file you need to update I believe. 

BTW, I notice you are also working on support multiple versions of spark in one livy instance. You also need to update doc for this as well. But you can do it in another ticket",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-11-28 05:39:12.0,,,,,,,,,,"0|i3j00v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTTP Auth support for Scala LivyClient,LIVY-252,13095890,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,21/Nov/16 21:54,22/Dec/16 08:34,19/Dec/25 04:15,22/Dec/16 08:34,0.3,,,0.3,,RSC,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-11-21 21:54:17.0,,,,,,,,,,"0|i3j007:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn session leaked if PingJob times out,LIVY-251,13095889,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,linchan,linchan,gmcdonald,21/Nov/16 17:00,13/Dec/16 05:37,19/Dec/25 04:15,13/Dec/16 05:37,0.3,,,0.3,,RSC,,,,,,,,,,0,,,,,,"When starting an interactive session, a fake ping job is submitted to determine the state of the repl driver. However, if yarn is out of resources and the repl application is stuck in Accepted state, the ping job will fail and session will go into dead state. However, the yarn application is not killed. As a result, when Yarn got resources back this application will start running and would be leaked.
Need to make sure the application is killed in Yarn before transition to dead in this case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-11-21 17:00:35.0,,,,,,,,,,"0|i3izzz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change livy.repl.jars to work with multiple repl jars,LIVY-250,13095888,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,tc0312,gmcdonald,19/Nov/16 03:13,13/Dec/16 05:37,19/Dec/25 04:15,13/Dec/16 05:37,0.3,,,0.3,,REPL,,,,,,,,,,0,,,,,,"Spark 2.0 changes added 2 repls (for Scala 2.10 & 2.11). Livy will automatically select the right repl.

This doesn't work nicely with livy.repl.jars. livy.repl.jars was designed to pass a list of jars for a repl. We should replace livy.repl.jars with a config that works with auto repl selection.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Nov 22 06:43:46 UTC 2016,,,,,,,,,,"0|i3izzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Nov/16 06:43;jerryshao;An easy way to address it is to configured all the dependencies to {{livy.repl.jars}} and filter out unmatched dependencies in the runtime according to Spark's scala version and dependency's scala version suffix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
livy-server script doesn't handle error cases.,LIVY-249,13095887,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,Sloan,gmcdonald,17/Nov/16 22:15,17/Mar/17 07:50,19/Dec/25 04:15,17/Mar/17 07:50,0.2,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"In livy-server-0.2.0, livy-server start returns 0 regardless of whether the server starts successfully",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 17 02:03:00 UTC 2017,,,,,,,,,,"0|i3izzj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/Nov/16 22:18;Sloan;In livy-server-0.2.0, `livy-server start` returns 0 regardless of whether the server starts successfully (edited)

 ```Listening for transport dt_socket at address: 8800
16/11/17 16:48:31 INFO LivyServer: Using spark-submit version 1.6.0
16/11/17 16:48:31 WARN RequestLogHandler: !RequestLog
16/11/17 16:48:31 WARN AbstractLifeCycle: FAILED ServerConnector@4cf1e2d9{HTTP/1.1}{0.0.0.0:8998}: java.net.BindException: Address already in use
java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method)
        at sun.nio.ch.Net.bind(Net.java:463)
        at sun.nio.ch.Net.bind(Net.java:455)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
        at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:321)
        at org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
        at org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at org.eclipse.jetty.server.Server.doStart(Server.java:366)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at com.cloudera.livy.server.WebServer.start(WebServer.scala:94)
        at com.cloudera.livy.server.LivyServer.start(LivyServer.scala:116)
        at com.cloudera.livy.server.LivyServer$.main(LivyServer.scala:203)
        at com.cloudera.livy.server.LivyServer.main(LivyServer.scala)
16/11/17 16:48:31 WARN AbstractLifeCycle: FAILED org.eclipse.jetty.server.Server@2d14b355: java.net.BindException: Address already in use
java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method)
        at sun.nio.ch.Net.bind(Net.java:463)
        at sun.nio.ch.Net.bind(Net.java:455)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
        at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:321)
        at org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
        at org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at org.eclipse.jetty.server.Server.doStart(Server.java:366)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at com.cloudera.livy.server.WebServer.start(WebServer.scala:94)
        at com.cloudera.livy.server.LivyServer.start(LivyServer.scala:116)
        at com.cloudera.livy.server.LivyServer$.main(LivyServer.scala:203)
        at com.cloudera.livy.server.LivyServer.main(LivyServer.scala)
Exception in thread ""main"" java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method)
        at sun.nio.ch.Net.bind(Net.java:463)
        at sun.nio.ch.Net.bind(Net.java:455)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
        at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:321)
        at org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
        at org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:236)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at org.eclipse.jetty.server.Server.doStart(Server.java:366)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at com.cloudera.livy.server.WebServer.start(WebServer.scala:94)
        at com.cloudera.livy.server.LivyServer.start(LivyServer.scala:116)
        at com.cloudera.livy.server.LivyServer$.main(LivyServer.scala:203)
        at com.cloudera.livy.server.LivyServer.main(LivyServer.scala)
```


That's what the logs throw, but the service start command always returns 0;;;","17/Nov/16 22:18;Sloan;There's also no `livy-server status` command, so I can't even check after it starts;;;","17/Mar/17 02:03;ajbozarth;Opened a pr: https://github.com/cloudera/livy/pull/311

Addressed both return value and server status;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up how PySpark python intepreters are set.,LIVY-248,13095886,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,17/Nov/16 03:05,06/Dec/16 00:55,19/Dec/25 04:15,06/Dec/16 00:55,0.3,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"Right now they are set using env `PYSPARK3_DRIVER_PYTHON` and `PYSPARK_DRIVER_PYTHON`.

Firstly, they should be called `PYSPARK3_PYTHON` and `PYSPARK_PYTHON`, because we are passing them to PYSPARK_PYTHON. Executors will use this python too, not just the driver.

We should also document pyspark kind in README.rst.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-11-17 03:05:10.0,,,,,,,,,,"0|i3izzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sharing variables across Scala Jobs in RSC.,LIVY-245,13095883,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,tc0312,gmcdonald,11/Nov/16 21:38,20/Oct/17 06:59,19/Dec/25 04:15,20/Oct/17 06:58,0.3,,,0.5.0,,RSC,,,,,,,,,,0,,,,,,Users are asking for somthing similar to NamedRDD & NamedObject in JobServer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 20 06:58:46 UTC 2017,,,,,,,,,,"0|i3izyn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Mar/17 20:06;calvince;Hi Alex, tracking this issue. What is its status? Could you point me to the code to update to support SharedRDDs?;;;","07/Apr/17 05:36;tc0312;Do you need the ability to share RDDs/objects across languages? e.g. make RDD accessible to Python and Scala/Java jobs?;;;","23/Aug/17 22:30;ajbozarth;Open PR by [~jerryshao] https://github.com/apache/incubator-livy/pull/19;;;","13/Oct/17 04:56;githubbot;Github user jerryshao closed the pull request at:

    https://github.com/apache/incubator-livy/pull/19
;;;","13/Oct/17 04:56;githubbot;GitHub user jerryshao reopened a pull request:

    https://github.com/apache/incubator-livy/pull/19

    [LIVY-245][RSC] Add support shared variables across Jobs

    Currently we cannot share variables across different Jobs in Livy, so here propose to add a cache layer in RSC to store shared objects. This cache followed LRU, the least not used will be removed when exceeding limits.
    
    This work is based on @alex-the-man 's work.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jerryshao/incubator-livy LIVY-245

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/19.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #19
    
----
commit cb024b4559942df8dbb00cf530708c45252d721d
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-05-18T09:01:02Z

    Add support shared variables across Jobs
    
    Change-Id: I6eccd5d2efebe153e3be4d94b80aa964ed095fc1

commit 1e1c9775bd3514f6bbb850a92c445697c94c6424
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-05-18T12:45:30Z

    Fix IT failure
    
    Change-Id: I14d688caf94950a4dcd9ad17bf7edfeba2339eed

commit 766ca6e262a70897d7013afc6c37664c00781db9
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-07-24T08:00:08Z

    Update the code

commit fe6edbae71ede5ee58c90dfc58b74658b67afb41
Author: jerryshao <sshao@hortonworks.com>
Date:   2017-10-13T03:57:23Z

    Fix rebase issue

----
;;;","13/Oct/17 05:37;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/19
  
    @ajbozarth , would you please help to review, thanks!
;;;","20/Oct/17 06:58;jerryshao;Issue resolved by pull request 19
[https://github.com/apache/incubator-livy/pull/19];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace livy-server repl state polling thread with RPC.,LIVY-244,13095882,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,11/Nov/16 05:53,06/Dec/16 00:56,19/Dec/25 04:15,06/Dec/16 00:56,0.3,,,0.3,,REPL,Server,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-11-11 05:53:17.0,,,,,,,,,,"0|i3izyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperStateStore ignores settings for RetryPolicy,LIVY-243,13095881,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,,meisam,gmcdonald,10/Nov/16 18:47,16/Nov/16 08:45,19/Dec/25 04:15,16/Nov/16 08:45,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,"ZooKeeper state store reads the settings for the retry policy from {{livy.server.recovery.zk-state-store.retry-policy}} and parses them using a regex, but it ignore them and uses hard coded values.",started in commit: 8b546dc95ba215b6b8f3f9c7d4106e8ce564a006,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Nov 10 22:45:19 UTC 2016,,,,,,,,,,"0|i3izy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/Nov/16 22:45;meisam;Pull request raised: https://github.com/cloudera/livy/pull/228;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shorten the time to get RSC driver URI for interactive session,LIVY-242,13095880,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,jerryshao,jerryshao,gmcdonald,10/Nov/16 07:27,14/Nov/16 22:27,19/Dec/25 04:15,14/Nov/16 22:27,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,"Currently RSC driver URI is gotten when {{PingJob}} is finished, which means only when SparkContext and other Spark entry point are fully started and job are finished we could get the RSC driver URI. This unnecessarily prolong the intermediate state of recovery metadata, if LivyServer is failed at this time period, it will never talk the RSC application again.

So we should shorten the time to get RSC driver URI to mitigate the intermediate state of recovery metadata, that will make session recovery more robust.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-11-10 07:27:23.0,,,,,,,,,,"0|i3izxz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad parameter for batch/session creation should return error 400,LIVY-241,13095879,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,linchan,linchan,gmcdonald,08/Nov/16 22:58,11/Nov/16 05:46,19/Dec/25 04:15,11/Nov/16 05:46,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,"Currently, at session creation, if the parameter contains an unknown value or type mismatch, livy is returning error 500(Internal error). We should convert that to error 400 (Bad request).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-11-08 22:58:08.0,,,,,,,,,,"0|i3izxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aggressively gc the inactive sessions for SessionManager,LIVY-240,13095878,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,07/Nov/16 13:28,22/Feb/17 22:26,19/Dec/25 04:15,22/Feb/17 22:26,0.3,,,0.4.0,,Server,,,,,,,,,,0,,,,,,"Current {{SessionManager}} will only clean the session in a timely triggered GC mechanism, and the default timeout is 1hr, which means the session metadata will be in memory for 1hr, this is OK without recovery enabled.

But with session recovery enabled, stored session data will also only be cleaned out after timeout, this will unnecessarily prolong the metadata persisted in the external storage. If Livy server is failed in this time period, though session is already inactive, recovered Livy server will recover and mark as inactive again, and the TTL will again set to 1hr. To the extreme, if Livy server always failed with this time period, the persisted metadata will never have a chance to clean up.

So we should aggressively clean the metadata once session is stopped.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-11-07 13:28:25.0,,,,,,,,,,"0|i3izxj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Job API throws NPE when return value is Void (null),LIVY-236,13095874,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,28/Oct/16 08:39,02/Nov/16 01:03,19/Dec/25 04:15,02/Nov/16 01:03,0.3,,,0.3,,API,,,,,,,,,,0,,,,,,"Job API will throw NPE if return value is explicitly defined as {{Void}} and return null. {{Void}} should be expected since some Jobs actually doesn't require returned value.

{noformat}
  java.util.concurrent.ExecutionException: java.lang.NullPointerException
  at com.cloudera.livy.client.http.JobHandleImpl.get(JobHandleImpl.java:201)
  at com.cloudera.livy.client.http.JobHandleImpl.get(JobHandleImpl.java:101)
  at com.cloudera.livy.test.JobApiIT.com$cloudera$livy$test$JobApiIT$$waitFor(JobApiIT.scala:295)
  at com.cloudera.livy.test.JobApiIT$$anonfun$13.apply$mcV$sp(JobApiIT.scala:219)
  at com.cloudera.livy.test.JobApiIT$$anonfun$13.apply(JobApiIT.scala:210)
  at com.cloudera.livy.test.JobApiIT$$anonfun$13.apply(JobApiIT.scala:210)
  at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
  at org.scalatest.Transformer.apply(Transformer.scala:22)
  ...
  Cause: java.lang.NullPointerException:
  at java.nio.ByteBuffer.wrap(ByteBuffer.java:396)
  at com.cloudera.livy.client.http.JobHandleImpl$JobPollTask.run(JobHandleImpl.java:239)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
  at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
  ...
{noformat}

To reproduce this issue, job implementation could be:

{code}
val job = new Job[Void] {
      override def call(jc: JobContext): Void = {
        null
      }
    }

    val result = waitFor(client2.submit(job))
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-10-28 08:39:28.0,,,,,,,,,,"0|i3izwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Livy Scala API to support both Scala 2.10 and 2.11,LIVY-234,13095872,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,26/Oct/16 08:33,18/Aug/17 18:20,19/Dec/25 04:15,29/Oct/16 06:33,0.3,,,0.3,,API,,,,,,,,,,0,,,,,,"Current Livy supports running on Spark with different Scala build, but Scala API only release 2.10 build, so here propose to also have a 2.11 build for livy-scala-api.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-10-26 08:33:58.0,,,,,,,,,,"0|i3izw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support SparkSession in Job API,LIVY-233,13095871,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,26/Oct/16 06:13,18/Aug/17 18:20,19/Dec/25 04:15,13/Dec/16 05:37,0.3,,,0.3,,API,RSC,,,,,,,,,0,,,,,,"Spark 2.0 brings in new entry point {{SparkSession}} to replace {{SQLContext}} or {{HiveContext}}. In the current Livy code, interpreters already support {{SparkSession}}, while Job API still uses old entry point. So it would be better to have Job API also supports new Spark 2.0 entry point. 

The main obstacle is how to bring into {{SparkSession}}, since it is a Spark 2.0 only class, Job API uses explicit type which makes it hard to use reflection.

Another thing is to support {{SparkSession}} in {{JobContextImpl}}.

So here as far as I can think there're possibly two solutions:

* One solution is to build separate jars for spark 1.x and 2.x and pick right jars according to Spark version detected in the runtime.
* Another solution is to use Object for {{SparkSession}}, which means user has to do type conversion manually.

Both these two solutions are not so elegant, it would be great to hear suggestions and comments on how to handle this issue in Job API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Oct 26 06:19:12 UTC 2016,,,,,,,,,,"0|i3izvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Oct/16 06:19;jerryshao;\cc [~vanzin] [~tc0312] [~zjffdu], it would be great to hear your suggestions on how to well address it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Spark version cap in livy-server,LIVY-232,13095870,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,jerryshao,tc0312,gmcdonald,25/Oct/16 21:24,11/Nov/16 05:48,19/Dec/25 04:15,11/Nov/16 05:48,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,"livy-server should print a warning when it's running with untested new Spark version, instead of erroring out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 25 21:25:12 UTC 2016,,,,,,,,,,"0|i3izvr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Oct/16 21:25;tc0312;https://github.com/cloudera/livy/pull/205#discussion-diff-84818446R32;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSession is not avalible for PySparkInterperter and SparkRInterperter,LIVY-230,13095868,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,19/Oct/16 08:27,25/Oct/16 21:21,19/Dec/25 04:15,25/Oct/16 21:21,0.2,,,0.3,,REPL,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-10-19 08:27:43.0,,,,,,,,,,"0|i3izvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark version check fails when it's used with HDP built Spark,LIVY-229,13095867,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,18/Oct/16 22:12,21/Oct/16 01:28,19/Dec/25 04:15,21/Oct/16 01:28,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,"Spark version check fails when it's used with HDP built Spark.
{noformat}
Exception in thread ""main"" java.lang.IllegalArgumentException: Fail to parse Spark version from 2.0.0.2.5.1.0-56.
{noformat}
As 2.0.0.2.5.1.0-56 doesn't match regex 

{code:java}
  def formatSparkVersion(version: String): (Int, Int) = {
    val versionPattern = """"""(\d)+\.(\d)+(?:\.\d*)?"""""".r
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-10-18 22:12:43.0,,,,,,,,,,"0|i3izv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shade some repl dependencies to mitigate the building difference between Spark1 and 2,LIVY-228,13095866,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,14/Oct/16 07:18,18/Aug/17 18:20,19/Dec/25 04:15,25/Oct/16 01:22,0.3,,,0.3,,REPL,,,,,,,,,,0,,,,,,"Some dependencies are different between Spark 1 and Spark 2, like json4s, py4j, it makes Livy hard to support multiple versions simultaneously, especially Json4s dependency, so here propose to shade these dependencies to mitigate the difference. For json4s, the final goal is to remove the dependency, which will be addressed in LIVY-118.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-10-14 07:18:21.0,,,,,,,,,,"0|i3izuv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propose to ship two REPL bundles in one assembly,LIVY-227,13095865,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,13/Oct/16 07:09,18/Aug/17 18:20,19/Dec/25 04:15,26/Oct/16 08:35,0.3,,,,,REPL,,,,,,,,,,0,,,,,,"Current Livy support two different scala REPLs, one for Scala-2.10 and another for Scala-2.11, users have to specify which version of Scala they wanted in build time through ""-Dscala-2.11"" (by default 2.10 is picked), this is not so intuitive. So here propose to ship two repl bundles in one Livy assembly, this requires:

* assembly pom to package two repl folders, one is repl-jars-2.10 and another is repl-jars-2.11.
* Interactive session to upload repl jars according to Scala version of Spark.
* Interactive session to figure out which Scala version Spark uses.
** Spark provides a way to output Scala version (https://issues.apache.org/jira/browse/SPARK-17686).
** Specified by user in creating interactive session.
** Default mapping relation (spark2.0 -> scala-2.11, spark 1.6 -> scala-2.10).
* Changes in integrations test.

This is one step to support different Spark + Scala version in one Livy instance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Oct 13 07:13:39 UTC 2016,,,,,,,,,,"0|i3izun:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Oct/16 07:13;jerryshao;CC [~tc0312];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Netty version to 4.0.29.Final,LIVY-226,13095864,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,jerryshao,jerryshao,gmcdonald,12/Oct/16 02:46,14/Oct/16 01:51,19/Dec/25 04:15,14/Oct/16 01:51,0.3,,,0.3,,RSC,,,,,,,,,,0,,,,,,"To mitigate the difference between supporting Spark 1 and Spark 2, upgrading Netty version to 4.0.29.Final.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-10-12 02:46:23.0,,,,,,,,,,"0|i3izuf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix several bugs in Spark 2.0 integration test.,LIVY-225,13095863,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,12/Oct/16 00:44,28/Oct/16 06:24,19/Dec/25 04:15,28/Oct/16 06:24,0.3,,,0.3,,Tests,,,,,,,,,,0,,,,,,"Due to some changes in Spark 2.0, it will make integration test fail to run with Spark 2.0. So here propose to fix these issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-10-12 00:44:02.0,,,,,,,,,,"0|i3izu7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Datanucleus jars are not detected for spark 2.x,LIVY-224,13095862,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,10/Oct/16 09:50,18/Oct/16 07:35,19/Dec/25 04:15,18/Oct/16 07:35,0.2,,,0.3,,Core,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-10-10 09:50:19.0,,,,,,,,,,"0|i3iztz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't create a pyspark interactive session using livy in spark 2.0.X,LIVY-223,13095861,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Cannot Reproduce,,ofer,gmcdonald,08/Oct/16 13:29,02/Nov/16 01:03,19/Dec/25 04:15,02/Nov/16 01:03,0.3,,,,,Server,,,,,,,,,,0,usability,,,,,"I tried to run an interactive pyspark session using spark 2.0.1 and i failed:
the session state was in ""starting"" for a long time and then went directly to ""died"" (or ""died"" equivalent since i don't remember the exact string)

I was able to run successfully a pyspark batch job againt spark 2.0.1 using the same livy installation.","latest livy master, spark 2.0.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 28 06:26:26 UTC 2016,,,,,,,,,,"0|i3iztr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Oct/16 06:37;ofer;the state went from ""starting"" to ""dead"" and not ""died""....;;;","12/Oct/16 03:57;jerryshao;Would you please elaborate the problem you met, I tried pyspark interactive session manually, seems it can be started correctly.;;;","12/Oct/16 16:03;ofer;did u try on spark 2.0.x?
As i mentioned, I can't create an interactive session.
In spark 1.6.2 it works fine. 2.0.1 doesn't work.


;;;","13/Oct/16 00:37;jerryshao;Of course I'm running on Spark 2.0. In any case if pyspark interactive session cannot be started, there should be logs or something else. Would you please provide this kind of information? Also did you build Livy against Spark 2.0 by ""mvn clean package -DskipTests -Dspark-2.0 -Dscala-2.11"" ?;;;","13/Oct/16 05:25;ofer;I worked with 2.0.1.
I didn't use the -Dscala-2.11 flag. 
Is this mandatory?
;;;","13/Oct/16 05:33;jerryshao;Of course, since Spark 2.0 by default uses Scala-2.11.;;;","13/Oct/16 05:38;ofer;I'll try and let u know (will take me some time as i downgraded to 1.6.2 :))
;;;","28/Oct/16 06:26;tc0312;I cannot reproduce this. Do you have a repo?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default Livy name is overwriting conf,LIVY-221,13095859,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,linchan,linchan,gmcdonald,30/Sep/16 17:09,12/Oct/16 01:41,19/Dec/25 04:15,12/Oct/16 01:41,0.2,,,0.3,,Core,,,,,,,,,,0,,,,,,"Currently, there is a default value ""Livy"" for the name parameter. This is causing the spark.app.name provided in conf being overwritten. Given the direction to deprecate the explicit parameters and use conf instead, should remove the default value for name parameter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-09-30 17:09:57.0,,,,,,,,,,"0|i3iztb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark zipfile uploading is not working,LIVY-220,13095858,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,ofer,ofer,gmcdonald,29/Sep/16 10:12,01/Sep/17 01:40,19/Dec/25 04:15,25/Oct/16 21:20,0.3,,,0.3,,Batch,,,,,,,,,,0,packaging,pyFiles,pyspark,python,python-packages,"submitting a pyspark batch with pyFiles is not working.

i created a simple program containing 2 files.
a.py with the content:


{code:python}
from pyspark import SparkContext
import os
import bubu

sc = SparkContext(os.environ['SPARKMASTER'], 'lll')
rdd = sc.parallelize([1,2,3,4,5])
res =  rdd.map(bubu.func).collect()
print res
{code}

i also created bubu.py:

{code:python}
def func(x):
   return x+1
{code}


when i run this as a batch:

curl -X POST --data '{""file"": ""/apps/try/a.py"", ""pyFiles"": [""/apps/try/out.zip""]}' -H ""Content-Type: application/json"" localhost:8998/batches

""ImportError: No module named bubu"","")

the root cause is that the zip file is not uploaded into the executors:


6/09/29 09:57:06 INFO Utils: Copying /try/a.py to /tmp/spark-eabfc8a2-5f41-4d10-8c81-0d97be08f1ea/userFiles-8dc3cf7e-f122-4f4c-8a6d-4d2672a3e7fe/a.py
16/09/29 09:57:06 INFO SparkContext: Added file file:/try/a.py at http://172.19.0.3:42761/files/a.py with timestamp 1475143026005

and i don't see the same line for the zip file as it should (when working from spark directly).","debian:jessie, latest repository master branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Oct 08 13:20:45 UTC 2016,,,,,,,,,,"0|i3izt3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Oct/16 11:27;ofer;I finally got this thing over - what a nightmare lol.
It turns out that this never worked before.

The livy code takes the pyFiles variable and passes it as a --conf ""spark.submit.pyFiles=<whatever>"" as a command line to spark.

The spark.submit.pyFiles was only introduced in spark 2.0

I tried to run it with spark 2.0 but it turns out that spark 2.0 has also a bug!!! that ignores this option lolol....

What i did is i passed it as --py-files as well - this solved the problem..

The fix is in:
utils/SparkProcessBuilder.scala

look for the array that looks like this:

_conf.foreach { case (key, value) =>                                                                           
      arguments += ""--conf""
      arguments += f""$key=$value""
 }           

and change this into:

_conf.foreach { case (key, value) =>                                                                           
      arguments += ""--conf""
      arguments += f""$key=$value""

      if (key == ""spark.submit.pyFiles"") {
         arguments += ""--py-files""                                                                                 
         arguments += f""$value""                                                                                    
      }
}         


that's it - and good luck.;;;","05/Oct/16 11:54;ofer;i created a pull request for this issue:
https://github.com/cloudera/livy/pull/197

hope this helps.;;;","08/Oct/16 07:48;jerryshao;1. Spark 1.6 also has ""spark.submit.pyFiles"", it is not newly added in Spark 2.0.
2. Yes Spark also has some issues related to this (https://issues.apache.org/jira/browse/SPARK-17512).;;;","08/Oct/16 13:20;ofer;1.
The reason i thought it wasn't introduced in spark 1.6.X is that it is not documented in:
http://spark.apache.org/docs/1.6.2/configuration.html
However in 2.0 it appears:
http://spark.apache.org/docs/latest/configuration.html

2. 
Spark-17512 is only about yarn, and also mentions that --py-files doesn't work.
I tested it in ""stand alone"" and only the ""spark.submit.pyFiles"" didn't work (the --py-files works fine).
I tested this both on spark 1.6.2 and on spark 2.0.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change Livy IT to support different scala version of livy-repl,LIVY-219,13095857,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,jerryshao,gmcdonald,28/Sep/16 02:29,18/Aug/17 18:20,19/Dec/25 04:15,29/Oct/16 06:34,0.3,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"Current interactive IT is using scala-2.10 interactive interpreter to run test, it is necessary to enable IT to test Scala-2.11 interpreter. This requires not only building file changes, also some codes should be changed to get correct jar file, also test script is necessary to run different tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-09-28 02:29:33.0,,,,,,,,,,"0|i3izsv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upload binary distribution of livy,LIVY-218,13095856,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jonalter,zjffdu,gmcdonald,22/Sep/16 12:37,22/Feb/17 22:19,19/Dec/25 04:15,22/Feb/17 22:19,0.2,,,0.4.0,,Core,,,,,,,,,,0,,,,,,It would be nice to have binary distribution of livy so that user can download it and try it easier without building by himself.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 16 23:41:52 UTC 2017,,,,,,,,,,"0|i3izsn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/Feb/17 20:12;akchin;Hello, 

I noticed that the currently link to download livy is still for the 0.2.0 distribution. Can anyone update to 0.3.0?;;;","16/Feb/17 23:41;jonalter;Updated link to build 0.3.0
PR: [#294|https://github.com/cloudera/livy/pull/294];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
executor number is not correctly configured in interactive session,LIVY-216,13095854,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,22/Sep/16 03:44,07/Oct/16 01:06,19/Dec/25 04:15,07/Oct/16 01:06,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,"Current in the code, if executor number is set in session creation body, this will be converted to spark configuration as:

{code}
      ""spark.dynamicAllocation.maxExecutors"" -> request.numExecutors.map(_.toString)
{code}

It is not correct to use ""spark.dynamicAllocation.maxExecutors"" to set executor number, instead we should use ""spark.executor.instances"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-09-22 03:44:22.0,,,,,,,,,,"0|i3izs7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support canceling of statement,LIVY-215,13095853,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,zjffdu,gmcdonald,19/Sep/16 08:48,16/Dec/16 06:59,19/Dec/25 04:15,16/Dec/16 06:59,0.2,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-09-19 08:48:50.0,,,,,,,,,,"0|i3izrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Statement result recovery for interactive sessions,LIVY-213,13095851,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,16/Sep/16 17:02,18/Aug/17 18:08,19/Dec/25 04:15,11/Nov/16 05:46,0.3,,,,,REPL,RSC,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 18 07:46:52 UTC 2016,,,,,,,,,,"0|i3izrj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Oct/16 00:51;tc0312;I implemented this by proxying statement result queries to livy-repl. I'm following existing code and am using JValue to represent statement output.
This works fine in Scala 2.10. But in Scala 2.11, this doesn't work because Kyro (used by Rpc) cannot serialize scala.collection.immutable.$colon$colon (JValue uses :: list). I'm not sure why it works fine in Scala 2.10.

There are a few ways to solve this:
1. Use some Kryo serialization library for Scala like twitter/chill to serialize lists and JValue.
2. Don't use JValue to store statement results. Use java.util.Map<String, Object> instead. And use Jackson to convert an object to a map.

I like approach 2 more because we need to get rid of json4s anyway.
Any comments?;;;","18/Oct/16 07:46;tc0312;I decided to remove json4s in another workitem. To workaround the Kyro serialization issue of non POJO types, I'm changing repl to send statement results to server in form of json string.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Session recovery for interactive sessions,LIVY-212,13095850,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,16/Sep/16 17:02,18/Aug/17 18:08,19/Dec/25 04:15,28/Oct/16 06:24,0.3,,,0.3,,REPL,Server,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-09-16 17:02:00.0,,,,,,,,,,"0|i3izrb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Session recovery for batch sessions,LIVY-211,13095849,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,16/Sep/16 17:01,18/Aug/17 18:26,19/Dec/25 04:15,07/Oct/16 01:06,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-11,,,,,,,,,,,,,,,,,,LIVY-231,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-09-16 17:01:36.0,,,,,,,,,,"0|i3izr3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add test context for Livy unit test output,LIVY-210,13095848,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,jerryshao,gmcdonald,09/Sep/16 03:14,07/Oct/16 01:06,19/Dec/25 04:15,07/Oct/16 01:06,0.3,,,0.3,,Tests,,,,,,,,,,0,,,,,,"Currently Livy unit test will output the log into unit-tests.log, but it is not separated by each test, so it is hard to identify which log belongs to which task, so here propose to add context before and after unit test, so that user could easily identify the related logs.

Here is the example of unit test output:

{noformat}
==== TEST OUTPUT FOR com.cloudera.livy.repl.SparkSessionSpec: 'should start in the starting or idle state' ====

16/09/09 10:59:15.735 pool-17-thread-1 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/09/09 10:59:15.871 pool-17-thread-1 INFO SecurityManager: Changing view acls to: sshao
16/09/09 10:59:15.872 pool-17-thread-1 INFO SecurityManager: Changing modify acls to: sshao
16/09/09 10:59:15.873 pool-17-thread-1 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(sshao); users with modify permissions: Set(sshao)
16/09/09 10:59:16.040 pool-17-thread-1 INFO HttpServer: Starting HTTP Server
16/09/09 10:59:16.119 pool-17-thread-1 INFO Utils: Successfully started service 'HTTP class server' on port 56636.
16/09/09 10:59:18.840 pool-17-thread-1 INFO SparkContext: Running Spark version 1.6.2
16/09/09 10:59:18.878 pool-17-thread-1 INFO SecurityManager: Changing view acls to: sshao
16/09/09 10:59:18.878 pool-17-thread-1 INFO SecurityManager: Changing modify acls to: sshao
16/09/09 10:59:18.878 pool-17-thread-1 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(sshao); users with modify permissions: Set(sshao)
16/09/09 10:59:19.093 pool-17-thread-1 INFO Utils: Successfully started service 'sparkDriver' on port 56638.
16/09/09 10:59:19.695 sparkDriverActorSystem-akka.actor.default-dispatcher-3 INFO Slf4jLogger: Slf4jLogger started
16/09/09 10:59:19.734 sparkDriverActorSystem-akka.actor.default-dispatcher-3 INFO Remoting: Starting remoting
16/09/09 10:59:19.897 sparkDriverActorSystem-akka.actor.default-dispatcher-3 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.0.103:56640]
16/09/09 10:59:19.904 pool-17-thread-1 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 56640.
16/09/09 10:59:19.916 pool-17-thread-1 INFO SparkEnv: Registering MapOutputTracker
16/09/09 10:59:19.937 pool-17-thread-1 INFO SparkEnv: Registering BlockManagerMaster
16/09/09 10:59:19.953 pool-17-thread-1 INFO DiskBlockManager: Created local directory at /Users/sshao/projects/livy/repl/target/tmp/blockmgr-ba269a9b-0b0e-4d3e-86a0-6a384e1f6d6e
16/09/09 10:59:19.958 pool-17-thread-1 INFO MemoryStore: MemoryStore started with capacity 1140.4 MB
16/09/09 10:59:20.025 pool-17-thread-1 INFO SparkEnv: Registering OutputCommitCoordinator
16/09/09 10:59:20.122 pool-17-thread-1 INFO Executor: Starting executor ID driver on host localhost
16/09/09 10:59:20.130 pool-17-thread-1 INFO Executor: Using REPL class URI: http://192.168.0.103:56636
16/09/09 10:59:20.145 pool-17-thread-1 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56641.
16/09/09 10:59:20.145 pool-17-thread-1 INFO NettyBlockTransferService: Server created on 56641
16/09/09 10:59:20.146 pool-17-thread-1 INFO BlockManagerMaster: Trying to register BlockManager
16/09/09 10:59:20.149 dispatcher-event-loop-2 INFO BlockManagerMasterEndpoint: Registering block manager localhost:56641 with 1140.4 MB RAM, BlockManagerId(driver, localhost, 56641)
16/09/09 10:59:20.151 pool-17-thread-1 INFO BlockManagerMaster: Registered BlockManager
16/09/09 10:59:20.427 pool-17-thread-1 INFO SparkInterpreter: Created sql context.
16/09/09 10:59:23.108 dispatcher-event-loop-6 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/09/09 10:59:23.113 ScalaTest-main-running-SparkSessionSpec INFO MemoryStore: MemoryStore cleared
16/09/09 10:59:23.114 ScalaTest-main-running-SparkSessionSpec INFO BlockManager: BlockManager stopped
16/09/09 10:59:23.122 ScalaTest-main-running-SparkSessionSpec INFO BlockManagerMaster: BlockManagerMaster stopped
16/09/09 10:59:23.126 dispatcher-event-loop-3 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/09/09 10:59:23.128 ScalaTest-main-running-SparkSessionSpec INFO SparkContext: Successfully stopped SparkContext
16/09/09 10:59:23.133 sparkDriverActorSystem-akka.actor.default-dispatcher-2 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/09/09 10:59:23.137 sparkDriverActorSystem-akka.actor.default-dispatcher-2 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/09/09 10:59:23.173 sparkDriverActorSystem-akka.actor.default-dispatcher-2 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
16/09/09 10:59:23.182 ScalaTest-main-running-SparkSessionSpec INFO SparkSessionSpec:

==== FINISHED com.cloudera.livy.repl.SparkSessionSpec: 'should start in the starting or idle state' ====
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-09-09 03:14:52.0,,,,,,,,,,"0|i3izqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getPathInfo will return null,LIVY-209,13095847,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,,jerryshao,gmcdonald,05/Sep/16 07:26,08/Sep/16 14:30,19/Dec/25 04:15,08/Sep/16 14:30,0.2,0.3,,0.3,,Server,,,,,,,,,,0,,,,,,"When creating a ineractive Livy session and get Location from headers, Livy server may return a wrong {{Location}} will {{null}} included.

To produce:

{code}
>>> import json, pprint, requests, textwrap
>>> host = 'http://localhost:8998'
>>> data = {'kind': 'spark'}
>>> headers = {'Content-Type': 'application/json'}
>>> r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
>>> r.json()
{u'kind': u'spark', u'log': [], u'proxyUser': None, u'state': u'starting', u'appId': None, u'owner': None, u'id': 0}
>>> r.headers['location']
'null/sessions/0'
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-09-05 07:26:08.0,,,,,,,,,,"0|i3izqn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose Spark UI URL and Driver Log URL,LIVY-208,13095846,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,tc0312,tc0312,gmcdonald,31/Aug/16 21:47,20/Sep/16 15:54,19/Dec/25 04:15,20/Sep/16 15:54,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,"Users need access to Spark UI and Driver Log to diagnose their applications. We should expose them.

This doesn't resolve [LIVY-55]. For that JIRA, Livy should return the actual driver log instead of the URL to the log.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-08-31 21:47:10.0,,,,,,,,,,"0|i3izqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy Supports Different Versions of Spark,LIVY-206,13095844,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,jerryshao,gmcdonald,22/Aug/16 05:57,28/Sep/16 01:33,19/Dec/25 04:15,28/Sep/16 01:33,0.2,,,,,API,Core,Interpreter,Tests,,,,,,,1,,,,,,"Currently, Livy is built against specific version of Spark and can be run with different Spark 1.X versions without recompilation. But with the compatibility broken version of Spark 2 released, Livyï¿½ï¿½ï¿½s code base should be changed to be compiled with Spark 2, which means Livy should maintain 2 different code bases to support different versions of Spark. So according to this, Livy should be compatible with different versions of Spark:

# Make Livy be compatible with different Spark versions, like Spark 1.6, Spark 2.0, etc.
# Make Livy support for different versions of Scala, Scala 2.10, 2.11, etc.
# Different Livy sessions can run against different Spark versions, which means Livy could launch different version of Spark applications simultaneously.

Based on the requirements we proposed above, several Livy parts should be changed:

# Change the pom file to support different Spark profiles, like -Pspark1.6, -Pspark2.0, etc.
# Change the livy scala API pom to support to cross-compile with different Scala versions, this will make Livy code work against Spark with different Scala versions.
# Support SparkSession in Batch API.
# Change the repl related codes to use reflection to make it version independent.
# Make SPARK_HOME configuration session independent, so that different session could launch different versions of Spark.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 28 01:33:18 UTC 2016,,,,,,,,,,"0|i3izpz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Aug/16 02:02;tc0312;The plan looks great in general.

For 3.Support SparkSession in Batch API, can we implement it as a separate feature enhancement to reduce the scope a bit?;;;","24/Aug/16 02:59;zjffdu;Make sense. ;;;","26/Aug/16 06:22;jerryshao;I will start addressing two things first:

1. Livy support different versions of Spark.
2. Livy support different versions of Scala.

Because Spark 2.0 officially moves to Scala 2.11, so these two things are a little mixed. The main obstacle is interpreter, scala-2.10 and 2.11 interpreters can quite different, so we have to implement them separately. The changes are mainly focused on pom file, repl module.

Also there're several similar JIRAs LIVY-104 and LIVY-105.;;;","28/Sep/16 01:33;jerryshao;Duplicated to LIVY-106;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sparkr.zip is unzipped on executor side,LIVY-205,13095843,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,20/Aug/16 01:03,07/Sep/16 23:38,19/Dec/25 04:15,07/Sep/16 23:38,0.2,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"The following code will fail in SparkRInterpreter.
{code}
sqlContext <- sparkRSQL.init(sc)
df <- createDataFrame(sqlContext, faithful)
head(df)
{code}

The root cause is that sparkr.zip is not unzipped in the executor side.

{noformat}
16/08/20 09:31:59 INFO r.BufferedStreamThread: Fatal error: cannot open file '/Users/jzhang/Temp/hadoop_tmp/nm-local-dir/usercache/jzhang/appcache/application_1471337400283_0082/container_1471337400283_0082_01_000002/sparkr/SparkR/worker/daemon.R': No such file or directory
16/08/20 09:32:09 ERROR executor.Executor: Exception in task 0.0 in stage 1.0 (TID 1)
java.net.SocketTimeoutException: Accept timed out
    at java.net.PlainSocketImpl.socketAccept(Native Method)
    at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:404)
    at java.net.ServerSocket.implAccept(ServerSocket.java:545)
    at java.net.ServerSocket.accept(ServerSocket.java:513)
    at org.apache.spark.api.r.RRDD$.createRWorker(RRDD.scala:432)
    at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:63)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Sep 07 23:38:21 UTC 2016,,,,,,,,,,"0|i3izpr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Sep/16 23:38;vanzin;Commit [02841487|https://github.com/cloudera/livy/commit/0284148713a9e21666992519d70cb797312ae9a8] by  Jeff Zhang <zjffdu@...> in cloudera/livy:
{code}
LIVY-205. sparkr.zip is unzipped on executor side

This PR is to unzipped sparkr.zip on executor side.

Closes #181
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Statement retention policy,LIVY-204,13095842,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Duplicate,jerryshao,paulw,gmcdonald,19/Aug/16 22:54,03/Mar/17 00:52,19/Dec/25 04:15,03/Mar/17 00:52,0.3,,,,,Server,,,,,,,,,,0,usability,,,,,Add statement retention policy to retain only last N statements. N should be a LivyConf.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 03 00:52:04 UTC 2017,,,,,,,,,,"0|i3izpj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"31/Aug/16 21:33;tc0312;Should we add a feature to keep just the last N statements?
Should N be a LivyConf or a per session property?
Any thoughts?;;;","28/Oct/16 06:27;tc0312;LIVY-213 will fix this.;;;","16/Feb/17 23:55;ajbozarth;LIVY-213 is closed as fixed, should this be closed or does it still need to be addressed?;;;","03/Mar/17 00:52;ajbozarth;Fixed as LIVY-303;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark version check fails on CDH,LIVY-203,13095841,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,purechoc,jhalfpenny,gmcdonald,18/Aug/16 16:33,22/Feb/17 22:21,19/Dec/25 04:15,22/Feb/17 22:21,0.3,,,,,Server,,,,,,,,,,0,,,,,,"The Spark version check introduced to Livy server in LIVY-184 fails on CDH. The regex that matches the version number looks for the pattern n.n.n, whereas the CDH version of Spark adds a suffix denoting the version of CDH e.g. n.n.n.-cdh5.5.2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 31 19:42:37 UTC 2017,,,,,,,,,,"0|i3izpb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Aug/16 16:48;jhalfpenny;Pull request #180 opened with a patch to fix the regex and drop the minimum version of Spark required for Livy server down to 1.5.;;;","31/Aug/16 21:34;tc0312;I posted some feedback in the PR. Mind if you take a look?;;;","31/Jan/17 19:42;akchin;Hello, 
Should this JIRA be closed now? Looks like https://github.com/cloudera/livy/pull/278 resolved this JIRA as well as LIVY-302.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Statement output is null even though state is ""available""",LIVY-202,13095840,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,tc0312,paulw,gmcdonald,15/Aug/16 09:40,11/Nov/16 05:48,19/Dec/25 04:15,11/Nov/16 05:48,0.3,,,0.3,,RSC,Server,,,,,,,,,0,usability,,,,,"During some concurrency testing with an interactive session, I got the following response back:

{{{""id"":0,""state"":""available"",""output"":null}}}

The issue is related to the following code in [InteractiveSessionServlet.scala|https://github.com/cloudera/livy/blob/master/server/src/main/scala/com/cloudera/livy/server/interactive/InteractiveSessionServlet.scala], which assumes the statement result will be available within 100 milliseconds:

{code:title=InteractiveSessionServlet.scala|borderStyle=solid}
  val output = try {
    Await.result(statement.output(), Duration(100, TimeUnit.MILLISECONDS))
  } catch {
    case _: TimeoutException => null
  }
{code}

Workaround is simple: retry the statement REST call until the output is not null.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Oct 28 06:28:09 UTC 2016,,,,,,,,,,"0|i3izp3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"15/Aug/16 10:17;paulw;Note that making an overloaded server do more work (by retrying) because it's overloaded is the wrong thing to do in general, so a better workaround would be desirable.;;;","28/Oct/16 06:28;tc0312;LIVY-213 will fix this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pick up R executable from configuration first,LIVY-201,13095839,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,10/Aug/16 06:43,15/Aug/16 17:21,19/Dec/25 04:15,15/Aug/16 17:21,0.2,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"For now, I can not run sparkr in cluster mode as livy pick up R executable from environment variable DRIVER_R, but spark didn't pass DRIVER_R to AM in yarn-cluster mode (This can be fixed in spark side). And since spark introduce spark.sparkr.r.command & spark.r.command for configuring R executable. Livy should also take this configuration over environment variable DRIVER_R.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 15 17:21:52 UTC 2016,,,,,,,,,,"0|i3izov:",9223372036854775807,,,,,,,,,,,,,,,,,,,"15/Aug/16 17:21;vanzin;Commit [741defdf|https://github.com/cloudera/livy/commit/741defdf388f77d1433b135b5c9354e1b37ed074] by  Jeff Zhang <zjffdu@...> in cloudera/livy:
{code}
LIVY-201. Pick up R executable from configuration first

For now, I can not run sparkr in cluster mode as livy pick up R executable from environment variable DRIVER_R, but spark didn't pass DRIVER_R to AM in yarn-cluster mode (This can be fixed in spark side). And since spark introduce spark.sparkr.r.command & spark.r.command for configuring R executable. Livy should also take this configuration over environment variable DRIVER_R.

Closes #177
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kyro deserialization fails when trying to return java.util.List on javaRDD.collect(),LIVY-200,13095838,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,mnarayan,gmcdonald,29/Jul/16 15:09,24/Feb/17 23:38,19/Dec/25 04:15,24/Feb/17 23:38,0.3,,,,,RSC,,,,,,,,,,0,,,,,,"Kyro deserialization fails when trying to return java.util.List when the collect API is called on a javaRDD 

CODE
public class TestJob implements Job<List<Integer>> {

    public List<Integer> call(JobContext jobContext) throws Exception {
        Function<Integer, Integer> multiplyFunc = new Function<Integer, Integer>() {
            public Integer call(Integer v1) throws Exception {
                return v1 * v1;
            }
        };
        List<Integer> list = jobContext.sc().parallelize(Arrays.asList(1, 2, 3)).map(multiplyFunc).collect();
        return list;
    }
}

EXCEPTION STACK TRACE 
java.lang.NullPointerException
	at scala.collection.convert.Wrappers$IterableWrapperTrait$class.size(Wrappers.scala:23)
	at scala.collection.convert.Wrappers$SeqWrapper.size(Wrappers.scala:64)
	at java.util.AbstractList.add(AbstractList.java:108)
	at com.cloudera.livy.shaded.kryo.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)
	at com.cloudera.livy.shaded.kryo.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)
	at com.cloudera.livy.shaded.kryo.kryo.Kryo.readClassAndObject(Kryo.java:776)
	at com.cloudera.livy.client.common.Serializer.deserialize(Serializer.java:63)
	at com.cloudera.livy.client.http.JobHandleImpl$JobPollTask.run(JobHandleImpl.java:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:

WORK AROUND
 Populate the results of the collect() function onto a java.util.ArrayList",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 23 05:07:00 UTC 2017,,,,,,,,,,"0|i3izon:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Oct/16 07:43;tc0312;This is because collect() is not returning a real Java list, but a wrapper over a Scala Seq. That wrapper is not Kryo serializable.
Implementing a specialized Kryo serializer for SeqWrapper could fix this. I think this's a common Scala issue, we should check do some popular Scala libraries like chill fixes the issue.;;;","22/Feb/17 08:32;zzzhy;@Alex Man  
I found this problem too. 
my class is like the following but problem exists either.
{code:java}
class SQLQueryJob extends Job[java.util.List[String]]
{code}
;;;","22/Feb/17 22:14;tc0312;What object did you return? Can you post the code here?;;;","23/Feb/17 02:28;zzzhy;@Alex Man , please check 
{code:java}
  override def call(jc: JobContext): JList[String] = {
    val session:SparkSession = jc.sparkSession()
    implicit val hConfig = HbConfig()
    val rows = query(session, tableName, sql, schemasToStructType(schemas))
    import scala.collection.JavaConverters._
    rows.collect().map(_.getString(0)).toSeq.asJava
  }
{code}
;;;","23/Feb/17 04:19;tc0312;That's exactly what I've described. To workaround it, do `new ArrayList(ows.collect().map(_.getString(0)).toSeq.asJava)`.
You can also try twitter/chill.;;;","23/Feb/17 05:07;zzzhy;yes, it works;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement user whitelisting as an interim authorisation control.,LIVY-199,13095837,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Fixed,paulw,paulw,gmcdonald,26/Jul/16 14:25,28/Jul/16 20:51,19/Dec/25 04:15,28/Jul/16 20:51,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,"Until a more full-featured security authorisation feature is in place, a simple alternative would be to create a user whitelisting feature. This could be implemented as a servlet filter, that checks to see that the connecting remote user is allowed to use Livy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 28 20:51:14 UTC 2016,,,,,,,,,,"0|i3izof:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Jul/16 14:30;paulw;In combination with LIVY-198, this will allow Livy to be used securely against a secure cluster.

The deployment scenario is that Livy is running as a service user on an edge node. It only accepts jobs from that service user, and it only runs jobs as that service user. Jobs are run using the spark.yarn.[keytab|principal] parameters, which reference a keytab stored locally on the Livy server. The jobs are run without impersonation.;;;","28/Jul/16 20:51;vanzin;Commit [d5615621|https://github.com/cloudera/livy/commit/d561562178f5a6c18a64818bef7d8dff4b27bf3a] by  Paul Wilkinson <paulw@...> in cloudera/livy:
{code}
LIVY-199. Add a simple user whitelist capability.

Adding allowed users to LivyConf.
Adding a servlet filter to control who has access.
Validating auth type when enabling access control.

Closes #173
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow impersonation to be disabled when using Kerberos.,LIVY-198,13095836,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Fixed,paulw,paulw,gmcdonald,26/Jul/16 13:53,28/Jul/16 20:32,19/Dec/25 04:15,28/Jul/16 20:32,0.3,,,0.3,,Server,,,,,,,,,,0,usability,,,,,"Livy should be able to use Kerberos and SPNEGO without forcing the use of impersonation.

The deployment scenario is Livy running as a single user that is only able to run Spark jobs as that user on a secure cluster.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-07-26 13:53:46.0,,,,,,,,,,"0|i3izo7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy doesn't support RM HA,LIVY-197,13095835,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,pbeauvois,gmcdonald,22/Jul/16 07:31,13/Jul/17 16:38,19/Dec/25 04:15,27/Jul/16 16:17,0.3,,,,,Server,,,,,,,,,,0,resource-management,,,,,"Hi,
I'm currently trying to configure Livy. 
I already set up impersonation in the core-site.xml like the following:
{noformat}
<property>
        <name>hadoop.proxyuser.hue.groups</name>
        <value>*</value>
        <description>Allow the user to impersonate any members of group1 and group2</description>
</property>

<property>
        <name>hadoop.proxyuser.hue.hosts</name>
        <value>hue01.bigdata.fr</value>
        <description>Allow the user to connect only from host1 and host2 to impersonate a user</description>
</property>

<property>
        <name>hadoop.proxyuser.livy.groups</name>
        <value>*</value>
        <description>Allow the user to impersonate any members of group1 and group2</description>
</property>
<property>
        <name>hadoop.proxyuser.livy.hosts</name>
        <value>hue01.bigdata.fr</value>
        <description>Allow the user to connect only from host1 and host2 to impersonate a user</description>
</property>
{noformat}
The cluster is also secured with Kerberos so I set the auth type, the principal and the keytab path in livy.conf (attached).

Unfortunately, my livy sessions are not working properly. Sessions are killed after 10 minutes in a ""startup"" state. My resourcemanager HA is not working for testing purpose and I think it's the source of the error here. The machine ""resourcemanager01.bigdata.fr is stopped"" but ""resourcemanager02.bigdata.fr"" is running. 
You can find below the stacktrace.
{noformat}
Jul 22 09:02:38 hue01.bigdata.fr livy INFO - com.cloudera.livy.server.LivyServerUsing spark-submit version 1.6.1
Jul 22 09:02:38 hue01.bigdata.fr livy INFO - org.eclipse.jetty.util.logLogging initialized @1536ms
Jul 22 09:02:38 hue01.bigdata.fr livy INFO - com.cloudera.livy.server.LivyServerSPNEGO auth enabled (principal = HTTP/hue01.bigdata.fr@SANDBOX.HADOOP)
Jul 22 09:02:38 hue01.bigdata.fr livy INFO - org.eclipse.jetty.server.Serverjetty-9.2.16.v20160414
Jul 22 09:02:39 hue01.bigdata.fr livy INFO - org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandlerLogin using keytab /opt/application/Livy/current/spnego.keytab, for principal HTTP/hue01.bigdata.fr@SANDBOX.HADOOP
Jul 22 09:02:39 hue01.bigdata.fr livy INFO - org.eclipse.jetty.server.handler.ContextHandlerStarted o.e.j.s.ServletContextHandler@6c814dbd{/,file:/src/main/com/cloudera/livy/server,AVAILABLE}
Jul 22 09:02:39 hue01.bigdata.fr livy WARN - org.eclipse.jetty.server.handler.RequestLogHandler!RequestLog
Jul 22 09:02:39 hue01.bigdata.fr livy INFO - org.eclipse.jetty.server.ServerConnectorStarted ServerConnector@57c3eb95{HTTP/1.1}{0.0.0.0:8998}
Jul 22 09:02:39 hue01.bigdata.fr livy INFO - org.eclipse.jetty.server.ServerStarted @2571ms
Jul 22 09:02:39 hue01.bigdata.fr livy INFO - com.cloudera.livy.server.WebServerStarting server on http://hue01.bigdata.fr:8998
Jul 22 09:08:24 hue01.bigdata.fr livy INFO - com.cloudera.livy.server.interactive.InteractiveSessionCreating LivyClient for sessionId: 0
Jul 22 09:08:24 hue01.bigdata.fr livy WARN - com.cloudera.livy.rsc.RSCConfYour hostname, hue01.bigdata.fr, resolves to a loopback address, but we couldn't find any external IP address!
Jul 22 09:08:24 hue01.bigdata.fr livy WARN - com.cloudera.livy.rsc.RSCConfSet livy.rsc.rpc.server.address if you need to bind to another address.
Jul 22 09:08:24 hue01.bigdata.fr livy INFO - com.cloudera.livy.sessions.SessionManagerRegistering new session 0
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher16/07/22 09:15:49 ERROR SparkContext: Error initializing SparkContext.
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncherjava.net.ConnectException: Call From hue01.bigdata.fr/192.168.200.208 to resourcemanager01.bigdata.fr:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.ipc.Client.call(Client.java:1479)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.ipc.Client.call(Client.java:1412)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at com.sun.proxy.$Proxy10.getNewApplication(Unknown Source)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:221)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at java.lang.reflect.Method.invoke(Method.java:606)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at com.sun.proxy.$Proxy11.getNewApplication(Unknown Source)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:219)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:227)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:132)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.spark.SparkContext.<init>(SparkContext.scala:530)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at py4j.Gateway.invoke(Gateway.java:214)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at py4j.GatewayConnection.run(GatewayConnection.java:209)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at java.lang.Thread.run(Thread.java:745)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncherCaused by: java.net.ConnectException: Connection refused
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011at org.apache.hadoop.ipc.Client.call(Client.java:1451)
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher#011... 28 more
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.ContextLauncher16/07/22 09:15:49 ERROR PythonInterpreter: Process has died with 1
Jul 22 09:15:49 hue01.bigdata.fr livy INFO - com.cloudera.livy.rsc.RSCClientReceived result for b256b2e5-0739-43ab-827a-0d2e9d4f6020
{noformat}","Hadoop 2.7.2, Hue 3.10.0, Spark 1.6.1, Livy 0.3 (github snapshot), Cluster secured with Kerberos",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 13 16:38:24 UTC 2017,,,,,,,,,,"0|i3iznz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Jul/16 06:46;pbeauvois;Nothing new about this JIRA?;;;","27/Jul/16 16:17;vanzin;Livy doesn't ever talk directly to YARN. So there's no way that this bug can be in Livy.

You either are giving Livy the wrong YARN configuration (which then Spark uses), or you're using a Spark built against a version of Hadoop that doesn't support RM HA. In either case, it's not a Livy issue.;;;","28/Jul/16 07:45;pbeauvois;Hi Marcelo,

I have Spark 1.6.1 built for Hadoop 2.7.2. Nothing special here. Unless I'm wrong but Hadoop 2.7.2 fully supports RM HA. Anyway. I still don't understand why I can't create a livy session successfully. The session is killed all the time.;;;","28/Jul/16 16:21;vanzin;I don't understand either, but Livy is not causing that problem, because Livy does not talk to YARN. Spark does.;;;","13/Jul/17 04:36;roman.kovalik;@Marcelo Vanzin Please correct me if I'm wrong but I have found that Livy does in fact talk to Yarn.In the `SparkYarnApp` (https://github.com/cloudera/livy/blob/v0.3.0/server/src/main/scala/com/cloudera/livy/utils/SparkYarnApp.scala) livy creates a new Yarn client which it uses to list the running applcations, get application container info and kill applications.

I have also found that providing a yarn-site.xml that does not contain `yarn.resourcemanager.hostname` which is the case when running in high availability mode will make it try to connect to `0.0.0.0/0.0.0.0:8032` eventually leading to:

{code:java}
ERROR SparkYarnApp: Error whiling refreshing YARN state: java.net.ConnectException: Call From ******/****** to 0.0.0.0:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
{code}
;;;","13/Jul/17 16:38;vanzin;Livy did not talk to YARN when this bug was filed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy not loading yarn-site temporarily after being restarted,LIVY-196,13095834,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,,kbadani,gmcdonald,17/Jul/16 19:15,17/Jul/16 23:26,19/Dec/25 04:15,17/Jul/16 23:26,0.2,,,,,Core,,,,,,,,,,0,,,,,,"After the livy server is restarted, and the first spark job is submitted through Livy - we see in the livy server logs that it is trying to connect to the RM at default address 0.0.0.0:8032 even though RM webaddress in yarn-site.xml is a non-default 


{code:java}
6/07/15 19:45:45 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
16/07/15 19:45:46 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
16/07/15 19:45:47 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
16/07/15 19:45:48 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
16/07/15 19:45:49 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
16/07/15 19:45:50 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
16/07/15 19:45:51 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
16/07/15 19:45:52 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
16/07/15 19:45:53 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
16/07/15 19:45:54 INFO Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
Exception in thread ""session app state monitor thread"" java.net.ConnectException: Call From XX.XX.XX.XX to 0.0.0.0:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor11.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1555)
	at org.apache.hadoop.ipc.Client.call(Client.java:1495)
	at org.apache.hadoop.ipc.Client.call(Client.java:1395)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at com.sun.proxy.$Proxy8.getApplicationReport(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplicationReport(ApplicationClientProtocolPBClientImpl.java:191)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy9.getApplicationReport(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplicationReport(YarnClientImpl.java:431)
	at com.cloudera.livy.sessions.SessionManager$$anonfun$2.apply(SessionManager.scala:108)
	at com.cloudera.livy.sessions.SessionManager$$anonfun$2.apply(SessionManager.scala:105)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at com.cloudera.livy.sessions.SessionManager.checkAppState(SessionManager.scala:105)
	at com.cloudera.livy.sessions.SessionManager$SessionAppStateMonitor.run(SessionManager.scala:142)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jul 17 23:26:36 UTC 2016,,,,,,,,,,"0|i3iznr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/Jul/16 22:08;tc0312;Were did you get Livy? HDInsight or Cloudera's repo?;;;","17/Jul/16 23:26;zjffdu;Close this, as this is due to our inhouse change. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Killing interactive session while spark-submit is running,LIVY-195,13095833,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,16/Jul/16 01:43,29/Jul/16 00:02,19/Dec/25 04:15,29/Jul/16 00:02,0.2,,,0.3,,Server,,,,,,,,,,0,,,,,,"User might submit a Spark job that uploads a huge jar to cluster fs. If it's taking too long, user might want to cancel the job submission.

With current RSC code, this is impossible because RSCClient doesn't expose anything about the spark-submit process. ChildProcess is hidden in ContextLauncher API and is not exposed to RSCClient and InteractiveSession. There's no way for InteractiveSession to kill the spark-submit process.

We need to expose the spark-submit process to InteractiveSession.

Does anyone know what's the best way to expose it? I'm thinking about changing ChildProcess from a private class to public and return it as a 2nd return value from ContextLauncher.create(). But this sounds hacky.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 29 00:02:12 UTC 2016,,,,,,,,,,"0|i3iznj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Jul/16 17:03;vanzin;I don't think you need to expose the underlying process to achieve this. {{RSCClient}} has a reference to the promise that monitors the spark-submit process (when {{ContextLauncher}} is used). If you close the session, you can cancel that promise, and if you install a listener on the promise in {{ContextLauncher}}, you can act on cancellation and kill the spark-submit process if it's still running.;;;","18/Jul/16 17:03;vanzin;A quick example that can be run on spark-shell to show what I mean:

{code}
import io.netty.channel.nio.NioEventLoopGroup
import io.netty.util.concurrent._

val grp = new NioEventLoopGroup(1)
val p: Promise[String] = grp.next().newPromise()
val lsnr = new GenericFutureListener[Promise[String]] {
  override def operationComplete(future: Promise[String]): Unit = {
    println(s""cancel: ${p.isCancelled}, done: ${p.isDone}, success: ${p.isSuccess} "")
  }
}
p.addListener(lsnr)
p.cancel(true)
{code}
;;;","18/Jul/16 22:10;tc0312;Thanks. I will implement it this way.
Do we have unit test for ContextLauncher in its original project?;;;","18/Jul/16 22:13;vanzin;There's no explicit test for {{ContextLauncher}} - it's tested indirectly in {{TestSparkClient}}.;;;","29/Jul/16 00:02;vanzin;Commit [d1386405|https://github.com/cloudera/livy/commit/d1386405f1ada8be5e899594cecbd34fffdba244] by  Alex Man <tc.technetium@...> in cloudera/livy:
{code}
LIVY-195. Killing interactive session while spark-submit is running.

Closes #170
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Share SparkContext across languages,LIVY-194,13095832,13098817,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,zjffdu,gmcdonald,15/Jul/16 05:08,01/Sep/17 06:27,19/Dec/25 04:15,31/Aug/17 13:08,0.2,,,0.5.0,,Interpreter,,,,,,,,,,4,,,,,,"It would be nice to share the same SparkContext across languages, so that we can leverage multiple languages for one big task.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-19,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Aug 28 14:30:06 UTC 2017,,,,,,,,,,"0|i3iznb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/Aug/16 17:17;jeffersonezra;We have a design here - https://docs.google.com/document/d/1rbcEVSg4HN_FBz4PdDTSXVATnKkhOPBIEhcAIWzzDQE/edit?usp=sharing

Please take a look and add your comments.;;;","18/Aug/17 17:59;ajbozarth;[~jerryshao]'s comment that missed the import:

Propose a compromise solution to meet with Jeff's requirement (https://docs.google.com/document/d/16HOt8yJyPlvbtDY_VyPfF9iJOmPmT4xvVtBPh0BJF9A/edit?usp=sharing).;;;","23/Aug/17 22:31;ajbozarth;Open PR by [~jerryshao] https://github.com/apache/incubator-livy/pull/28;;;","24/Aug/17 07:44;githubbot;Github user zjffdu commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/28#discussion_r134947131
  
    --- Diff: repl/scala-2.10/src/main/scala/org/apache/livy/repl/SparkInterpreter.scala ---
    @@ -102,33 +102,28 @@ class SparkInterpreter(conf: SparkConf)
               classLoader = classLoader.getParent
             }
           }
    -
    -      createSparkContext(conf)
    +      postStart(jobContext)
         }
    -
    -    sparkContext
       }
     
    -  protected def bind(name: String, tpe: String, value: Object, modifier: List[String]): Unit = {
    +  override protected def bind(name: String,
    +      tpe: String,
    +      value: Object,
    +      modifier: List[String]): Unit = {
         sparkIMain.beQuietDuring {
           sparkIMain.bind(name, tpe, value, modifier)
         }
       }
     
       override def close(): Unit = synchronized {
    -    if (sparkContext != null) {
    -      sparkContext.stop()
    -      sparkContext = null
    -    }
    -
         if (sparkIMain != null) {
    --- End diff --
    
    Where is SparkContext closed ?
;;;","24/Aug/17 07:45;githubbot;Github user zjffdu commented on the issue:

    https://github.com/apache/incubator-livy/pull/28
  
    @jerryshao Could you add document about how to use shared interpreter via rest-api ?
;;;","24/Aug/17 07:49;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/28#discussion_r134948084
  
    --- Diff: repl/scala-2.10/src/main/scala/org/apache/livy/repl/SparkInterpreter.scala ---
    @@ -102,33 +102,28 @@ class SparkInterpreter(conf: SparkConf)
               classLoader = classLoader.getParent
             }
           }
    -
    -      createSparkContext(conf)
    +      postStart(jobContext)
         }
    -
    -    sparkContext
       }
     
    -  protected def bind(name: String, tpe: String, value: Object, modifier: List[String]): Unit = {
    +  override protected def bind(name: String,
    +      tpe: String,
    +      value: Object,
    +      modifier: List[String]): Unit = {
         sparkIMain.beQuietDuring {
           sparkIMain.bind(name, tpe, value, modifier)
         }
       }
     
       override def close(): Unit = synchronized {
    -    if (sparkContext != null) {
    -      sparkContext.stop()
    -      sparkContext = null
    -    }
    -
         if (sparkIMain != null) {
    --- End diff --
    
    It is closed in JobContextImpl.
;;;","24/Aug/17 07:50;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/28
  
    I will do the doc things in the following PR.
;;;","24/Aug/17 07:51;githubbot;Github user zjffdu commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/28#discussion_r134948334
  
    --- Diff: repl/src/main/scala/org/apache/livy/repl/Session.scala ---
    @@ -77,40 +76,56 @@ class Session(
     
       private val newStatementId = new AtomicInteger(0)
     
    +  private val defaultInterpKind = Kind(livyConf.get(RSCConf.Entry.SESSION_KIND))
    +
       stateChangedCallback(_state)
     
    -  def start(): Future[SparkContext] = {
    +  private def sc: SparkContext = jobContext.sc().sc
    +
    +  def start(): Future[Unit] = {
         val future = Future {
           changeState(SessionState.Starting())
    -      val sc = interpreter.start()
    -      _sc = Option(sc)
    +
    +      // Start SparkInterpreter first to make sure SparkContext is created within Scala REPL's
    +      // classloader, otherwise the class created by REPL cannot be found by SparkContext.
    +      interpGroup.get(Spark()).foreach(_.start(jobContext))
    +      interpGroup.get(PySpark()).foreach(_.start(jobContext))
    +      interpGroup.get(SparkR()).foreach(_.start(jobContext))
    --- End diff --
    
    Does that means even scala-repl, python process and R process would always get started at the begining ? It would better to start them lazily, which means only start python process when user submit python code. 
;;;","24/Aug/17 07:53;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/28#discussion_r134948807
  
    --- Diff: repl/src/main/scala/org/apache/livy/repl/Session.scala ---
    @@ -77,40 +76,56 @@ class Session(
     
       private val newStatementId = new AtomicInteger(0)
     
    +  private val defaultInterpKind = Kind(livyConf.get(RSCConf.Entry.SESSION_KIND))
    +
       stateChangedCallback(_state)
     
    -  def start(): Future[SparkContext] = {
    +  private def sc: SparkContext = jobContext.sc().sc
    +
    +  def start(): Future[Unit] = {
         val future = Future {
           changeState(SessionState.Starting())
    -      val sc = interpreter.start()
    -      _sc = Option(sc)
    +
    +      // Start SparkInterpreter first to make sure SparkContext is created within Scala REPL's
    +      // classloader, otherwise the class created by REPL cannot be found by SparkContext.
    +      interpGroup.get(Spark()).foreach(_.start(jobContext))
    +      interpGroup.get(PySpark()).foreach(_.start(jobContext))
    +      interpGroup.get(SparkR()).foreach(_.start(jobContext))
    --- End diff --
    
    Yes, it will be started at beginning, my thinking is that the overhead is not so big. Let me think about how to handle this.
;;;","24/Aug/17 08:02;githubbot;Github user zjffdu commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/28#discussion_r134950547
  
    --- Diff: repl/src/main/scala/org/apache/livy/repl/Session.scala ---
    @@ -77,40 +76,56 @@ class Session(
     
       private val newStatementId = new AtomicInteger(0)
     
    +  private val defaultInterpKind = Kind(livyConf.get(RSCConf.Entry.SESSION_KIND))
    +
       stateChangedCallback(_state)
     
    -  def start(): Future[SparkContext] = {
    +  private def sc: SparkContext = jobContext.sc().sc
    +
    +  def start(): Future[Unit] = {
         val future = Future {
           changeState(SessionState.Starting())
    -      val sc = interpreter.start()
    -      _sc = Option(sc)
    +
    +      // Start SparkInterpreter first to make sure SparkContext is created within Scala REPL's
    +      // classloader, otherwise the class created by REPL cannot be found by SparkContext.
    +      interpGroup.get(Spark()).foreach(_.start(jobContext))
    +      interpGroup.get(PySpark()).foreach(_.start(jobContext))
    +      interpGroup.get(SparkR()).foreach(_.start(jobContext))
    --- End diff --
    
    Overhead is one concern, another concern is that the starting of one interpreter may affect other interpreters. e.g. session may fail to start if R is not installed, but user may don't need RInterpreter. 
;;;","24/Aug/17 08:27;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/28#discussion_r134955605
  
    --- Diff: repl/src/main/scala/org/apache/livy/repl/Session.scala ---
    @@ -77,40 +76,56 @@ class Session(
     
       private val newStatementId = new AtomicInteger(0)
     
    +  private val defaultInterpKind = Kind(livyConf.get(RSCConf.Entry.SESSION_KIND))
    +
       stateChangedCallback(_state)
     
    -  def start(): Future[SparkContext] = {
    +  private def sc: SparkContext = jobContext.sc().sc
    +
    +  def start(): Future[Unit] = {
         val future = Future {
           changeState(SessionState.Starting())
    -      val sc = interpreter.start()
    -      _sc = Option(sc)
    +
    +      // Start SparkInterpreter first to make sure SparkContext is created within Scala REPL's
    +      // classloader, otherwise the class created by REPL cannot be found by SparkContext.
    +      interpGroup.get(Spark()).foreach(_.start(jobContext))
    +      interpGroup.get(PySpark()).foreach(_.start(jobContext))
    +      interpGroup.get(SparkR()).foreach(_.start(jobContext))
    --- End diff --
    
    @zjffdu , currently in the repl code, if Python interpreter is created first (with SparkContext intialized), then creating Spark interpreter will have classloader issue. Please see the comments above, I'm not sure how to handle it now.
;;;","24/Aug/17 08:29;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/28#discussion_r134956079
  
    --- Diff: repl/src/main/scala/org/apache/livy/repl/Session.scala ---
    @@ -77,40 +76,56 @@ class Session(
     
       private val newStatementId = new AtomicInteger(0)
     
    +  private val defaultInterpKind = Kind(livyConf.get(RSCConf.Entry.SESSION_KIND))
    +
       stateChangedCallback(_state)
     
    -  def start(): Future[SparkContext] = {
    +  private def sc: SparkContext = jobContext.sc().sc
    +
    +  def start(): Future[Unit] = {
         val future = Future {
           changeState(SessionState.Starting())
    -      val sc = interpreter.start()
    -      _sc = Option(sc)
    +
    +      // Start SparkInterpreter first to make sure SparkContext is created within Scala REPL's
    +      // classloader, otherwise the class created by REPL cannot be found by SparkContext.
    +      interpGroup.get(Spark()).foreach(_.start(jobContext))
    +      interpGroup.get(PySpark()).foreach(_.start(jobContext))
    +      interpGroup.get(SparkR()).foreach(_.start(jobContext))
    --- End diff --
    
    SparkContext should be created with this wrapper in SparkInterpreter, I'm not sure how Zeppelin handle this issue.
    
    ```scala
    
        restoreContextClassLoader {
          // Call sparkIMain.setContextClassLoader() to make sure SparkContext and repl are using the
          // same ClassLoader. Otherwise if someone defined a new class in interactive shell,
          // SparkContext cannot see them and will result in job stage failure.
          val setContextClassLoaderMethod = sparkIMain.getClass().getMethod(""setContextClassLoader"")
          setContextClassLoaderMethod.setAccessible(true)
          setContextClassLoaderMethod.invoke(sparkIMain)
    
          // With usejavacp=true, the Scala interpreter looks for jars under System Classpath. But it
          // doesn't look for jars added to MutableURLClassLoader. Thus extra jars are not visible to
          // the interpreter. SparkContext can use them via JVM ClassLoaders but users cannot import
          // them using Scala import statement.
          //
          // For instance: If we import a package using SparkConf:
          // ""spark.jars.packages"": ""com.databricks:spark-csv_2.10:1.4.0""
          // then ""import com.databricks.spark.csv._"" in the interpreter, it will throw an error.
          //
          // Adding them to the interpreter manually to fix this issue.
          var classLoader = Thread.currentThread().getContextClassLoader
          while (classLoader != null) {
            if (classLoader.getClass.getCanonicalName == ""org.apache.spark.util.MutableURLClassLoader"")
            {
              val extraJarPath = classLoader.asInstanceOf[URLClassLoader].getURLs()
                // Check if the file exists. Otherwise an exception will be thrown.
                .filter { u => u.getProtocol == ""file"" && new File(u.getPath).isFile }
                // Livy rsc and repl are also in the extra jars list. Filter them out.
                .filterNot { u => Paths.get(u.toURI).getFileName.toString.startsWith(""livy-"") }
                // Some bad spark packages depend on the wrong version of scala-reflect. Blacklist it.
                .filterNot { u =>
                  Paths.get(u.toURI).getFileName.toString.contains(""org.scala-lang_scala-reflect"")
                }
    
              extraJarPath.foreach { p => debug(s""Adding $p to Scala interpreter's class path..."") }
              sparkIMain.addUrlsToClassPath(extraJarPath: _*)
              classLoader = null
            } else {
              classLoader = classLoader.getParent
            }
          }
          postStart(jobContext)
        }
    ```
;;;","28/Aug/17 07:17;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/28
  
    @zjffdu , can you please help to review again. Based on your comments, now `PythonInterpreter` and `SparkRInterpreter` are started lazily until the related statement submitted. I think the core logics should not be so difficult to review (getting rid of all UI changes).
    
    Also CC @ajbozarth to help to review.
    
    Thanks a lot.
;;;","28/Aug/17 14:13;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/28#discussion_r135535099
  
    --- Diff: client-common/src/main/java/org/apache/livy/client/common/HttpMessages.java ---
    @@ -82,13 +82,15 @@ private SessionInfo() {
       public static class SerializedJob implements ClientMessage {
     
         public final byte[] job;
    +    public final String jobType;
     
    -    public SerializedJob(byte[] job) {
    +    public SerializedJob(byte[] job, String jobType) {
    --- End diff --
    
    Because we now support different interpreters in one session, so we should have a way to figure out which type of job it is, is it a Spark job or a python job that should go to related interpreters.
;;;","28/Aug/17 14:27;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/28#discussion_r135538524
  
    --- Diff: core/src/main/scala/org/apache/livy/msgs.scala ---
    @@ -28,7 +28,7 @@ case class Msg[T <: Content](msg_type: MsgType, content: T)
     
     sealed trait Content
     
    -case class ExecuteRequest(code: String) extends Content {
    +case class ExecuteRequest(code: String, kind: Option[String]) extends Content {
    --- End diff --
    
    Also for the statement submission, we need to a field to differentiate what kind of code submitted. Here using `Option` means if it is `None` then we will choose the default kind which is specified in session creation (like Spark, PySpark, SparkR). If we specify `Shared` in session creation, here we must include kind in statement submission, otherwise it will throw exception.
;;;","28/Aug/17 14:30;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/28#discussion_r135539313
  
    --- Diff: repl/scala-2.11/src/main/scala/org/apache/livy/repl/SparkInterpreter.scala ---
    @@ -89,16 +90,17 @@ class SparkInterpreter(conf: SparkConf)
             }
           }
     
    -      createSparkContext(conf)
    +      entries = new SparkEntries(conf)
    --- End diff --
    
    The SparkInterpreter is responsible to create a `SparkEntries` which is the entries for Spark application. Python interpreter and R interpreter will require this `SparkEntries` to initialize python and R context.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove field timeout of Session,LIVY-193,13095831,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,ajbozarth,zjffdu,gmcdonald,15/Jul/16 01:51,01/Aug/17 05:15,19/Dec/25 04:15,22/Feb/17 01:36,0.2,,,,,Core,,,,,,,,,,0,,,,,,"For now it seems timeout in Session is just for a minimum value of timeout. I don't think we should hard code the minimum value of timeout as it would make it difficult to do session timeout testing. I think we can just make 1 hour as the default timeout and user can set whatever value they want through livy.server.session.timeout


{code}
    def expired(session: Session): Boolean = {
      val currentTime = System.nanoTime()
      currentTime - session.lastActivity > math.max(sessionTimeout, session.timeout)
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 14 23:29:20 UTC 2017,,,,,,,,,,"0|i3izn3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Feb/17 23:29;ajbozarth;New JIRA duplicates this and has an open pr;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python Client API - Server side changes,LIVY-192,13095830,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mnarayan,mnarayan,gmcdonald,14/Jul/16 21:50,25/Aug/16 21:28,19/Dec/25 04:15,25/Aug/16 21:28,0.3,,,0.3,,Interpreter,RSC,,,,,,,,,0,,,,,,"Changes involve

* Change BypassRequests of RSC to process jobs from the python-client
* Deserialization of the pickled objects sent over HTTP from the python-client in the Python Interpreter",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 25 21:28:16 UTC 2016,,,,,,,,,,"0|i3izmv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Aug/16 21:28;vanzin;Commit [c49ee5ae|https://github.com/cloudera/livy/commit/c49ee5ae24477bdd33b554df3356267f051d5585] by  manikandan.nagarajan <manikandan.n2010@...> in cloudera/livy:
{code}
LIVY-192. Python Client API - Server side changes

- Server side changes to handle requests from the Python Client API
- Added Integration tests

Closes #182
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rest api for getting all sessions should order sessions by id,LIVY-189,13095827,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,zjffdu,zjffdu,gmcdonald,08/Jul/16 21:13,11/Jul/16 18:08,19/Dec/25 04:15,11/Jul/16 18:08,0.2,,,0.3,,Core,,,,,,,,,,0,,,,,,"Since we also provide parameters of ""from"" and ""size"" for this rest api. It is better to order sessions by session id.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jul 11 18:08:46 UTC 2016,,,,,,,,,,"0|i3izm7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Jul/16 18:08;vanzin;Commit [4dcfbadd|https://github.com/cloudera/livy/commit/4dcfbadd15e69c96aabf9251f0253aa24f4da705] by  Jeff Zhang <zjffdu@...> in cloudera/livy:
{code}
LIVY-189. Rest api for getting all sessions should order sessions by id

Change default HashMap to LinkedHashMap to make the sessions ordered.

Closes #162
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table magic crash on large dataset included unicode string,LIVY-188,13095826,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,purechoc,purechoc,gmcdonald,07/Jul/16 07:47,13/Jul/16 22:20,19/Dec/25 04:15,13/Jul/16 22:20,0.2,0.3,,0.3,,Interpreter,RSC,,,,,,,,,0,,,,,,"in python notebook, using table magic with large dataset include unicode string.
return Exception.

Test Code
{code:none}
test_map = []
for x in range(0, 100, 1) :
    test_list = []
    for y in range(0, 100, 1) :
        test_list.append(x)
        test_list.append(y)
        test_list.append(u""\u263A"")
    test_map.append(test_list)

%table test_map
{code}

Livy Server Exception
{code:none}
io.netty.handler.codec.DecoderException: com.cloudera.livy.shaded.kryo.kryo.KryoException: Buffer underflow.
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:280)
        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:149)
        at io.netty.handler.codec.ByteToMessageCodec.channelRead(ByteToMessageCodec.java:108)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
        at java.lang.Thread.run(Thread.java:745)
Caused by: com.cloudera.livy.shaded.kryo.kryo.KryoException: Buffer underflow.
        at com.cloudera.livy.shaded.kryo.kryo.io.Input.require(Input.java:164)
        at com.cloudera.livy.shaded.kryo.kryo.io.Input.readUtf8_slow(Input.java:561)
        at com.cloudera.livy.shaded.kryo.kryo.io.Input.readUtf8(Input.java:535)
        at com.cloudera.livy.shaded.kryo.kryo.io.Input.readString(Input.java:465)
        at com.cloudera.livy.shaded.kryo.kryo.serializers.DefaultSerializers$StringSerializer.read(DefaultSerializers.java:171)
        at com.cloudera.livy.shaded.kryo.kryo.serializers.DefaultSerializers$StringSerializer.read(DefaultSerializers.java:160)
        at com.cloudera.livy.shaded.kryo.kryo.Kryo.readClassAndObject(Kryo.java:776)
        at com.cloudera.livy.client.common.Serializer.deserialize(Serializer.java:63)
        at com.cloudera.livy.rsc.rpc.KryoMessageCodec.decode(KryoMessageCodec.java:77)
        at io.netty.handler.codec.ByteToMessageCodec$1.decode(ByteToMessageCodec.java:42)
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:249)
{code}

tested By master branch, spark 1.6.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 13 22:20:58 UTC 2016,,,,,,,,,,"0|i3izlz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"08/Jul/16 01:11;purechoc;another testcase.
{code:none}
 #TestSerializer.java
  @Test
  public void testUnicodeSerializer() throws Exception {
    List list = new ArrayList();
    for (int x = 0; x < 1000; x++) {
      list.add(""""+x);
      list.add(""\u263A"");
    }
    String testMessage = list.toString();
    Object decoded = doSerDe(testMessage);
    assertEquals(testMessage, decoded);
  }
{code};;;","11/Jul/16 08:35;purechoc;this is not table magic problem.
I think this is KryoSerializer bug.

need to change kryo serde method usage.;;;","13/Jul/16 22:20;vanzin;Commit [a685ecc9|https://github.com/cloudera/livy/commit/a685ecc9e07803bb4ef1660c1daff3dd0274e5cd] by  purechoc <purechoc.en@...> in cloudera/livy:
{code}
LIVY-188. 	change kryo serde method usage

currently kryo serde method usage has bug when SerDe long string include unicode.

may be this is kryo bug.
but, may be this is wrong method usage.

so using byte array instead of ByteBufferInputStream.

Closes #168
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
livy think session is still alive when yarn app is killed,LIVY-187,13095825,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,zjffdu,zjffdu,gmcdonald,07/Jul/16 03:45,07/Jul/16 18:16,19/Dec/25 04:15,07/Jul/16 18:16,0.2,,,,,RSC,Server,,,,,,,,,0,,,,,,,"Start a session in yarn-cluster mode and then kill the yarn app, but livy still think the session is alive, state is idle but should be dead",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 07 18:16:08 UTC 2016,,,,,,,,,,"0|i3izlr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Jul/16 06:43;tc0312;LIVY-186 will fix this;;;","07/Jul/16 17:52;zjffdu;Thanks @Alex Man, do you have ETA for that ? Because I think this is very critical bug.  And I suspect this would also happens when yarn app is failed as it is very likely to happen after a period of time. If livy can not detect the yarn app state after it is launched properly, this is very bad. ;;;","07/Jul/16 17:55;vanzin;Alex mentioned that LIVY-186 would fix it for YARN, but I still think implementing LIVY-139 would be a better solution for this particular problem, since it's master-agnostic.;;;","07/Jul/16 17:56;vanzin;(In fact this could be considered a duplicate of LIVY-139.);;;","07/Jul/16 18:16;zjffdu;Thanks Marcelo. Close it as duplicate.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better YARN integration for Interactive Session,LIVY-186,13095824,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,07/Jul/16 01:06,07/Sep/16 17:58,19/Dec/25 04:15,07/Sep/16 17:58,0.2,,,0.3,,Server,,,,,,,,,,0,,,,,,"# State
Interactive session's state does not transit to dead state if the RSCDriver YARN application has been killed in YARN. User won't see any error until they submit another job/statement.
To fix this, when running with YARN, interactive session should poll and reflect RSCDriver's YARN application state.
# Ability to kill driver when it's unresponsive
Interactive session relies on client.stop() to stop the RSCDriver. It cannot force kill the application when RSCDriver is unresponsive. Interactive session should first try client.stop(). If it fails, it should issue a YARN kill to make sure no YARN application is leaked.

Behaviour in local mode should be unchanged.

This is also a subtask of Livy HA/Recovery.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 04 23:17:15 UTC 2016,,,,,,,,,,"0|i3izlj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/Jul/16 00:31;tc0312;We noticed a bunch of RSC related classes are in Java but not Scala. What's the reason behind this?;;;","16/Jul/16 00:43;vanzin;Because they were developed as part of a Java application. No point in converting them to Scala. Also it means that those classes can be deployed on different scala runtimes without recompilation (unlike, for example, all the repl stuff).;;;","03/Aug/16 02:42;tc0312;To prevent RSCDriver from leaking, I'm modifying InteractiveSession to issue a YARN kill if RSCClient.stop() fails.

I noticed that RSCClient.stop() doesn't throw when it fails to stop the RSCDriver. Instead it prints warnings.
Is it a good idea to modify the following catch block in RSCClient to throw an error when it times out? Then InteractiveSession will catch it and call app.kill() to prevent RSCDriver leakage.

{code:java}
try {
	driverRpc.get().getChannel().closeFuture().get(stopTimeout, TimeUnit.MILLISECONDS);
} catch (Exception e) {
	LOG.warn(""Error waiting for context to shut down: {} ({})."", e.getClass().getSimpleName(), e.getMessage());
}
{code};;;","03/Aug/16 23:11;vanzin;I don't see a better way of doing it; also, since the method doesn't declare any exceptions, you'll have to make sure you throw an unchecked exception.

(While there, you could get rid of the nested {{try}} blocks and just keep the one catch right under the lines you mention.);;;","04/Aug/16 21:39;tc0312;Is there a requirement on maintaining RSCClient interface unchanged? So instead of adding a throws clause to stop(), throwing an unchecked exception is preferred?;;;","04/Aug/16 23:17;vanzin;That method overrides a public API. Adding a throws clause changes the method signature = potentially breaks existing code. Don't remember if adding exceptions also breaks binary compatibility.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update jacoco version and generate aggregate reports,LIVY-185,13095823,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,07/Jul/16 01:03,07/Jul/16 21:19,19/Dec/25 04:15,07/Jul/16 21:19,0.3,,,0.3,,Core,,,,,,,,,,0,,,,,,"The most recent version of jacoco can create aggregate reports for all modules, which better reflects our test coverage given that we don't necessarily test all code paths for a module as part of the module's unit tests. For example, this allows us to account for integration test coverage too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 07 21:19:49 UTC 2016,,,,,,,,,,"0|i3izlb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Jul/16 01:10;tc0312;This will render https://github.com/cloudera/livy/pull/157 useless. I will close it.;;;","07/Jul/16 01:13;vanzin;It might still be valid; there are some codecov.io configs that can be added to ignore certain code if it's showing up in the coverage.;;;","07/Jul/16 21:19;vanzin;Commit [38343cc6|https://github.com/cloudera/livy/commit/38343cc680254c6c0870b1ad69f62b2d534f3658] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-185. Update jacoco and generate aggregate reports.

To account for coverage added by integration tests, generate aggregate reports,
and hook up the jacoco command line arguments to the Livy server run as part of
the integration tests.

Closes #161
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better YARN integration for Batch Session,LIVY-184,13095822,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,07/Jul/16 00:54,16/Aug/16 18:42,19/Dec/25 04:15,16/Aug/16 18:42,0.2,,,0.3,,Server,,,,,,,,,,0,,,,,,"Right now batch session is just a controller of spark-submit.
It's possible to provide better YARN integration when Livy is running with YARN. We can wire log() and stop() to go through YARN.
log() can return YARN diagnostics instead of returning spark-submit's not that useful stdout & stderr.
stop() can call YARN kill instead of killing spark-submit process which will guarantee killing the YARN application to avoid resource leakage in the cluster.

With this, we can also set waitAppCompletion to false to ditch the long running spark-submit process.

Behaviour in local mode should be unchanged.

This is also a subtask of Livy HA/Recovery.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 16 18:42:18 UTC 2016,,,,,,,,,,"0|i3izl3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Jul/16 01:00;tc0312;To implement this, we can introduce interface ""SparkApp"" that implements log() and stop(). SparkApp will also emit events when the underlying application is dead.

It has 2 implementations:
# SparkYarnApp will route calls to YARN. It's used only when livy.spark.master is YARN.
# SparkProcApp will be a controller of spark-submit. It's used when livy.spark.master is not YARN.

log() and stop() logic will be refactored from BatchSession to SparkApp implementations.;;;","11/Jul/16 23:53;tc0312;This change requires spark.yarn.tags, which is introduced in Spark 1.6.
It means after this change, Livy will no longer support Spark older than 1.6.

We discussed with Cloudera a while ago and think this is fine since Spark is moving fast. We would like to know are there any concerns in the community?;;;","12/Jul/16 23:29;vanzin;Yes, we're fine with moving to 1.6. If someone wants to pick up LIVY-153 to make it more official, that'd also be great. :-);;;","16/Aug/16 18:42;vanzin;Commit [6a32fd55|https://github.com/cloudera/livy/commit/6a32fd55a4e552c309253b3d6c7460ba95c9bcd3] by  Alex Man <tc.technetium@...> in cloudera/livy:
{code}
LIVY-184. Better YARN integration for Batch Session.

When livy is running with YARN master:
- Stop and log session using YARN API.
- Exposed app id.
- Livy now requires Spark 1.6+. Added a check so Livy won't start if it's running with older Spark.

Test changes:
- Added a test to verify killing the batch session will kill its YARN app.
- Added a test to verify killing the YARN app will change the batch session state.
- Added YarnClient integration in test framework.
- Added halt mode in SimpleSparkApp in test lib for kill session test.
- Added a method to dump logs when Batch test fails.
- Added a Clock interface to allow unit testing to mock out time.

Closes #163
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
livy-main pom file is not being published to repo,LIVY-183,13095821,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,vanzin,vanzin,gmcdonald,05/Jul/16 17:12,05/Jul/16 20:07,19/Dec/25 04:15,05/Jul/16 20:07,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"As title says.

https://repository.cloudera.com/artifactory/cloudera-repos/com/cloudera/livy/livy-main/0.2.0/livy-main-0.2.0.pom",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jul 05 20:07:17 UTC 2016,,,,,,,,,,"0|i3izkv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Jul/16 20:07;vanzin;Fixed poms, and refreshed the 0.2 repo to contain the parent pom.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add SPARK_HOME/conf to classpath when SPARK_CONF_DIR is not defined,LIVY-182,13095820,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,jonalter,zjffdu,gmcdonald,05/Jul/16 17:01,23/Feb/17 20:22,19/Dec/25 04:15,23/Feb/17 20:22,0.2,,,,,Core,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 23 20:22:57 UTC 2017,,,,,,,,,,"0|i3izkn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/Feb/17 22:38;jonalter;PR: [#296|https://github.com/cloudera/livy/pull/296];;;","23/Feb/17 20:22;jonalter;Will be fixed by LIVY-246;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
addJar support for interactive sessions,LIVY-181,13095819,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Duplicate,,tc0312,gmcdonald,29/Jun/16 01:00,14/Jul/16 00:12,19/Dec/25 04:15,14/Jul/16 00:11,0.3,,,,,Server,,,,,,,,,,0,,,,,,"From [PR#146|https://github.com/cloudera/livy/pull/146]
bq. vanzin@: Also, I know that's currently not used by interactive sessions, but Livy allows adding jars to running sessions (that's exposed in the REST API); it would be good at some point to allow that to work for interactive sessions too. No need to do anything here, but filing a bug would be nice.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 14 00:12:26 UTC 2016,,,,,,,,,,"0|i3izkf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Jul/16 00:12;tc0312;Dup of LIVY-191;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add docs to the Scala-API,LIVY-180,13095818,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mnarayan,mnarayan,gmcdonald,28/Jun/16 21:15,09/Aug/16 17:53,19/Dec/25 04:15,09/Aug/16 17:53,0.3,,,0.3,,Docs,,,,,,,,,,0,,,,,,"Add docs for the Livy scala client. This should have been a sub-task of 
https://issues.cloudera.org/browse/LIVY-92 
Since that task is complete, I have created a new task",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 09 17:53:15 UTC 2016,,,,,,,,,,"0|i3izk7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Aug/16 17:53;vanzin;Commit [eaf99f53|https://github.com/cloudera/livy/commit/eaf99f53563f732a2001b4301a2defd23f7b24e7] by  manikandan.nagarajan <manikandan.n2010@...> in cloudera/livy:
{code}
LIVY-180. Add docs to the Scala-API

Closes #175
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark sessions don't support unicode output,LIVY-177,13095815,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,17/Jun/16 23:10,19/Jun/16 20:27,19/Dec/25 04:15,19/Jun/16 20:27,0.2,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"{code:none}
print(""\u2713"")
{code}
will return an error: 'ascii' codec can't decode byte",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Jun 19 20:27:49 UTC 2016,,,,,,,,,,"0|i3izjj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"19/Jun/16 20:27;vanzin;Commit [9071339f|https://github.com/cloudera/livy/commit/9071339f5c1a508c66d33b8651dd5bcc0fe48764] by  Alex Man <tc.technetium@...> in cloudera/livy:
{code}
LIVY-177. Added unicode support output for PySpark session.

fake_shell is using cStringIO to capture stdout and stderr.
cStringIO doesn't support unicode. Replacing it with io.StringIO.
io.StringIO supports unicode only, convert every output to utf8.

Closes #155
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add docs for the Scala API,LIVY-175,13095813,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,mnarayan,gmcdonald,16/Jun/16 22:35,23/Jan/18 20:22,19/Dec/25 04:15,23/Jan/18 20:22,0.3,,,0.5.0,,Docs,,,,,,,,,,0,,,,,,Add appropriate docs for the methods exposed by Scala API,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-141,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 23 20:22:52 UTC 2018,,,,,,,,,,"0|i3izj3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"21/Aug/17 23:52;ajbozarth;Given the Scala API is an implementation/extension of the JAVA API I plan to look into and possibly address this alongside LIVY-141;;;","22/Aug/17 06:07;jerryshao;I think we can address this in the next version.;;;","22/Aug/17 19:42;ajbozarth;[~jerryshao] I agree, I don't expect this work to go out in 0.4.0;;;","18/Jan/18 00:32;ajbozarth;Note: [https://github.com/apache/incubator-livy/pull/38]Â will add scaladocs to the Livy Documentation, thus addressing this JIRA.;;;","18/Jan/18 22:42;ajbozarth;The above PR addresses this Jira by building the scaladoc for the Scala API module and including them in the Livy Docs build which is included on the Apache Livy website. The current scaladoc comments in the code seemed adequate for documentation. Given the Scala API is an extensionÂ of the Java API I did not add an example of how to use the API in your code since the Java example already shows that. If a such an example is further desired a new Jira can be opened in the future.Â ;;;","23/Jan/18 20:22;ajbozarth;Issue resolved by pull request 38
[https://github.com/apache/incubator-livy/pull/38];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSRF protection in livy rest api,LIVY-174,13095812,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,16/Jun/16 01:44,22/Jun/16 03:56,19/Dec/25 04:15,22/Jun/16 03:56,0.2,,,0.3,,API,Core,,,,,,,,,0,,,,,,"Some details on CSRF https://www.owasp.org/index.php/Cross-Site_Request_Forgery_(CSRF)_Prevention_Cheat_Sheet

Here's one blog post about how to fix it. 
http://blog.alutam.com/2011/09/14/jersey-and-cross-site-request-forgery-csrf/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jun 22 03:56:31 UTC 2016,,,,,,,,,,"0|i3iziv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Jun/16 03:56;vanzin;Commit [dd9b036e|https://github.com/cloudera/livy/commit/dd9b036e15724354bc3f88cb647454a43f266f2d] by  Jeff Zhang <zjffdu@...> in cloudera/livy:
{code}
LIVY-174. CSRF protection in livy rest api

Add CSRFFilter in livy server. By default it is false, so it won't bring incomptability and break the existing application.

Closes #154
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add examples sub module,LIVY-173,13095811,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mnarayan,zjffdu,gmcdonald,15/Jun/16 08:48,11/Aug/16 18:40,19/Dec/25 04:15,11/Aug/16 18:40,0.2,,,0.3,,Core,,,,,,,,,,0,,,,,,There's no much document and examples of how to use the new client api. It should be better to have that in a examples sub modules.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Aug 11 18:40:45 UTC 2016,,,,,,,,,,"0|i3izin:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Aug/16 18:40;vanzin;Commit [200086e4|https://github.com/cloudera/livy/commit/200086e4bd11b8131f1e9301acd24a77c0e6e977] by  manikandan.nagarajan <manikandan.n2010@...> in cloudera/livy:
{code}
LIVY-173. Examples module

Simple WordCount example using Scala-API

Closes #176
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client side implementation,LIVY-171,13095809,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mnarayan,mnarayan,gmcdonald,14/Jun/16 21:01,30/Aug/16 17:39,19/Dec/25 04:15,30/Aug/16 17:38,0.3,,,0.3,,API,,,,,,,,,,0,,,,,,Implementation of the Client side API,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Aug 30 17:38:50 UTC 2016,,,,,,,,,,"0|i3izi7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/Aug/16 17:38;vanzin;Commit [4a681a2f|https://github.com/cloudera/livy/commit/4a681a2f3f2b6de3fcb5de80f55dbbf9fafbf733] by  manikandan.nagarajan <manikandan.n2010@...> in cloudera/livy:
{code}
LIVY-171. Client side Python Job API.

Closes #183
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python-friendly API for Livy client sessions,LIVY-169,13095807,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mnarayan,mnarayan,gmcdonald,14/Jun/16 20:58,18/Aug/17 18:23,19/Dec/25 04:15,06/Dec/16 06:21,0.3,,,0.3,,API,,,,,,,,,,0,,,,,,"Currently, there's an existing Java and Scala API for Livy. This will be a Python API which will have similar functionality to the Java API",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-170,LIVY-172,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-06-14 20:58:35.0,,,,,,,,,,"0|i3izhr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy Scala interpreter doesn't handle comments at the end of statements properly,LIVY-167,13095805,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,13/Jun/16 21:27,16/Jun/16 21:34,19/Dec/25 04:15,16/Jun/16 21:34,0.2,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"{code:java}
1

// comment
{code}
{code:java}
1
/*

multi line

*/
{code}
return incomplete statement.
The correct output should be ""1"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 16 21:34:20 UTC 2016,,,,,,,,,,"0|i3izhb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/Jun/16 21:34;vanzin;Commit [c6c12e02|https://github.com/cloudera/livy/commit/c6c12e02edecb50ed4a280eb236ab99d7ae32559] by  Lin Chan <linchan@...> in cloudera/livy:
{code}
LIVY-167. Fixed last line of interactive statements cannot be comments.

[LIVY-167] (https://issues.cloudera.org/projects/LIVY/issues/LIVY-167)

Closes #152
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
session.id need to be increase continuously,LIVY-166,13095804,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,purechoc,gmcdonald,13/Jun/16 08:24,11/Nov/16 05:49,19/Dec/25 04:15,11/Nov/16 05:49,0.3,,,0.3,,Server,,,,,,,,,,0,,,,,,"currently, when livy startup session.id started always zero.

# user A save notebook {session,id:0, kind:pyspark}
# livy server restart.
# user A create notebook"" {session,id:0, kind:pyspark}
# user A fetch saved notebook's result. but unfortunately user A will see notebook"" result.  

so. i think session.id need to be increase continuously. even if livy server restarted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 20:22:07 UTC 2016,,,,,,,,,,"0|i3izh3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Jun/16 08:42;zjffdu; Do you save the sessionId when saving notebook and reuse that sessionId when restoring the notebook? I don't feel it make sense to do that as session will lost due to lots of reasons like session failure , expired and etc. I think it is make more sense to fix it in notebook side. BTW, what kind of notebook do you use ?;;;","13/Jun/16 13:08;purechoc;may be, this is notebook side issue. 
but, notebook connect to livy, and received sessionId 4, that notebook execute code like this
POST http://livy-host:8998/sessions/4 ...
and... 30 min after.
GET http://livy-host:8998/sessions/4
in this time, livy restarted and another user make notebook 4, 
i don't think, this is notebook side issue,

i'm using Hue notebook. (Hue 3.10)
when I load saved notebook and execute code, hue try connect livy reuse sessionId.
 ;;;","13/Jun/16 13:17;zjffdu;I think here security can help. Each user's session should be protected so that everyone can only access his own session. ;;;","13/Jun/16 20:22;tc0312;Livy session recovery will fix this. I'm working on upstreaming the feature.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stack trace is shown in evalue in Interactive Scala session,LIVY-164,13095802,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,tc0312,tc0312,gmcdonald,11/Jun/16 01:08,16/Jun/16 21:07,19/Dec/25 04:15,16/Jun/16 21:07,0.2,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"It should be shown in traceback.
Should also hide Livy internal frames in the stack trace.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 16 21:07:40 UTC 2016,,,,,,,,,,"0|i3izgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/Jun/16 21:07;vanzin;Commit [a8af54ca|https://github.com/cloudera/livy/commit/a8af54cae23a34e7575e445d371414d65ee16213] by  Alex Man <tc.technetium@...> in cloudera/livy:
{code}
LIVY-164. Fixed and cleaned up stack trace return from SparkInterpreter.

- Moved stack trace from ""evalue"" to ""traceback"".
- Filtered out internal frames from stack trace.

Closes #150
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interactive Python session doesn't return traceback on error,LIVY-163,13095801,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,tc0312,tc0312,gmcdonald,11/Jun/16 01:07,13/Jun/16 19:03,19/Dec/25 04:15,13/Jun/16 19:03,0.2,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,Should also hide Livy internal frames in the stack trace.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 13 19:03:21 UTC 2016,,,,,,,,,,"0|i3izgf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Jun/16 19:03;vanzin;Commit [230e3d08|https://github.com/cloudera/livy/commit/230e3d08b1ae293f0c12a63726da6f568a70880c] by  Alex Man <tc.technetium@...> in cloudera/livy:
{code}
LIVY-163. Fixed missing Python traceback in interactive mode.

- Also stripped the frame of fake_shell in the traceback. It should be transparent to users.

Closes #147
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Spark 2,LIVY-162,13095800,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,zjffdu,gmcdonald,11/Jun/16 00:01,18/Aug/17 18:20,19/Dec/25 04:15,03/Oct/16 17:45,0.2,,,0.3,,Core,REPL,,,,,,,,,5,,,,,,"Spark 2 has some new features and is incompatible with spark 1.x, just create this ticket to track the work to support spark 2 in livy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 03 17:45:39 UTC 2016,,,,,,,,,,"0|i3izg7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Aug/16 20:56;zyork;Spark 2 has been released. Any idea when Livy will support it?;;;","16/Aug/16 03:14;jerryshao;Is there anyone working on this issue [~vanzin]?

We are planning to make a proposal to support Livy with different versions of Spark :

1. Make Livy build against different versions of Spark, even major release, like what Zeppelin did for Spark interpreter.
2. Ideally we would like Livy to launch different sessions which can support different versions of Spark simultaneously.

It would be great to hear the feedbacks from community, thanks a lot.;;;","03/Oct/16 17:45;vanzin;Commit [0fa2120a|https://github.com/cloudera/livy/commit/0fa2120a2444ec4f7f6277372b1aca71598275c5] by  jerryshao <sshao@...> in cloudera/livy:
{code}
LIVY-162. Livy to support Spark 2.0 + Scala 2.11 cross build

This PR adds support to build against Spark 2.0 and Scala 2.11, what this PR addressed:

1. Support building against different Spark version with profile like -Pspark-1.6, -Pspark-2.0.
2. Support building against different version of Scala through profile: -Pscala-2.10 -Pscala-2.11.
3. Refactor SparkInterpret to support different Scala version + Spark version.

What this PR doesn't address:

1. Batch API to support SparkSession. Currently batch API still uses old API for `SparkContext`, `SQLContext` and `HiveContext`.

This PR is tested with following scenarios:

1. Spark 1.6 + Scala 2.10
2. Spark 1.6 + Scala 2.11
3. Spark 2.0 + Scala 2.10
3. Spark 2.0 + Scala 2.11

Please suggest and comment, any feedback is greatly appreciated.

Closes #186
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy internal configs show up in Spark History Server,LIVY-161,13095799,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,jerryshao,vanzin,gmcdonald,06/Jun/16 20:50,21/Oct/16 01:28,19/Dec/25 04:15,21/Oct/16 01:28,0.2,,,0.3,,RSC,,,,,,,,,,0,,,,,,"To pass Livy-specific options to the Spark driver, the RSC stashes them in the Spark configuration with the {{spark.__livy__}} prefix. The issue is that those show up in the ""Environment"" tab of the Spark UI and get written down to Spark event logs; so things like the secret used for authentication are visible.

We need to remove those from the Spark config to be able to run Livy with security enabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-06-06 20:50:29.0,,,,,,,,,,"0|i3izfz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support selecting python 2 or 3 for pyspark batch,LIVY-159,13095797,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,,KevinGre,gmcdonald,03/Jun/16 21:39,11/Nov/16 05:49,19/Dec/25 04:15,11/Nov/16 05:49,0.2,,,,,Batch,,,,,,,,,,1,,,,,,"currently pyspark working with only python2.
(fake_shell.py don't work on python3)

Might need a support both python version.

how about this.
AS-IS
{code:none}
kind = [pyspark, spark, sparkr]
{code}
AS-IS
{code:none}
# if user choice pyspark3,  PythonInterperter run fake_shell3.py
kind = [pyspark, spark, sparkr, pyspark3]
{code}

If i misunderstood, this ticket close please.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 21 22:02:04 UTC 2016,,,,,,,,,,"0|i3izfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Jun/16 23:13;KevinGre;Related issue for interactive support.;;;","03/Jun/16 23:28;KevinGre;Description: (unable to edit description) so adding detail here:

A batch submitter needs to be able to specify Python2 or Python3 for each PySpark job. Setting the spark conf setting spark.yarn.appMasterEnv.PYSPARK_PYTHON on the POST is not sufficient as the PYSPARK_PYTHON environment variable needs to be set for the spark-submit execution (when the app master is running it is too late). Currently there is no way to do this via Livy batch POST, and the Python version can only be set globally, changing all batch (and interactive) sessions.

The simplest change to support the python 2 or 3 choice on each batch submit would be adding an optional PYSPARK_PYTHON value to batch POST that would override the default. This would commonly be set to ""python3"" or ""python2"". Just as batch POST already exposes the other parameters of spark-submit, this allows this additional setting to passed through to spark-submit (via exporting an environment variable before calling it).

An alternative would be not to just pass through the environment variable value, but have Livy accept an enumerated value (say pythonversion with value 2 or 3) where each value maps to a specific value for PYSPARK_PYTHON. That is, Livy would gain two new settings; PYSPARK_PYTHON2, PYSPARK_PYTHON3. I'm assuming it is preferred instead to just pass the value through rather than add more Livy configuration. Please comment if you have reason for doing this more complex fix.
;;;","03/Jun/16 23:38;vanzin;You could do this in yarn-cluster mode using {{spark.yarn.appMasterEnv.PYSPARK_PYTHON}}, right?

For non-yarn-cluster mode I'm a little worried about exposing something like this, since without some kind of white list it could be used to execute arbitrary commands on the Livy server as the Livy user. But then, non-yarn-cluster mode can't really be secured anyway (not without using containers or something like that).;;;","04/Jun/16 03:37;KevinGre;I tried that conf setting. ""Setting the spark conf setting spark.yarn.appMasterEnv.PYSPARK_PYTHON on the POST is not sufficient"" as it appears it gets applied too late and spark-submit has already set the Python version, at least that is what my experiments found when running spark-submit at the command line. I copied the spark-submit command exactly from the livy logs and added setting that conf item and it had no effect on the Python version run. Let me know if you find differently.

On security, I was assuming the code they are submitting and the spark-submit are running with the same permissions. When is that not the case? Containers do not provide security barriers, just component isolation is my understanding. The value could be sanitized to only expected values but something has to then map those to the right values for the cluster, hence the alternative suggestion. I'm not sure what something in-between would be? Could one assume that the env variable set to ""python"" or ""python3"" would always work? Seems unlikely and you are then less flexible than spark-submit. ;;;","06/Jun/16 17:24;vanzin;bq. at least that is what my experiments found when running spark-submit at the command line

You have to try that in yarn-cluster mode. It won't work in any other mode. I'll run an experiment locally to see whether it works (seems like it would from the code).

bq. the code they are submitting and the spark-submit are running with the same permissions

Again, the key here is yarn-cluster mode. In that mode, the user code will run inside a YARN container, not as a sub-process of the Livy server. spark-submit just submits the application. If kerberos / impersonation are enabled, the code will run as the submitting user, not as Livy.

bq. Containers do not provide security barriers, just component isolation is my understanding.

There are containers and containers. YARN containers, with kerberos / impersonation, will run as the user, not as Livy, and that's how you get security. That's why yarn-cluster mode is the only way to run Livy securely right now.

For client mode, that's not an option right now. You could run spark-submit inside a docker container, for example, and that would still run the code as the livy user, but then the code wouldn't have access to the Livy server's file system, meaning it can't ready Livy's SSL certificate or kerberos credentials, so there's very limited things it would be able to do. Anyway, it's just easier to say that security only works in yarn-cluster mode. ;;;","06/Jun/16 18:46;vanzin;I just tested locally and overriding the app master env works. You need to set {{spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON}} in the Livy session config, since Livy only looks at that variable and not {{PYSPARK_PYTHON}}.;;;","06/Jun/16 19:21;KevinGre;All above assumes yarn-cluster mode. I'll try the appMasterEnv again.;;;","09/Jun/16 04:06;KevinGre;So I ran my experiments again and confirmed that setting spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON in the Livy session config does _not_ work. I ran this end to end and confirmed that the conf setting is being passed by looking at the Livy logs which show it getting correctly passed to spark-submit:
{noformat}16/06/09 02:17:00 INFO SparkProcessBuilder: Running '/usr/hdp/current/spark-client/bin/spark-submit' '--master' 'yarn' '--deploy-mode' 'cluster' '--name' 'Livy' '--py-files' ... ... '--conf' 'spark.yarn.maxAppAttempts=1' '--conf' 'spark.yarn.tags=livy_a17e8ded-2121-4529-95c5-7cc1791311a0' '--conf' 'spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON=python3' '--conf' 'spark.yarn.appMasterEnv.PYSPARK_PYTHON=python3'  '...py' {noformat}

You may consider this more a Spark issue than a Livy one as I can reproduce the issue at the command prompt using just spark-submit. Spark-submit requires the environment variable to be set. I did this on a Azure HDInsight Spark cluster (HDI 3.4) working at the command prompt on the head node (where the Livy service runs). Attached is a detailed capture of such a session showing running a simple pySpark program submitted three times. Once with the defaults (getting python 2.7), again with the conf with spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON set and again with the local environment variable set. Only the third runs the job with Python 3.

This brings me back to proposed fixes, either spark-submit needs to honor the conf setting like you think it should, or Livy needs a way to pass values for PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON through to spark-submit via environment variables. Assuming the later, seems we could accept a whitelist of values (python2, python3) which would work most of the time and if more control is needed python2 and python3 commands can be setup correctly in spark-env.sh. [^runloginteresting.txt]  [^runlogfull.txt] ;;;","10/Jun/16 05:26;KevinGre;[https://github.com/cloudera/livy/compare/master...KevinGrealish:batchPythonVersion?expand=1];;;","10/Jun/16 22:29;KevinGre;Pull request: [https://github.com/cloudera/livy/pull/149|https://github.com/cloudera/livy/pull/149];;;","17/Jun/16 19:15;vanzin;[~KevinGre] I still think you're doing something wrong here. I finally got some time to try this on my cluster, this is what I have. I wrote this python script:

{code}
import sys
print(sys.version)
{code}

And submitted it as a batch. With no extra config I get this (from YARN logs):

{noformat}
Log Type: stdout
Log Upload Time: Fri Jun 17 12:03:23 -0700 2016
Log Length: 79
2.7.5 (default, Nov 20 2015, 02:00:19) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)]
{noformat}

Setting {{spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON}} to {{/usr/bin/python3}} I get this:

{noformat}
Log Type: stdout
Log Upload Time: Fri Jun 17 12:12:04 -0700 2016
Log Length: 79
3.4.3 (default, Jan 26 2016, 02:25:35) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)]
{noformat}

This mimics the previous behavior I observed with interactive sessions too.;;;","17/Jun/16 19:19;KevinGre;Are you running Livy on a different machine to the YARN master? I verified this twice with a HDInsight cluster with all logs attached. Can you see from the logs what is wrong there?;;;","17/Jun/16 19:42;vanzin;Hmm... one thing that might be tripping you is that Spark gives preferences to env variables over the config parameters when dealing with {{PYSPARK_DRIVER_PYTHON}} and friends. I see that you tried both (config and env variables), but maybe your spark-env.sh script is overriding those and always setting the python interpreter?;;;","21/Jun/16 03:17;KevinGre;Yes, that appears to be the case. If on the head node I do:
{noformat}
KevinGre@hn0-keving:~$ unset PYSPARK_PYTHON PYSPARK_DRIVER_PYTHON
KevinGre@hn0-keving:~$ '/usr/hdp/current/spark-client/bin/spark-submit' '--master' 'yarn' '--deploy-mode' 'cluster' '--name' 'probeVersion' '--conf' 'spark.yarn.maxAppAttempts=1' '--conf' 'spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON=python3' '--conf' 'spark.yarn.appMasterEnv.PYSPARK_PYTHON=python3' probe.py
{noformat}
I get:
{noformat}
driver:
3.4.3 (default, Oct 14 2015, 20:28:29) 
[GCC 4.8.4]
PYTHONHASHSEED=not set.
PYSPARK_PYTHON=python3
executors:
3.4.3 (default, Oct 14 2015, 20:28:29) 
[GCC 4.8.4]
PYTHONHASHSEED=0
PYSPARK_PYTHON=/usr/bin/anaconda/bin/python
{noformat}
Although I did need to set both to not get a worker/driver mismatched version error.

However, this does not solve the problem. If a cluster needs to be setup to use a Python that is not the system one (as HDInsight clusters do), the only mechanism that covers both YARN cluster and client mode is setting the environment variable, which then blocks using conf to set it per submission.;;;","21/Jun/16 04:23;KevinGre;Iï¿½ï¿½ï¿½m open to ideas on how to solve this. As far I can see the only mechanism that covers both YARN cluster and client mode is setting the environment variable. Am I missing something? It would be better if there was a single mechanism to set the Python to use, but there is not. The proposed pull request for Livy still provides a way to expose the primary way this is set and does solve the problem.

An alternative would be to change [Client.scala|https://github.com/apache/spark/blob/61d729abdaf9bdd54aea74a643828d570b036d87/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala] so that propagating the env vars (lines 807,808) is moved to before setting those from conf (line 735 and following)?;;;","21/Jun/16 10:12;purechoc;spark driver use ""appMasterEnv.PYSPARK_PYTHON"", and worker use ""spark.executorEnv.PYSPARK_PYTHON""
so.. are you try this?
{code:none}
'--conf' 'spark.yarn.appMasterEnv.PYSPARK_PYTHON=python3' '--conf' 'spark.executorEnv.PYSPARK_PYTHON=python3'
{code};;;","21/Jun/16 19:41;vanzin;[~zjffdu] has a Spark PR that might help with this: https://github.com/apache/spark/pull/13146

Unfortunately I haven't had the time to look at it.

;;;","21/Jun/16 21:12;KevinGre;Marcelo, I'll have a look. I'm also going to put in a pull request for a one liner that should fix the issue in Spark. [SPARK-1611|https://issues.apache.org/jira/browse/SPARK-16110].

Hyunwoo, I did not have any issues with getting the executor to be the right version once the env var was unset.;;;","21/Jun/16 22:02;KevinGre;Marcelo, [https://github.com/apache/spark/pull/13146] looks like a more complete change to add these as explicit submit parameters, which would also require changes to LIVY to support the new parameters. I've submitted a tighter fix to Spark that will just get the ordering right with the existing conf in the YARN cluster case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
First class support for --packages,LIVY-158,13095796,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Won't Fix,tc0312,tc0312,gmcdonald,03/Jun/16 20:32,06/Jun/16 19:01,19/Dec/25 04:15,06/Jun/16 19:01,0.3,,,,,Server,,,,,,,,,,0,,,,,,"A lot of people are using --packages to include packages like spark.csv.

Right now users and manaully specify spark.jars.packages, spark.jars.ivy and spark.jars.excludes in conf in the session request.

But it might be easier for users if we provide a first class options in in the session request like:
{code:javascript}
{
    packges: [""com.databricks:spark-csv_2.10:1.4.0""]
    excludePackages: [""org.apache.hadoop:*""]
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Jun 06 18:50:52 UTC 2016,,,,,,,,,,"0|i3izfb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Jun/16 20:34;vanzin;As a general question, why are explicit parameters better than using the corresponding Spark configuration when those exist? I really don't like duplicating things like that.;;;","03/Jun/16 21:07;tc0312;Because a lot of users are switching from spark-submit to Livy. And spark-submit provides explicit parameters (--packages, --exclude-packages) for them.;;;","03/Jun/16 21:13;vanzin;The answer to that is not to create yet another API. Users already have 2 (spark-submit and the spark config options), this would only be adding another one to the mix with no clear benefits.;;;","03/Jun/16 21:18;tc0312;We are not adding another API. We are proposing this as a parameter in the create session request, like other parameters like proxyUser, jars, pyFiles, etc in our create session requests. 
This will improve usability a lot. We already have a lot of incoming support cases asking us how to specify ""--package"" in Livy.;;;","03/Jun/16 21:20;vanzin;bq. We are not adding another API. We are proposing this as a parameter in the create session request

That's the definition of an API. You're exposing something to the user to write against.

The other explicit parameters are there for backwards compatibility; I'd have removed them if it wouldn't break existing clients. I even removed them from the documentation.;;;","03/Jun/16 21:21;vanzin;As for:

bq.  a lot of incoming support cases asking us how to specify ""--package"" in Livy.

That's more of a documentation or UI issue.;;;","03/Jun/16 21:32;tc0312;So a request used to look like:
{code:javascript}
{
    ""driverMemory"": ""5G"",
    ""numExecutors"": 4,
    ""executorCores"": 5,
    ""executorMemory"": ""20G""
}
{code}
will become
{code:javascript}
{
    conf: {
        ""spark.driver.memory"": ""5G"",
        ""spark.executor.instances"": 4,
        ""spark.executor.cores"": 5,
        ""spark.executor.memory"": ""20G""
    }
}
{code}
What do we gain in this process in terms of usability? If I take your argument further, spark-submit shouldn't offer those parameters and just provide --conf, correct?
There are a lot of usability issues for this approach. SparkConf values themselves are not that well documented. Users switching from spark-submit to Livy will need to learn all these SparkConf values. spark-submit provided these shortcut parameters for a reason: They are easier to use.;;;","03/Jun/16 21:38;vanzin;bq. What do we gain in this process in terms of usability?

Nothing, but you also don't lose anything in my view. And you avoid a lot of potential issues: backwards compatibility in future versions of Livy, forwards compatibility with future version of Spark, maintaining documentation for these things, extra code to translate and potentially merge these options with the Spark configuration, etc. And you avoid falling into the trap of ""why is option x exposed but option y isn't?"".

bq. spark-submit shouldn't offer those parameters and just provide --conf, correct?

That's already true for the vast majority of options; most spark-submit options existed before they even existed as config options. But, if you pay attention, most conf options are not exposed as explicit spark-submit arguments.

bq. SparkConf values themselves are not that well documented

Not true:
- http://spark.apache.org/docs/latest/configuration.html
- http://spark.apache.org/docs/latest/running-on-yarn.html#configuration

Those pages have most of the options people will want to use. Are you saying we should be exposing all of them too?
;;;","03/Jun/16 21:43;tc0312;Have you even tried to search for these parameters in those pages you provided?
spark.jars.packages, spark.jars.ivy and spark.jars.excludes are not documented in those pages. They are only mentioned in the source code. I found them in spark-submit source code.

I'm not saying we should provide a 1:1 mapping from Livy to SparkConf values. I'm saying we should provide a 1:1 mapping from Livy to spark-submit parameters.;;;","03/Jun/16 21:46;vanzin;bq. Have you even tried to search for these parameters in those pages you provided?

That's a bug in Spark's docs then.

bq. we should provide a 1:1 mapping from Livy to spark-submit parameters

That's slightly better but still I don't think it's necessary.;;;","04/Jun/16 02:14;tc0312;It's fine to switch from spark-submit model to SparkConf model. But IMHO it's a change in design and it should be explicitly communicated as a JIRA or as an email to livy-dev@ or livy-user@. This makes collaboration easier.

We've just found out docs for session request fields in Livy's README.md are removed. If we are switching to SparkConf model, we shouldn't just remove documentations for those fields. We should replace them with documentation of popular SparkConf keys, or provide at least a link to necessary SparkConf keys like spark.driver.memory/spark.executor.cores/etc. Livy's README.md should provide enough info to get users started.;;;","06/Jun/16 16:12;linchan;I think this is a tradeoff between ease of use vs maintainability. I do see the value in both.

I do agree keeping everything as conf makes things easy for us to maintain. However, I believe just keeping a ""conf"" parameter in the documentation won't help livy users. Imagine someone who's a bit new to Spark and might only be familiar with spark-submit. How will he figure out the basic spark settings, e.g. executor memory, no of executor etc? We should make the first experience easier for Livy users. If it is not in code, at least some documentation. 
I would suggest in the API documentation, we do a mapping table of the basic/common spark-submit parameters to the conf settings and give simple examples. How about that? Thanks;;;","06/Jun/16 17:17;vanzin;bq.  it's a change in design and it should be explicitly communicated

LIVY-28 was filed a long time ago, so the desire to do it has always existed. I just chose to not remove the code to avoid dealing with API versioning, since it's just easier to leave the existing code there and not document it.

re: documentation, I'm fine with writing better documentation. Some basic configs + a link to Spark's documentation is fine. I just don't want to get into the business of replicating half of Spark's documentation in Livy, just like I don't want to replicate Spark's functionality in this area.;;;","06/Jun/16 18:45;tc0312;or *provide at least a link to necessary SparkConf keys* like spark.driver.memory/spark.executor.cores/etc.

We are not saying we should duplicate all the conf keys;;;","06/Jun/16 18:46;tc0312;In LIVY-28,
@romain raised up ""One issue though is that we won't map with the CLI options, which is pretty handy"" in the issue, which was never addressed.;;;","06/Jun/16 18:50;vanzin;That's addressed by better documentation (both Spark and Livy). I sincerely don't think that not having a 1:1 mapping of every spark-submit option is that big of a deal.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specifying Spark master and Spark deploy mode for Livy session recovery,LIVY-157,13095795,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,tc0312,gmcdonald,01/Jun/16 19:49,28/Jun/16 20:05,19/Dec/25 04:15,28/Jun/16 20:05,0.3,,,0.3,,Server,,,,,,,,,,2,,,,,,"Hi, I'm merging Livy session recovery to upstream. 

Right now, Livy is indifferent to master & deploy mode. When Livy calls spark-submit, spark-submit will pick the value specified in spark-defaults.conf.

However, session recovery depends on the cluster manager. Different cluster manager requires different session recovery implementation. (Note: Right now, session recovery supports YARN only.). Livy has to be aware of current master and deploy mode.

To enable that, Livy should read master & deploy mode when Livy is starting. Cache it and pass them to spark-submit explicitly.

We have a few options to specify master & deploy mode:
1: Add 2 new configs in livy.conf.
 * Pro: We've seen users who want different default master & deploy mode for Livy and other jobs. (yarn-cluster for Livy, yarn-client for manually called spark-submit). This makes configuration for this use case easier.
 * Con: More configurations in Livy.

2: Read it from SparkConf.
 * Pro: Less configurations in Livy. Livy should mimic Spark's behaviour: Read from --properties-file then SPARK_HOME/conf/spark-defaults.conf. Values should be override-able with Java options.
 * Con: The configuration logic is quite complicated. New users might have a hard time configuring Livy.

Note: With this change, master and deploy mode remain unchanged even if spark-defaults.conf is changed while Livy is running. New values won't apply until Livy restarts. Which is a desirable side effect.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jun 28 20:05:19 UTC 2016,,,,,,,,,,"0|i3izf3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"15/Jun/16 04:11;tc0312;We are in favor of approach 1. In our use case, we need different default master for job submitted remotely thru Livy (yarn-cluster) and job submitted locally thru spark-submit (yarn-client).;;;","28/Jun/16 20:05;vanzin;This was fixed in commit 7448c7c (https://github.com/cloudera/livy/commit/7448c7c), my script failed to update the bug...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid java command line when setting memory configs in session create request,LIVY-156,13095794,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,vanzin,vanzin,gmcdonald,31/May/16 17:12,31/May/16 18:03,19/Dec/25 04:15,31/May/16 18:00,0.2,,,0.2,0.3,Server,,,,,,,,,,0,,,,,,"From the mailing list:

{quote}
livy/server/src/main/scala/com/cloudera/livy/server/interactive/InteractiveSession.scala

The seems to be an issue on line 106:

SparkLauncher.DRIVER_MEMORY -> request.driverMemory.map(_.toString + ""b"")
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 31 18:00:56 UTC 2016,,,,,,,,,,"0|i3izev:",9223372036854775807,,,,,,,,,,,,,,,,,,,"31/May/16 18:00;vanzin;Commit [fe565cc3|https://github.com/cloudera/livy/commit/fe565cc30af2e1da2c1638774dc264f0208182d1] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-156. Fix propagation of driver memory config.

Closes #143
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkInterpreter causes SparkIMain instances to leak,LIVY-155,13095793,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,vanzin,vanzin,gmcdonald,24/May/16 22:36,25/May/16 18:26,19/Dec/25 04:15,25/May/16 18:15,0.2,,,0.2,0.3,REPL,,,,,,,,,,0,,,,,,"When the Spark REPL is invoked, it sets its class loader as the currently active class loader in the thread, but there's no code to unset that.

What happens is that, since Livy uses the global execution context to invoke the REPL, all threads in that context end up referencing the REPL class loader, and thus references to it are never cleaned up.

In most cases this is fine since there REPL is running on its own separate VM; but this is bad for our unit tests since it causes {{SparkIMain}} instances to leak, and those reference a lot of memory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 25 18:25:46 UTC 2016,,,,,,,,,,"0|i3izen:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/May/16 18:15;vanzin;Commit [06c99561|https://github.com/cloudera/livy/commit/06c995614b626b0761fa5835ab5cdd77926eda52] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-155. Make sure original context class loader is restored after calling REPL.

Anytime the Spark REPL is called, it may mess with the thread's context
class loader. There was code to restore the original one after the
interpreter is stopped, but that only applied to a single thread, which
might not even be the right one.

The issue showed up mainly during unit tests, where multiple REPL instances
were created in the same VM. The REPL's class loader would remain as the
active context class loader in many threads in the global executor, and
that would prevent the GC from collecting a whole lot of unused memory
referenced from the REPL.

With this patch, the heap size after a full GC remains constant at around
300M while the repl unit tests are running (while it would balloon to more
than 1G before).

I also added some explicit shut down code to the R interpreter, to make sure
that its backend thread is cleaned up.

Closes #142
{code}
;;;","25/May/16 18:25;vanzin;Fix backported to branch-0.2 ([ae6ac577|https://github.com/cloudera/livy/commit/ae6ac5774d26deccc723b07032954d78466e3323]).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent exceptions when uploading files to Livy,LIVY-154,13095792,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,20/May/16 18:08,03/Jun/16 23:06,19/Dec/25 04:15,03/Jun/16 23:06,0.2,,,0.3,,Server,,,,,,,,,,0,,,,,,"Every once in a while, integration tests fail for me, especially on slow machines. The error looks like this:

{noformat}
  java.util.concurrent.ExecutionException: com.cloudera.livy.shaded.apache.http.NoHttpResponseException: The target server failed to respond
  at java.util.concurrent.FutureTask.report(FutureTask.java:122)
  at java.util.concurrent.FutureTask.get(FutureTask.java:202)
  at com.cloudera.livy.test.JobApiIT.com$cloudera$livy$test$JobApiIT$$waitFor(JobApiIT.scala:208)
  at com.cloudera.livy.test.JobApiIT$$anonfun$1.apply$mcV$sp(JobApiIT.scala:78)
  at com.cloudera.livy.test.JobApiIT$$anonfun$1.apply(JobApiIT.scala:67)
  at com.cloudera.livy.test.JobApiIT$$anonfun$1.apply(JobApiIT.scala:67)
  at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
  at org.scalatest.Transformer.apply(Transformer.scala:22)
{noformat}

The only hint on the server logs is this:

{noformat}
16/05/20 10:59:09 WARN HttpParser: badMessage: java.lang.IllegalStateException: too much data after closed for HttpChannelOverHttp@36b1307a{r=1,c=false,a=IDLE,uri=}
{noformat}

Need to figure out what's going on here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 03 23:06:36 UTC 2016,,,,,,,,,,"0|i3izef:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Jun/16 23:06;vanzin;Commit [6bfd6e21|https://github.com/cloudera/livy/commit/6bfd6e2165f07c9d056ed35fe58875035389191c] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-154. Don't reuse connections in client-http.

Either the current client code is doing something wrong when setting
up the http client, or there's a bug in the http client library, but
the result is that a connection ends up being closed while still in
use, causing errors when the client is talking to Livy.

While investigating that I found a couple of other issues that I also
fixed here:

- the client-http packaging was relocating too much stuff, which broke
  logging for the relocated classes, making it hard to debug things.

- the server wasn't configuring multipart request handling for servlets;
  fixing that means that we can't use asynchronous tasks, since Jetty
  (at least the version we use) does not allow that. Async tasks weren't
  really needed anyway, since they're either pretty quick or, otherwise,
  would block a thread in another shared thread pool anyway, so there
  wasn't really any gain.

Closes #144
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade default spark version to 1.6,LIVY-153,13095791,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,zjffdu,gmcdonald,20/May/16 06:50,13/Jul/16 22:24,19/Dec/25 04:15,13/Jul/16 22:24,0.2,,,0.3,,Core,,,,,,,,,,0,,,,,,"Livy unit test can not pass with spark 1.5 due to one issue in snappy-java 1.0.4.1 which is shipped with spark 1.5. 

{noformat}
Tests in error:
  TestSparkClient.testAddJarsAndFiles:175->runTest:434 ï¿½ï¿½ Execution java.lang.Run...
  TestSparkClient.testHiveJob:244->runTest:434 ï¿½ï¿½ Execution java.lang.RuntimeExce...
  TestSparkClient.testRemoteClient:164->runTest:434 ï¿½ï¿½ Execution java.lang.Runtim...
  TestSparkClient.testSimpleSparkJob:115->runTest:434 ï¿½ï¿½ Execution java.lang.Runt...
  TestSparkClient.testSparkSQLJob:230->runTest:434 ï¿½ï¿½ Execution java.lang.Runtime...
{noformat}
{noformat}
Caused by: java.lang.UnsatisfiedLinkError: no snappyjava in java.library.path
    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1865)
    at java.lang.Runtime.loadLibrary0(Runtime.java:870)
    at java.lang.System.loadLibrary(System.java:1122)
    at org.xerial.snappy.SnappyNativeLoader.loadLibrary(SnappyNativeLoader.java:52)
    ... 50 more
{noformat}

Specify spark 1.6 when running unit test can solve this issue. And since spark 1.6.1 has been released for quite a while, I think we can upgrade the default spark version to 1.6.1 in pom",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 13 22:24:43 UTC 2016,,,,,,,,,,"0|i3ize7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/May/16 08:24;zjffdu;But InteractiveIT will fail under spark 1.6, very weird, this is the log in AM

{noformat}
16/05/20 17:43:17.045 pool-6-thread-1: STARTED PooledBuffers [0/1024@6144,0/1024@16384,0/1024@-]/PooledBuffers [0/1024@6144,0/1024@32768,0/1024@-]
16/05/20 17:43:17.046 pool-6-thread-1: Started SocketConnector@0.0.0.0:55313
16/05/20 17:43:17.046 pool-6-thread-1: STARTED SocketConnector@0.0.0.0:55313
16/05/20 17:43:17.046 pool-6-thread-1: STARTED org.spark-project.jetty.server.Server@670ffc4a
16/05/20 17:43:17.046 pool-6-thread-1: Successfully started service 'HTTP file server' on port 55313.
16/05/20 17:43:17.047 pool-6-thread-1: HTTP file server started at: http://127.0.0.1:55313
16/05/20 17:43:17.060 pool-6-thread-1: Added JAR file:/Users/jzhang/github/livy/rsc/target/jars/livy-api-0.2.0-SNAPSHOT.jar at http://127.0.0.1:55313/jars/livy-api-0.2.0-SNAPSHOT.jar with timestamp 1463737397060
16/05/20 17:43:17.062 pool-6-thread-1: Added JAR file:/Users/jzhang/github/livy/rsc/target/jars/livy-rsc-0.2.0-SNAPSHOT.jar at http://127.0.0.1:55313/jars/livy-rsc-0.2.0-SNAPSHOT.jar with timestamp 1463737397062
16/05/20 17:43:17.064 pool-6-thread-1: Added JAR file:/Users/jzhang/github/livy/rsc/target/jars/netty-all-4.0.23.Final.jar at http://127.0.0.1:55313/jars/netty-all-4.0.23.Final.jar with timestamp 1463737397064
16/05/20 17:43:17.065 pool-6-thread-1: Added JAR file:/Users/jzhang/github/livy/repl/target/jars/commons-codec-1.9.jar at http://127.0.0.1:55313/jars/commons-codec-1.9.jar with timestamp 1463737397065
16/05/20 17:43:17.066 pool-6-thread-1: Added JAR file:/Users/jzhang/github/livy/repl/target/jars/livy-core-0.2.0-SNAPSHOT.jar at http://127.0.0.1:55313/jars/livy-core-0.2.0-SNAPSHOT.jar with timestamp 1463737397066
16/05/20 17:43:17.067 pool-6-thread-1: Added JAR file:/Users/jzhang/github/livy/repl/target/jars/livy-repl-0.2.0-SNAPSHOT.jar at http://127.0.0.1:55313/jars/livy-repl-0.2.0-SNAPSHOT.jar with timestamp 1463737397067
16/05/20 17:43:17.095 pool-6-thread-1: Created YarnClusterScheduler
Exception in thread ""pool-6-thread-1"" java.lang.NoClassDefFoundError: org/apache/spark/scheduler/cluster/YarnSchedulerBackend
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:760)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:174)
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2697)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2304)
	at com.cloudera.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:89)
	at com.cloudera.livy.repl.Session$$anonfun$1.apply(Session.scala:59)
	at com.cloudera.livy.repl.Session$$anonfun$1.apply(Session.scala:57)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.scheduler.cluster.YarnSchedulerBackend
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 30 more
{noformat};;;","20/May/16 16:52;vanzin;Hi @zjffdu,

Are you running on MacOS? The CDH build of Snappy doesn't include MacOS native libs, so you would get that error. I haven't tried building with a different version so far, but I do run Livy (built against 1.5) on top of 1.6 all the time.;;;","23/May/16 01:14;zjffdu;yes, I run it on MacOS;;;","13/Jul/16 22:24;vanzin;Commit [8ab21281|https://github.com/cloudera/livy/commit/8ab212818553afda07dff584cbcba47cc619c0e3] by  Alex Man <tc.technetium@...> in cloudera/livy:
{code}
LIVY-153. Upgraded Spark from 1.5 to 1.6.2.

Changed default Spark version to 1.6.2 (Apache build).

Closes #169
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race in HTTP client's JobHandleImpl when setting result,LIVY-151,13095789,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,19/May/16 22:26,20/May/16 00:06,19/Dec/25 04:15,20/May/16 00:06,0.2,,,0.2,,API,,,,,,,,,,0,,,,,,"In {{JobHandleImpl}}, setting the result and updating the state are different methods, so the job handle may report a final result before the handle's state changes to succeeded (for example). 

This affects the unit tests in that module in certain cases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 20 00:06:13 UTC 2016,,,,,,,,,,"0|i3izdr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/May/16 00:06;vanzin;Commit [50432159|https://github.com/cloudera/livy/commit/5043215914d0334acdf66ddc47e6f66dfb155000] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-151. Make sure handle state is updated before result can be returned.

Closes #140
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update RealCluster integration tests to use new livy-server script features,LIVY-150,13095788,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,16/May/16 20:42,17/May/16 17:40,19/Dec/25 04:15,17/May/16 17:40,0.2,,,0.2,,Tests,,,,,,,,,,0,,,,,,"Instead of manually using nohup / pkill, use the new ""start"" and ""stop"" arguments to livy-server in our integration tests, to also test those code paths.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 17 17:40:34 UTC 2016,,,,,,,,,,"0|i3izdj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/May/16 17:40;vanzin;Commit [386663d7|https://github.com/cloudera/livy/commit/386663d7752a62aeed1e880e3eede113daaba1fa] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-150. Use ""livy-server start|stop"" in RealCluster ITs.

This makes sure those code paths are tested too. While at it, I did some
small clean up in the server script, and added documentation for some more
env variables.

Closes #136
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RSC should timeout and exit if Livy doesn't connect back,LIVY-149,13095787,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,16/May/16 19:52,17/May/16 18:20,19/Dec/25 04:15,17/May/16 18:20,0.2,,,0.2,,RSC,,,,,,,,,,0,,,,,,"After we reversed the way that Livy and RSC connections are established, the RSC may now listen forever for a Livy connection that may not arrive. This would leave the session using resources in the cluster and would require manual intervention from users to kill the application.

Instead, there should be a timeout: if there are no connections to the RSC after a configurable x amount of time, it just stops itself.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 17 18:20:52 UTC 2016,,,,,,,,,,"0|i3izdb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/May/16 18:20;vanzin;Commit [816976f5|https://github.com/cloudera/livy/commit/816976f57e6ceeea615205f94df55993345fd58e] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-149. Add idle timeout to RSC server.

The RSC server currently listens indefinitely waiting for a Livy server
to connect to it. That can result in sessions that never go away and need
to be manually cleaned up when the Livy server crashes or is otherwise
uncleanly shut down.

So add a timeout, default 10 minutes, that allows sessions to shut themselves
down if the Livy server goes away and doesn't come back.

Closes #137
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy should not allow local files to be added to sessions,LIVY-148,13095786,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Fixed,vanzin,vanzin,gmcdonald,13/May/16 19:48,16/May/16 18:24,19/Dec/25 04:15,16/May/16 18:24,0.2,,,0.2,,Server,,,,,,,,,,0,,,,,,"Users can provide local (i.e. {{file}} scheme) files in the list of files or jars to be added to sessions; this shouldn't be allowed (at least not by default), since it means people can abuse that to read files that only the Livy server has access to.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 18:24:52 UTC 2016,,,,,,,,,,"0|i3izd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/May/16 18:24;vanzin;Commit [7f10ed0b|https://github.com/cloudera/livy/commit/7f10ed0bba08b2d20ca5c1f3122e6e9318a0260e] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-148. Add explicit whitelist for local files users can read.

This makes it so by default users cannot provide local URIs as input
to their jobs; this prevents users from using Livy to read arbitrary
files on the Livy server's file system.

It's possible to add explicit directories that users can read, though;
adding ""/"", for example, would open up the whole file system to users.

There are some extra changes to merge the code that processes file lists
in the Spark configuration into a single shared method used by both
batches and interactive sessions; this makes sure that both modes perform
the appropriate checks and translations before starting Spark. This
new code also looks at all known file lists that Spark processes (as
of the current 2.0 branch) and performs validaton on those.

Closes #135
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default URI for files in session requests should be fs.defaultFS,LIVY-147,13095785,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,12/May/16 17:41,13/May/16 17:31,19/Dec/25 04:15,13/May/16 17:31,0.2,,,0.2,,Server,,,,,,,,,,0,,,,,,"Currently, paths are just passed directly to Spark for processing; that means that by default they refer to local files. That doesn't make a lot of sense in Livy since ""local"" means the server's filesystem, which most clients won't even have access to.

Instead, paths without a proper scheme should inherit the {{fs.defaultFS}} configuration from Hadoop; that should also make the 0.2 session creation API more backwards compatible, since it was possible to set up a custom default FS in 0.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 13 17:31:02 UTC 2016,,,,,,,,,,"0|i3izcv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/May/16 17:31;vanzin;Commit [24e10b26|https://github.com/cloudera/livy/commit/24e10b265572b9666d329e0b2f275f5c836a0a7e] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-147. Prepend default FS to user-provided paths when needed.

Spark's default behavior is to treat schema-less URIs as local, which
doesn't make a whole lot of sense for Livy (since clients most probably
don't share Livy's local filesystem).

So, instead, prepend Hadoop's default FS to paths when they don't already
have a scheme, so that users don't have to figure that out by themselves.
This should also make session requests more backwards compatible with v0.1,
since a similar functionality existed there in certain cases.

Closes #133
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interactive session should not spin waiting for statement result,LIVY-143,13095781,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,vanzin,gmcdonald,11/May/16 18:26,07/Dec/16 05:06,19/Dec/25 04:15,07/Dec/16 05:06,0.2,,,0.3,,Server,,,,,,,,,,0,,,,,,"Currently, when submitting statements for execution, the session code does this:

{code}
    val future = Future {
      val id = client.submitReplCode(content.code)
      info(s""Running statement $id: ${content.code}"")
      waitForStatement(id)
    }
{code}

{{waitForStatement}} calls itself recursively until the statement is finished; that means that a thread in the global pool will be blocked until that statement finishes, which is bad and limits how many statements can be submitted concurrently.

We either should make the polling a recurring task (instead of a blocking task), or make the monitoring asynchronous.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Dec 07 05:06:50 UTC 2016,,,,,,,,,,"0|i3izbz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/Dec/16 05:06;tc0312;livy-server does not poll and store statements result in memory anymore.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to latest Jetty,LIVY-142,13095780,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,,vanzin,gmcdonald,11/May/16 00:25,11/May/16 20:52,19/Dec/25 04:15,11/May/16 20:52,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"Let's upgrade to the latest Java 7-compatible Jetty before the 0.2 release, to make sure we have the latest fixes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 20:52:53 UTC 2016,,,,,,,,,,"0|i3izbr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/May/16 20:52;vanzin;Fixed in bcdfd60154cd5aec5edd1dc6d31b712e7da4fe34.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document Java API,LIVY-141,13095779,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,vanzin,gmcdonald,11/May/16 00:15,23/Jan/18 20:22,19/Dec/25 04:15,23/Jan/18 20:22,0.2,,,0.5.0,,Docs,,,,,,,,,,0,,,,,,"We should add some documentation about the Java API to the top-level README. It should include info about how to get the artifacts (which haven't been published yet), and some examples of how to use the code.

We also should look at how to generate and publish javadocs; not sure whether it's possible to publish them through github somehow.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-175,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 23 20:22:40 UTC 2018,,,,,,,,,,"0|i3izbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"21/Aug/17 23:49;ajbozarth;This was partially addressed by [~vanzin]'s [commit|https://github.com/apache/incubator-livy/commit/b99bc14c9f0214785b77b3a245e6a4014fcf5dda] last year where he added a Doc section on the API. I'm going to pick this up and address the javadocs as well as look into improving the API Doc.;;;","21/Aug/17 23:55;vanzin;The docs I wrote still reference Cloudera repos, something that needs to change too.;;;","22/Aug/17 19:43;ajbozarth;[~vanzin] I actually already updated it when we moved the Docs out of the README;;;","30/Aug/17 22:53;ajbozarth;I opened a WIP PR https://github.com/apache/incubator-livy/pull/38;;;","18/Jan/18 22:36;ajbozarth;The above PR addresses this Jira in the following way:
 * Adds ability to build javadocs for Livy
 * Fleshes out various javadoc comments in the code and addresses javadoc build errors in current code
 * Includes Java API javadocs in the Livy Docs build (which is includedÂ on the Apache Livy website)

Issues previously addressed already:
 * There is a Livy Doc on how to use the Java API in your code
 * Livy Docs are publicly available on the Apache Livy website

Â ;;;","23/Jan/18 20:22;ajbozarth;Issue resolved by pull request 38
[https://github.com/apache/incubator-livy/pull/38];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Change RSC URIs from ""local:"" to ""rsc:""",LIVY-140,13095778,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,11/May/16 00:13,11/May/16 23:00,19/Dec/25 04:15,11/May/16 23:00,0.2,,,0.2,,RSC,,,,,,,,,,0,,,,,,"Let's use ""rsc:"" as the scheme for creating RSC clients, instead of the current ""local:"". We need to do that before the 0.2 release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 23:00:40 UTC 2016,,,,,,,,,,"0|i3izbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/May/16 23:00;vanzin;Commit [ed8585c4|https://github.com/cloudera/livy/commit/ed8585c45061c8f2d4128dc3d23b5d007062755d] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-140. Change RSC's URI scheme from ""local"" to ""rsc"".

Closes #130
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update documentation to match current code.,LIVY-138,13095775,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,10/May/16 20:25,10/Jan/20 17:22,19/Dec/25 04:15,11/May/16 20:42,0.2,,,0.2,,Docs,,,,,,,,,,0,,,,,,"The documentation has gotten a little stale with all the changes made to Livy; we need to update it before we can make a release.",,"fenimore commented on pull request #270: [LIVY-138] Specify batch session ID in documentation 
URL: https://github.com/apache/incubator-livy/pull/270
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Jan/20 17:22;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 20:42:04 UTC 2016,,,,,,,,,,"0|i3izan:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/May/16 20:42;vanzin;Commit [94561388|https://github.com/cloudera/livy/commit/94561388bcdcebaaf78d78f8fc6159da182c63bf] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-138. Update documentation and clean up some code.

Bring the documentation up to speed with the code, removing references
to removed functionality, and adding missing things. Here's a list of
specific changes:

- Removed mentions of ""YARN"" mode since now there's just ""whatever the
  Spark config says"" mode.
- Renamed the config file to ""livy.conf""; the ""defaults"" suffix didn't
  make a ton of sense, and it's ok to have the file there without the
  "".template"" extension - all settings are commented out.
- Removed outdated configs and added new configs to the config file.
- Renamed ""spark-blacklist.properties"" to "".conf"" for consistency.
- Removed the verbose, specific options from the create session / batch
  REST documentation; users should use the ""conf"" map instead (as per
  LIVY-28). The code itself still handles the old parameters, but we
  really shouldn't encourage using them.
- Stopped using env variables for configuration in several places. Some
  were just not used, so they're removed. Others already had equivalents
  in the config file, so the code was changed to reflect that.
- Restore support for LIVY_CONF_DIR.

Closes #128
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI tests fail sporadically on Travis,LIVY-136,13095774,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,30/Apr/16 00:06,02/May/16 17:54,19/Dec/25 04:15,02/May/16 17:54,0.2,,,0.2,,Tests,,,,,,,,,,0,,,,,,"CI tests have been failing sporadically lately, more often than not. I'm pretty sure this is caused by the kernel's OOM killer, given this line in the log:

{noformat}
16/04/29 23:57:37.254 ContextLauncher-1: Child process exited with code 137.^M
{noformat}

That's a SIGKILL. We can look at a few things, including:
- lower memory settings for the test processes (currently -Xmx4g)
- lower memory settings for launched Spark processes (currently the default, 1g per process)
- switch to a larger env (see https://docs.travis-ci.com/user/ci-environment/, trusty provides ~7G).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 02 17:54:01 UTC 2016,,,,,,,,,,"0|i3izaf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"02/May/16 17:54;vanzin;Commit [cf73a660|https://github.com/cloudera/livy/commit/cf73a6604ae490bbf50ce2f03fc07a08a3f24522] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-136. Put tests on a memory diet.

Reduce the amount of memory requested by tests so that they
don't put too much pressure on the machines used by CI builds,
and avoid triggering the kernel's OOM killer.

Closes #122
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Scala-based tests for job API,LIVY-135,13095773,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,28/Apr/16 18:32,18/Aug/17 18:16,19/Dec/25 04:15,05/May/16 17:25,0.2,,,0.2,,Tests,,,,,,,,,,0,,,,,,"We should add some Scala tests for the job API, to make sure it's reasonably easy to run Scala code through the Livy API without LIVY-92, and that serialization of Scala types works.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 05 17:25:14 UTC 2016,,,,,,,,,,"0|i3iza7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/May/16 17:25;vanzin;Commit [3ae23e6c|https://github.com/cloudera/livy/commit/3ae23e6c67b10157d9e128ffebafd2bed37070a8] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-135. Add Scala-based tests to integration suite.

These tests are mostly targeted at making sure that Scala classes
(standard library types, case classes, etc) are properly serialized
by the Livy RPC code. They also show a little bit how to use the
Livy API in Scala, although it's not really optimal right now.

I also moved around some code in the test suite so that certain
errors don't cause subsequent tests to throw nasty exceptions,
causing them to be skipped instead.

Closes #125
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix real cluster support for integration tests,LIVY-134,13095772,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,28/Apr/16 18:17,18/Aug/17 18:16,19/Dec/25 04:15,09/May/16 17:47,0.2,,,0.2,,Tests,,,,,,,,,,0,,,,,,"As part of implementing integration tests based on mini clusters, I've explicitly broken real cluster support to get the ball rolling. Now it's probably the time to get it working again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 17:47:58 UTC 2016,,,,,,,,,,"0|i3iz9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/May/16 17:47;vanzin;Commit [304cc46e|https://github.com/cloudera/livy/commit/304cc46e8fd520b3a0d61aa80466c191f4372f8c] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-134. Fix real cluster support for integration tests.

This change bring real cluster support back to integration tests,
by fixing and simplifying the code that starts Livy on a remote
machine. Since the tests now need that, there's also support to
download the Hadoop configuration from the cluster so that tests
can write directly to HDFS. At the moment, that means kerberized
cluster do not work.

The change also cleans up a lot of unused features of the test
framework; unused methods were removed, and the ClusterPool type
was removed, since right now there's only ever a single cluster.
I also removed the ""withClue"" calls since log fetching is now
done differently, and logs end up in local files in the build
directory.

Closes #126
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"For interactive sessions, spark.master is hardcoded",LIVY-133,13095771,,Bug,Closed,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,mattinbits,gmcdonald,28/Apr/16 09:31,13/Mar/19 21:29,19/Dec/25 04:15,04/May/16 16:48,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"In the InteractiveSession class at this line:
https://github.com/cloudera/livy/blob/a5f3cfc9ebf1a5b3caf9ac23583c105fcf49aa6a/server/src/main/scala/com/cloudera/livy/server/interactive/InteractiveSession.scala#L74

The master of the Client Builder is hard coded to be ""yarn-cluster"". This overrides any value set in config as described in a comment within the client builder:

{code:java}
  /**
   * Creates a new builder that will optionally load the default Livy and Spark configuration
   * from the classpath.
   *
   * Livy client configuration is stored in a file called ""livy-client.conf"", and Spark client
   * configuration is stored in a file called ""spark-defaults.conf"", both in the root of the
   * application's classpath. Livy configuration takes precedence over Spark's (in case
   * configuration entries are duplicated), and configuration set in this builder object will
   * override the values in those files.
   */
{code}

Locally, with this line removed, I could start an interactive session (using the simple example from the project README) with master set to 'local' in my spark-defaults.conf.

It looks like an InteractiveSession would only work with YARN in the current version.","Centos Linux, Standalone Spark 1.5.1, Livy master branch as of 28-04-2016.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 04:26:50 UTC 2016,,,,,,,,,,"0|i3iz9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Apr/16 17:11;vanzin;You can set ""spark.master"" in the ""conf"" field of the request, and that will override the default; you may have to edit {{conf/spark-blacklist.properties}} to allow overriding it, though.

I'll look at why the Spark config is not taking precedence.;;;","01/May/16 15:09;mattinbits;Overriding client side is not possible in this case unfortunately, since I wouldn't want the client to have control of that setting. I would be happy to submit a PR to go with this issue but not clear what the expected functionality should be? Could the line be removed which sets the `master` programmatically, requiring that setting to come from config, as is the case with Spark itself? Or does it need to be changed so that the default parameters are set programmatically but are overridden by configuration rather than vice-versa?;;;","04/May/16 16:48;vanzin;Commit [d306a73c|https://github.com/cloudera/livy/commit/d306a73c87a985e443b0b41d982233bcdc980f31] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-133. Remove default master from interactive session config.

This makes it so that admins have to provide a proper Spark config
to start things up in yarn cluster mode, which is really the only
recommended way to run things.

I also removed setting the master from tests that allow that (those
that run through spark-submit); tests that create a SparkContext
in-process still need to define the master manually.

While testing I found out that the code that caused RSCClient initialization
to fail fast was not working anymore, so I fixed that too. Also, found a
race in the integration tests, where an assert might fail because a session
from a previous test was still in the process of shutting down.

Closes #124
{code}
;;;","10/May/16 09:14;zjffdu;Looks like this ticket make user can run local mode by default. If I want to run yarn-cluster mode, I have to specify spark.master in conf of request and also modify spark-blacklist.properties which is not convinent. I think we should have a followup ticket to make it user friendly. ;;;","10/May/16 16:35;vanzin;Livy will use Spark's configuration. That's whatever is in {{$SPARK_HOME}} (or {{$SPARK_CONF_DIR}} if you define that). We need to update all of Livy's documentation, and this info will be there, but there's no need to write special code for this.;;;","11/May/16 04:26;zjffdu;oh, I use the wrong SPARK_CONF_DIR which make me think the spark-defaults.conf doesn't take affect. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Named Session Support,LIVY-132,13095770,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Duplicate,,Spankymonkey,gmcdonald,26/Apr/16 14:54,26/Apr/16 17:25,19/Dec/25 04:15,26/Apr/16 17:25,0.2,,,,,API,,,,,,,,,,0,,,,,,Allow the User to Name a session.  The Session can then be referred to by name on subsequent requests.  I will also create an enhancement for name based load balancing.  The name will be specified during session creation session_name attribute.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 26 17:25:49 UTC 2016,,,,,,,,,,"0|i3iz9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Apr/16 17:25;vanzin;Same as LIVY-41.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in LivyServer when starting LivyServer,LIVY-131,13095769,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,25/Apr/16 08:35,25/Apr/16 17:43,19/Dec/25 04:15,25/Apr/16 17:43,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"{noformat}
Exception in thread ""main"" java.lang.NullPointerException

at com.cloudera.livy.server.LivyServer.stop(LivyServer.scala:127)

at com.cloudera.livy.server.LivyServer$.main(LivyServer.scala:198)

at com.cloudera.livy.server.LivyServer.main(LivyServer.scala)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 25 17:43:22 UTC 2016,,,,,,,,,,"0|i3iz9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Apr/16 17:43;vanzin;Commit [c3d57093|https://github.com/cloudera/livy/commit/c3d570933d3cd280fe59b759f475dea6aabd6add] by  Jeff Zhang <zjffdu@...> in cloudera/livy:
{code}
LIVY-131. NPE in LivyServer when starting LivyServer

Closes #116
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support pyspark and sparkr in mini cluster,LIVY-130,13095768,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,22/Apr/16 21:30,18/Aug/17 18:16,19/Dec/25 04:15,03/May/16 17:23,0.2,,,0.2,,Tests,,,,,,,,,,0,,,,,,"Current the mini cluster cannot run pyspark not sparkr. It uses a fake Spark installation based on maven dependencies, and those do not include the necessary packages to run those (since those packages are not found in maven).

We need to figure out a way to run these kinds of tests, though. A couple of options:

- Require users to set a ""REAL_SPARK_HOME"" env variable, and only run those tests when the variable is set.

- Download a Spark tarball to set up a proper Spark installation.

The first approach will not help with CI tests on github, while the second would download a large file every time you cleaned up your build directory... so I'm kinda leaning towards the first one, since we can set up an internal jenkins job with a proper Spark install.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue May 03 17:23:54 UTC 2016,,,,,,,,,,"0|i3iz93:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/May/16 17:23;vanzin;Commit [7fe44542|https://github.com/cloudera/livy/commit/7fe445429a4d603c975000a37254cf2ac21c37f4] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-130. Run pyspark and sparkr tests if a real Spark is available.

A new setting was added to allow a real Spark installation to be used
for integration tests. When set, this allows pyspark and sparkr to run;
this also allows testing different versions of Spark than the one
referenced in the pom easily. I tested this with 1.6.1.

Closes #123
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration tests for error conditions,LIVY-129,13095767,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,22/Apr/16 03:05,18/Aug/17 18:16,19/Dec/25 04:15,11/May/16 20:32,0.2,,,0.2,,Tests,,,,,,,,,,0,,,,,,"We should add tests to exercise error paths, such as when the Spark context dies, and make sure the server reacts accordingly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 20:32:13 UTC 2016,,,,,,,,,,"0|i3iz8v:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/May/16 20:32;vanzin;Commit [43b153fd|https://github.com/cloudera/livy/commit/43b153fdbb04bd702d302b87cbf3eb764960abfd] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-129. Add tests for errors in interactive sessions.

The largest portion of this change is a slight refactoring of
how statements are tested in InteractiveIT, to make it easier to
test for errors. With that done, I added a few tests to check error
statement in Scala and Python, and also a test to make sure that
Livy behaves properly when the Spark context dies unexpectedly.

That last one required a fix in the code that handles interactive
sessions, so that errors are properly propagated.

Closes #127
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create integration tests for client sessions,LIVY-127,13095765,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,22/Apr/16 02:04,18/Aug/17 18:16,19/Dec/25 04:15,28/Apr/16 18:20,0.2,,,0.2,,Tests,,,,,,,,,,0,,,,,,"Self-explanatory. I'm leaning towards doing this after merging the APIs for interactive sessions and client sessions, to avoid having to fix these tests after that work is done.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 28 18:20:59 UTC 2016,,,,,,,,,,"0|i3iz8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Apr/16 18:20;vanzin;Commit [55aba4bd|https://github.com/cloudera/livy/commit/55aba4bdaadf2a6d5a9750c712042f597d5dccee] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-127. Add integration tests for the public job API.

A few changes in other areas were made:

- Always overwrite the URI used to create the RSC in InteractiveSession,
  otherwise things fail.
- Removed the ""timeout"" from the session creation structure, because the
  existing interactive session API doesn't have it. We'll add it back
  somehow (probably as a configuration stuck in the ""conf"" map).
- Removed the logging from LivyClientBuilder and just let exceptions from
  the factories propagate; this removes the logging dependency, which also
  avoids having to install the jul-to-slf4j bridge to avoid ugly log messages
  in test output. As a bonus, it makes the exceptions from the builder less
  cryptic.
- Renamed a few classes and methods to make things cleaner and also to avoid
  a naming clash.
- Removed some dead code and dependencies.

Closes #118
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create integration test for batches,LIVY-126,13095764,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,22/Apr/16 02:02,18/Aug/17 18:16,19/Dec/25 04:15,02/May/16 18:03,0.2,,,0.2,,Tests,,,,,,,,,,0,,,,,,Self-explanatory.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 02 18:03:02 UTC 2016,,,,,,,,,,"0|i3iz87:",9223372036854775807,,,,,,,,,,,,,,,,,,,"02/May/16 18:03;vanzin;Commit [d609977d|https://github.com/cloudera/livy/commit/d609977de2c124520cd14776de658ce5509a759c] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-126. Add integration tests for batches.

Run a couple of simple apps using the batch API. Some adjustment was
needed to make things work; mainly, ""spark.submit.deployMode"" is not
supported by the default Spark dependency (1.5.0 from CDH), so we need
to use the old style ""yarn-cluster"" otherwise batches will fail to
launch.

For the tests to work I also needed explicit access to the HDFS cluster,
which means I broke RealCluster in the process. Need to figure out how
to implement that kind of functionality when not using a mini cluster.

Closes #119
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create test library with jobs and mini apps for integration testing,LIVY-125,13095763,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,22/Apr/16 02:02,18/Aug/17 18:16,19/Dec/25 04:15,25/Apr/16 22:08,0.2,,,0.2,,Tests,,,,,,,,,,0,,,,,,"To enable testing of Java/Scala based client sessions and batches, let's create a small library with test jobs and small Spark apps. This could be a refactoring of existing jobs that are scattered around existing unit tests, making those unit tests use the shared code instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 25 22:08:33 UTC 2016,,,,,,,,,,"0|i3iz7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Apr/16 22:08;vanzin;Commit [af28e7b1|https://github.com/cloudera/livy/commit/af28e7b1f73c6cbcfc864c3901935951a85c6370] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-125. Refactor existing test jobs into a new library.

This will help with writing integration tests, by avoiding
having to create new versions of these simple jobs. It also
serves as a nice cleanup of the existing code.

I also removed the async job from the rsc tests, since the
support code for dealing with async Spark jobs has been
removed from Livy.

Closes #115
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create framework for integration tests,LIVY-123,13095761,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,20/Apr/16 21:36,18/Aug/17 18:16,19/Dec/25 04:15,21/Apr/16 22:29,0.2,,,0.2,,Tests,,,,,,,,,,0,,,,,,"See discussion in parent bug. This covers creating a framework for running tests against a real cluster (running HDFS, YARN and Livy).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 21 22:32:32 UTC 2016,,,,,,,,,,"0|i3iz7j:",9223372036854775807,,,,,,,,,,,,,,,,,,,"21/Apr/16 22:29;vanzin;Commit [03209b4f|https://github.com/cloudera/livy/commit/03209b4f42820e5cfaebdeb2a8b3562a1c7d21c9] by  Alex Man <tc.technetium@...> in cloudera/livy:
{code}
LIVY-123. Initial framework for integration tests.

This change adds an initial framework for integration tests, based on the code
from https://github.com/hdinsight/livy/tree/int-test/.

It allows both real clusters and ""mini"" clusters (managed by the framework) to be
used. The default is to run a small mini cluster. The code also allows defining your
own cluster config file without having to modify files in the repo for that; that could be
used to run different invocations of the tests as part of the build later on, for example.

There's probably code that needs to be cleaned up or fixed up, but this provides
the basic functionality, at least for mini clusters. I haven't (or hope I haven't) modified
the real cluster code so it should be working as well as it was in the original branch.

Two blatant shortcomings of the current mini cluster is the inability to run pyspark
or SparkR tests. Because it uses a fake Spark installation, it doesn't have access
to the needed python / R libraries. That will be taken care of in separate changes.

Closes #114
{code}
;;;","21/Apr/16 22:32;vanzin;[~tc0312] my merge script uses the first commit's author as the squashed commit's author, and since I worked on top of your original work, you ended up as the owner of the commit. Don't be alarmed. :-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVM crash because of unsafe.copymemory,LIVY-122,13095760,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,GayathriMurali,gmcdonald,20/Apr/16 20:18,09/May/16 18:12,19/Dec/25 04:15,09/May/16 18:12,0.1,,,,,Core,,,,,,,,,,0,,,,,,"The same job with unsafe.copymemory usage was able to successfully complete in Spark Shell. But it fails when launched the second time thru Livy. Here is an example piece of code we tried to emulate the error with Livy.

codes = {'code': textwrap.dedent(""""""\
   val test = sc.parallelize(0 to 200, 200).mapPartitions{ _ => 
     val UNSAFE: sun.misc.Unsafe = {
     val unsafeField = classOf[sun.misc.Unsafe].getDeclaredField(""theUnsafe"")
       unsafeField.setAccessible(true)
       unsafeField.get().asInstanceOf[sun.misc.Unsafe]
     }
	 
	 if(scala.util.Random.nextInt(100) == 1){
       val buffer = new Array[Byte](100)
	   val offset = UNSAFE.arrayBaseOffset(classOf[Array[Byte]])
	   val address = UNSAFE.allocateMemory(buffer.length)
	   UNSAFE.copyMemory(buffer, offset, null, address, 200)
	 }
     Iterator(1)
   }.collect()
   println(test)   
   """""")
 }
 # run the code
r = requests.post(statements_url, data=json.dumps(codes), headers=headers) 

This code caused the JVM to crash when launched using Livy , but completes successfully when launched directly on Spark Shell.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 18:12:40 UTC 2016,,,,,,,,,,"0|i3iz7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Apr/16 22:51;vanzin;Your example crashes even in spark-shell, because you're trying to write past the end of the array you created. a.k.a., you're causing a segfault. It probably passed in one test because of your use of random. Try this code instead:

{code}
val test = sc.parallelize(0 to 200, 200).mapPartitions{ _ =>
  val UNSAFE: sun.misc.Unsafe = {
    val unsafeField = classOf[sun.misc.Unsafe].getDeclaredField(""theUnsafe"")
    unsafeField.setAccessible(true)
    unsafeField.get().asInstanceOf[sun.misc.Unsafe]
  }
  val buffer = new Array[Byte](1000)
  val offset = UNSAFE.arrayBaseOffset(classOf[Array[Byte]])
  val address = UNSAFE.allocateMemory(buffer.length)
  UNSAFE.copyMemory(buffer, offset, null, address, 200)
  Iterator(1)
}.collect()
println(test)
{code}


;;;","22/Apr/16 18:13;vanzin;[~GayathriMurali] did you have time to look at this? I'm leaning towards closing this as invalid, since your test code is broken.;;;","22/Apr/16 20:19;GayathriMurali;We are still facing the issue that for a given benchmark, a job submitted second time fails when launched from Livy. Sorry about the test code. Please keep this open until early next week while we try to get a better representation of the problem;;;","03/May/16 17:14;vanzin;[~GayathriMurali] any update here? I still only see the broken test case.;;;","09/May/16 18:12;vanzin;There's not enough info to do anything here. If you're able to provide a valid reproduction, please re-open (or file a new bug).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RSC should use canonical localhost hostname,LIVY-120,13095758,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,erickt,erickt,gmcdonald,14/Apr/16 22:32,22/Apr/16 20:54,19/Dec/25 04:15,22/Apr/16 20:54,0.2,,,0.2,,RSC,,,,,,,,,,0,,,,,,Machines that are configured to use a short hostname by way of {{/etc/hosts}} cause problems for the livy repl because it's currently passing the short name to the livy server instead of the fully qualified domain name. This can cause problems if the livy server is in a different subdomain.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Apr 22 20:54:30 UTC 2016,,,,,,,,,,"0|i3iz6v:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Apr/16 20:54;vanzin;Fixed in 430135af45a88532c346e278f2a31c642e352c0c.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Move LivyConf out of ""core"" module",LIVY-119,13095757,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,vanzin,vanzin,gmcdonald,14/Apr/16 22:06,18/Apr/16 17:07,19/Dec/25 04:15,18/Apr/16 17:07,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"In an effort to try to slim down the ""core"" module and eventually get rid of it, let's move LivyConf to the server module, the only place where it's actually used.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 18 17:07:25 UTC 2016,,,,,,,,,,"0|i3iz6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Apr/16 17:07;vanzin;Commit [430135af|https://github.com/cloudera/livy/commit/430135af45a88532c346e278f2a31c642e352c0c] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-119. Move LivyConf to the server module.

The core module is small and, after some other adjustments to dependencies,
can probably be removed. LivyConf is only used in the server module (except
for a single static field, now moved to ClientConf), so move it there.

Closes #111
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Build Zip File Clarification to Readme,LIVY-117,13095755,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Duplicate,,Spankymonkey,gmcdonald,13/Apr/16 21:17,14/Apr/16 09:35,19/Dec/25 04:15,14/Apr/16 09:35,0.1,0.2,,,,Docs,,,,,,,,,,0,,,,,,There have been several users who have tried to start the livy-sever from the build directory.  This does not currently work.  The readme should be updated to fix this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 14 09:35:12 UTC 2016,,,,,,,,,,"0|i3iz67:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Apr/16 09:35;vanzin;See LIVY-115.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow Livy to submit jobs when run from the build directory,LIVY-115,13095753,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,11/Apr/16 05:19,14/Apr/16 10:41,19/Dec/25 04:15,14/Apr/16 10:41,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"Lots of people try to run Livy directly from the build directory, and that currently doesn't work well. Let's try to make that work and respect the principle of least astonishment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 14 10:41:27 UTC 2016,,,,,,,,,,"0|i3iz5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Apr/16 10:41;vanzin;Commit [14c594cc|https://github.com/cloudera/livy/commit/14c594cc50360bc568ab4a0a3792980e83b90887] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-115. Allow Livy to be run from the build directory.

Add extra code so that jar files that need to be added to Spark
apps can be retrieved from the build directories when not found
in the expected location.

Closes #107
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge client and interactive sessions,LIVY-114,13095752,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,11/Apr/16 05:14,27/Apr/16 18:33,19/Dec/25 04:15,27/Apr/16 18:33,0.2,,,0.2,,REPL,RSC,,,,,,,,,0,,,,,,"Right now client and interactive sessions have completely different APIs, although under the hood they're mostly the same code. With some tweaks, we could allow the same backend session to handle both pre-compiled Java/Scala code and also code snippets. For Python and R, at least initially, we'd be restricted to code snippets.

This should make the exposed Livy API much simpler.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 27 18:33:31 UTC 2016,,,,,,,,,,"0|i3iz5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Apr/16 18:33;vanzin;Commit [a5f3cfc9|https://github.com/cloudera/livy/commit/a5f3cfc9ebf1a5b3caf9ac23583c105fcf49aa6a] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-114. Merge interactive and client sessions.

The code to handle both session types was very, very similar. So instead
of having two completely separate endpoints, merge them, and allow sessions
to be used both in interactive mode and to run pre-compiled jobs. Note
that pre-compiled jobs are still restricted to Scala sessions.

Most of this change is code motion; I also cleaned up the repl package
hierarchy since there was no need to a bunch of different packages containing
a single class each.

The unit tests for interactive and pre-compiled jobs are still kept separate
in the server module, since the former uses mock sessions while the latter
requires a proper running Spark to run.

Closes #117
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Use different URIs for ""client"" and ""repl"" RSC sessions",LIVY-113,13095751,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Do,,vanzin,gmcdonald,08/Apr/16 20:41,11/Apr/16 05:12,19/Dec/25 04:15,11/Apr/16 05:12,0.2,,,,,REPL,RSC,,,,,,,,,0,,,,,,"Currently the code uses a config option and the same URI for client and repl sessions started via the RSC. It would be better to use different URIs, which would also make it possible to move the REPL-specific jobs out of {{LocalClient}} and into and ""server"" module.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 11 05:12:18 UTC 2016,,,,,,,,,,"0|i3iz5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Apr/16 05:12;vanzin;Kostas suggested in an unrelated conversation that we use the same session type for both client and repl sessions, and allow both pre-compiled jobs and code snippets to be submitted. I think this is doable and would help keep the API (and code) simpler, so I'll file a bug for that instead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Rename ""client-local"" to ""client-rsc""",LIVY-112,13095750,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,08/Apr/16 20:37,14/Apr/16 21:45,19/Dec/25 04:15,14/Apr/16 21:45,0.2,,,0.2,,RSC,,,,,,,,,,0,,,,,,"As much as I hate to rename things this late, ""client-local"" is becoming increasingly innacurate and confusing at the same time. Let's rename it to ""client-rsc"" before it's too late.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 14 21:45:30 UTC 2016,,,,,,,,,,"0|i3iz53:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Apr/16 21:45;vanzin;Commit [0206d392|https://github.com/cloudera/livy/commit/0206d392be6d0c495e8b70f383538b3849d36efb] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-112. Rename ""client-local"" to ""rsc"".

""Local"" is becoming more and more confusing as more work is done
on the RSC. Let's just call it by its usual name.

Closes #109
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove job monitoring from public API,LIVY-111,13095749,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,08/Apr/16 20:03,11/Apr/16 18:25,19/Dec/25 04:15,11/Apr/16 18:25,0.2,,,0.2,,API,,,,,,,,,,0,,,,,,"We removed the automatic metrics collection in LIVY-59 since it's possible for users to do that themselves. But the job monitoring API ({{JobContext.monitor}} is still in place, and without metrics, it's mostly useless and results in a lot of kinda hacky code.

Better to leave it out, at least for now, and let users who really want something like that deal with it themselves.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 11 18:25:08 UTC 2016,,,,,,,,,,"0|i3iz4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Apr/16 18:25;vanzin;Commit [e1a9282c|https://github.com/cloudera/livy/commit/e1a9282ca627c99872f2cf86948b639c1d0568bf] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-111. Remove async job monitoring API.

This API was a lot more useful when the RSC collected metrics
about running jobs. Since it doesn't do that anymore, this API
is kinda out of place, and there's a lot of hacky code to support
it, so let's just remove it.

Because the HTTP-based client is poll-based, there's no benefit
in having these callback-based APIs. Users can implement the same
functionality by collecting the data they need themselves, and
submitting a job to retrieve that data when desired.

Closes #103
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up RSC driver initialization,LIVY-110,13095748,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,05/Apr/16 18:17,11/Apr/16 01:01,19/Dec/25 04:15,11/Apr/16 01:01,0.2,,,0.2,,RSC,,,,,,,,,,0,,,,,,"The RSC driver initialization is a little hacky now; especially after the patch for LIVY-103. There's even the potential of blocking the RPC channel if somehow the app misbehaves.

That code needs some clean up so that it's more understandable and has fewer pitfalls. Extra points if less work is done in the constructor, which should make event handling during driver initialization predictable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 11 01:01:28 UTC 2016,,,,,,,,,,"0|i3iz4n:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Apr/16 01:01;vanzin;Commit [8cdc4590|https://github.com/cloudera/livy/commit/8cdc4590a5e477feab0fd30dc00894bc6389f642] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-110, LIVY-93. Simplify RSC driver class hierarchy.

The separation between ""client-mode"" driver and REPL driver was a little
hacky after recent changes. This patch aims at simplifying things.

Now, there is a single entry point to the RSC (RSCDriverBootstrapper),
and the actual driver implementation (which must extend RSCDriver, the
new name of RemoteDriver) can be configured.

The driver is responsible for handling RPC messages, so it must provide
""handle()"" methods for any messages that might arrive. RSCDriver handles
all messages directed at the client-mode driver. The REPL driver extends
it and adds handlers to REPL-specific messages, and re-uses the client-mode
logic for running the PingJob used to detect when the session is ready.
This simplified code a lot.

On top of that I made some other clean up changes:

- the code that launches the driver in-process and out-of-process was
  slightly modified to share more setup code.
- when jobs send results back to the client, all clients now receive
  the message. This code path is mostly unused in Livy (most uses are in
  the unit tests), since the Livy API uses bypass jobs which have different
  logic. In any case, there should be very few, if any, situations when
  there are two clients connected to a remote driver.
- fixed some sketchy error handling code, especially in the shutdown
  path of the driver.

Closes #101
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark is using Kryo 2.21 while Livy is using 2.22,LIVY-109,13095747,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,tc0312,gmcdonald,05/Apr/16 01:18,05/Apr/16 03:29,19/Dec/25 04:15,05/Apr/16 03:29,0.2,,,,,RSC,,,,,,,,,,0,,,,,,"There's a backward incompatible signature change in Kyro 2.22: https://github.com/twitter/chill/issues/209
It's causing errors when Spark/Hive call Kyro. An example:

{noformat}
An error occurred while calling o851.showString.
: java.lang.NoSuchMethodError: com.esotericsoftware.kryo.Kryo.setInstantiatorStrategy(Lorg/objenesis/strategy/InstantiatorStrategy;)V
        at org.apache.hadoop.hive.ql.exec.Utilities$3.initialValue(Utilities.java:1097)
        at org.apache.hadoop.hive.ql.exec.Utilities$3.initialValue(Utilities.java:1089)
        at java.lang.ThreadLocal.setInitialValue(ThreadLocal.java:160)
        at java.lang.ThreadLocal.get(ThreadLocal.java:150)
        at org.apache.spark.sql.hive.HiveShim$HiveFunctionWrapper.serializePlan(HiveShim.scala:155)
        at org.apache.spark.sql.hive.HiveShim$HiveFunctionWrapper.writeExternal(HiveShim.scala:168)
{noformat}


Should we downgrade from 2.22 to 2.21 since Spark is using 2.21?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 05 02:57:06 UTC 2016,,,,,,,,,,"0|i3iz4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Apr/16 02:57;vanzin;Kryo is shaded / relocated in the master branch; there shouldn't be any conflicts in the latest master (or after my open patches are merged). We really shouldn't depend on what version of libraries Spark supports since we may need to support different versions of Spark.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Guava dependency,LIVY-108,13095746,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,vanzin,vanzin,gmcdonald,04/Apr/16 20:33,18/Apr/16 16:50,19/Dec/25 04:15,18/Apr/16 16:50,0.2,,,0.2,,RSC,,,,,,,,,,0,,,,,,"Livy uses Guava for very few things, some of which are covered by ""new"" syntax in Java 7. To avoid having to manage another dependency and potentially run into issues with different versions of Guava needed by Hadoop / Spark / user code, let's just get rid of Guava.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 18 16:50:15 UTC 2016,,,,,,,,,,"0|i3iz47:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Apr/16 16:50;vanzin;Commit [dfdcbc2c|https://github.com/cloudera/livy/commit/dfdcbc2cc013c14e6dcdaf1fed84aecad0d9d28d] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-108. Remove Guava dependency from RSC.

This avoids two sets of issues: the first is the need to localize the
Guava jar every time an app is started, in case the jars haven't been
cached in HDFS. The second is that we can't really know which version
of Guava will be used; most of the time it will be Hadoop's, but user
configuration and dependencies might change that, and since Guava is
a very common dependency, let's not make user's lives more difficult.

Guava is not explicitly excluded from dependencies, because it's needed
to run the tests (and in some cases is even provided by different
transitive dependencies, such as hive-exec). The patch was tested with
the exclusions in place, to make sure the module compiles without Guava.

Closes #110
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Restore ""configuration blacklist"" functionality",LIVY-107,13095745,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,04/Apr/16 18:16,13/Apr/16 15:50,19/Dec/25 04:15,13/Apr/16 15:50,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"There used to be code to provide a ""black list"" of Spark configuration entries that users weren't allowed to set. That code was removed since a lot of the configurations it was trying to protect didn't really have security implications.

But that depends on running Spark in ""yarn-client"" mode. So, at the very list, Livy admins should be able to block users from choosing which Spark master they want to use.

Given that, restoring the black list itself is probably better, since it gives admins more control over how users can run things on their cluster.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 13 15:50:54 UTC 2016,,,,,,,,,,"0|i3iz3z:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Apr/16 15:50;vanzin;Commit [9da7bb4a|https://github.com/cloudera/livy/commit/9da7bb4a443c48f015b09eaf29778362ecddb268] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-107. Add config blacklist file.

This file lists Spark (and Livy) settings that cannot be set by users,
to avoid security issues (and possibly other runtime issues). For example,
Livy can only start sessions really securely in YARN cluster mode, so admins
should blacklist options that would allow users to override that.

Closes #105
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Spark + Scala 2.11,LIVY-105,13095743,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,jerryshao,vanzin,gmcdonald,04/Apr/16 18:12,11/Nov/16 05:50,19/Dec/25 04:15,11/Nov/16 05:50,0.2,,,0.3,,REPL,,,,,,,,,,3,,,,,,"The current interactive session backend only supports Spark built against Scala 2.10. The 2.11 Spark REPL is very different internally, and we need to add support for that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-04-04 18:12:38.0,,,,,,,,,,"0|i3iz3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switch to scala 2.11,LIVY-104,13095742,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,vanzin,gmcdonald,04/Apr/16 18:11,18/Jan/18 00:27,19/Dec/25 04:15,18/Jan/18 00:27,0.2,,,0.5.0,,Core,,,,,,,,,,1,,,,,,"Livy should switch to Scala 2.11; 2.10 is EOL.

Note this doesn't mean dropping support for Spark + 2.10; we should still support running on Spark built for 2.10. This just affects the Livy server itself.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-116,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 18 00:27:31 UTC 2018,,,,,,,,,,"0|i3iz3b:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/Apr/17 20:16;ajbozarth;[~tc0312] and [~zjffdu] is this something we still want? I did some looking into this and, outside of one really weird issue, updating to 2.11 seems as simple as updating the pom.

The one issue I found was in the integrations tests, specifically JobApiIT. The ""run scala jobs"" test crashes the entire build. It seems related to LIVY-219 so perhaps [~jerryshao] might be able to help me sort it out if this is something we want to move forward on.;;;","29/Apr/17 00:37;ajbozarth;Been a couple weeks but I'm still interested in following up on this. [~tc0312] [~zjffdu] [~jerryshao];;;","01/Dec/17 08:50;jerryshao;Hi [~ajbozarth] would you please drive this issue if you have time, I'm thinking of shifting to 2.11 for Livy 0.5. Any thought?;;;","01/Dec/17 17:38;vanzin;At this point in time, maybe it makes more sense to jump to 2.12?;;;","01/Dec/17 22:44;ajbozarth;I agree, I think jumping to 2.12 alongside LIVY-423 makes the most sense. I would also love this for 0.5 and be willing to drive it, but I won't have time until next calendar year. If we want to do this in January and delay 0.5 until early February then I'm in, otherwise someone else should pick this up.;;;","04/Dec/17 00:19;jerryshao;Livy server itself requires Spark artifacts to run unit tests, if we change Scala version to 2.12, it also requires Spark Scala 2.12 dependencies, which are not available currently. So from my understanding a workable solution is to change to Scala 2.11, which is available from Spark 1.6 to 2.3.;;;","04/Dec/17 20:46;ajbozarth;Thanks [~jerryshao], I believe you're right, even after adding Scala 2.12 support with Spark 2.3 we'll still need that back compatibility. I think we should still look into seeing if building with 2.12 will work though just to be sure. As I said before, if this isn't picked up by January I'll take it and will check compatibility, either way the IT's will need some tweaking for any update.;;;","11/Jan/18 22:40;ajbozarth;Opened a PR: https://github.com/apache/incubator-livy/pull/72;;;","18/Jan/18 00:27;ajbozarth;Issue resolved by pull request 72
[https://github.com/apache/incubator-livy/pull/72];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interactive sessions cannot see jars defined by user,LIVY-103,13095741,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,01/Apr/16 21:09,05/Apr/16 18:28,19/Dec/25 04:15,05/Apr/16 18:28,0.2,,,0.2,,REPL,,,,,,,,,,0,,,,,,"This was fixed by @tc0312 with this: https://github.com/hdinsight/livy/commit/f11cdcb5357046fe555a4ee53c85fb1bccf646b5

There are some minor issues on top of that because master is using the RSC now, so I'll fix those as part of this also.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 05 18:28:24 UTC 2016,,,,,,,,,,"0|i3iz33:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Apr/16 18:28;vanzin;Commit [340a6c9f|https://github.com/cloudera/livy/commit/340a6c9fa10532d86fe6b1320a3141a5128ae4b3] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-103. Make repl session see user jars, correctly report idle state.

Because of how Spark's REPL deals with class loaders, it's necessary to
poke inside of it to make jars distributed by Livy be seen. This fix is
the same as in the patch below:
  https://github.com/hdinsight/livy/commit/f11cdcb5357046fe555a4ee53c85fb1bccf646b5

Aside from that, the REPL sessions were reporting ""idle"" state too early.
I had to shuffle a little bit of code around so that the REPL can execute
basic RSC commands, so that the client side can know when it's safe to
send commands. This is a little bit hacky at the moment until we can spend
more time working on LIVY-93.

Finally, I included some changes to propagate jacoco configuration to child
processes, so that we can run code coverage analysis when we run the RSC
driver in a child process too.

Closes #100
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't leave spark-submit launchers lying around,LIVY-102,13095740,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,01/Apr/16 20:13,11/Apr/16 18:30,19/Dec/25 04:15,11/Apr/16 18:30,0.2,,,0.2,,Core,RSC,,,,,,,,,0,,,,,,"When Livy launches Spark in cluster mode, the {{spark-submit}} process used to launch Spark hangs around in the background, consuming resources and printing lots of things to the log.

Instead, we should let the process go away since it's not useful anymore after the app is started. That's the {{spark.yarn.submit.waitAppCompletion}} config option (default = true), but we also need to make sure the RSC (and batch sessions) won't detect that as an error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 11 18:30:49 UTC 2016,,,,,,,,,,"0|i3iz2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Apr/16 18:30;vanzin;Commit [e36145f6|https://github.com/cloudera/livy/commit/e36145f6a3bf4c501b4e919d9f9afed150342349] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-102. Set the config to dispose of the yarn-cluster launcher.

This saves resources on the Livy server by not keeping the launcher
process around; Livy is already connected to the driver so it knows
how to monitor its status.

Closes #104
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove warning about Spark 1.6,LIVY-101,13095739,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,vanzin,vanzin,gmcdonald,01/Apr/16 20:10,11/Apr/16 01:06,19/Dec/25 04:15,11/Apr/16 01:06,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,Stop warning users that Spark 1.6 hasn't been tested.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 11 01:06:58 UTC 2016,,,,,,,,,,"0|i3iz2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Apr/16 01:06;vanzin;Commit [7aef0095|https://github.com/cloudera/livy/commit/7aef009521f09fff3dcb7ae1e2dfc5c7d2d802c5] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-101. Remove message about unsupported Spark versions.

We've actually tested on 1.6; instead of a white list, let's just
print the found Spark version in the code. Also added a couple of
small tests just to exercise that code during the build.

Closes #102
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Driver for client sessions needs to add uploaded jars to driver's class path,LIVY-100,13095738,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,31/Mar/16 18:17,05/Apr/16 18:21,19/Dec/25 04:15,05/Apr/16 18:21,0.2,,,0.2,,RSC,,,,,,,,,,0,,,,,,"When you upload a jar file using {{LivyClient.uploadJar}}, it adds the jar to the SparkContext, but that only affect executors, not the driver. So currently a client cannot use {{uploadJar}} to upload its main jar file, which contains the bytecode for the jobs that need to run in the driver.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 05 18:21:01 UTC 2016,,,,,,,,,,"0|i3iz2f:",9223372036854775807,,,,,,,,,,,,,,,,,,,"31/Mar/16 23:12;tc0312;We fixed this issue in the old interactive code. I'm not sure it will help with RSC but here's the fix: https://github.com/hdinsight/livy/commit/f11cdcb5357046fe555a4ee53c85fb1bccf646b5;;;","31/Mar/16 23:21;vanzin;Thanks; that might help with the RSC-based repl sessions, but there's more that needs to be written for the non-interactive (""client"") sessions. I'll make sure to test repl sessions too.;;;","01/Apr/16 21:09;vanzin;I filed LIVY-103 to track the REPL fix since I ran into another issue when testing your patch, and since it's pretty isolated from this fix, I'll tackle that separately.;;;","05/Apr/16 18:21;vanzin;Commit [dff04a77|https://github.com/cloudera/livy/commit/dff04a77a30625777fcfbd44e34b79a9adf15ca4] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-100. Make jars uploaded by the user available in RSC's driver.

Spark's ""addJar"" only applies to executors, so we need more code to get
jars uploaded by users using the RSC API available to the driver. This
change sets up a new class loader that is used by the ""add jar"" job to
add uploaded files to the driver's class path.

Closes #99
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support _HOST pattern for principal,LIVY-99,13095737,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,31/Mar/16 09:51,20/Apr/16 17:53,19/Dec/25 04:15,20/Apr/16 17:53,0.1,,,0.2,,Core,,,,,,,,,,0,,,,,,"hadoop support _HOST pattern for principal, it would be better to support it too in livy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Apr 20 17:53:27 UTC 2016,,,,,,,,,,"0|i3iz27:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Apr/16 17:53;vanzin;Commit [ee7fca35|https://github.com/cloudera/livy/commit/ee7fca358c30da20a4e07b897016d4c687101094] by  Jeff Zhang <zjffdu@...> in cloudera/livy:
{code}
LIVY-99. Support _HOST pattern for principal

Closes #97
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run kinit thread periodly in secure mode,LIVY-98,13095736,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,31/Mar/16 06:22,18/Aug/17 18:10,19/Dec/25 04:15,27/Jul/16 23:59,0.1,,,0.3,,Core,,,,,,,,,,1,,,,,,"livy.server.auth.kerberos.principal & livy.server.auth.kerberos.keytab is for spengo, but launching yarn application also require principal and keytab. Nowï¿½ï¿½ï¿½user need to invoke kinit to get the ticket. Since livy server is a long running service, it would be better to allow user specify principal & keytab in livy-defaults.conf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Jul 27 23:59:15 UTC 2016,,,,,,,,,,"0|i3iz1z:",9223372036854775807,,,,,,,,,,,,,,,,,,,"31/Mar/16 09:52;zjffdu;spark don't support use keytab and proxy user together (not sure why), try to find way to do in livy side. ;;;","31/Mar/16 21:25;vanzin;bq. spark don't support use keytab and proxy user together (not sure why)

That's because Spark's use of principal / keytab is not as a generic kerberos login feature. It needs that information to log in as the user *in the cluster*, so that it can renew delegation tokens used to talk to Hadoop services when running applications in cluster mode.

That means that if you use both options, you'd be logging in as the Livy subject, submitting the application as the proxied user, and uploading the Livy server's keytab to that application; meaning that all users running applications through Livy would be able to read Livy's keytab, which is a huge security problem.

Livy's login needs to be handled by Livy itself; {{UserGroupInformation}} has methods to login, but those do not extend to child processes (because they don't write a ticket cache file). Hive, for example, handles that by running ""kinit"" before running ""spark-submit"", which is probably fine, and can be optimized to not run kinit if the underlying tgt is still valid.

Another thing that can be tried is to fetch delegation tokens for the target proxy user, write them to a file, and set {{HADOOP_TOKEN_FILE_LOCATION}} to tell Spark where to find it (that's also handled by {{UserGroupInformation}}). Some code in Spark that explicitly tries to fetch delegation token needs to be disabled in that case, but hopefully it would work. This is somewhat similar to SPARK-13148. For that, though, Livy would put itself in a situation where it needs to know which delegation tokens the application will need, since it's the only entity that can get them.

An alternative to Livy managing application delegation tokens is LIVY-44, where instead of impersonation Livy would instead just log in as the requested user with the credentials given in the session creation request.;;;","01/Apr/16 00:20;zjffdu;Does that mean livy currently still have the ticket expiration issue as long running service ? ;;;","01/Apr/16 00:30;vanzin;Do you mean Livy itself or the sessions it creates?

The answer to both is yes, but for different reasons. For Livy itself, the discussion above applies, since it's related to Livy managing its own kerberos logins, and that code doesn't currently exist. For sessions, LIVY-44 applies, since currently the only way to support long-running sessions is to give your principal and keytab to Spark.;;;","01/Apr/16 03:23;zjffdu;Can LIVY-44 solve both cases ? As my understanding, livy just use the ticket for launching yarn application. So as long as we allow specifying keytab & principal in spark-submit, seems livy server can launch yarn application successfully and livy session also will renew its ticket.  Do I miss anything here ?;;;","01/Apr/16 06:53;zjffdu;And seems impersonation can't solve the delegation token expiration issue in spark session.  Not sure whether LIVY-44 is secure enough.  I think there's 2 ways for LIVY-44.
* One is passing keytab file content in CreateSessionRequest (SSL needs to be enabled, not sure whether this is secure enough)
* Another way is just passing keytab path in CreateSessionRequest  (Seems not good, because that would mean user-1 can use user-2's keytab/principal as long as he know the keytab file path, there may need another layer to verify whether the principal & user are matched);;;","01/Apr/16 16:55;vanzin;bq. Can LIVY-44 solve both cases?

It can, but the user experience is a little odd. It's weird to require users to upload their keytabs. Users might not even have keytabs.

bq. And seems impersonation can't solve the delegation token expiration issue in spark session. 

That's correct, because Spark doesn't have a mechanism for an external entity to upload new delegation tokens.

bq. Another way is just passing keytab path in CreateSessionRequest (Seems not good

That should be ok if authentication is enabled; you just force the principal to be the authenticated user, instead of allowing the principal to be defined in the request. But in reality, if user 1 can read user 2's keytab, that's all he needs to just log in as user 2 and do anything he wants.
;;;","27/Jul/16 23:59;vanzin;Commit [ef8e6304|https://github.com/cloudera/livy/commit/ef8e6304cc0b6f3f627443bc15706fa214c79fcb] by  Jeff Zhang <zjffdu@...> in cloudera/livy:
{code}
LIVY-98. Run kinit thread periodically in secure mode

The solution here is always execute kinit command periodically as long as the refresh interval is less than the ticket lifetime which is specified in /etc/krb5.conf

Closes #145
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve livy launch script,LIVY-97,13095735,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,25/Mar/16 01:12,16/May/16 18:05,19/Dec/25 04:15,16/May/16 18:05,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"Several things to be added:
1. livy-env.sh (specify SPAPRK_HOME, LIVY_LOG_DIR, LIVY_PID_DIR)
2. Enhance livy-server.sh to make it can stop server too. And redirect the log to LIVY_LOG_DIR and generate pid file under LIVY_PID_DIR",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 18:05:54 UTC 2016,,,,,,,,,,"0|i3iz1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/May/16 18:05;vanzin;Commit [a385cbdf|https://github.com/cloudera/livy/commit/a385cbdf17184dd5b9ee9f8d7d078f819f01ab2c] by  Jeff Zhang <zjffdu@...> in cloudera/livy:
{code}
LIVY-97. Improve livy launch script

using command ""livy-server start"" to start livy server and ""livy-server stop"" to stop livy server.
LIVY_LOG_DIR is for log directory location and LIVY_PID_DIR is for pid location.

Closes #95
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Callback URL is not correctly when SSL is enabled,LIVY-96,13095734,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,,zjffdu,gmcdonald,22/Mar/16 03:40,23/Mar/16 21:12,19/Dec/25 04:15,23/Mar/16 21:12,0.2,,,,,Core,,,,,,,,,,0,,,,,,It should be https when ssl is enabled.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 21:11:06 UTC 2016,,,,,,,,,,"0|i3iz1j:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/Mar/16 21:11;vanzin;This is not an issue anymore after LIVY-46 was merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy-repl driver is not secured,LIVY-95,13095733,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,zjffdu,zjffdu,gmcdonald,18/Mar/16 02:43,23/Mar/16 21:11,19/Dec/25 04:15,23/Mar/16 21:11,0.2,,,,,Interpreter,,,,,,,,,,0,,,,,,"As long as I know the livy-repl server address, I can send command to this server even in secure mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 21:11:52 UTC 2016,,,,,,,,,,"0|i3iz1b:",9223372036854775807,,,,,,,,,,,,,,,,,,,"18/Mar/16 17:08;vanzin;LIVY-46 will fix this.;;;","23/Mar/16 21:11;vanzin;This is not an issue anymore after LIVY-46 was merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLContext/HiveContext is not available in Interpreter,LIVY-94,13095732,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,17/Mar/16 10:21,07/Jul/16 01:17,19/Dec/25 04:15,06/Jul/16 22:57,0.2,,,0.3,,Interpreter,,,,,,,,,,0,,,,,,"Only SparkContext is available in Interpreter, it would be better to have SQLContext/HiveContext to make the behavior consistent with spark-shell",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 07 01:17:08 UTC 2016,,,,,,,,,,"0|i3iz13:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/May/16 05:44;zjffdu;It is straightforward to add SQLContext. But for HiveContext is a little complicated, because it require hive-site.xml and datanucleus jars. There's 2 ways to specify that
1.  Ask the client to specify that in rest api, but client should have no idea where the hive-site.xml and datanucleus jars are located in the livy server machine. Although they put these files on hdfs and specify them using hdfs path. But this way still have one issue in yarn-cluster mode due to SPARK-14845
2.  Add a option like enableHiveContext in rest api and livy server pick up the hive-site.xml and datanucleus jars automatically. But this way has limitation that it always use the same hive-site.xml, prevent user to user another custom hive-site.xml if he want to connect to another hive metastore. 

In this PR, I just use option 1. ;;;","06/Jul/16 22:57;vanzin;Commit [0a8f8dab|https://github.com/cloudera/livy/commit/0a8f8dab39d3b487ba4544d6bbfe57584c62b897] by  Jeff Zhang <zjffdu@...> in cloudera/livy:
{code}
LIVY-94. SQLContext/HiveContext is not available in Interpreter

It is straightforward to add SQLContext. But for HiveContext is a little complicated, because it require hive-site.xml and datanucleus jars. There's 2 ways to specify that
1. Ask the client to specify that in rest api, but client should have no idea where the hive-site.xml and datanucleus jars are located in the livy server machine. Although they put these files on hdfs and specify them using hdfs path. But this way still have one issue in yarn-cluster mode due to SPARK-14845
2. Add a option like enableHiveContext in rest api and livy server pick up the hive-site.xml and datanucleus jars automatically. But this way has limitation that it always use the same hive-site.xml, prevent user to user another custom hive-site.xml if he want to connect to another hive metastore.
In this PR, I just use option 1.

Closes #132
{code}
;;;","07/Jul/16 01:17;vanzin;Updated the commit description, new commit is https://github.com/cloudera/livy/commit/14aedd94;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor RSC protocol,LIVY-93,13095731,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,hshreedharan,gmcdonald,16/Mar/16 20:01,11/Apr/16 01:01,19/Dec/25 04:15,11/Apr/16 01:01,0.1,,,0.2,,Core,,,,,,,,,,0,,,,,,"The RSC protocol is pretty tightly coupled with the implementation of the driver. There are calls from the driver to the protocol and vice versa. We should ideally have these implemented as callbacks and the driver should probably never have a protocol handler reference. I like the way Avro defines the protocol in the Netty implementation, having used it in Flume. Define a bunch of interfaces that are implemented by the ""handler"" - in this case the driver - cleanly separates the protocol from the driver.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 11 01:01:38 UTC 2016,,,,,,,,,,"0|i3iz0v:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Apr/16 01:01;vanzin;Commit [8cdc4590|https://github.com/cloudera/livy/commit/8cdc4590a5e477feab0fd30dc00894bc6389f642] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-110, LIVY-93. Simplify RSC driver class hierarchy.

The separation between ""client-mode"" driver and REPL driver was a little
hacky after recent changes. This patch aims at simplifying things.

Now, there is a single entry point to the RSC (RSCDriverBootstrapper),
and the actual driver implementation (which must extend RSCDriver, the
new name of RemoteDriver) can be configured.

The driver is responsible for handling RPC messages, so it must provide
""handle()"" methods for any messages that might arrive. RSCDriver handles
all messages directed at the client-mode driver. The REPL driver extends
it and adds handlers to REPL-specific messages, and re-uses the client-mode
logic for running the PingJob used to detect when the session is ready.
This simplified code a lot.

On top of that I made some other clean up changes:

- the code that launches the driver in-process and out-of-process was
  slightly modified to share more setup code.
- when jobs send results back to the client, all clients now receive
  the message. This code path is mostly unused in Livy (most uses are in
  the unit tests), since the Livy API uses bypass jobs which have different
  logic. In any case, there should be very few, if any, situations when
  there are two clients connected to a remote driver.
- fixed some sketchy error handling code, especially in the shutdown
  path of the driver.

Closes #101
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala-friendly API for Livy client sessions,LIVY-92,13095730,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,mnarayan,vanzin,gmcdonald,16/Mar/16 18:57,16/Jun/16 22:28,19/Dec/25 04:15,16/Jun/16 22:23,0.2,,,0.3,,API,,,,,,,,,,0,,,,,,"Currently, the RSC API is in Java. While it's possible to access all the Scala functionality from the Java wrappers, we should add a more Scala-friendly API that exposes the Scala types directly and works more cleanly with Scala code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 16 22:24:21 UTC 2016,,,,,,,,,,"0|i3iz0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/Jun/16 22:23;vanzin;Commit [e00ac738|https://github.com/cloudera/livy/commit/e00ac738ea69b0c8f9fcce567846a24f997320ca] by  Manikandan Nagarajan<...> in cloudera/livy:
{code}
LIVY-92. Scala-friendly API for Livy client sessions

- Added a Scala API which is a wrapper over the existing Java API
- Jobs can be submitted with a Scala-friendly JobContext eliminating the overhead of creating new job instances
- The submit API returns a Scala JobHandle which supports the existing Java Listener callbacks

Closes #151
{code}
;;;","16/Jun/16 22:24;vanzin;Wow that commit got messed up, let me fix the author.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch mode don't support app jar in hdfs,LIVY-91,13095729,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,zjffdu,gmcdonald,15/Mar/16 03:01,06/May/16 21:20,19/Dec/25 04:15,06/May/16 21:20,0.2,,,,,Core,Interpreter,,,,,,,,,0,,,,,,Use can not specify app jar on hdfs. This is very inconvenient for batch mode.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri May 06 21:20:26 UTC 2016,,,,,,,,,,"0|i3iz0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/May/16 17:24;vanzin;Is this still an issue? The tests I added LIVY-130 use HDFS for the paths and work fine. Or maybe I'm misunderstanding the bug.;;;","06/May/16 21:20;vanzin;Jar files on HDFS seem to be working fine. Note that Spark only supports that when running in yarn-cluster mode. That's, not surprisingly, the recommended way for configuring Livy.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add livy-client.conf.template,LIVY-90,13095728,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,ajbozarth,zjffdu,gmcdonald,11/Mar/16 06:18,04/Mar/17 00:55,19/Dec/25 04:15,04/Mar/17 00:55,0.2,,,0.4.0,,API,,,,,,,,,,0,,,,,,"LivyClient can load livy-client.conf, but there's not template file under conf about what can be in livy-client.conf. Although there's one sample livy-client.conf in livy/api/src/test/resources/livy-client.conf,  it is not available in distribution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Feb 27 17:46:25 UTC 2017,,,,,,,,,,"0|i3iz07:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Feb/17 23:34;ajbozarth;Is this something we still want, the sample conf is only two lines? 

Similarly do we want to replace the sample conf files that are already in /conf with *.template files for easier git adds?

If yes to both it would be easier to do them together, otherwise I'll open the second idea as a new JIRA.;;;","16/Feb/17 23:32;ajbozarth;[~zjffdu] ^;;;","24/Feb/17 00:43;ajbozarth;PR: https://github.com/cloudera/livy/pull/300;;;","27/Feb/17 17:46;ajbozarth;separated out a new pr:
https://github.com/cloudera/livy/pull/301
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySparkInterpreter doesn't work,LIVY-89,13095727,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,09/Mar/16 05:42,11/Mar/16 18:25,19/Dec/25 04:15,11/Mar/16 18:25,0.2,,,0.2,,Interpreter,,,,,,,,,,0,,,,,,It is due to commit 3c314b11777459e10984ab408aaf2cbd47edf6db Create a fake SPARK_HOME so tests that fork Spark can run.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Mar 11 18:25:48 UTC 2016,,,,,,,,,,"0|i3iyzz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Mar/16 18:25;vanzin;Commit [03fe9aa2|https://github.com/cloudera/livy/commit/03fe9aa286dab8a6d5594393149ed9ee360f906c] by  Jeff Zhang <zjffdu@...> in cloudera/livy:
{code}
LIVY-89. PySparkInterpreter doesn't work

Closes #88
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace net.databinder.dispatch with com.ning.async-http-client,LIVY-88,13095726,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,zjffdu,zjffdu,gmcdonald,09/Mar/16 01:42,17/Mar/16 19:28,19/Dec/25 04:15,17/Mar/16 19:28,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"net.databinder.dispatch's licence is LGPL, it might be a problem for users. I suggest to use com.ning.async-http-client which is apache license.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 17 19:28:40 UTC 2016,,,,,,,,,,"0|i3iyzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/Mar/16 19:28;vanzin;Commit [761599f9|https://github.com/cloudera/livy/commit/761599f9fa08b4e2ce73c4d848b5d2b2690ff455] by  Jeff Zhang <zjffdu@...> in cloudera/livy:
{code}
LIVY-88. Replace net.databinder.dispatch with com.ning.async-http-client

Closes #87
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create livy UI to manage sessions,LIVY-87,13095725,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ajbozarth,zjffdu,gmcdonald,08/Mar/16 12:30,16/Jan/18 20:05,19/Dec/25 04:15,16/Jan/18 20:05,0.2,,,0.4.0,,Core,,,,,,,,,,1,,,,,,"Currently sessions status is only available through rest api, it would be nice to have centralized livy ui to manage all the sessions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-346,LIVY-342,LIVY-353,LIVY-343,LIVY-354,LIVY-387,LIVY-344,LIVY-366,LIVY-345,LIVY-352,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Jan 16 20:05:42 UTC 2018,,,,,,,,,,"0|i3iyzj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/Mar/17 00:25;ajbozarth;Is this still a feature we are interested in pursuing? If so I would be willing to start looking into what it would entail.;;;","17/Mar/17 01:19;zjffdu;It is nice to have, but not urgent. I would be appreciated if you can help on this. ;;;","17/Mar/17 01:22;ajbozarth;I'll do some work on it whenever I'm not focused on higher priority tasks then and I'll open a pr when I have some enough WIP to start a review.;;;","01/Apr/17 00:53;ajbozarth;So I've coded up a POC for a Livy Web UI and pushed it to GitHub:
https://github.com/ajbozarth/livy/tree/livy-ui-poc
I've also typed up a rough word doc of my plans/thoughts/notes and attached here.
Since I'm posting this at the end of the week I'll reach out the email list and slack channel about on Monday for further discussion.
If I get the ok on this code design I'll split off sub-jiras and start work on the phases outlined in my word doc. ;;;","03/Apr/17 19:18;ajbozarth;[~zjffdu] and [~tc0312] I sent a email to the mailing list asking for feedback on this but wanted to ping you directly also;;;","07/Apr/17 05:33;tc0312;This's a very good idea. Can we do this relying just on the public API?;;;","07/Apr/17 07:01;ajbozarth;Like I mentioned on slack, my plan is to dynamically call the current API in javascript code to fill each webpage in the UI. The UI will not have access to any of the Livy Session data in it's scala code.;;;","10/Apr/17 18:06;ajbozarth;Based on [~tc0312]'s feedback on here and slack unless [~zjffdu] has any issues I'll split off some sub-jiras and start work on this this week;;;","18/Apr/17 21:27;ajbozarth;Split off incremental sub-tasks;;;","26/Apr/17 00:34;ajbozarth;pr open for the first part (LIVY-342): https://github.com/cloudera/livy/pull/319;;;","09/May/17 21:32;ajbozarth;Next PRs:
https://github.com/cloudera/livy/pull/326
https://github.com/cloudera/livy/pull/327;;;","13/Jul/17 01:16;ajbozarth;https://github.com/apache/incubator-livy/pull/16;;;","26/Jul/17 21:17;ajbozarth;latest pr: https://github.com/apache/incubator-livy/pull/25;;;","02/Aug/17 00:38;jerryshao;Hi Alex, I suggest LIVY-387 would be the last UI JIRA for Livy 0.4. And for the left JIRAs I would suggest to work on it after a first Apache release, what do you think?;;;","02/Aug/17 00:59;ajbozarth;That sounds good to me, I didn't even intend for it to make it, but I had some time today. Also LIVY-366 is giving me some hangups, I'm not sure how or even if I should tackle it.;;;","16/Jan/18 20:05;ajbozarth;Given the core web ui is finished I going to close this and either break off or close the remaining subtasks, which only consisted of followup and phase 2 type tasks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interactive mode doesn't work in yarn-cluster mode,LIVY-86,13095724,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,,zjffdu,gmcdonald,04/Mar/16 04:11,23/Mar/16 21:13,19/Dec/25 04:15,23/Mar/16 21:13,0.2,,,,,Core,Interpreter,,,,,,,,,0,,,,,,"livy-server will read the livy-repl address from driver. Now it read it from the driver process stdout. But if it is yarn-cluster mode, the driver is in a remote machine that it can not read its stdout.

{code}
private val stdoutThread = new Thread {
    override def run() = {
      val regex = """"""Starting livy-repl on (https?://.*)"""""".r

      val lines = process.inputIterator

      // Loop until we find the ip address to talk to livy-repl.
      @tailrec
      def readUntilURL(): Unit = {
        if (lines.hasNext) {
          val line = lines.next()

          line match {
            case regex(url_) => url = new URL(url_)
            case _ => readUntilURL()
          }
        }
      }

      readUntilURL()
    }
  }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 21:13:51 UTC 2016,,,,,,,,,,"0|i3iyzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Mar/16 07:40;zjffdu;It's my code change cause this issue. But seems this thread reading from process stdout is not necessary. Because the callback will set the url. ;;;","23/Mar/16 21:13;vanzin;This is not an issue anymore after LIVY-46 was merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support pyspark with python3,LIVY-85,13095723,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,purechoc,purechoc,gmcdonald,04/Mar/16 00:40,07/Jul/16 18:26,19/Dec/25 04:15,07/Jul/16 18:26,0.2,,,0.3,,Interpreter,,,,,,,,,,1,,,,,,"currently pyspark working with only python2.
(fake_shell.py don't work on python3)

Might need a support both python version.

how about this.
AS-IS
{code:none}
kind = [pyspark, spark, sparkr]
{code}
AS-IS
{code:none}
# if user choice pyspark3,  PythonInterperter run fake_shell3.py
kind = [pyspark, spark, sparkr, pyspark3]
{code}

If i misunderstood, this ticket close please.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jul 07 18:26:00 UTC 2016,,,,,,,,,,"0|i3iyz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Jun/16 00:05;linchan;In your design, will pyspark and pyspark3 work concurrently on a single cluster (only difference is the type submitted at startup)? Or in a single cluster, only 1 of the 2 python types would work correctly?;;;","03/Jun/16 02:18;purechoc;TO-BE 
# if user choice pyspark3,  PythonInterpreter exec ""python3 fake_shell.py"", user can executed python3 code.
# if user choice pyspark,  PythonInterpreter exec ""python2 fake_shell.py"" user can executed python2 code.
kind = [pyspark, spark, sparkr, pyspark3]

if user run pyspark and pyspark3 notebook, 2 PythonInterpreter process will launched.

;;;","03/Jun/16 21:09;KevinGre;The proposed pull request covers interactive but batch also needs changes so that a batch submitter can specify Python2 or Python3 for a PySpark job. Setting the spark conf setting spark.yarn.appMasterEnv.PYSPARK_PYTHON is not sufficient as the PYSPARK_PYTHON environment variable needs to be set for the spark-submit and set according to a batch POST setting. Currently there is no way to do this, and the python version can only be set globally.

The simplest change to support python 2 or 3 choice on batch submit would be adding a PYSPARK_PYTHON setting to batch POST that would override the default and would commonly be set to ""python3"" or ""python2"". Just as batch POST echoes the parameters to spark-submit, this allows this additional setting to passed through to spark-submit.

An alternative would be not to just pass through the environment variable value but have Livy accept an enumerated value (say 2 or 3) that maps to settings for PYSPARK_PYTHON for each value. Please comment if you have reason for doing this more complex fix.;;;","03/Jun/16 21:37;KevinGre;I see this issue is for interactive. I'll create a new one for the batch case.;;;","07/Jul/16 18:26;vanzin;Commit [d3d20316|https://github.com/cloudera/livy/commit/d3d203160436abb37b1136035d534f3d1c1a0533] by  purechoc <purechoc.en@...> in cloudera/livy:
{code}
LIVY-85. support pyspark with python3

support python3 using pyspark3

added kind type 'pyspark3'
`kind = [pyspark, pyspark3, spark, sparkr]`

Hue Notebook using like this
```
  [[interpreters]]
    [[[pyspark]]]
      name=PySpark
      interface=livy
    [[[pyspark3]]]
      name=PySpark3
      interface=livy
```

if user want to using python3, all cluster node need to be installed python3.

Closes #93
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy repl does not work on CDH,LIVY-84,13095722,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Fixed,,vanzin,gmcdonald,29/Feb/16 19:19,23/Mar/16 21:14,19/Dec/25 04:15,23/Mar/16 21:14,0.2,,,,,Core,,,,,,,,,,0,,,,,,"The repl currently runs its own web server, and it needs a newer version of the servlet API than that provided by Spark. Until we work on LIVY-46, we need a way to work around this so that the repl can run.

Currently, you get a {{java.lang.ClassNotFoundException: javax.servlet.http.HttpUpgradeHandler}} when trying to run the repl on CDH, at least.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 21:14:04 UTC 2016,,,,,,,,,,"0|i3iyyv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"29/Feb/16 19:40;vanzin;Here's another exception that happens in CDH because of other CDH dependencies in Spark's classpath:

{noformat}
16/02/29 11:39:16 ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NoSuchMethodError: org.eclipse.jetty.server.Server.setStopTimeout(J)V
java.lang.NoSuchMethodError: org.eclipse.jetty.server.Server.setStopTimeout(J)V
	at com.cloudera.livy.WebServer.<init>(WebServer.scala:40)
	at com.cloudera.livy.repl.Main$.main(Main.scala:68)
	at com.cloudera.livy.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:542)
{noformat}

(That's from a jetty 7.x jar that ends up in Spark's classpath through some CDH dependency.);;;","01/Mar/16 12:17;purechoc;I met that problem, 
i try ""livy/server/target/jars/javax.servlet-api-3.1.0.jar"" copy to ""livy/repl/target/jars/""
and then problem is fixed.
;;;","01/Mar/16 17:43;vanzin;bq. i try ""livy/server/target/jars/javax.servlet-api-3.1.0.jar"" copy to ""livy/repl/target/jars/""

That might work in the short term, but it's a bug waiting to happen. Classes that exist in the old servlet jar loaded by Spark's system class loader will still have the old interfaces, so the Livy code might try to call methods that don't exist and throw errors.;;;","03/Mar/16 21:42;quanghoc@yahoo.com;I tried to copy `javax.servlet-api-3.1.0.jar` and that method didn't work either. I used CDH QuickStart 5.5 with Spark 1.5.;;;","23/Mar/16 21:14;vanzin;This is not an issue anymore after LIVY-46 was merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release process and artifact deployment,LIVY-83,13095721,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Done,,hshreedharan,gmcdonald,24/Feb/16 18:57,02/Jun/16 02:13,19/Dec/25 04:15,02/Jun/16 02:13,0.2,,,,,Core,,,,,,,,,,0,,,,,,"Let's come up with a release process:
- Where do we plan to build the release?
- Keys to sign the release?
- Maven artifact deployment?
- Where do we host the final tarballs?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 02 02:13:15 UTC 2016,,,,,,,,,,"0|i3iyyn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"02/Jun/16 02:13;vanzin;We're storing archives and artifacts on Cloudera's servers; we can look at providing signatures and checksum files for the next release. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gateway timeout if client session takes long to start,LIVY-81,13095719,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,24/Feb/16 18:54,02/May/16 18:12,19/Dec/25 04:15,02/May/16 18:12,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"If the session takes a while to start, the client receives a gateway timeout from the Livy server:

{noformat}
Caused by: java.io.IOException: Gateway Timeout: {""msg"":""Gateway timeout""}
        at com.cloudera.livy.client.http.LivyConnection.sendRequest(LivyConnection.java:193)
        at com.cloudera.livy.client.http.LivyConnection.sendJSONRequest(LivyConnection.java:169)
        at com.cloudera.livy.client.http.LivyConnection.post(LivyConnection.java:145)
        at com.cloudera.livy.client.http.HttpClient.<init>(HttpClient.java:67)
{noformat}

It would be better, instead, if the start of a client session was monitored asynchronously, so that the code is less susceptible to these timeouts. If that's too much work, we need to at least figure out the source of this timeout, since it seems to come from the server and I didn't notice anything wrong in the server output.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 02 18:12:09 UTC 2016,,,,,,,,,,"0|i3iyy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Mar/16 01:54;GayathriMurali;[~MarceloVanzin] Is there way I can reproduce this bug on my setup? I understand its a time out issue,but is there a way I can simulate it? ;;;","04/Mar/16 02:51;vanzin;If you put a long sleep in the constructor for {{LocalClient}} that should simulate the problem.;;;","26/Apr/16 16:24;erickt;If I recall correctly, the source of this problem is that the yarn job is submitted synchronously in the session creation route handler. If one doesn't pre-upload the spark assembly jar into hdfs, it might take some time for the job to actually be submitted, which can cause some internal timeouts.

One simple way to avoid this problem is to instead start the yarn job in a background thread. This would allow the route handler to return a ""session is starting"" state rather than this confusing error message.;;;","02/May/16 18:12;vanzin;Commit [0e714535|https://github.com/cloudera/livy/commit/0e7145352ed52510977f2d90fd088b9f1f87ec3f] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-81. Monitor start of remote context asynchronously.

The remote context can take a while to start, and doing it
synchronously in the builder call makes it possible, even
likely, that you'll get ugly timeouts from the Livy server,
and cause contexts to stay around and be lost while still
consuming resources.

Instead, just start the submission process synchronously, but
wait for the context to connect back asynchronously. This makes
the client create call return much more quickly, but, as with
anything done asynchronously, requires a lot of code to be moved
around and placed in listeners, and to use promises instead of
actual instances in a few places.

The first tricky change was to support submitting jobs before
the context was up. This was done by chaining listeners so that
the actual submission only happens when the context's RPC channel
is actually created; for clients of the API, nothing changes.

The trickier part was monitoring context shut down; this mostly
affected unit tests, since you can't run two Spark contexts in
the same VM (so tests need to make sure the contexts are really
stopped when the stop call returns). That is now done by waiting
for the RPC socket to close, instead of explicitly waiting for
the context thread or process to go away.

As part of the above change I also removed Rpc.Listener since it
can be implemented by adding a listener to the channel's close
future.

Another unrelated thing I ran into while testing was that jobs
can jump directly from SENT to STARTED, and tests were expecting
them to go through QUEUED first. I fixed the test and wrote a
clarification in the API docs.

Closes #120
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade,LIVY-80,13095718,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,hshreedharan,gmcdonald,24/Feb/16 18:53,16/May/16 18:14,19/Dec/25 04:15,16/May/16 18:14,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"We must come up with an upgrade story. If upgrading from 0.2.0 to the next release - do all currently running jobs die and is the state lost unless the user specifically checkpoints RDDs and recovers by themselves? Without HA or Livy providing a way to do this, I think this is pretty much the only option. 

Whatever we go with, lets make sure we document it",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 18:14:59 UTC 2016,,,,,,,,,,"0|i3iyxz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/May/16 18:14;vanzin;Commit [e454969c|https://github.com/cloudera/livy/commit/e454969c137482806cf4468cbd3a5cdd3b21f59e] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-80. Add blurb in README about upgrading from 0.1.

Closes #134
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wire protocol finalization,LIVY-79,13095717,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Abandoned,,hshreedharan,gmcdonald,24/Feb/16 18:51,16/May/16 18:40,19/Dec/25 04:16,16/May/16 18:40,0.2,,,,,API,,,,,,,,,,0,,,,,,"We must finalize wire compatibility story. Basically apps with 0.2.0 client jars in the classpath must be able to talk to future server versions. If not, we must ensure we document that we don't plan to be wire-compatible.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 18:40:54 UTC 2016,,,,,,,,,,"0|i3iyxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/May/16 18:40;vanzin;I'm gonna close this since we've decided to look at this as part of subsequent releases; as in ""don't break compatibility with 0.2"" instead of taking any explicit action in 0.2 for that.

(API versioning, which is there, helps a little though.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
API stability,LIVY-78,13095716,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Abandoned,,hshreedharan,gmcdonald,24/Feb/16 18:49,16/May/16 18:41,19/Dec/25 04:16,16/May/16 18:41,0.2,,,,,API,,,,,,,,,,0,,,,,,"Let's finalize the API, such that we have a stable client API which the users can rely on between releases. My take is that the client API must be compatible between releases until the next major release. So apps written 0.2.0 clients should work with future client jars at the least.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 18:41:41 UTC 2016,,,,,,,,,,"0|i3iyxj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/May/16 18:41;vanzin;We've decided to not tackle this explicitly in 0.2; we can build compatibility tests later, and use the API versioning support to make breaking changes if needed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy 0.2.0 release,LIVY-77,13095715,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Done,,hshreedharan,gmcdonald,24/Feb/16 18:45,02/Jun/16 02:12,19/Dec/25 04:16,02/Jun/16 02:12,0.2,,,,,Core,,,,,,,,,,0,,,,,,This is an umbrella jira for 0.2.0 release. Let's create sub-tasks for this one which will help us figure out when we are ready to release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 02 02:12:18 UTC 2016,,,,,,,,,,"0|i3iyxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"02/Jun/16 02:12;vanzin;0.2 is out, so closing this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local client is broken in cluster mode,LIVY-76,13095714,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,24/Feb/16 01:51,26/Feb/16 07:33,19/Dec/25 04:16,26/Feb/16 06:37,0.2,,,,,Core,,,,,,,,,,0,,,,,,"A recent change of mine to explicit pass client configuration in a file to the RemoteDriver doesn't really work in cluster mode, since the file is never uploaded to the cluster. You'll see things like this:

{noformat}
16/02/23 17:46:10 ERROR yarn.ApplicationMaster: User class threw exception: java.io.FileNotFoundException: /tmp/livy.1778663115509384185.properties (No such file or directory)
java.io.FileNotFoundException: /tmp/livy.1778663115509384185.properties (No such file or directory)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 26 07:33:33 UTC 2016,,,,,,,,,,"0|i3iyx3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Feb/16 07:33;kostas;Fixed in: https://www.github.com/cloudera/livy/pull/82
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to start livy session in secure mode,LIVY-75,13095713,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,,zjffdu,gmcdonald,23/Feb/16 07:16,23/Mar/16 21:14,19/Dec/25 04:16,23/Mar/16 21:14,0.1,,,,,Core,,,,,,,,,,0,,,,,,"When creating livy session, get the following exception:

{noformat}
16/02/23 05:13:43 INFO impl.ContainerManagementProtocolProxy: Opening proxy : hostname:45454
16/02/23 05:13:46 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hostname:50214
16/02/23 05:13:46 INFO cluster.YarnClusterSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@ hostname:52598/user/Executor#-623778451]) with ID 1
16/02/23 05:13:46 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
16/02/23 05:13:46 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
16/02/23 05:13:46 INFO storage.BlockManagerMasterEndpoint: Registering block manager hostname:47108 with 265.4 MB RAM, BlockManagerId(1, hostname, 47108)
16/02/23 05:13:52 INFO repl.ScalatraBootstrap: Calling http://10.0.2.15:8998/sessions/1/callback...
16/02/23 05:13:52 ERROR repl.ScalatraBootstrap: Exception is thrown in notifyCallback()
java.util.concurrent.ExecutionException: dispatch.StatusCode: Unexpected response status: 401
	at com.ning.http.client.providers.netty.NettyResponseFuture.abort(NettyResponseFuture.java:342)
	at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.abort(NettyAsyncHttpProvider.java:1418)
	at com.ning.http.client.providers.netty.NettyAsyncHttpProvider$HttpProtocol.handle(NettyAsyncHttpProvider.java:2257)
	at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.messageReceived(NettyAsyncHttpProvider.java:1228)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:142)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.handler.codec.http.HttpClientCodec.handleUpstream(HttpClientCodec.java:92)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: dispatch.StatusCode: Unexpected response status: 401
	at dispatch.OkHandler$class.onStatusReceived(handlers.scala:37)
	at dispatch.OkFunctionHandler.onStatusReceived(handlers.scala:29)
	at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.updateStatusAndInterrupt(NettyAsyncHttpProvider.java:1560)
	at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.access$2600(NettyAsyncHttpProvider.java:161)
	at com.ning.http.client.providers.netty.NettyAsyncHttpProvider$HttpProtocol.handle(NettyAsyncHttpProvider.java:2212)
	at com.ning.http.client.providers.netty.NettyAsyncHttpProvider.messageReceived(NettyAsyncHttpProvider.java:1228)
	at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:142)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
	at org.jboss.netty.handler.codec.http.HttpClientCodec.handleUpstream(HttpClientCodec.java:92)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	... 3 more
16/02/23 05:13:52 INFO spark.SparkContext: Invoking stop() from shutdown hook
16/02/23 05:13:52 INFO server.ServerConnector: Stopped ServerConnector@6eb6add7{HTTP/1.1}{0.0.0.0:0}
16/02/23 05:13:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
16/02/23 05:13:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
16/02/23 05:13:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
16/02/23 05:13:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
16/02/23 05:13:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
16/02/23 05:13:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
16/02/23 05:13:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
16/02/23 05:13:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
16/02/23 05:13:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
16/02/23 05:13:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
16/02/23 05:13:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
16/02/23 05:13:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
16/02/23 05:13:52 INFO servlet.ScalatraListener: Destroying life cycle class: com.cloudera.livy.repl.ScalatraBootstrap
{noformat}",single node secure cluster.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 21:14:29 UTC 2016,,,,,,,,,,"0|i3iywv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Feb/16 05:14;zjffdu;This is auth issue (status code 401), livy-server is spnego enabled. And this issue happens when livy-repl try to connect to livy-server ;;;","17/Mar/16 16:56;vanzin;I have a PR open for this: https://github.com/cloudera/livy/pull/84

But I'm holding off on it since we're implementing LIVY-46, and that fixes the problem in a different way.;;;","23/Mar/16 01:58;zjffdu;I also have a PR to fix this issue (but in a different way). What's your plan for it ? Wait for LIVY-46 to be completed or commit this as a temporary solution ?

BTW, seems @ function doesn't work here.  ;;;","23/Mar/16 02:09;vanzin;I already closed my PR and posted the fix for LIVY-46 out. Once I check it in I'll just close all these related bugs.;;;","23/Mar/16 21:14;vanzin;This is not an issue anymore after LIVY-46 was merged.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit session count,LIVY-74,13095712,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,,purechoc,gmcdonald,22/Feb/16 01:08,04/May/16 01:15,19/Dec/25 04:16,03/May/16 19:37,0.2,,,,,Core,,,,,,,,,,0,,,,,,"now, create new session API is no limitation
if some user can make to many session, and using almost cluster resource.

how about limit session count per ""proxyUser""
{code:none}
livy.session.proxyusers.limit = 3
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 04 01:15:16 UTC 2016,,,,,,,,,,"0|i3iywn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/May/16 19:37;vanzin;I don't think this belongs in Livy. ""Number of sessions"" is not a good metric; you really need to control how much CPU and memory and other resources are allocated to each session. And that's YARN's job (or some other resource manager).;;;","04/May/16 00:33;purechoc;My intention was
{code:none}
# if set
livy.session.proxyusers.limit = 3

each users make only 3 session. 

user1 make 2 session using some resource, 
user2 make 3 session using some resource.
user3 make 3 session ...
...
no limit total session.

when user3 make 4th notebook, livy server return some message (""you can only make 3 notebook, try later... )

{code}
all session's resources management by yarn,  it is no problem.

Resource management of each user is hard to control. (is this yarn's scheduler can do this?)
- specified users resource limit;;;","04/May/16 01:15;vanzin;bq. yarn's scheduler can do this?

Yes, that's the whole purpose of using something like YARN.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow HTTP client to connect to existing session,LIVY-73,13095711,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,,vanzin,gmcdonald,17/Feb/16 18:23,26/Feb/16 07:40,19/Dec/25 04:16,26/Feb/16 07:02,0.2,,,,,API,,,,,,,,,,0,,,,,,The HTTP client currently only allows creating new sessions. It should also allow connecting to an existing session.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 26 07:40:44 UTC 2016,,,,,,,,,,"0|i3iywf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Feb/16 07:29;kostas;Hari, when was this fixed? I thought the existing client always creates a new session. Was there a PR that fixed this?;;;","26/Feb/16 07:38;kostas;Nevermind, found the PR.;;;","26/Feb/16 07:38;hshreedharan;Hmm, the client I used was supposed to put in the PR details on the jira. Here you are: https://github.com/cloudera/livy/pull/81;;;","26/Feb/16 07:40;kostas;We should probably setup something like what there is on the Spark jira. Databricks has built: https://github.com/databricks/spark-pr-dashboard;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Output value is always plain/text instead of JSON,LIVY-72,13095710,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Not A Bug,,milad.bourhani@gmail.com,gmcdonald,11/Feb/16 13:20,13/Feb/16 20:06,19/Dec/25 04:16,13/Feb/16 01:16,0.2,,,,,API,Core,Interpreter,,,,,,,,0,,,,,,"According to the docs at https://github.com/cloudera/hue/tree/master/apps/spark/java#statement-output, by specifying ""application/json"" as MIME I should get back a JSON output result:

bq. an object mapping a mime type to the result. If the mime type is application/json, the value will be a JSON value

However, executing this code I get back a ""plain/text"" result:

{noformat}
$ curl -H ""Content-Type: application/json"" -H ""Accept: application/json"" -X POST http://localhost:8998/sessions/ -d '{""kind"": ""spark""}'
{""id"":0,""state"":""starting"",""kind"":""spark"",""log"":[]}

$ curl -H ""Accept: application/json"" -X GET http://localhost:8998/sessions/0
{""id"":0,""state"":""idle"",""kind"":""spark"",""log"":[""16/02/11 14:15:08 INFO SparkUI: Started SparkUI at http://10.100.44.51:4040"",""16/02/11 14:15:08 INFO SparkContext: Added JAR file:/home/milad/projects/hue/apps/spark/java/livy-assembly/target/scala-2.10/livy-assembly-0.2.0-SNAPSHOT.jar at http://10.100.44.51:54600/jars/livy-assembly-0.2.0-SNAPSHOT.jar with timestamp 1455196508415"",""16/02/11 14:15:08 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set."",""16/02/11 14:15:08 INFO Executor: Starting executor ID driver on host localhost"",""16/02/11 14:15:08 INFO Executor: Using REPL class URI: http://10.100.44.51:17347"",""16/02/11 14:15:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42576."",""16/02/11 14:15:08 INFO NettyBlockTransferService: Server created on 42576"",""16/02/11 14:15:08 INFO BlockManagerMaster: Trying to register BlockManager"",""16/02/11 14:15:08 INFO BlockManagerMasterEndpoint: Registering block manager localhost:42576 with 530.0 MB RAM, BlockManagerId(driver, localhost, 42576)"",""16/02/11 14:15:08 INFO BlockManagerMaster: Registered BlockManager""]}
 
$ curl -H ""Content-Type: application/json"" -H ""Accept: application/json"" -X POST http://localhost:8998/sessions/0/statements -d '{""code"": ""41 + 1""}'
{""id"":0,""state"":""running"",""output"":null}

$ curl -H ""Accept: application/json"" -X GET http://localhost:8998/sessions/0/statements/0
{""id"":0,""state"":""available"",""output"":{""status"":""ok"",""execution_count"":0,""data"":{""text/plain"":""res0: Int = 42""}}}
{noformat}

I'd expect to see something different than this:
{noformat}
{""text/plain"":""res0: Int = 42""}
{noformat}

For example:
{noformat}
{""application/json"":""42""}
{noformat}","Git: master, rev. aa0742107df8715c3fa83e83b94c39259c896295",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Feb 13 20:06:44 UTC 2016,,,,,,,,,,"0|i3iyw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"13/Feb/16 01:16;vanzin;The response to your request is actually JSON.

The actual output of the command you executed - which is in the ""output"" property of the JSON payload representing the statement you're getting - may not be json, and that's why it has its own mime type in the response.;;;","13/Feb/16 12:24;milad.bourhani@gmail.com;Thank you for your response. The intention was really to get a parseable output value, instead of trying and fiddle with text {{""res0: Int = 42""}} (which is just the {{toString}} of the return value) -- is there any way to accomplish this?;;;","13/Feb/16 19:28;vanzin;Not with that endpoint. That endpoint is for implementing shells on top of Spark, and that's the output of the shell. There might be ""magics"" that do what you're looking for but I'm not familiar with that part of the code.;;;","13/Feb/16 20:06;milad.bourhani@gmail.com;Could you tell me what's the right endpoint for my purpose? Or point me a the right docs, thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix path handling for user files in Livy,LIVY-71,13095709,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,11/Feb/16 01:09,04/Apr/16 17:49,19/Dec/25 04:16,04/Apr/16 17:49,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"Livy has some spotty file path handling in a few places.

The first one is the weird ""absolute"" vs. ""relative"" paths, which I'm removing in LIVY-70. The result after that is that users have to always provide an absolute path, which is fine.

But that means users should not provide paths to local files, because there's no (easy) way for them to exist on the Livy server. Instead, one of two options should be taken:

- disallow paths without a proper scheme (e.g. ""http://..."" is fine, ""/foo"" is not)
- set up a default URI (could be ""fs.defaultFS"" read from Hadoop's configuration?) that is prepended to scheme-less paths.

The second problem is in handling uploaded files for client sessions. Those are written to ""livyHome"", which is actually a local path, not an HDFS path. We should have a separate configuration for where to write these files on HDFS. On top of that, we need to write these files with the proper permissions, respecting the session owner and proxy user when applicable (related to LIVY-52).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Apr 04 17:49:46 UTC 2016,,,,,,,,,,"0|i3iyvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/Feb/16 01:51;hshreedharan;livyHome can be on HDFS as well. We actually us the HDFS API to access anything in livyHome already: https://github.com/cloudera/livy/blob/master/spark/src/main/scala/com/cloudera/livy/spark/client/ClientSession.scala#L107

We only need to make sure that the fs.defaultFS in the config points to HDFS.;;;","11/Feb/16 01:54;hshreedharan;livyHome comes from SessionManager: https://github.com/cloudera/livy/blob/master/core/src/main/scala/com/cloudera/livy/sessions/SessionManager.scala#L45

which in turn gets it from LivyConf:  https://github.com/cloudera/livy/blob/master/core/src/main/scala/com/cloudera/livy/LivyConf.scala#L79

As long as this is a schema-less path, it resolves to HDFS. I left it that way to actually make it testable. 

I am open to changing this, but this seems to be a configurable method anyway.;;;","11/Feb/16 01:57;vanzin;I don't like to overload ""livy home"" with multiple meanings. Other parts of the code already refer to ""livy home"" as the place where the livy installation is. We should have a separate setting for the ""HDFS session cache path"".

Also, that part of the code would probably have to change anyway to work with multiple users.;;;","11/Feb/16 02:04;hshreedharan;I wasn't aware of livy home being used elsewhere. We can call it something else (all of livyHome stuff from livy conf to session manager was added for the upload - so we can change that).

We already have directories per session - we'd need to make sure ownership is taken care of etc. with multiple users.;;;","04/Apr/16 17:49;vanzin;Commit [410adf40|https://github.com/cloudera/livy/commit/410adf40d7098fa27846ea05536c0a94997299c1] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-71. Write session files to HDFS as the correct user.

When uploading files using `LivyClient.uploadJar` (or `uploadFile`),
the user files should not be readable by other users. Since we know
who the session is running as, impersonate that user before uploading
the file to HDFS. The file is stored in a session-specific staging
directory, which is deleted when the session is stopped.

The staging directory, by default, is under the user's home directory
(under the "".livy-sessions"" subdirectory); the location can be modified
in the config file. When setting a custom location, the directory needs
to be world-writable, so that all users can create directories, since
Livy cannot change ownership of directories it creates as its own user.

Two other minor changes are included (found during testing):

- bin/livy-server now adds config directories (Spark and Hadoop) do Livy's
  classpath, so that Livy can read the correct Hadoop configuration (and
  Spark's, if needed later).
- client-http/pom.xml now shades livy-client-common since we need to relocate
  kryo classes in that artifact too.

Closes #98
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplify session type hierarchy,LIVY-70,13095708,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,09/Feb/16 23:28,17/Feb/16 18:12,19/Dec/25 04:16,17/Feb/16 18:12,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,"The session hierarchy in Livy is currently over-engineered; too many session types, too many factories for those sessions, and a weird ""manager"" concept that creates yet another layer of indirection.

There are more session types in the code than actual session types you can create!

For an example of the headaches, see the patch for LIVY-68 where most of the code has nothing to do with the problem being solved, just deals with this type hierarchy.

We should simplify this so that more code is shared and there's a simpler hierarchy. This should make code maintenance cheaper and also increase our test coverage.

In the process, we should also merge the ""server"" and ""spark"" sub-modules since there's no point in them being separate. We could potentially also get rid of the ""yarn"" sub-module, merging its code with the server module if it's really needed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 17 18:12:31 UTC 2016,,,,,,,,,,"0|i3iyvr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/Feb/16 18:12;vanzin;Commit [10e86c77|https://github.com/cloudera/livy/commit/10e86c7776026db95b4c5b048835958537dfd986] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-70. Simplify Livy type and module hierarchy.

Fasten your seatbelts; lots going on here.

The first big change is the removal of the manager / factory / session
hierarchy. Instead, each session servlet knows how to create their own
sessions. There's no need for a ""manager"" concept anymore, and factories
are all replaced with a single method in each servlet.

This leads to the loss of two existing functions:

- the whole ""yarn"" module, which wasn't very helpful anyway. All it did
  was monitor the stdout of spark-submit; we should instead switch to
  Spark 1.6 and use the enhanced launcher API for that.

- the Spark config whitelist; that was touted as a security feature, but
  there's really no security if you're running use code as the same
  user as the Livy server. So it didn't really serve any real purpose.

The second big change is the removal of the ""spark"" module; well, it isn't
really removed, but actually merged with the ""server"" module which was its
only consumer. This makes development easier since you just need to rebuild
one module when you touch those files. For the same reason, backend-specific
session classes were moved from the ""core"" module to the ""server"" module.

There is still more code in ""core"" that could potentially be cleaned up or
moved, but that's left for the future.

Third but not last, the change removes the concept of absolute or relative
paths, and filesystem root. Clients should always provide paths as they would
for a ""spark-submit"" invocation; the difference being that local paths
wouldn't really work. There are probably extra enhancements to be made in
this area, but those wouldn't be related to this cleanup.

Finally, some intermediary classes (like ""InteractiveWebSession"" and
""BaseInteractiveSessionSpec"") were merged with their (single) concrete
implementations. That also allowed the removal of the livy-core test
jar.

Closes #64
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy cannot execute any interactive statements,LIVY-69,13095707,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,vanzin,tc0312,gmcdonald,09/Feb/16 05:10,10/Feb/16 23:30,19/Dec/25 04:16,10/Feb/16 23:30,0.2,,,,,Core,,,,,,,,,,0,,,,,,"<livy-server>/sessions/0/statements is not returning valid response. Instead of a list of statements, it returns: 
{noformat}
Map(total_statements -> 0, statements -> SeqViewSM(...))
{noformat}

The handler in InteractiveSessionServlet does not use jpost/jget:

{code:java}
  get(""/:id/statements"") {
    withSession { session =>
      val from = params.get(""from"").map(_.toInt).getOrElse(0)
      val size = params.get(""size"").map(_.toInt).getOrElse(session.statements.length)

      Map(
        ""total_statements"" -> session.statements.length,
        ""statements"" -> session.statements.view(from, from + size).map(statementView)
      )
    }
  }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 09 18:01:36 UTC 2016,,,,,,,,,,"0|i3iyvj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Feb/16 18:01;vanzin;A fix for LIVY-64 would probably fix this too. Let me look at that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add SPNEGO auth to server, perform auth checks.",LIVY-68,13095706,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,08/Feb/16 19:54,08/Feb/16 22:27,19/Dec/25 04:16,08/Feb/16 22:27,0.2,,,,,Core,,,,,,,,,,0,,,,,,Add SPNEGO auth and authorization checks for sessions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2016-02-08 19:54:43.0,,,,,,,,,,"0|i3iyvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add impersonation support to client sessions,LIVY-67,13095705,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,08/Feb/16 19:53,24/Feb/16 21:58,19/Dec/25 04:16,24/Feb/16 21:58,0.2,,,0.2,,Core,,,,,,,,,,2,,,,,,"Client session currently don't support impersonation. We need to add that.

On top of it, current impersonation support in batch / interactive sessions need to be fixed up accordingly. We need a whitelist of users who can impersonate other users, similar to Hadoop's. This is so services that use Livy to start sessions (e.g. something like Hue) can connect to Livy as their user and be able to impersonate the actual user making the request. If the user is not on that whitelist, they shouldn't be allowed to impersonate anyone.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 24 21:58:22 UTC 2016,,,,,,,,,,"0|i3iyv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Feb/16 21:58;vanzin;Commit [8287f4c7|https://github.com/cloudera/livy/commit/8287f4c7156b67edad0d3fa4f3afc1059b0baea6] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-67. Support impersonation in client sessions.

There are a few different things going on here to reach that goal:

- client-local now supports spark submit's --proxy-user command line
  argument, and a new configuration to define the user to be proxied.

- the Livy server now has a list of ""super users"" who can do anything;
  relevant to this change, super users can impersonate anyone.

- SessionServlet now has a helper method to figure out who should be
  impersonated and check for appropriate permissions.

- The three session servlets now use that method; the semantics are:

  * if there is no user information (i.e. auth is off), users are
    allowed to impersonate anyone
  * when users are authenticated, normal users can only impersonate
    themselves; even if they don't specify a proxy user, the server
    will impersonate the calling user.
  * super users can impersonate anyone; if a specific proxy user is not
    requested, the session will run as the super user.

The rest of the changes are mostly test code, and a small tweak to
JsonServlet to be able to serialize raw String into a JSON object
until LIVY-54 is fixed.

Closes #74
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPNEGO support in client-http,LIVY-66,13095704,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,08/Feb/16 19:48,18/Apr/17 18:29,19/Dec/25 04:16,26/Feb/16 22:49,0.2,,,,,API,,,,,,,,,,0,,,,,,Need to add SPNEGO auth support to client-http. See LIVY-52.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Apr 18 18:29:13 UTC 2017,,,,,,,,,,"0|i3iyuv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"10/Feb/17 22:39;krish.dey;SPNEGO is not working for multiple domains. Though it works for default one.
For example, the following rule which is in our Hadoop auth_to_local mappings and is working, doesnt work for livy.
Though in default domain which is NAEAST.AD.KRISHCOM.COM is fine. But for other domain it says no rule found.

livy.server.auth.kerberos.name_rules = RULE:[1:$1@$0](.@\QNAEAST.AD.KRISHCOM.COM\E$)s/@\QNAEAST.AD.KRISHCOM.COM\E$///L\nRULE:[1:$1@$0](.@\QEMEA.AD.KRISHCOM.COM\E$)s/@\QEMEA.AD.KRISHCOM.COM\E$///L\nRULE:[1:$1@$0](.@\QASIAPAC.AD.KRISHCOM.COM\E$)s/@\QASIAPAC.AD.KRISHCOM.COM\E$///L\nRULE:[1:$1@$0](.@\QEMEA.AD.KRISHCOM.COM\E$)s/@\QEMEA.AD.KRISHCOM.COM\E$//\nRULE:[2:$1@$0](.@\QEMEA.AD.KRISHCOM.COM\E$)s/@\QEMEA.AD.KRISHCOM.COM\E$//\nRULE:[1:$1@$0](.@\QNAEAST.AD.KRISHCOM.COM\E$)s/@\QNAEAST.AD.KRISHCOM.COM\E$//\nRULE:[2:$1@$0](.@\QNAEAST.AD.KRISHCOM.COM\E$)s/@\QNAEAST.AD.KRISHCOM.COM\E$//\nRULE:[1:$1@$0](.@\QASIAPAC.AD.KRISHCOM.COM\E$)s/@\QASIAPAC.AD.KRISHCOM.COM\E$//\nRULE:[2:$1@$0](.*@\QASIAPAC.AD.KRISHCOM.COM\E$)s/@\QASIAPAC.AD.KRISHCOM.COM\E$//\nDEFAULT
;;;","18/Apr/17 18:29;mikesk-livy-import;I can confirm this on my end too. Please reopen this ticket!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Clean up server dependencies, deployed packages, package names",LIVY-65,13095703,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,vanzin,vanzin,gmcdonald,05/Feb/16 03:27,16/Feb/16 21:43,19/Dec/25 04:16,16/Feb/16 21:43,0.2,,,0.2,,API,,,,,,,,,,0,,,,,,"There are some problems with the current package structure in Livy that we should clean up. I'll pile all of them in the same task since it's all modifications to the POM files, so better to do them all at once.

- Livy should not have Spark jars in the final package

Livy server should not depend on any Spark classes to run (with the eventual exception of the ""spark-launcher"" library). It doesn't run Spark code, it just starts Spark applications, which doesn't require anything Spark in the classpath.

The modules that actually depend on Spark (like the REPL or client-local) are Spark applications, so they are run with spark-submit (or the equivalent), and so don't need Spark jars either.

- Only ""public"" artifacts should be deployed.

Deploy here means ""upload to maven central (or equivalent repository) when the time arrives"". Only a handful of packages are user-facing: things like client-http, client-local (useful for testing) and, potentially, repl, although I'm not sold on that one.

All others should not be uploaded anywhere. They're packaged in the final Livy server zip and that's all that's needed.

- The Scala version for package names is not needed.

The version of Scala the server is compiled against shouldn't matter, since no one is linking against the server. client-http is pure java. client-local depends on Spark, but it depends on Spark's Java bindings, so the version of Scala there hopefully won't matter. So we should be able to remove ""_2.10"" from all artifact names.

I know this might be a little painful if you got used to typing it, but I'm getting kinda tired of typing ""_2.10"" all the time. :-)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 16 21:43:54 UTC 2016,,,,,,,,,,"0|i3iyun:",9223372036854775807,,,,,,,,,,,,,,,,,,,"16/Feb/16 21:43;vanzin;Commit [52e2cf82|https://github.com/cloudera/livy/commit/52e2cf8285db621457d833930cc5b3a7d8f33656] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-65. Dependency and package cleanup.

A few changes to the build and packaging:

- Drop the ""_2.10"" prefix from Livy artifacts. The version of Scala does
  not matter for Livy; the server can talk to backends running whatever
  version of Scala (or no Scala at all). So it doesn't make sense to
  expose that.

- Remove Spark classes that leaked through the public API paths; this makes
  sure clients can run without Spark classes in the classpath.

- Clean up the module poms and assembly descriptor a bit.

- Make deployment to a maven repo ""opt in"", so that we control artifacts that
  should be published.

Note that Scala dependencies are still stuck at 2.10. This means Livy cannot
currently run (or even compile) its repl against a Spark that's using 2.11.
That's a separate future enhancement, though.

Closes #61
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make JsonServlet less awkward for non-JSON args,LIVY-64,13095702,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,vanzin,vanzin,gmcdonald,05/Feb/16 00:31,10/Feb/16 23:30,19/Dec/25 04:16,10/Feb/16 23:30,0.2,,,,,Core,,,,,,,,,,0,,,,,,"JsonServlet currently requires some weird syntax for handlers that do not take JSON input:

{code}
  jpost[Unit](...) { _ =>
  }
{code}

We should instead change things so that you could use the standard Scalatra {{post}} in that case, and automatically serialize the result if the content type is JSON.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 05 04:04:42 UTC 2016,,,,,,,,,,"0|i3iyuf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Feb/16 04:04;vanzin;BTW, this would probably make {{jget}} and {{jdelete}} obsolete, too.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to create more than 4 sessions in Livy,LIVY-63,13095701,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Cannot Reproduce,,boricuastock,gmcdonald,04/Feb/16 19:41,09/May/16 18:16,19/Dec/25 04:16,09/May/16 18:16,0.2,,,,,Core,,,,,,,,,,0,,,,,,"After I create 4 sessions in spark using livy, I start to receive a 504 timeout after submitting subsequent sessions.  You can see in the logs that other attempts to create sessions don't seem to show in the Livy logs at all.  According to my dashboards, I still have 20 GB of yarn memory left.  I am creating each session with 1 Executor, 1 Process per Executor and 800M of memory.  I have 12 worker cores available across the cluster.

I tried to find the Yarn Application logs but failed to actually get them.  The YARN UI does not show them.  If these are needed, please assist me in finding them and I will be happy to send them over.  I can say that I can see the 4 applications running in Yarn but no other Application start attempts are shown in yarn.  

I also do not know the version of livy.  I put .2 but I can't find anything that shows me the version.  This is installed as part of the HDInsight cluster and so I do not know the version. Any guidance here would be helpful.  

Below is the output of a command run with curl

""Content-Type: application/json""  -X POST -d '{'kind': 'pyspark'}' https://azurehdinsight.net/livy/sessions
* Hostname was NOT found in DNS cache
*   Trying 
* Connected to azurehdinsight.net () port 443 (#0)
* successfully set certificate verify locations:
*   CAfile: none
  CApath: /etc/ssl/certs
* SSLv3, TLS handshake, Client hello (1):
* SSLv3, TLS handshake, Server hello (2):
* SSLv3, TLS handshake, CERT (11):
* SSLv3, TLS handshake, Server key exchange (12):
* SSLv3, TLS handshake, Server finished (14):
* SSLv3, TLS handshake, Client key exchange (16):
* SSLv3, TLS change cipher, Client hello (1):
* SSLv3, TLS handshake, Finished (20):
* SSLv3, TLS change cipher, Client hello (1):
* SSLv3, TLS handshake, Finished (20):
* SSL connection using ECDHE-RSA-AES256-SHA384
* Server certificate:
*        subject: CN=*.azurehdinsight.net
*        start date: 2014-11-12 20:55:19 GMT
*        expire date: 2016-11-11 20:55:19 GMT
*        issuer: C=US; ST=Washington; L=Redmond; O=Microsoft Corporation; OU=Microsoft IT; CN=Microsoft IT SSL SHA1
*        SSL certificate verify ok.
* Server auth using Basic 
> POST /livy/sessions HTTP/1.1
> Authorization: Basic c2NvYmllOlBhcml2ZWRhMLKJ=
> User-Agent: curl/7.35.0
> Host: azurehdinsight.net
> Accept: */*
> Content-Type: application/json
> Content-Length: 15
>
* upload completely sent off: 15 out of 15 bytes
< HTTP/1.1 504 Gateway Timeout
< Content-Length: 15
< Content-Type: application/json; charset=UTF-8
* Server Microsoft-IIS/8.5 is not blacklisted
< Server: Microsoft-IIS/8.5
< X-Powered-By: ARR/2.5
< X-Powered-By: ASP.NET
< Date: Thu, 04 Feb 2016 19:33:09 GMT
<
* Connection #0 to host azurehdinsight.net left intact
Gateway timeout","This is running on a Microsoft HDInsight cluster.  The cluster consists of the two head nodes and 3 worker nodes.  

Spark - 1.5.2
OS - ubuntu14 (x86_64)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 18:16:45 UTC 2016,,,,,,,,,,"0|i3iyu7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Feb/16 20:45;boricuastock;Here is the log with some thread dumps;;;","09/May/16 18:16;vanzin;I can't reproduce this with current master. I can bring up any number of simultaneous sessions just fine. If you can still reproduce this, might be worth it to bring it up with MS, since it might be particular to their service.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayStoreException using custom class with SparkContext,LIVY-62,13095700,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,milad.bourhani@gmail.com,gmcdonald,04/Feb/16 10:33,07/May/16 00:18,19/Dec/25 04:16,07/May/16 00:18,0.2,,,,,API,Core,Interpreter,,,,,,,,0,,,,,,"The following code defines a class and then uses the with the Spark Context:
{code}
class MyClass( val myInt: Int ) extends Serializable { override def toString: String = myInt.toString }
val myRes = sc.parallelize(List(1,2,3)).map((n) => new MyClass(n)).collect()
myRes(1).myInt
{code}
The following error appears:
{noformat}
org.apache.spark.SparkDriverExecutionException: Execution error
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:909)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:908)
	at $iwC$$iwC.<init>(<console>:9)
	at $iwC.<init>(<console>:18)
	at <init>(<console>:20)
	at .<init>(<console>:24)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at com.cloudera.hue.livy.repl.scala.SparkInterpreter$$anonfun$executeLine$1.apply(SparkInterpreter.scala:242)
	at com.cloudera.hue.livy.repl.scala.SparkInterpreter$$anonfun$executeLine$1.apply(SparkInterpreter.scala:242)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.Console$.withOut(Console.scala:126)
	at com.cloudera.hue.livy.repl.scala.SparkInterpreter.executeLine(SparkInterpreter.scala:241)
	at com.cloudera.hue.livy.repl.scala.SparkInterpreter.executeLines(SparkInterpreter.scala:216)
	at com.cloudera.hue.livy.repl.scala.SparkInterpreter.execute(SparkInterpreter.scala:79)
	at com.cloudera.hue.livy.repl.Session.com$cloudera$hue$livy$repl$Session$$executeCode(Session.scala:96)
	at com.cloudera.hue.livy.repl.Session$$anonfun$3.apply(Session.scala:71)
	at com.cloudera.hue.livy.repl.Session$$anonfun$3.apply(Session.scala:71)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayStoreException: [L$iwC$$iwC$MyClass;
	at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1837)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1837)
	at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1020)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{noformat}
I've executed the code via curl from the command line:
{noformat}
cd /home/milad/projects/hue/apps/spark/java
export SPARK_HOME=/home/milad/spark-1.5.2-bin-hadoop2.6
./bin/livy-server
curl -H ""Content-Type: application/json"" -X POST http://localhost:8998/sessions/0/statements -d '{""code"": ""class MyClass( val myInt: Int ) extends Serializable { override def toString: String = myInt.toString }; val myRes = sc.parallelize(List(1,2,3)).map((n) => new MyClass(n)).collect(); myRes(1).myInt""}'

curl -X GET http://localhost:8998/sessions/0/statements/0

{""id"":0,""state"":""available"",""output"":{""status"":""error"",""execution_count"":0,""ename"":""Error"",""evalue"":""org.apache.spark.SparkDriverExecutionException: Execution error\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1024)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1837)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1850)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1921)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:909)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:908)\n\tat $iwC$$iwC.<init>(<console>:9)\n\tat $iwC.<init>(<console>:18)\n\tat <init>(<console>:20)\n\tat .<init>(<console>:24)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat com.cloudera.hue.livy.repl.scala.SparkInterpreter$$anonfun$executeLine$1.apply(SparkInterpreter.scala:242)\n\tat com.cloudera.hue.livy.repl.scala.SparkInterpreter$$anonfun$executeLine$1.apply(SparkInterpreter.scala:242)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat scala.Console$.withOut(Console.scala:126)\n\tat com.cloudera.hue.livy.repl.scala.SparkInterpreter.executeLine(SparkInterpreter.scala:241)\n\tat com.cloudera.hue.livy.repl.scala.SparkInterpreter.executeLines(SparkInterpreter.scala:216)\n\tat com.cloudera.hue.livy.repl.scala.SparkInterpreter.execute(SparkInterpreter.scala:79)\n\tat com.cloudera.hue.livy.repl.Session.com$cloudera$hue$livy$repl$Session$$executeCode(Session.scala:96)\n\tat com.cloudera.hue.livy.repl.Session$$anonfun$3.apply(Session.scala:71)\n\tat com.cloudera.hue.livy.repl.Session$$anonfun$3.apply(Session.scala:71)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ArrayStoreException: [L$iwC$$iwC$MyClass;\n\tat scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1837)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1837)\n\tat org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1020)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)"",""traceback"":[]}}
{noformat}","Git: master, revision 1fa1d37d9b32361e2fa6389da294e7445ce065d4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat May 07 00:18:04 UTC 2016,,,,,,,,,,"0|i3iytz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"07/May/16 00:18;vanzin;This is a weird issue, but it doesn't seem to be a Livy problem.

Trying your test in the spark-shell, I get this:

- running everything in the same statement, as in your ""curl"" example, works in local mode; it fails with a super-weird error in yarn mode.
- running each statement separately works in both modes,
- running each statement separately through Livy also works fine
- running everything as one statement through Livy seems to cause Spark to never finish the job

From the above it seems to me like this is some real weirdness in the Spark shell. Feel free to file a Spark bug for that if you really want it investigated.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup LivyConf,LIVY-61,13095699,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,vanzin,vanzin,gmcdonald,02/Feb/16 23:51,05/Feb/16 19:50,19/Dec/25 04:16,05/Feb/16 19:50,0.2,,,0.2,,Core,,,,,,,,,,0,,,,,,While working on a separate thing I kinda started getting annoyed with LivyConf and how it replicates a lot of existing code from other places. We can merge some of that code and clean up its usage a big.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 05 19:50:34 UTC 2016,,,,,,,,,,"0|i3iytr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Feb/16 19:50;vanzin;Commit [7f16ef6c|https://github.com/cloudera/livy/commit/7f16ef6c4e50d8812a5fb8c720ad672fcd7eae6a] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-61. Simplify LivyConf by extending ClientConf.

That reuses a lot of the type conversion logic; also, declare
some constants in the form of ""ConfEntry"" objects so that it's
easier to spot their names and default values.

I also changed the server's Main to use a listener instead of
a ScalaBootstrap since that allows using the same LivyConf instance,
instead of having both places initialize the conf object in
different ways.

Finally, I changed the default value for the ""environment"" config
to ""production"", since well, the default should not be ""development"".
The unit tests will set it to ""development"" explicitly; although it's
all kinda moot since that's not used anywhere.

Closes #52
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Json deserialization in JsonServlet ignores default values,LIVY-60,13095698,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Fixed,vanzin,tc0312,gmcdonald,02/Feb/16 22:16,04/Feb/16 22:38,19/Dec/25 04:16,04/Feb/16 22:38,0.2,,,,,API,,,,,,,,,,0,,,,,,"Json deserialization in JsonServlet ignores default value defined in case class definitions.
Instead of using the default value, Jackson uses null.

Default values in following classes are ignored and substitute by null.

{code:none}
case class CreateInteractiveRequest(
    kind: Kind,
    proxyUser: Option[String] = None,
    jars: List[String] = List(),
    pyFiles: List[String] = List(),
    files: List[String] = List(),
    driverMemory: Option[String] = None,
    driverCores: Option[Int] = None,
    executorMemory: Option[String] = None,
    executorCores: Option[Int] = None,
    numExecutors: Option[Int] = None,
    archives: List[String] = List(),
    queue: Option[String] = None,
    name: Option[String] = None,
    conf: Map[String, String] = Map())

case class CreateBatchRequest(
    file: String,
    proxyUser: Option[String] = None,
    args: List[String] = List(),
    className: Option[String] = None,
    jars: List[String] = List(),
    pyFiles: List[String] = List(),
    files: List[String] = List(),
    driverMemory: Option[String] = None,
    driverCores: Option[Int] = None,
    executorMemory: Option[String] = None,
    executorCores: Option[Int] = None,
    numExecutors: Option[Int] = None,
    archives: List[String] = List(),
    queue: Option[String] = None,
    name: Option[String] = None,
    conf: Map[String, String] = Map())
{code}

Right now both batch and interactive modes are broken because of NullPointerException unless users specify every attribute in the create requests.

It's a known bug in the library: https://github.com/FasterXML/jackson-module-scala/issues/87",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 04 05:01:46 UTC 2016,,,,,,,,,,"0|i3iytj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"04/Feb/16 02:48;vanzin;Mildly related to LIVY-28 (which suggests removing all those fields from the request, something I approve of). But I'll take a look at what to do in the meantime.;;;","04/Feb/16 02:50;vanzin;Also, interesting that all unit tests pass. Kinda makes you wonder about how much those tests are covering, if those backends are not working.;;;","04/Feb/16 05:01;kostas;Alex expressed that this is a serious issue for them. Lets make sure we fix this for 0.2 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove metrics from public client API,LIVY-59,13095697,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,01/Feb/16 23:39,23/Feb/16 19:46,19/Dec/25 04:16,23/Feb/16 19:46,0.2,,,0.2,,API,,,,,,,,,,0,,,,,,"-Currently the HTTP client does not get job metrics from the remote Spark application. We need to implement that since it's part of the public API.-

Let's remove metrics from the public API; we can revisit adding metrics in the future if there are requests for it, but for now, let's leave it for applications to implement metrics gathering by themselves.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 23 19:46:20 UTC 2016,,,,,,,,,,"0|i3iytb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Feb/16 17:34;vanzin;I'm actually leaning towards removing metrics from the API altogether. People who care about it can install listeners on the Spark side and write small jobs to get metrics; that also shields us from changes in Spark's metrics.;;;","23/Feb/16 19:46;vanzin;Commit [00819210|https://github.com/cloudera/livy/commit/00819210e72d70dff1b7a8b757a9967276ac1ceb] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-59. Remove MetricsCollection from the public API.

This was inherited from Hive, and it's used there; but let's wait
until there's an actual need in Livy for this, especially since
the API is highly coupled with APIs that have changed in Spark,
and applications can still write code themselves to gather metrics
if they need to.

Closes #78
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allows Livy server to reconnect to running RSC instances,LIVY-58,13095696,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,kostas,gmcdonald,30/Jan/16 07:12,18/Aug/17 18:07,19/Dec/25 04:16,12/Mar/16 01:00,0.1,,,0.2,,Core,,,,,,,,,,1,,,,,,"To make sure applications survive livy server restarts, livy should be able to reconnect to running RSC instances (and RSC instances should not die when livy disconnects). Client sessions should also be preserved across restarts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sat Mar 12 01:00:34 UTC 2016,,,,,,,,,,"0|i3iyt3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Mar/16 19:56;vanzin;After discussion via e-mail, the easiest path forward seems to be to invert the connection direction: Livy connects to the RSC instead of the current RSC connects to Livy. This allows the Livy server to change addresses after a restart without needing to figure out a way to tell the running RSC instances about that.

This would work similarly to how the REPL currently works: Livy starts the RSC, RSC starts a listening socket, and connects back to Livy to register the address of the listening socket.

My initial inclination is to use a similar approach to the REPL, and use an HTTP route for the callback, instead of having a separate listen socket talking the RSC protocol. This would mean:

- the RSC would need an HTTP client inside it
- the Livy endpoint would need to provide different authentication to know which RSC is connecting back (e.g. some hacked up version of basic auth)

For the initial implementation we can keep the two sockets approach, to keep things simple, and leave the optimization to get rid of the second socket for later.;;;","12/Mar/16 01:00;hshreedharan;I committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client configuration,LIVY-56,13095694,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,,hshreedharan,gmcdonald,30/Jan/16 04:49,02/Feb/16 00:20,19/Dec/25 04:16,02/Feb/16 00:20,0.2,,,,,API,,,,,,,,,,0,,,,,,We should consider creating the concept of a client configuration. This would be useful to easily pickup default configs for the cluster basics (address/port/security configuration etc).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 02 00:20:21 UTC 2016,,,,,,,,,,"0|i3iysn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"02/Feb/16 00:20;vanzin;This is mostly covered by https://github.com/cloudera/livy/commit/35b96d09. If there's anything else we should create a new task.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement missing features in HTTP client,LIVY-53,13095691,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,vanzin,gmcdonald,29/Jan/16 19:37,05/Feb/16 00:12,19/Dec/25 04:16,05/Feb/16 00:12,0.2,,,0.2,,API,,,,,,,,,,0,,,,,,"From the code:

{quote}
What is currently missing:
- cancel jobs
- monitoring of spark job IDs launched by jobs
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 05 00:12:41 UTC 2016,,,,,,,,,,"0|i3iyrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"05/Feb/16 00:12;vanzin;Commit [315f7dee|https://github.com/cloudera/livy/commit/315f7dee0b26a5dc2e38d82bc35019b2ffdbcf66] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-53. Implement job cancellation, Spark job monitoring in http client.

These two features were missing from the HTTP client. Some changes
were made to the local client implementation to send diffs of the
Spark jobs being monitored back to the client; previously, only the
first job started by Spark would be reported.

Closes #48
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Security support: SPNEGO/Impersonation,LIVY-52,13095690,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Done,,hshreedharan,gmcdonald,28/Jan/16 23:49,18/Aug/17 18:10,19/Dec/25 04:16,09/May/16 18:18,0.1,,,,,Core,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Feb 10 22:38:51 UTC 2017,,,,,,,,,,"0|i3iyrr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"31/Jan/16 00:41;Tagar;This might be implemented just by passing spark-submit --proxy-user parameter?
Thank you.;;;","15/Feb/16 20:05;Tagar;This one might be relevant as well - [LIVY-49|https://issues.cloudera.org/projects/LIVY/issues/LIVY-49] ;;;","09/May/16 18:18;vanzin;All sub-tasks have been implemented, so closing this one.;;;","10/Feb/17 22:38;krish.dey;SPNEGO is not working for multiple domains. Though it works for default one.
For example, the following rule which is in our Hadoop auth_to_local mappings and is working, doesnt work for livy. 
Though in default domain which is NAEAST.AD.KRISHCOM.COM is fine. But for other domain it says no rule found.

livy.server.auth.kerberos.name_rules = RULE:[1:$1@$0](.*@\QNAEAST.AD.KRISHCOM.COM\E$)s/@\QNAEAST.AD.KRISHCOM.COM\E$///L\nRULE:[1:$1@$0](.*@\QEMEA.AD.KRISHCOM.COM\E$)s/@\QEMEA.AD.KRISHCOM.COM\E$///L\nRULE:[1:$1@$0](.*@\QASIAPAC.AD.KRISHCOM.COM\E$)s/@\QASIAPAC.AD.KRISHCOM.COM\E$///L\nRULE:[1:$1@$0](.*@\QEMEA.AD.KRISHCOM.COM\E$)s/@\QEMEA.AD.KRISHCOM.COM\E$//\nRULE:[2:$1@$0](.*@\QEMEA.AD.KRISHCOM.COM\E$)s/@\QEMEA.AD.KRISHCOM.COM\E$//\nRULE:[1:$1@$0](.*@\QNAEAST.AD.KRISHCOM.COM\E$)s/@\QNAEAST.AD.KRISHCOM.COM\E$//\nRULE:[2:$1@$0](.*@\QNAEAST.AD.KRISHCOM.COM\E$)s/@\QNAEAST.AD.KRISHCOM.COM\E$//\nRULE:[1:$1@$0](.*@\QASIAPAC.AD.KRISHCOM.COM\E$)s/@\QASIAPAC.AD.KRISHCOM.COM\E$//\nRULE:[2:$1@$0](.*@\QASIAPAC.AD.KRISHCOM.COM\E$)s/@\QASIAPAC.AD.KRISHCOM.COM\E$//\nDEFAULT

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a new configuration for livy.repl to always include additional files for interactive sessions,LIVY-51,13095689,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,tc0312,gmcdonald,28/Jan/16 21:35,09/May/16 18:19,19/Dec/25 04:16,09/May/16 18:19,0.2,,,,,Interpreter,,,,,,,,,,0,,,,,,"On our cluster, we want to include a hive-site.xml for every Livy interactive session. Right now, we have to specify the hive-site.xml in every interactive request, which is less than ideal.

We would like to add a configuration to automatically include a list of files in livy.repl. It would look like:

{noformat}
# Additional files livy will include when starting interactive sessions.
# It is equivalent to setting ""--files"" on every session request.
## livy.repl.additional.files = hdfs://localhost:8020/hive-site.xml
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 18:19:34 UTC 2016,,,,,,,,,,"0|i3iyrj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"22/Mar/16 05:35;zjffdu;I think now you can put any spark configuration in spark-defaults.xml which will be picked up when launching spark application.;;;","01/Apr/16 02:23;tc0312;Do you mean by using spark.\*.extraClassPath & spark.\*.extraLibraryPath?;;;","01/Apr/16 02:53;zjffdu;Yes, I think it would work. ;;;","09/May/16 18:19;vanzin;As Jeff mentions, Livy will use whatever Spark configuration you provide to it (either {{$SPARK_HOME/conf}} or {{$SPARK_CONF_DIR}}), so this functionality already exists.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy command does't detect external dependency jars,LIVY-50,13095688,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,,kostas,gmcdonald,28/Jan/16 18:23,09/May/16 18:21,19/Dec/25 04:16,09/May/16 18:21,0.1,,,,,Core,,,,,,,,,,0,,,,,,"Filed by: https://github.com/skillful-tech
https://github.com/cloudera/livy/issues/29

Couldn't submit the spark job by Livy because the Livy command didn't reach to external jars to include it on spark job, I run this command to submit the jar:

{code}
 curl -X POST --data '{""jars"":""/home/smartappi/smart/colt.jar, /home/smartappi/smart/geocalc-0.5.1.jar"", ""className"":""org.tssg.smartappi.analytics.mlr.alg.MLRSupplierPredicter"", ""file"": ""/home/smartappi/smart/smartappi.jar""}' -H ""Content-Type: application/json"" localhost:8998/batches
{code}

but I got this error:

{code}
{""id"":91,""state"":""error"",""log"":[""\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:639)"",""\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)"",""Caused by: java.lang.ClassNotFoundException: cern.colt.matrix.DoubleMatrix1D java.net.URLClassLoader.findClass(URLClassLoader.java:381)
java.lang.ClassLoader.loadClass(ClassLoader.java:424)
java.lang.ClassLoader.loadClass(ClassLoader.java:357)"",""\t... 8 more""]},
{code}

Note:when I submit the jar on spark directly it's working fine",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 18:21:17 UTC 2016,,,,,,,,,,"0|i3iyrb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/May/16 18:21;vanzin;The request references local jars that Livy cannot read. Without a smart client that can upload jars to Livy, the only solution is to place these jars or HDFS (or some other shared location that Spark can access).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark + Sentry + Kerberos don't add up?,LIVY-49,13095687,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Workaround,,kostas,gmcdonald,28/Jan/16 18:19,24/Dec/19 14:23,19/Dec/25 04:16,24/Feb/16 23:36,0.1,,,,,Core,,,,,,,,,,1,,,,,,"File by: https://github.com/Tagar
https://github.com/cloudera/livy/issues/36

Getting following error stack

{code}
The Spark session could not be created in the cluster: 
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671) 
    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:160) 
    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205) 
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120) 
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) ) 
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:466) 
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:234) 
    at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74) 
    ... 35 more
{code}

My understanding that hive.server2.enable.impersonation and hive.server2.enable.doAs should be enabled to make UserGroupInformation.doAs() work?

When I try to enable these parameters, Cloudera Manager shows error:

Hive Impersonation is enabled for Hive Server2 role 'HiveServer2 (hostname)'. 
Hive Impersonation should be disabled to enable Hive authorization using Sentry
So Spark-Hive conflicts with Sentry?

Environment: Hue 3.9 Spark Notebooks + Livy Server (built from master). CDH 5.5.

This is a kerberized cluster with Sentry.

ps. I was using hue's keytab as hue user is normally (by default in CDH) is allowed to impersonate to other users. So very convenient for Spark Notebooks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Dec 24 14:23:24 UTC 2019,,,,,,,,,,"0|i3iyr3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Jan/16 18:58;Tagar;Thank you Kostas.;;;","24/Feb/16 20:59;vanzin;I got a similar exception during my testing and this turns out to be a Spark bug. As soon as I finish some debugging on the Livy side, I'll file a Spark bug and work on the fix.

In the meantime, these options should allow more progress:

{code}
spark.yarn.security.tokens.hive.enabled=false
spark.yarn.security.tokens.hbase.enabled=false
{code}
;;;","24/Feb/16 21:00;Tagar;Thank you Marcelo.;;;","24/Feb/16 23:36;vanzin;Filed SPARK-13478 for the Spark fix. The workaround above should work for current versions of Spark (but you'll lose ability to talk to Hive and HBase secure clusters).;;;","24/Feb/16 23:43;Tagar;that's great. I'll try the workaround... thank you for filing SPARK-13478.;;;","24/Dec/19 14:23;sdhalex;Excuse me,  has this bug been fixed in a later version?  Or need we still do the same workaround by losing ability to talk to Hive and HBase secure clusters ? Thx.  [~vanzin]
                             
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot start livy repl,LIVY-48,13095686,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,kostas,gmcdonald,28/Jan/16 18:16,28/Jan/16 22:15,19/Dec/25 04:16,28/Jan/16 22:15,0.1,,,,,Core,,,,,,,,,,0,,,,,,"Filed by: https://github.com/lammic
https://github.com/cloudera/livy/issues/41

Tried starting with ./bin/livy-repl and got

{code}
java.lang.ClassNotFoundException: com.cloudera.livy.repl.Main
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:278)
        at org.apache.spark.util.Utils$.classForName(Utils.scala:173)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:641)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}
I noticed that in livy-reply the env variable $ASSEMBLY_JAR gets never set.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 28 22:15:54 UTC 2016,,,,,,,,,,"0|i3iyqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Jan/16 22:15;vanzin;Should be fixed now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude Hadoop & Spark's jars in Livy assembly,LIVY-47,13095685,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,kostas,gmcdonald,28/Jan/16 18:12,08/Mar/16 22:43,19/Dec/25 04:16,08/Mar/16 22:43,0.1,,,0.2,,Core,,,,,,,,,,0,,,,,,"In most environment, users already have Hadoop & Spark's jars available on their clusters.
These jars are huge and are bloating up Livy assembly.

From https://github.com/cloudera/livy/issues/43",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Mar 08 22:43:29 UTC 2016,,,,,,,,,,"0|i3iyqn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Jan/16 20:18;tc0312;If everyone agrees on this, I have a change in pom.xml for this and will send a PR.;;;","08/Mar/16 22:43;vanzin;Commit [d8842102|https://github.com/cloudera/livy/commit/d8842102dd151f7568b0a6a9feb0a0064dbc32bf] by  Alex Man <tc.technetium@...> in cloudera/livy:
{code}
LIVY-47. Added an override for the scope of Hadoop and Spark dependencies.

In most environment, users already have Hadoop & Spark's jars available on their clusters.
This change added an override so users can exclude them in Livy assembly and provide them to Livy at runtime using CLASSPATH.
To use it, build with: mvn -Dhadoop.scope=provided -Dspark.scope=provided package

Closes #72
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make REPL sessions use the RSC,LIVY-46,13095684,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Fixed,vanzin,kostas,gmcdonald,28/Jan/16 18:08,23/Mar/16 21:02,19/Dec/25 04:16,23/Mar/16 21:02,0.1,,,0.2,,Core,,,,,,,,,,0,,,,,,"The current backends for the batch and REPL sessions use custom code to run and communicate with a Spark process; they also have their own YARN client for running things on YARN.
Instead, we should standardize on using the RSC for everything.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 21:02:30 UTC 2016,,,,,,,,,,"0|i3iyqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"01/Mar/16 00:25;hshreedharan;Hi [~GayathriMurali], have you started working on this? I am planning to do start doing some work on this.

I am planning to get rid of the additional web server for interactive, but I am still trying to figure out how to do the interactive version with the RSC. To do this, we might have to run a driver which then starts a shell (spark/python/R etc). ;;;","01/Mar/16 18:54;hshreedharan;[~vanzin] - Any comments on that approach? 

I'd rather have the underlying shell do the interpretation than us doing it - so that seems like the best option, but the classpath stuff can be tricky.;;;","01/Mar/16 18:59;vanzin;I haven't really thought this through (it would require some prototyping to see whether things would work). My original idea was to run the RSC as is and run a task that starts the underlying shell and stores it in some static variable. Then incoming code is send as another task that just writes the command to the object stored in that static field.

But I'm not sure whether that would work for python and R.

So it might be the case that we'd have to modify the RSC to run different drivers depending on what's being run. There'd be some common class to do the communication with the RSC client, but the actual driver would differ.

But then that's all just brainstorming and I didn't really get down to looking at the code and trying to make any of this work.;;;","01/Mar/16 19:45;hshreedharan;The concern I have (which is true even today) is that the interpreter starts spark shells which are in client mode (afaik, the shell cannot run in yarn-cluster mode). This can use up too much memory on the machine running Livy itself. By outsourcing this to run via YARN (even indirectly via the remote deriver), we can manage resources more efficiently

So if I understand correctly, your idea is similar to what I mentioned (where the shell is started inside a driver, thus requiring some modification on the driver and some on the client?). ;;;","01/Mar/16 19:54;vanzin;bq. The concern I have (which is true even today) is that the interpreter starts spark shells which are in client mode 

I don't thing that's true. It starts the shell in whatever container runs the repl's Main class; if you start that class in cluster mode, you'll be running, e.g., the underlying python or R interpreter in some cluster node.;;;","01/Mar/16 20:07;hshreedharan;Ah, correct. I misread that code. It looks like the repl is started as a Spark job - so yeah it could run in a cluster. That makes it much easier.;;;","01/Mar/16 20:21;hshreedharan;Chatted with [~GayathriMurali] offline, and she is ok with me picking this up. So I will start spending some time prototyping this.;;;","01/Mar/16 22:13;tc0312;We are actively working on Livy HA and it's not designed with RSC in mind. We have changes in batch mode and REPL driver which will conflict with this change.
Would you mind sharing some details on the design so we can coordinate work and minimize throwaway work in HA? Thanks!;;;","01/Mar/16 22:13;tc0312;Another question: How does batch use RSC?;;;","01/Mar/16 22:17;hshreedharan;Today, nothing except client mode uses RSC. So the idea is to change the current RSC RemoteDriver class (or create a new driver), that basically does the same thing that the repl/Main class does - create a shell and run commands and get results (I will probably reuse a lot of the repl/Main code). To communicate with the RSC client running inside Livy, we will use the same protocol as now via which we send the results back. This is sent back via the current HTTP protocol from Livy to the client (I don't think anything has to change here).

;;;","01/Mar/16 22:19;hshreedharan;I am not sure if there is a whole lot of benefit running the batch session via the RSC. I am inclined to just leave that as is.;;;","01/Mar/16 22:44;tc0312;Thanks Hari for the prompt response. I'm going to rename the JIRA to indicate that batch is not included in this change.

With this change, repl sessions will still be started with SparkProcessBuilder, but with jar and classname replaced with RemoteDriver's. Is that correct? We have changes in SparkProcessBuilder for HA (For YARN Application Tag) so we would prefer to keep repl sessions creation the same.

In current HA design, repl callbacks to notify server its endpoint, server will persist it. After a HA failover, server will recover and contact repl using the persist endpoint. Is this possible still with RSC?;;;","01/Mar/16 22:54;hshreedharan;Actually in the RSC case, the server is run inside {{LocalClient}}: https://github.com/cloudera/livy/blob/master/client-local/src/main/java/com/cloudera/livy/client/local/LocalClientFactory.java#L67

We'd have to figure out how to get the {{RemoteDriver}} class to figure out where the new client is. I think this is more robust as an app can die and get restarted by YARN. So if the app gets restarted we want to still be able to re-connect to Livy. If the app runs the server, then Livy will not know where the app is. But in either case, we need to find where the ""server"" is, so the other side can connect to it.;;;","01/Mar/16 22:59;vanzin;bq. With this change, repl sessions will still be started with SparkProcessBuilder, but with jar and classname replaced with RemoteDriver's. Is that correct?

No. The RSC has its own code to run Spark. But it shouldn't affect adding any configuration to the Spark app to be run; you just provide a map with the Spark configuration to the RSC, so you just add whatever configuration you want there.

bq. We'd have to figure out how to get the RemoteDriver class to figure out where the new client is.

Yes. There's a bug for modifying the RSC to retry connections (LIVY-58); both the RSC needs to be modified to not commit suicide if the Livy server goes away, and the server needs to be modified to save enough state to be able to recognize existing RSCs connecting back. That's probably not a trivial change.;;;","01/Mar/16 23:25;tc0312;Is it possible to unify RSC to use SparkProcessBuilder to run Spark?
Besides YARN app tag, there's a separate change we are making to SparkProcessBuilder, it will return a SparkProcessYarn object, its log() and stop() go to YARN rather than the spark-submit process.
If each session has its own way to run Spark, it's hard to not duplicate code.;;;","01/Mar/16 23:32;vanzin;bq. Is it possible to unify RSC to use SparkProcessBuilder to run Spark?

Why would you want that? `SparkProcessBuilder` is basically a reimplementation of `SparkLauncher` without any extra functionality that I remember. I think being able to set any arbitrary Spark option is much more flexible, and the RSC interface allows that.;;;","01/Mar/16 23:35;vanzin;bq. Besides YARN app tag,

That just a Spark configuration. You don't need an explicit API for that when running spark-submit.

It would be much easier if you guys shared some more detailed spec of what you guys are thinking. The current HA spec is very high level. If we could actually see interfaces and maybe some code, we'd have more to work with.;;;","01/Mar/16 23:44;linchan;Thanks Marcelo/Hari. It should be no problem for us to start sharing code to you guys for early review. We can discuss more about the design choice and reach an agreement.

To me, the biggest concern right now is the non-trivial change to make RSC survive a Server restart. That is a fundamental requirement for the success of Livy HA. I think we should discuss how we can coordinate to make sure both work can be implemented successfully.;;;","01/Mar/16 23:50;tc0312;Previously there were process and YARN modes in Livy. They were removed.
We need YARN mode back because when Livy's master is YARN, log() and kill() should go thru YARN, instead of the local spark-submit process which could die at any time.
Hence we need an abstraction to handle Spark app running on YARN and generic process mode.
I don't care about SparkProcessBuilder as long as there's a central place where Livy creates Spark app.;;;","02/Mar/16 00:08;hshreedharan;I agree that LIVY-58 is a pre-req for the HA work. 

Since Livy server restarts are less likely (hopefully), than Spark apps getting restarted - I think the approach of running the server on Livy makes more sense, plus this gives us fewer pieces to secure (single RPC server within livy compared to many running servers in the repl instances). So I  think we must find a way to make HA work in such a way that the server is still on the Livy side.;;;","02/Mar/16 01:49;vanzin;bq. Hence we need an abstraction to handle Spark app running on YARN and generic process mode.

We can discuss this better over the phone or when we actually see a more detailed spec, but that's not what you need.

What you need is two different things:
- code that knows how to run Spark and talk to the running Spark app regardless of which mode Spark runs in. That's the RSC.
- code that knows how to talk to YARN and figure out running applications started by Livy and doing the other things you mention (e.g. kill apps or get logs)

The old yarn mode tried to conflate the two and that's why it was removed. These two need to be separate things for HA to work, because when you're recovering, you do not have a child processes anymore, so you need to re-build session state through some other means.
;;;","02/Mar/16 03:12;kostas;Alex & Lin, I think a lot of confusion might be sorted out if before our call, you guys can just point us to the code changes you have made for HA. A pointer to a personal WIP branch is fine here - it doesn't need to be pretty, just needs to be illustrative so we understand the changes you are proposing. ;;;","22/Mar/16 05:32;zjffdu;Sorry, what does RSC stand for here ?;;;","22/Mar/16 06:11;zjffdu;Never mind, I think I got it. That means the new livy client library (RSC means remote spark client ?);;;","23/Mar/16 21:02;vanzin;Commit [0af8a274|https://github.com/cloudera/livy/commit/0af8a274f6a09683d9625cdb11620db1208d1368] by  Hari Shreedharan <hshreedharan@...> in cloudera/livy:
{code}
LIVY-46. Use the RSC to drive REPL sessions.

This change modifies interactive sessions so they use the RSC (livy-client-local) to
run the REPL driver. This removes the embedded Jetty server in the REPL process,
so it solves other issues too; namely, LIVY-95, LIVY-84, and maybe others that I have
forgotten about.

The change works by having the RSC instantiate a REPL-specific driver when a certain
config option is set. This is currently a little hacky and not very extensible, but we can work
on those parts later if there's a need. The `LocalClient` class exposes some REPL-specific
methods so that its main client (the ""server"" module) doesn't need REPL jars in its classpath.
These methods just send RPCs to the REPL driver and return their results.

One notable loss of functionality is that the interactive session servlet does not expose REPL
logs anymore.  But because those logs were the output of ""spark-submit"", they're not very useful.
In yarn-cluster mode, there's only app status information there, nothing related about what's
really happening in Spark. LIVY-55 is the proper way of exposing logs. Another loss is the removal of the `bin/livy-repl` script; if that functionality is desired, a new module that brings back the HTTP frontend to the repl will have to be created, to avoid the issues in LIVY-84.

Closes #94
{code}
;;;",,,,,,,,,,,,,,,,,,,
Make YARN file system root configurable,LIVY-45,13095683,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,tc0312,gmcdonald,28/Jan/16 03:30,09/May/16 18:23,19/Dec/25 04:16,09/May/16 18:23,0.2,,,,,Core,,,,,,,,,,0,,,,,,"Livy is hardcoded to prefix relative path with ï¿½ï¿½ï¿½hdfs://ï¿½ï¿½ï¿½ in YARN mode. There are alternative file systems like s3, s3n, wasb, etc.

Instead of hardcoding hdfs:// in the code, I think we should make it an configuration. Then users can use Livy with whatever filesystem their Spark support.
The configuration will look like:

{noformat}
# When Livy is running in YARN mode, if it sees a relative path, it will automatically prefix the path with yarn-filesystem-root.
## livy.server.yarn-filesystem-root = hdfs://
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 18:23:46 UTC 2016,,,,,,,,,,"0|i3iyq7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"28/Jan/16 03:40;kostas;Does it make sense to also just remove the hardcoded path and make the user be more explicit? That would be a breaking change though.;;;","28/Jan/16 20:14;tc0312;I agree. Without the file system root, we can just pass the local path to spark-submit. Spark-submit will upload them to cluster fs for us.;;;","09/May/16 18:23;vanzin;There's no hardcoded hdfs URI on master anymore; Livy will just use the default FS from the configuration ({{fs.defaultFS}}).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add Livy parameters --principal and --keytab parameters to be passed to spark-submit,LIVY-44,13095682,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,,Tagar,gmcdonald,17/Jan/16 07:31,18/Aug/17 18:03,19/Dec/25 04:16,12/May/16 00:18,0.2,,,,,Core,,,,,,,,,,1,hue,kerberos,security,,,"It seems the only correct way to run Livy server is to run it under hue user in a kerberized cluster? 
There are two advantages:
1. hue user is normally (by default in CDH) is allowed to impersonate to other users;
2. hue already has a keytab maintened by Cloduera Manager.

This is explained in http://www.cloudera.com/content/www/en-us/documentation/enterprise/latest/topics/cm_sg_yarn_long_jobs.html
""Configuring Spark on YARN for Long-running Applications"".
Assuming Livy Server creates long-running Spark applications.

Without --principal and --keytab parameters (and without non-expired kerberos ticket), I am getting:

The Spark session could not be created in the cluster: ... 37 more 
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt) 
at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147) 
at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121) 
at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187) 
at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223) 
at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212) 
at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) 
at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193) ... 
46 more
Without --principal and --keytab parameters, but with a non-expired kerberos ticket for my personal principal, I am getting:

16/01/16 22:59:00 INFO RMProxy: 
Connecting to ResourceManager at xx.yy.com/10.20.xx.yy:8032 
16/01/16 22:59:00 WARN UserGroupInformation: 
PriviledgedActionException as:rdautkha (auth:PROXY) 
via rdautkhanov@XX.COM (auth:KERBEROS) 
cause:org.apache.hadoop.security.authorize.AuthorizationException: 
User: rdautkhanov@XX.COM is not allowed to impersonate rdautkha
ps. hue.keytab is located in

$ sudo ls -ltr /var/run/cloudera-scm-agent/process | grep hue-KT_RENEWER | tail -1 | awk '{print $9}'
on hosts running ""Kerberos Ticket Renewer"" Hue role. This is the directory that must be passed to --keytab parameter to spark-submit.

Irrespective of Hue, Livy Server should have parameters to allow to pass --principal and --keytab to spark-submit. So it's easier (or maybe the only way) to use in kerberized environmens.",kerberized hadoop,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jun 22 00:12:47 UTC 2017,,,,,,,,,,"0|i3iypz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"17/Jan/16 07:31;Tagar;Please also see https://github.com/cloudera/livy/issues/33 ;;;","12/May/16 00:18;vanzin;I'm going to close this as won't fix; I don't believe exposing these parameters are the right way to enable impersonation.

Most of the kerberos handling will already be implemented as part of LIVY-3. The only remaining part, at that point, is support for renewing delegation tokens. That requires support from the Spark side too, and the work is tracked in SPARK-14743.;;;","22/Jun/17 00:12;anderson.900;Any chance that the decision to close as won't fix could be revisited?  I have a specific, albeit strange, scenario that I think requires --principal and --keytab parameters.

I have a Spark job where part of the work done on the executors includes POSTing to a SPNEGO-enabled REST API endpoint, and unless I'm mistaken, when Spark is running a job as a proxy user, the doAs() chain doesn't have the necessary credentials to actually do a SPNEGO negotiation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add file r/w permission,LIVY-43,13095681,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,purechoc,gmcdonald,15/Jan/16 01:46,24/Feb/16 00:22,19/Dec/25 04:16,24/Feb/16 00:22,0.1,,,,,Interpreter,,,,,,,,,,0,,,,,,"test scenario
user ""livy"" run livy server with yarn cluster mode.
service to user ""a"", ""b"", ""c""

need to way prevent, user a, b, c access /home/livy/* directory

version .1,  user ""a"" can access and read/wirte file /home/livy/* using rest api",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Feb 24 00:22:12 UTC 2016,,,,,,,,,,"0|i3iypr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"24/Feb/16 00:22;vanzin;What you really want is proper security and impersonation which is being tracked in LIVY-52.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy Server can not be build if a user has a long id,LIVY-42,13095680,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,Tagar,gmcdonald,09/Jan/16 02:42,14/Jan/16 21:00,19/Dec/25 04:16,14/Jan/16 20:55,0.1,0.2,,0.2,,Core,,,,,,,,,,0,build,maven,,,,"Also posted on github: https://github.com/cloudera/livy/issues/20

Could not build livy from master:

[INFO] Reactor Summary:
[INFO]
[INFO] livy-main .......................................... SUCCESS [ 30.984 s]
[INFO] livy-api_2.10 ...................................... SUCCESS [03:32 min]
[INFO] livy-client-common_2.10 ............................ SUCCESS [ 1.972 s]
[INFO] livy-client-local_2.10 ............................. SUCCESS [ 19.896 s]
[INFO] livy-core_2.10 ..................................... SUCCESS [ 36.403 s]
[INFO] livy-repl_2.10 ..................................... SUCCESS [01:11 min]
[INFO] livy-yarn_2.10 ..................................... SUCCESS [ 11.750 s]
[INFO] livy-spark_2.10 .................................... SUCCESS [ 26.428 s]
[INFO] livy-server_2.10 ................................... SUCCESS [ 34.198 s]
[INFO] livy-assembly_2.10 ................................. FAILURE [ 9.494 s]
[INFO] livy-client-http_2.10 .............................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 07:35 min
[INFO] Finished at: 2016-01-08T19:28:50-07:00
[INFO] Final Memory: 47M/11135M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.6:single (livy-assembly) on project livy-assembly_2.10: Execution livy-assembly of goal org.apache.maven.plugins:maven-assembly-plugin:2.6:single failed: user id '2074918846' is too big ( > 2097151 ). -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.

$ id -u
2074918846

We have our corporate UNIX user ids synchronized with an Active Directory, so they are unique across all servers and .. long. Any idea of a workaround to get Livy Server compiled?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Jan 14 21:00:01 UTC 2016,,,,,,,,,,"0|i3iypj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Jan/16 21:00;Tagar;thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need a way to submit codes through Named Session instead of Session ID numbers,LIVY-41,13095679,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,meisam,prabhu1984,gmcdonald,29/Dec/15 20:51,05/Feb/19 19:25,19/Dec/25 04:16,05/Feb/19 19:25,0.1,,,0.6.0,,Core,,,,,,,,,,1,,,,,,"Currently, we are creating new pyspark session and submitting codes through newly created session id as below
curl -X POST --data '
{""kind"": ""pyspark"", ""queue"": ""spark"", ""name"": ""Livy_Example""}
' -H ""Content-Type: application/json"" localhost:8998/sessions
curl localhost:8998/sessions/1/statements -X POST -H 'Content-Type: application/json' -d '
{""code"":""1 + 1""}
'
But, it would be nice if we have a way to submit the codes through session name instead of session id. That way, we need not to maintain a lookup table for session-id to session-name. Users will be able to submit their codes easily as below through session names instead of worrying about session id.
curl -X POST --data '
{""kind"": ""pyspark"", ""queue"": ""spark"", ""name"": ""mkt_risk_batch""}
' -H ""Content-Type: application/json"" localhost:8998/sessions
curl localhost:8998/sessions/mkt_risk_batch/statements -X POST -H 'Content-Type: application/json' -d '
{""code"":""1 + 1""}
'",,"GitHub user meisam opened a pull request:

    https://github.com/apache/incubator-livy/pull/48

    [LIVY-41] Let users access sessions by session name

    This commit  enables Livy users to access sessions either by names or by auto-generated sessiond id's.
    
    It also prevents users from creating sessions that have the same name.
    
    This commit keeps API change  minimal. Thse are palces that API change
    is needed:
    - `Session` and its subclasses adds a new field, `name`.
    - `RecoveryMetadata` and its subclasses adds a new field, `name`.
    - `SessionManager` adds a new method `getSession(name: String)` which lookups sessions by name.
    
    A more clean implementation would change the signature of `SessionManager.register` so it returns a proper container around the session value to determine if it failed to register the given session.  For example,
    ```Scala
    def register(session S): Either[S, Throwable]
     ```
    
    Task-url: https://issues.apache.org/jira/browse/LIVY-41

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/meisam/incubator-livy LIVY-41-rebased

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/48.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #48
    
----

----
;19/Sep/17 21:17;githubbot;600","Github user codecov-io commented on the issue:

    https://github.com/apache/incubator-livy/pull/48
  
    # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=h1) Report
    > Merging [#48](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=desc) into [master](https://codecov.io/gh/apache/incubator-livy/commit/219fdac5cfea1780cfb4e52f29777d3b21f8a55e?src=pr&el=desc) will **decrease** coverage by `<.01%`.
    > The diff coverage is `73.01%`.
    
    [![Impacted file tree graph](https://codecov.io/gh/apache/incubator-livy/pull/48/graphs/tree.svg?width=650&src=pr&token=0MkVbiUFwE&height=150)](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree)
    
    ```diff
    @@             Coverage Diff             @@
    ##             master     #48      +/-   ##
    ===========================================
    - Coverage     70.71%   70.7%   -0.01%     
    - Complexity      791     793       +2     
    ===========================================
      Files            97      97              
      Lines          5389    5435      +46     
      Branches        800     809       +9     
    ===========================================
    + Hits           3811    3843      +32     
    - Misses         1040    1051      +11     
    - Partials        538     541       +3
    ```
    
    
    | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree) | Coverage Î” | Complexity Î” | |
    |---|---|---|---|
    | [.../main/scala/org/apache/livy/sessions/Session.scala](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXNzaW9ucy9TZXNzaW9uLnNjYWxh) | `73.33% <0%> (Ã¸)` | `17 <1> (Ã¸)` | :arrow_down: |
    | [...e/livy/server/interactive/InteractiveSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `61.58% <100%> (+0.49%)` | `42 <1> (Ã¸)` | :arrow_down: |
    | [...va/org/apache/livy/client/common/HttpMessages.java](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree#diff-Y2xpZW50LWNvbW1vbi9zcmMvbWFpbi9qYXZhL29yZy9hcGFjaGUvbGl2eS9jbGllbnQvY29tbW9uL0h0dHBNZXNzYWdlcy5qYXZh) | `93.02% <100%> (+0.16%)` | `0 <0> (Ã¸)` | :arrow_down: |
    | [...la/org/apache/livy/server/batch/BatchSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvYmF0Y2gvQmF0Y2hTZXNzaW9uLnNjYWxh) | `86.04% <100%> (+0.5%)` | `14 <1> (Ã¸)` | :arrow_down: |
    | [.../scala/org/apache/livy/server/SessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvU2Vzc2lvblNlcnZsZXQuc2NhbGE=) | `66.66% <100%> (+1.89%)` | `28 <4> (+1)` | :arrow_up: |
    | [...server/interactive/InteractiveSessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `60.6% <46.66%> (-2.73%)` | `5 <0> (Ã¸)` | |
    | [...apache/livy/server/batch/BatchSessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvYmF0Y2gvQmF0Y2hTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `71.42% <50%> (-16.08%)` | `4 <0> (+1)` | |
    | [...cala/org/apache/livy/sessions/SessionManager.scala](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXNzaW9ucy9TZXNzaW9uTWFuYWdlci5zY2FsYQ==) | `84.09% <90.9%> (+0.75%)` | `26 <1> (+2)` | :arrow_up: |
    | [...in/java/org/apache/livy/rsc/rpc/RpcDispatcher.java](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9ycGMvUnBjRGlzcGF0Y2hlci5qYXZh) | `64% <0%> (-3%)` | `19% <0%> (-1%)` | |
    | [...ain/java/org/apache/livy/rsc/driver/RSCDriver.java](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9kcml2ZXIvUlNDRHJpdmVyLmphdmE=) | `77.87% <0%> (-1.28%)` | `41% <0%> (-1%)` | |
    | ... and [6 more](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=tree-more) | |
    
    ------
    
    [Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=continue).
    > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
    > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
    > Powered by [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=footer). Last update [219fdac...8487bbb](https://codecov.io/gh/apache/incubator-livy/pull/48?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

;19/Sep/17 23:41;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r139850741
  
    --- Diff: server/src/main/scala/org/apache/livy/server/batch/BatchSessionServlet.scala ---
    @@ -43,8 +44,23 @@ class BatchSessionServlet(
       override protected def createSession(req: HttpServletRequest): BatchSession = {
         val createRequest = bodyAs[CreateBatchRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    +        name
    +      case Some(name) =>
    +        // this does NOT guarantee that by the time this session is ready to be registered in
    +        // sessionManager, another with the same name is not registered. But in most cases,
    +        // it prevents Livy from submitting applications to Spark.
    +        val msg = s""Session $name already exists! "" +
    +          s""Choose a different name or delete the existing session.""
    +        throw new IllegalArgumentException(msg)
    +      case None =>
    +        s""INTERACTIVE-SESSION-$sessionId""
    --- End diff --
    
    This wasn't changed in the copy-paste. Also I'm not a fan of this default name, it's a bit verbose. Just `batch-$sessionId` would suffice.
;20/Sep/17 00:40;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r139851402
  
    --- Diff: server/src/main/scala/org/apache/livy/sessions/SessionManager.scala ---
    @@ -92,13 +94,29 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
       def register(session: S): S = {
         info(s""Registering new session ${session.id}"")
         synchronized {
    -      sessions.put(session.id, session)
    +      // in InteractiveSessionServlet.createSession() and BatchSessionServlet.createSession(),
    +      // Livy checks to make sure another session with the same name does not exist already.
    +      // This case should not happen rarely.
    +      // already exists. But looking up a session name and adding it to the set of existing sessions
    +      // should happen atomically.
    +      if (sessionsByName.contains(session.name)) {
    +        val msg = s""Session ${session.name} already exists!""
    +        error(msg)
    +        delete(session)
    --- End diff --
    
    **Discussion Point**: Do we want to fail a request with a ""bad"" name or should we instead give a warning message and create a session with a default name instead? Just an idea, I'm still not 100% sure of this PR's use case.
;20/Sep/17 00:40;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r139850898
  
    --- Diff: server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala ---
    @@ -53,8 +53,23 @@ class InteractiveSessionServlet(
       override protected def createSession(req: HttpServletRequest): InteractiveSession = {
         val createRequest = bodyAs[CreateInteractiveRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId: Int = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    +        name
    +      case Some(name) =>
    +        // this does NOT guarantee that by the time this session is ready to be registered in
    +        // sessionManager, another with the same name is not registered. But in most cases,
    +        // it prevents Livy from submitting applications to Spark.
    +        val msg = s""Session $name already exists! "" +
    +          s""Choose a different name or delete the existing session.""
    +        throw new IllegalArgumentException(msg)
    +      case None =>
    +        s""INTERACTIVE-SESSION-$sessionId""
    --- End diff --
    
    As above, just `session-$sessionId` would suffice.
;20/Sep/17 00:40;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r139851201
  
    --- Diff: server/src/main/scala/org/apache/livy/sessions/SessionManager.scala ---
    @@ -92,13 +94,29 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
       def register(session: S): S = {
         info(s""Registering new session ${session.id}"")
         synchronized {
    -      sessions.put(session.id, session)
    +      // in InteractiveSessionServlet.createSession() and BatchSessionServlet.createSession(),
    +      // Livy checks to make sure another session with the same name does not exist already.
    +      // This case should not happen rarely.
    +      // already exists. But looking up a session name and adding it to the set of existing sessions
    +      // should happen atomically.
    --- End diff --
    
    Theres some confusing wording this comment block, might be artifacts from changes during development, but it makes the comment difficult to understand.
;20/Sep/17 00:40;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r139850104
  
    --- Diff: server/src/main/scala/org/apache/livy/server/SessionServlet.scala ---
    @@ -214,16 +215,23 @@ abstract class SessionServlet[S <: Session, R <: RecoveryMetadata](
       private def doWithSession(fn: (S => Any),
           allowAll: Boolean,
           checkFn: Option[(String, HttpServletRequest) => Boolean]): Any = {
    -    val sessionId = params(""id"").toInt
    -    sessionManager.get(sessionId) match {
    +    val idParam: String = params(""id"")
    --- End diff --
    
    I don't think re-using the id param for name is a good idea. It should have it's own optional param instead. I believe this is already how you are doing session creation anyway.
;20/Sep/17 00:40;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r139850911
  
    --- Diff: server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala ---
    @@ -53,8 +53,23 @@ class InteractiveSessionServlet(
       override protected def createSession(req: HttpServletRequest): InteractiveSession = {
         val createRequest = bodyAs[CreateInteractiveRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId: Int = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    +        name
    +      case Some(name) =>
    --- End diff --
    
    same as above
;20/Sep/17 00:40;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r139850365
  
    --- Diff: server/src/main/scala/org/apache/livy/server/batch/BatchSessionServlet.scala ---
    @@ -43,8 +44,23 @@ class BatchSessionServlet(
       override protected def createSession(req: HttpServletRequest): BatchSession = {
         val createRequest = bodyAs[CreateBatchRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    +        name
    +      case Some(name) =>
    --- End diff --
    
    I think this should be an else statement in the first case instead. This usage of match is a bit hacky.
;20/Sep/17 00:40;githubbot;600","Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/48
  
    @meisam , do you need to change Java and python job API to support this feature? As I remembered it can specify session id to reuse the existing session, so for now we should also support session name.
;20/Sep/17 03:29;githubbot;600","Github user meisam commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r140114815
  
    --- Diff: server/src/main/scala/org/apache/livy/server/SessionServlet.scala ---
    @@ -214,16 +215,23 @@ abstract class SessionServlet[S <: Session, R <: RecoveryMetadata](
       private def doWithSession(fn: (S => Any),
           allowAll: Boolean,
           checkFn: Option[(String, HttpServletRequest) => Boolean]): Any = {
    -    val sessionId = params(""id"").toInt
    -    sessionManager.get(sessionId) match {
    +    val idParam: String = params(""id"")
    --- End diff --
    
    I don't have any strong opinion for or against reusing `id param`. Here is why:
    
    If we do not reuse `id param`, we have to create a new REST endpoint for each existing one. Reusing `id param` allows Livy users access a session seamlessly by its ID or its name through the same endpoint:
    
    `sessions/{id}` or `sessions/{name}`
    `sessions/{id}/statements` or `sessions/{name}/statements`
    `sessions/{id}/state` or `sessions/{name}/state`
    `sessions/{id}/log` or `sessions/{name}/log`
    
    If we do not reuse `id param`,  we have to add separate endpoints for accessing sessions by name. Something like
    `named-sessions/{name}`
    `named-sessions/{name}/statements`
    `named-sessions/{name}/state`
    `named-sessions/{name}/log`
    or
    `sessions/by-name/{name}`
    `sessions/by-name/{name}/statements`
    `sessions/by-name/{name}/state`
    `sessions/by-name/{name}/log`
    
    or
    `sessions?name={name}`
    `sessions/statements?name={name}`
    `sessions/state?name={name}`
    `sessions/log?name={name}`
    
    @ajbozarth, what are your thoughts on adding a new endpoint?

;20/Sep/17 23:38;githubbot;600","Github user meisam commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r140115073
  
    --- Diff: server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala ---
    @@ -53,8 +53,23 @@ class InteractiveSessionServlet(
       override protected def createSession(req: HttpServletRequest): InteractiveSession = {
         val createRequest = bodyAs[CreateInteractiveRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId: Int = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    +        name
    +      case Some(name) =>
    +        // this does NOT guarantee that by the time this session is ready to be registered in
    +        // sessionManager, another with the same name is not registered. But in most cases,
    +        // it prevents Livy from submitting applications to Spark.
    +        val msg = s""Session $name already exists! "" +
    +          s""Choose a different name or delete the existing session.""
    +        throw new IllegalArgumentException(msg)
    +      case None =>
    +        s""INTERACTIVE-SESSION-$sessionId""
    --- End diff --
    
    ðŸ‘ 
;20/Sep/17 23:38;githubbot;600","Github user meisam commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r140117359
  
    --- Diff: server/src/main/scala/org/apache/livy/sessions/SessionManager.scala ---
    @@ -92,13 +94,29 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
       def register(session: S): S = {
         info(s""Registering new session ${session.id}"")
         synchronized {
    -      sessions.put(session.id, session)
    +      // in InteractiveSessionServlet.createSession() and BatchSessionServlet.createSession(),
    +      // Livy checks to make sure another session with the same name does not exist already.
    +      // This case should not happen rarely.
    +      // already exists. But looking up a session name and adding it to the set of existing sessions
    +      // should happen atomically.
    +      if (sessionsByName.contains(session.name)) {
    +        val msg = s""Session ${session.name} already exists!""
    +        error(msg)
    +        delete(session)
    --- End diff --
    
    This is an example use case we have here. I'll ask Prabhu to provide more.
    
    Team AAA has an ETL pipeline everyday at 1pm. The pipeline is a chain of jobs that work on the same dataset. They want to create a session on Livy and use it to run the pipeline. They also want to push the metrics and stats of their pipeline to altering and monitoring services.
    This PR allows them to access the session, manage it, and monitor it with a human readable name like team_aaa_daily_etl.
;20/Sep/17 23:38;githubbot;600","Github user meisam commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r140117998
  
    --- Diff: server/src/main/scala/org/apache/livy/server/batch/BatchSessionServlet.scala ---
    @@ -43,8 +44,23 @@ class BatchSessionServlet(
       override protected def createSession(req: HttpServletRequest): BatchSession = {
         val createRequest = bodyAs[CreateBatchRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    +        name
    +      case Some(name) =>
    --- End diff --
    
    ðŸ‘  I'll update this even though I am an advocate of if-guards in case statements.
;20/Sep/17 23:38;githubbot;600","Github user meisam commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r140115029
  
    --- Diff: server/src/main/scala/org/apache/livy/server/batch/BatchSessionServlet.scala ---
    @@ -43,8 +44,23 @@ class BatchSessionServlet(
       override protected def createSession(req: HttpServletRequest): BatchSession = {
         val createRequest = bodyAs[CreateBatchRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    +        name
    +      case Some(name) =>
    +        // this does NOT guarantee that by the time this session is ready to be registered in
    +        // sessionManager, another with the same name is not registered. But in most cases,
    +        // it prevents Livy from submitting applications to Spark.
    +        val msg = s""Session $name already exists! "" +
    +          s""Choose a different name or delete the existing session.""
    +        throw new IllegalArgumentException(msg)
    +      case None =>
    +        s""INTERACTIVE-SESSION-$sessionId""
    --- End diff --
    
    ðŸ‘  I'll update it.
;20/Sep/17 23:38;githubbot;600","Github user meisam commented on the issue:

    https://github.com/apache/incubator-livy/pull/48
  
    @jerryshao Yes, changing the Java and python API is a good idea.
;20/Sep/17 23:41;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r140138264
  
    --- Diff: server/src/main/scala/org/apache/livy/server/batch/BatchSessionServlet.scala ---
    @@ -43,8 +44,23 @@ class BatchSessionServlet(
       override protected def createSession(req: HttpServletRequest): BatchSession = {
         val createRequest = bodyAs[CreateBatchRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    +        name
    +      case Some(name) =>
    --- End diff --
    
    I was thinking an if/else inside the `Some(name)` case.
;21/Sep/17 02:42;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r140358101
  
    --- Diff: server/src/main/scala/org/apache/livy/sessions/SessionManager.scala ---
    @@ -92,13 +94,29 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
       def register(session: S): S = {
         info(s""Registering new session ${session.id}"")
         synchronized {
    -      sessions.put(session.id, session)
    +      // in InteractiveSessionServlet.createSession() and BatchSessionServlet.createSession(),
    +      // Livy checks to make sure another session with the same name does not exist already.
    +      // This case should not happen rarely.
    +      // already exists. But looking up a session name and adding it to the set of existing sessions
    +      // should happen atomically.
    +      if (sessionsByName.contains(session.name)) {
    +        val msg = s""Session ${session.name} already exists!""
    +        error(msg)
    +        delete(session)
    --- End diff --
    
    Your example use case does help me with understanding this PR's purpose, which I am liking more, but not this particular code decision. Is there a good reason we should be failing session creation on a ""bad"" session name rather than displaying a warning that the name was bad then creating a session with the default naming? I'm not saying either way is better, I am just wondering you reason for doing it this way.
;21/Sep/17 21:01;githubbot;600","Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r140360261
  
    --- Diff: server/src/main/scala/org/apache/livy/server/SessionServlet.scala ---
    @@ -214,16 +215,23 @@ abstract class SessionServlet[S <: Session, R <: RecoveryMetadata](
       private def doWithSession(fn: (S => Any),
           allowAll: Boolean,
           checkFn: Option[(String, HttpServletRequest) => Boolean]): Any = {
    -    val sessionId = params(""id"").toInt
    -    sessionManager.get(sessionId) match {
    +    val idParam: String = params(""id"")
    --- End diff --
    
    I checked the code to followup my argument and realized that the way these end-points are written the way you did it is actually the best way. The param name `id` is not user-facing so the clarity I want can be made in the Docs rather than the code. Given this change to the API though I would like you to double check that the Web UI will support these changes, both that it still works with the id, but also works by using a session name in the url. Actually adding the Session name to the UI can be done in a followup task (which I'm willing to do).
;21/Sep/17 21:12;githubbot;600","Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/48
  
    IMPO, for quicker pr reviews, I think adding Java and python API support can be done in a followup PR
;21/Sep/17 21:14;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jan/19 23:50;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   This commit  enables Livy users to access sessions either by names or by auto-generated sessiond id's.
   
   It also prevents users from creating sessions that have the same name.
   
   This commit keeps API change  minimal. Thse are palces that API change
   is needed:
   - `Session` and its subclasses adds a new field, `name`.
   - `RecoveryMetadata` and its subclasses adds a new field, `name`.
   - `SessionManager` adds a new method `getSession(name: String)` which lookups sessions by name.
   
   A more clean implementation would change the signature of `SessionManager.register` so it returns a proper container around the session value to determine if it failed to register the given session.  For example,
   ```Scala
   def register(session S): Either[S, Throwable]
    ```
   
   Task-url: https://issues.apache.org/jira/browse/LIVY-41
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Jan/19 23:50;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/19 22:20;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   This commit  enables Livy users to access sessions either by names or by auto-generated sessiond id's.
   
   It also prevents users from creating sessions that have the same name.
   
   This commit keeps API change  minimal. Thse are palces that API change
   is needed:
   - `Session` and its subclasses adds a new field, `name`.
   - `RecoveryMetadata` and its subclasses adds a new field, `name`.
   - `SessionManager` adds a new method `getSession(name: String)` which lookups sessions by name.
   
   A more clean implementation would change the signature of `SessionManager.register` so it returns a proper container around the session value to determine if it failed to register the given session.  For example,
   ```Scala
   def register(session S): Either[S, Throwable]
    ```
   
   Task-url: https://issues.apache.org/jira/browse/LIVY-41
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jan/19 22:20;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   This commit  enables Livy users to access sessions either by names or by auto-generated sessiond id's.
   
   It also prevents users from creating sessions that have the same name.
   
   This commit keeps API change  minimal. These are places that API change
   is needed:
   - `Session` and its sub-classes adds a new field, `name`.
   - `RecoveryMetadata` and its sub-classes adds a new field, `name`.
   - `SessionManager` adds a new method `getSession(name: String)` which looks sessions up by name.
   
   A more clean implementation would change the signature of `SessionManager.register` so it returns a proper container around the session value to determine if it failed to register the given session.  For example,
   ```Scala
   def register(session S): Either[S, Throwable]
    ```
   
   Task-url: https://issues.apache.org/jira/browse/LIVY-41
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/19 19:01;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/19 19:01;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/19 22:05;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   This commit  enables Livy users to access sessions either by names or by auto-generated sessiond id's.
   
   It also prevents users from creating sessions that have the same name.
   
   This commit keeps API change  minimal. These are places that API change
   is needed:
   - `Session` and its sub-classes adds a new field, `name`.
   - `RecoveryMetadata` and its sub-classes adds a new field, `name`.
   - `SessionManager` adds a new method `getSession(name: String)` which looks sessions up by name.
   
   A more clean implementation would change the signature of `SessionManager.register` so it returns a proper container around the session value to determine if it failed to register the given session.  For example,
   ```Scala
   def register(session S): Either[S, Throwable]
    ```
   
   Task-url: https://issues.apache.org/jira/browse/LIVY-41
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/19 22:05;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jan/19 03:03;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   This commit  enables Livy users to access sessions either by names or by auto-generated sessiond id's.
   
   It also prevents users from creating sessions that have the same name.
   
   This commit keeps API change  minimal. These are places that API change
   is needed:
   - `Session` and its sub-classes adds a new field, `name`.
   - `RecoveryMetadata` and its sub-classes adds a new field, `name`.
   - `SessionManager` adds a new method `getSession(name: String)` which looks sessions up by name.
   
   
   Task-url: https://issues.apache.org/jira/browse/LIVY-41
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jan/19 03:03;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jan/19 20:32;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   This commit  enables Livy users to access sessions either by names or by auto-generated sessiond id's.
   
   It also prevents users from creating sessions that have the same name.
   
   This commit keeps API change  minimal. These are places that API change
   is needed:
   - `Session` and its sub-classes adds a new field, `name`.
   - `RecoveryMetadata` and its sub-classes adds a new field, `name`.
   - `SessionManager` adds a new method `getSession(name: String)` which looks sessions up by name.
   
   
   Task-url: https://issues.apache.org/jira/browse/LIVY-41
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Jan/19 20:32;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jan/19 22:08;githubbot;600","meisam commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   This commit  enables Livy users to access sessions either by names or by auto-generated sessiond id's.
   
   It also prevents users from creating sessions that have the same name.
   
   This commit keeps API change  minimal. These are places that API change
   is needed:
   - `Session` and its sub-classes adds a new field, `name`.
   - `RecoveryMetadata` and its sub-classes adds a new field, `name`.
   - `SessionManager` adds a new method `getSession(name: String)` which looks sessions up by name.
   
   
   Task-url: https://issues.apache.org/jira/browse/LIVY-41
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;31/Jan/19 22:08;githubbot;600","asfgit commented on pull request #48: [LIVY-41] Let users access sessions by session name
URL: https://github.com/apache/incubator-livy/pull/48
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Feb/19 19:25;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,18600,,,0,18600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 05 19:25:41 UTC 2019,,,,,,,,,,"0|i3iypb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Jan/16 04:46;purechoc;session id is unique. but session names my be not unique.

how about this.
curl localhost:8998/sessions/username/mkt_risk_batch/statements ....

make session name unique is user's responsibility.;;;","22/Apr/16 21:26;vanzin;FYI I don't think this feature really belongs in Livy.;;;","26/Oct/16 23:32;meisam;I'd like to resurrect the discussion around this ticket. This is a very useful feature to have.
Here is the initial design document:
https://github.com/meisam/incubator-livy/wiki/Design-doc-for-Livy-41:-Accessing-sessions-by-name;;;","28/Sep/17 06:24;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141531378
  
    --- Diff: integration-test/src/main/scala/org/apache/livy/test/framework/LivyRestClient.scala ---
    @@ -205,12 +205,14 @@ class LivyRestClient(val httpClient: AsyncHttpClient, val livyEndpoint: String)
       }
     
       def startBatch(
    +      name: String,
    --- End diff --
    
    I would suggest to use `Option[String]` instead to avoid explicitly set to ""null"" as below.
;;;","28/Sep/17 06:24;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141532823
  
    --- Diff: server/src/main/scala/org/apache/livy/server/interactive/InteractiveSessionServlet.scala ---
    @@ -53,8 +53,23 @@ class InteractiveSessionServlet(
       override protected def createSession(req: HttpServletRequest): InteractiveSession = {
         val createRequest = bodyAs[CreateInteractiveRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId: Int = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    --- End diff --
    
    We'd better put this code into common place for both interactive and batch session.
;;;","28/Sep/17 06:24;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141531618
  
    --- Diff: server/src/main/scala/org/apache/livy/server/SessionServlet.scala ---
    @@ -214,16 +215,23 @@ abstract class SessionServlet[S <: Session, R <: RecoveryMetadata](
       private def doWithSession(fn: (S => Any),
           allowAll: Boolean,
           checkFn: Option[(String, HttpServletRequest) => Boolean]): Any = {
    -    val sessionId = params(""id"").toInt
    -    sessionManager.get(sessionId) match {
    +    val idParam: String = params(""id"")
    --- End diff --
    
    Let's rename this variable as `idOrNameParam` to be clear.
;;;","28/Sep/17 06:24;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141533416
  
    --- Diff: server/src/main/scala/org/apache/livy/server/batch/BatchSession.scala ---
    @@ -169,5 +174,5 @@ class BatchSession(
       override def infoChanged(appInfo: AppInfo): Unit = { this.appInfo = appInfo }
     
       override def recoveryMetadata: RecoveryMetadata =
    -    BatchRecoveryMetadata(id, appId, appTag, owner, proxyUser)
    +    BatchRecoveryMetadata(id, name, appId, appTag, owner, proxyUser)
    --- End diff --
    
    @meisam what if old recovery metadata which doesn't have `name` field, I think we should consider the compatibility of old metadata file with new Livy server, would you please check it?
;;;","28/Sep/17 06:24;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141532490
  
    --- Diff: server/src/main/scala/org/apache/livy/server/batch/BatchSessionServlet.scala ---
    @@ -43,8 +44,23 @@ class BatchSessionServlet(
       override protected def createSession(req: HttpServletRequest): BatchSession = {
         val createRequest = bodyAs[CreateBatchRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    +        name
    +      case Some(name) =>
    +        // this does NOT guarantee that by the time this session is ready to be registered in
    +        // sessionManager, another with the same name is not registered. But in most cases,
    +        // it prevents Livy from submitting applications to Spark.
    +        val msg = s""Session $name already exists! "" +
    +          s""Choose a different name or delete the existing session.""
    +        throw new IllegalArgumentException(msg)
    --- End diff --
    
    Is the exception here OK, shall we return some HTTP code to rest client about session name duplication?
;;;","28/Sep/17 06:24;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141534310
  
    --- Diff: server/src/main/scala/org/apache/livy/sessions/SessionManager.scala ---
    @@ -92,13 +94,29 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
       def register(session: S): S = {
         info(s""Registering new session ${session.id}"")
         synchronized {
    -      sessions.put(session.id, session)
    +      // in InteractiveSessionServlet.createSession() and BatchSessionServlet.createSession(),
    +      // Livy checks to make sure another session with the same name does not exist already.
    +      // This case should not happen rarely.
    +      // already exists. But looking up a session name and adding it to the set of existing sessions
    +      // should happen atomically.
    +      if (sessionsByName.contains(session.name)) {
    +        val msg = s""Session ${session.name} already exists!""
    +        error(msg)
    +        delete(session)
    --- End diff --
    
    I think fail the request and return user the specific bad http code would be better then choosing another name for user. Because user want to create a session with name ""A"" for example, if Livy cannot satisfy user's requirement, then user should choose another name. It is better to let user to decide than letting Livy to handle it.
;;;","28/Sep/17 06:24;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141533660
  
    --- Diff: server/src/main/scala/org/apache/livy/sessions/SessionManager.scala ---
    @@ -73,6 +73,8 @@ class SessionManager[S <: Session, R <: RecoveryMetadata : ClassTag](
     
       protected[this] final val idCounter = new AtomicInteger(0)
       protected[this] final val sessions = mutable.LinkedHashMap[Int, S]()
    +  private[this] final val sessionsByName = mutable.Map[String, S]()
    --- End diff --
    
    I think we can use `mutable.HashMap` to be clear.
;;;","28/Sep/17 18:42;githubbot;Github user ajbozarth commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141703062
  
    --- Diff: server/src/main/scala/org/apache/livy/server/batch/BatchSessionServlet.scala ---
    @@ -43,8 +44,23 @@ class BatchSessionServlet(
       override protected def createSession(req: HttpServletRequest): BatchSession = {
         val createRequest = bodyAs[CreateBatchRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    +        name
    +      case Some(name) =>
    +        // this does NOT guarantee that by the time this session is ready to be registered in
    +        // sessionManager, another with the same name is not registered. But in most cases,
    +        // it prevents Livy from submitting applications to Spark.
    +        val msg = s""Session $name already exists! "" +
    +          s""Choose a different name or delete the existing session.""
    +        throw new IllegalArgumentException(msg)
    --- End diff --
    
    nice catch, I not sure this would be surfaced to the submitter or not
;;;","29/Sep/17 19:22;githubbot;Github user meisam commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141947781
  
    --- Diff: server/src/main/scala/org/apache/livy/server/SessionServlet.scala ---
    @@ -214,16 +215,23 @@ abstract class SessionServlet[S <: Session, R <: RecoveryMetadata](
       private def doWithSession(fn: (S => Any),
           allowAll: Boolean,
           checkFn: Option[(String, HttpServletRequest) => Boolean]): Any = {
    -    val sessionId = params(""id"").toInt
    -    sessionManager.get(sessionId) match {
    +    val idParam: String = params(""id"")
    --- End diff --
    
    ðŸ‘ I'll rename it.
;;;","29/Sep/17 19:32;githubbot;Github user meisam commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141949640
  
    --- Diff: integration-test/src/main/scala/org/apache/livy/test/framework/LivyRestClient.scala ---
    @@ -205,12 +205,14 @@ class LivyRestClient(val httpClient: AsyncHttpClient, val livyEndpoint: String)
       }
     
       def startBatch(
    +      name: String,
    --- End diff --
    
    I'll update it to avoid explicit `null` values.
;;;","29/Sep/17 19:43;githubbot;Github user meisam commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141951874
  
    --- Diff: server/src/main/scala/org/apache/livy/server/batch/BatchSessionServlet.scala ---
    @@ -43,8 +44,23 @@ class BatchSessionServlet(
       override protected def createSession(req: HttpServletRequest): BatchSession = {
         val createRequest = bodyAs[CreateBatchRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    +        name
    +      case Some(name) =>
    +        // this does NOT guarantee that by the time this session is ready to be registered in
    +        // sessionManager, another with the same name is not registered. But in most cases,
    +        // it prevents Livy from submitting applications to Spark.
    +        val msg = s""Session $name already exists! "" +
    +          s""Choose a different name or delete the existing session.""
    +        throw new IllegalArgumentException(msg)
    --- End diff --
    
    One case that we should keep in mind about returning an error code. 
    Assume the existing session `s` with name `daily-job`.
    If the owner `s` is requesting a new session with name `daily-job`, we will reach this point and the user gets a http 400 bad request.
    But if a different user is requesting a session `s2` with name `daily-job`, the user gets a 403 Forbidden before reaching this point, because he/she does not have access to `/sessions/daily-job/`.

;;;","29/Sep/17 19:45;githubbot;Github user meisam commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141952416
  
    --- Diff: server/src/main/scala/org/apache/livy/server/batch/BatchSession.scala ---
    @@ -169,5 +174,5 @@ class BatchSession(
       override def infoChanged(appInfo: AppInfo): Unit = { this.appInfo = appInfo }
     
       override def recoveryMetadata: RecoveryMetadata =
    -    BatchRecoveryMetadata(id, appId, appTag, owner, proxyUser)
    +    BatchRecoveryMetadata(id, name, appId, appTag, owner, proxyUser)
    --- End diff --
    
    What do you suggest? Should we assign such sessions auto-generated names on recovery? Or should we leave them without a name?
;;;","29/Sep/17 19:53;githubbot;Github user meisam commented on the issue:

    https://github.com/apache/incubator-livy/pull/48
  
    > I think fail the request and return user the specific bad http code would be better then choosing another name for user. Because user want to create a session with name ""A"" for example, if Livy cannot satisfy user's requirement, then user should choose another name. It is better to let user to decide than letting Livy to handle it.
    
    I like failing the request option more. The only caveat is that with current approach, sometimes we submit the spark App and then kill it later (if two requests arrive at almost the same time, we launch both, but later when we want to add them to sessionManager, we have to keep only one and kill and and discard the other one).
    
    One way to avoid that is to refactor `Session`, `InteractiveSesion`, and `BatchSession` so it does not start the Spark process immediately, so we can start the Spark process after registering them in `SessionManager`.
    
    Are you okay with this approach?
;;;","29/Sep/17 22:52;githubbot;Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/48
  
    > Are you okay with this approach?
    
    I would have to see the change first but I think this could potentially be ok
;;;","30/Sep/17 01:17;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/48
  
    > I like failing the request option more. The only caveat is that with current approach, sometimes we submit the spark App and then kill it later (if two requests arrive at almost the same time, we launch both, but later when we want to add them to sessionManager, we have to keep only one and kill and and discard the other one).
    
    That's the problem of concurrency, I think we can first check the validity of session name, if it is conflicted, then directly return bad code. We should also postpone such check until session is created.
;;;","30/Sep/17 01:18;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141993541
  
    --- Diff: server/src/main/scala/org/apache/livy/server/batch/BatchSessionServlet.scala ---
    @@ -43,8 +44,23 @@ class BatchSessionServlet(
       override protected def createSession(req: HttpServletRequest): BatchSession = {
         val createRequest = bodyAs[CreateBatchRequest](req)
         val proxyUser = checkImpersonation(createRequest.proxyUser, req)
    +    val sessionId = sessionManager.nextId()
    +    val sessionName: String = createRequest.name match {
    +      case Some(name) if sessionManager.get(name).isEmpty =>
    +        name
    +      case Some(name) =>
    +        // this does NOT guarantee that by the time this session is ready to be registered in
    +        // sessionManager, another with the same name is not registered. But in most cases,
    +        // it prevents Livy from submitting applications to Spark.
    +        val msg = s""Session $name already exists! "" +
    +          s""Choose a different name or delete the existing session.""
    +        throw new IllegalArgumentException(msg)
    --- End diff --
    
    Yeah, that's true.
;;;","30/Sep/17 01:20;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/48#discussion_r141993619
  
    --- Diff: server/src/main/scala/org/apache/livy/server/batch/BatchSession.scala ---
    @@ -169,5 +174,5 @@ class BatchSession(
       override def infoChanged(appInfo: AppInfo): Unit = { this.appInfo = appInfo }
     
       override def recoveryMetadata: RecoveryMetadata =
    -    BatchRecoveryMetadata(id, appId, appTag, owner, proxyUser)
    +    BatchRecoveryMetadata(id, name, appId, appTag, owner, proxyUser)
    --- End diff --
    
    I'm saying with your changes, if new Livy server tries to recover from old data, what will be happened? Will it be failed? 
    
    Since the old recovery data doesn't have a session name field, so we could offer an auto-generated name, but we should not fail such scenario.
;;;","30/Sep/17 01:23;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/48
  
    > One way to avoid that is to refactor Session, InteractiveSesion, and BatchSession so it does not start the Spark process immediately, so we can start the Spark process after registering them in SessionManager.
    
    @meisam I would suggest to leave this work to another PR for the simplicity of review.
;;;","13/Oct/17 03:02;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/48
  
    @meisam can you please address the comments, thanks!
;;;","17/Oct/17 00:03;githubbot;Github user meisam commented on the issue:

    https://github.com/apache/incubator-livy/pull/48
  
    This is the output from last build:
    >No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.
    Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#Build-times-out-because-no-output-was-received
    The build has been terminated.
    
    Closing and re-opening this pull request to trigger a new build.
;;;","17/Oct/17 00:03;githubbot;Github user meisam closed the pull request at:

    https://github.com/apache/incubator-livy/pull/48
;;;","17/Oct/17 00:03;githubbot;GitHub user meisam reopened a pull request:

    https://github.com/apache/incubator-livy/pull/48

    [LIVY-41] Let users access sessions by session name

    This commit  enables Livy users to access sessions either by names or by auto-generated sessiond id's.
    
    It also prevents users from creating sessions that have the same name.
    
    This commit keeps API change  minimal. Thse are palces that API change
    is needed:
    - `Session` and its subclasses adds a new field, `name`.
    - `RecoveryMetadata` and its subclasses adds a new field, `name`.
    - `SessionManager` adds a new method `getSession(name: String)` which lookups sessions by name.
    
    A more clean implementation would change the signature of `SessionManager.register` so it returns a proper container around the session value to determine if it failed to register the given session.  For example,
    ```Scala
    def register(session S): Either[S, Throwable]
     ```
    
    Task-url: https://issues.apache.org/jira/browse/LIVY-41

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/meisam/incubator-livy LIVY-41-rebased

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/48.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #48
    
----
commit 8487bbbb155a132366f03b2d928cc2a0f9b94813
Author: Fathi Salmi, Meisam(mfathisalmi) <mfathisalmi@paypal.com>
Date:   2017-09-12T18:18:25Z

    [LIVY-41] Let users access sessions by session name
    
    This commit  enables Livy users to access sessions either by names or by auto-generated sessiond id's.
    
    It also prevents users from creating sessions that have the same name.
    
    This commit keeps API change  minimal. Thse are palces that API change
    is needed:
    - `Session` and its subclasses adds a new field, `name`.
    - `RecoveryMetadata` and its subclasses adds a new field, `name`.
    - `SessionManager` adds a new method `getSession(name: String)` which lookups sessions by name.
    
    A more clean implementation would change the signature of `SessionManager.register` so it returns a proper container around the session value to determine if it failed to register the given session.  For example,
    ```Scala
    def register(session S): Either[S, Throwable]
     ```
    
    Task-url: https://issues.apache.org/jira/browse/LIVY-41

commit 3f28b611ec7b587dbca2ff69cc99a7654d44b430
Author: Fathi Salmi, Meisam(mfathisalmi) <mfathisalmi@paypal.com>
Date:   2017-10-05T23:15:01Z

    [LIVY-41] renaming id tp idOrName
    
    Signed-off-by: Fathi Salmi, Meisam(mfathisalmi) <mfathisalmi@paypal.com>

commit dc644c7fbdb6da6ba2d573df9ea29cc2e04ff785
Author: Fathi Salmi, Meisam(mfathisalmi) <mfathisalmi@paypal.com>
Date:   2017-10-05T23:23:52Z

    [LIVY-41] refactoring guareded case statements
    
    Changing
    ```
    case ... if .... =>
    ```
    to
    ```
    case ... =>
      if ...
    ```

commit 967e896a0c5c4344de57dace5bc4fa62c9f3698a
Author: Fathi Salmi, Meisam(mfathisalmi) <mfathisalmi@paypal.com>
Date:   2017-10-10T03:30:17Z

    First shot at making brnach names optional

commit 0a0e1cf0f733b836461bb6add266836de03f5707
Author: Fathi Salmi, Meisam(mfathisalmi) <mfathisalmi@paypal.com>
Date:   2017-10-16T16:49:51Z

    [LIVY-41] Making session names optional
    
    If no session name is provided, do not use session names.
    This makes it possible to recover sessions that were stored with no
    name.

----
;;;","17/Oct/17 00:48;githubbot;Github user meisam commented on the issue:

    https://github.com/apache/incubator-livy/pull/48
  
    > @meisam can you please address the comments, thanks!
    
    @jerryshao I addressed comments in the previous commit.
;;;","05/Feb/19 19:25;vanzin;Issue resolved by pull request 48
[https://github.com/apache/incubator-livy/pull/48];;;",,,,,,,,,,,,,,,,,
Ability to create SQL Kind Session on Livy Job Server,LIVY-40,13095678,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Duplicate,,prabhu1984,gmcdonald,29/Dec/15 20:49,12/May/16 00:11,19/Dec/25 04:16,12/May/16 00:11,0.1,,,,,Core,,,,,,,,,,1,Spark-SQL,SQL,,,,"Currently Livy Job Server supports only PySpark and Scala Spark kind sessions to create and submit codes.
curl -X POST --data '
{""kind"": ""pyspark""}
' -H ""Content-Type: application/json"" localhost:8998/sessions
curl -X POST --data '
{""kind"": ""spark""}
' -H ""Content-Type: application/json"" localhost:8998/sessions
It would be better, if we are able to create '
{""kind"": ""sql""}
' kind session to submit spark sql through hiveContext.
eg: 
curl -X POST --data '
{""kind"": ""sql""}
' -H ""Content-Type: application/json"" localhost:8998/sessions
curl localhost:8998/sessions/0/statements -X POST -H 'Content-Type: application/json' -d '
{""code"":""INSERT OVERWRITE TABLE target SELECT cols FROM source""}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 12 00:11:37 UTC 2016,,,,,,,,,,"0|i3iyp3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/May/16 19:47;prabhu1984;Any updates on this issue. We are trying to migrate lot of hive queries to spark. And, we desperately looking for a SQL Kind Session to submit sql queries directly to hive context. Currently, we are executing each sql query through 4 steps like #-- Step1: Creating new Spark (Scala) Session and get status of created session, #-- Step2: If idle, then change created session context into HiveContext and check status, #-- Step3: If ok, then execute any spark sql, #-- Step4: Closing Spark (Scala) Session. We are looking for a user-friendly session kind.;;;","12/May/16 00:11;vanzin;Already tracked in LIVY-19 (just picking the older entry).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate Livy to using Spark Launcher Library,LIVY-39,13095677,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,GayathriMurali,kostas,gmcdonald,21/Dec/15 20:57,02/Mar/16 18:39,19/Dec/25 04:16,02/Mar/16 18:39,0.1,,,0.2,,Core,,,,,,,,,,0,,,,,,"Livy has custom code that launches the interpreter and remote context. This should instead use the Spark Launcher library. As of Spark 1.6, the launcher library also enables the programatic returning of application Id so that parsing the logging messages is not needed anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 02 18:39:13 UTC 2016,,,,,,,,,,"0|i3iyov:",9223372036854775807,,,,,,,,,,,,,,,,,,,"25/Jan/16 20:40;GayathriMurali;Pull request : https://github.com/cloudera/livy/pull/39

;;;","28/Jan/16 01:06;tc0312;I'm working on Livy HA and need access to the YARN application id.

Are we going to migrate process mode and yarn mode to use SparkLauncher too?
Will Livy always call SparkLauncher.startApplication() and thus always require Spark 1.6? Or will Livy automatically fallback to SparkLauncher.start() on Spark < 1.5?

Thanks.;;;","02/Mar/16 18:39;vanzin;Commit [d6515627|https://github.com/cloudera/livy/commit/d65156275a668bf0cd7f50cdc4ec460121000ac1] by  GayathriMurali <gayathri.m.softie@...> in cloudera/livy:
{code}
LIVY-39. Migrate Livy to use Spark Launcher Library

Removed Hashmap and modified code for values to be added directly to launcher.Fixed style issues

Closes #79
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backports missing on the new Livy repo,LIVY-35,13095673,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Won't Fix,romainr,romainr,gmcdonald,08/Dec/15 06:05,23/Feb/16 23:47,19/Dec/25 04:16,23/Feb/16 23:47,0.1,,,,,Core,,,,,,,,,,0,,,,,,"To cherry-pick on the new repo:

https://github.com/cloudera/hue/pull/276",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 23 23:47:39 UTC 2016,,,,,,,,,,"0|i3iynz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/Feb/16 23:47;vanzin;Users can add that to the catch all map available in the request object. I'm more fond of LIVY-28 which would get rid of all these explicit options.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Livy is returning bad ""Location: "" headers",LIVY-34,13095672,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Trivial,Fixed,erenavsarogullari,erickt,gmcdonald,20/Nov/15 04:36,02/Mar/16 19:49,19/Dec/25 04:16,23/Feb/16 19:02,0.1,,,0.2,,Core,,,,,,,,,,0,,,,,,"The session creation response is currently returning a bad {{Location}} header:

{code}
% http POST localhost:8998/sessions kind:='""pyspark""'
HTTP/1.1 201 Created
Content-Length: 53
Content-Type: application/json; charset=UTF-8
Date: Fri, 20 Nov 2015 04:31:49 GMT
Location: /0
Server: Jetty(9.2.z-SNAPSHOT)

{
    ""id"": 0,
    ""kind"": ""pyspark"",
    ""log"": [],
    ""state"": ""starting""
}
{code}

It should instead be returning {{/sessions/0}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 23 19:02:17 UTC 2016,,,,,,,,,,"0|i3iynr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"31/Jan/16 19:26;erenavsarogullari;Hi Livy Team,

I would like to start with this issue as the first commit. Please assign to me if it is possible ;)

Cheers,
Eren;;;","23/Feb/16 19:02;vanzin;Fixed by @erenavsarogullari, but I can't yet assign issues to him (will look at fixing that later).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[livy] Get livy to compile with Apache Spark 1.4.x and 1.5.x,LIVY-32,13095670,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,erickt,erickt,gmcdonald,04/Nov/15 01:01,20/Dec/15 08:54,19/Dec/25 04:16,06/Nov/15 01:34,0.1,,,,,Core,,,,,,,,,,0,,,,,,"Right now livy is pinned to the latest CDH version of Spark, but not everyone is using CDH's spark. We should make sure that livy can be built and pass all the tests with {{mvn package -Dspark.version=...}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 06 01:34:09 UTC 2015,,,,,,,,,,"0|i3iynb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Nov/15 00:58;erickt;Implemented in https://review.cloudera.org/r/6304/.;;;","06/Nov/15 01:34;erickt;landed in https://github.com/cloudera/hue/commit/e0f40d6e398a583eca6d12ef44cfb8df83159701.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add shutdown hook to make sure sessions are destroyed,LIVY-31,13095669,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,vanzin,erickt,gmcdonald,02/Nov/15 18:07,12/May/16 17:45,19/Dec/25 04:16,12/May/16 17:45,0.1,,,0.2,,Core,,,,,,,,,,0,,,,,,"Right now if you use ""ctrl-c"" to kill the livy server, it won't always kill all of it's sessions depending on which thread received the signal. So to be safe, we should add a [Runtime.addShutdownHook|http://docs.oracle.com/javase/6/docs/api/java/lang/Runtime.html#addShutdownHook%28java.lang.Thread%29] that also tries to kill the sessions before the JVM is killed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu May 12 17:45:57 UTC 2016,,,,,,,,,,"0|i3iyn3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"02/Nov/15 21:07;romainr;We should won't fix this one and do the DB persistence instead;;;","20/Dec/15 09:53;kostas;yes, once you have multiple Livy servers running, this kind of thing gets tricky. It is not clear what the right procedure here is in that circumstance. ;;;","11/May/16 23:37;vanzin;I think we should still add this for the case where HA / recovery is not enabled (a.k.a. the current state of things). Otherwise when Livy stops you'll end up with a bunch of orphan sessions that you can't really use, and have to manually kill using YARN's tools.;;;","12/May/16 17:45;vanzin;Commit [174851da|https://github.com/cloudera/livy/commit/174851da3e22fbe14647654c775197112ef80dc8] by  Marcelo Vanzin <vanzin@...> in cloudera/livy:
{code}
LIVY-31. Close sessions when server shuts down.

Without HA or recovery, not doing this means leaving orphan sessions
that will just hog resources and cannot be used, requiring people to
use YARN's (or other) tools to shut them down.

This change adds a shutdown hook that stops all the active sessions
when the server shuts down. This requires doing more work in a shutdown
hook than is generally palatable, but the alternative is to have some
explicit command (via REST API? monitoring local file? something else?)
to shut down the server, which we don't have.

Closes #131
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[livy] Make livy.repl.callback_url a parameter instead of java-opt,LIVY-30,13095668,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,linchan,linchan,gmcdonald,30/Oct/15 20:53,20/Dec/15 08:54,19/Dec/25 04:16,06/Nov/15 19:05,0.1,,,,,Core,,,,,,,,,,0,,,,,,"Currently livy.repl.callback_url is passed to the interactive mode as a java-opt. This made it incompatible with HDP clusters where they require a hdp.version to be included in java-opt.

We can pass that as a parameter instead of a java-opt. Also removing the livy.repl.port as it is not used.

See this thread (https://groups.google.com/a/cloudera.org/forum/#!topic/hue-user/UeYAW7AlpNA) for a little more details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 06 19:05:55 UTC 2015,,,,,,,,,,"0|i3iymv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"30/Oct/15 22:12;romainr;Nice! Assigning to Erick for review, then we can re-assign to Lin when closing it!;;;","02/Nov/15 23:01;erickt;I've tweaked the patch a little to switch the callback_url into environment variable: https://review.cloudera.org/r/6263/. This allows us to keep running {{./bin/livy-repl pyspark}} for debugging purposes.;;;","06/Nov/15 17:53;romainr;Is it in?;;;","06/Nov/15 19:05;erickt;@romain: yep, it was a part of https://github.com/cloudera/hue/commit/af01a4359ab6610195c15860ce6e0cdb6d6f60cd

@Lin: Unfortunately it turned out passing the callback url as an argument broke the tests and {{bin/livy-repl}}, so I rewrote the patch to make sure that setting the callback url didn't prevent others from setting the {{spark.driver.extraJavaOptions}} config option.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow admins to configure the spark.master,LIVY-29,13095667,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Not A Bug,,erickt,gmcdonald,30/Oct/15 20:38,23/Feb/16 23:49,19/Dec/25 04:16,23/Feb/16 23:49,0.1,,,,,Core,,,,,,,,,,0,,,,,,"Right now the {{spark.master}} is implicitly set by the {{livy.server.session.factory}}, but this means that users that want to submit spark batch jobs with the {{spark.master=yarn-client}} cannot. Given that the {{process}} and {{yarn}} modes are nearly identical, we should just get rid of those modes and allow an admin to specify the {{spark.master}}.

https://groups.google.com/a/cloudera.org/forum/#!topic/hue-user/7byMDkOhDRg",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 23 23:49:32 UTC 2016,,,,,,,,,,"0|i3iymn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/Feb/16 23:49;vanzin;This is already implemented through other changes. Just set ""spark.master"" in your Spark configuration (either the default Spark config, or by setting a custom {{SPARK_CONF_DIR}} when starting Livy).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[livy] Support java parameters,LIVY-27,13095665,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Not A Bug,erickt,romainr,gmcdonald,27/Oct/15 11:34,20/Dec/15 09:46,19/Dec/25 04:16,20/Dec/15 09:46,0.1,,,,,Core,,,,,,,,,,0,,,,,,,https://github.com/cloudera/hue/issues/244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Dec 20 09:46:35 UTC 2015,,,,,,,,,,"0|i3iym7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Dec/15 09:46;kostas;From the github issue it looks like this isn't an issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[livy] Add SSL support,LIVY-26,13095664,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,erickt,erickt,gmcdonald,27/Oct/15 04:24,20/Dec/15 08:54,19/Dec/25 04:16,27/Oct/15 16:08,0.1,,,,,Core,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 27 16:10:27 UTC 2015,,,,,,,,,,"0|i3iylz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Oct/15 04:27;erickt;Implemented in https://review.cloudera.org/r/6224/;;;","27/Oct/15 16:08;erickt;Committed in https://github.com/cloudera/hue/commit/01926a28e12851fb0868391c7030d5366c692cd0;;;","27/Oct/15 16:10;erickt;This command can generate the keystore:

{code}
keytool -genkey -alias sitename -keyalg RSA -keystore keystore.jks -keysize 2048
{code}

It will prompt you for a password and some other certificate information.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python batch mode does not print any stderr,LIVY-25,13095663,,Bug,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Not A Bug,,romainr,gmcdonald,26/Oct/15 15:03,16/May/16 18:45,19/Dec/25 04:16,16/May/16 18:45,0.1,,,,,Core,,,,,,,,,,0,,,,,,"{code}
spark-submit --class org.apache.spark.examples.SparkPi /usr/lib/spark/lib/spark-examples.jar
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/lib/flume-ng/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
15/10/26 11:29:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/10/26 11:29:45 WARN Utils: Your hostname, unreal resolves to a loopback address: 127.0.1.1; using 10.198.12.233 instead (on interface wlan0)
15/10/26 11:29:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
15/10/26 11:29:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
15/10/26 11:29:46 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
Pi is roughly 3.1365                                                            
{code}


{code}
curl -X POST --data '{""file"": ""/user/romain/pi.py""}' -H ""Content-Type: application/json"" localhost:8998/batches


romain@unreal:~$ curl localhost:8998/batches/4 |  python -m json.tool
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   502    0   502    0     0  53978      0 --:--:-- --:--:-- --:--:-- 55777
{
    ""id"": 4,
    ""log"": [
        ""\t diagnostics: N/A"",
        ""\t ApplicationMaster host: 192.168.0.38"",
        ""\t ApplicationMaster RPC port: 0"",
        ""\t queue: root.romain"",
        ""\t start time: 1445871101568"",
        ""\t final status: SUCCEEDED"",
        ""\t tracking URL: http://unreal:8088/proxy/application_1444917524249_0037/A"",
        ""\t user: romain"",
        ""15/10/26 07:51:52 INFO util.ShutdownHookManager: Shutdown hook called"",
        ""15/10/26 07:51:52 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f7874135-80e6-4324-aa8a-aeecee8b72ce""
    ],
    ""state"": ""success""
}
romain@unreal:~$ curl localhost:8998/batches/4/log |  python -m json.tool
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  5686    0  5686    0     0   609k      0 --:--:-- --:--:-- --:--:--  616k
{
    ""from"": 0,
    ""id"": 4,
    ""log"": [
        ""SLF4J: Class path contains multiple SLF4J bindings."",
        ""SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]"",
        ""SLF4J: Found binding in [jar:file:/usr/lib/flume-ng/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]"",
        ""SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation."",
        ""SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]"",
        ""15/10/26 07:51:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"",
        ""15/10/26 07:51:39 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032"",
        ""15/10/26 07:51:40 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers"",
        ""15/10/26 07:51:40 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)"",
        ""15/10/26 07:51:40 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead"",
        ""15/10/26 07:51:40 INFO yarn.Client: Setting up container launch context for our AM"",
        ""15/10/26 07:51:40 INFO yarn.Client: Setting up the launch environment for our AM container"",
        ""15/10/26 07:51:40 INFO yarn.Client: Preparing resources for our AM container"",
        ""15/10/26 07:51:40 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded."",
        ""15/10/26 07:51:40 INFO yarn.Client: Uploading resource file:/usr/lib/spark/lib/spark-assembly-1.5.0-cdh5.7.0-SNAPSHOT-hadoop2.6.0-cdh5.7.0-SNAPSHOT.jar -> hdfs://localhost:8020/user/romain/.sparkStaging/application_1444917524249_0037/spark-assembly-1.5.0-cdh5.7.0-SNAPSHOT-hadoop2.6.0-cdh5.7.0-SNAPSHOT.jar"",
        ""15/10/26 07:51:41 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/romain/pi.py"",
        ""15/10/26 07:51:41 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://localhost:8020/user/romain/.sparkStaging/application_1444917524249_0037/pyspark.zip"",
        ""15/10/26 07:51:41 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.8.2.1-src.zip -> hdfs://localhost:8020/user/romain/.sparkStaging/application_1444917524249_0037/py4j-0.8.2.1-src.zip"",
        ""15/10/26 07:51:41 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/romain/pyspark_csv.py"",
        ""15/10/26 07:51:41 INFO yarn.Client: Uploading resource file:/tmp/spark-f7874135-80e6-4324-aa8a-aeecee8b72ce/__spark_conf__1440053055516147207.zip -> hdfs://localhost:8020/user/romain/.sparkStaging/application_1444917524249_0037/__spark_conf__1440053055516147207.zip"",
        ""15/10/26 07:51:41 INFO spark.SecurityManager: Changing view acls to: romain"",
        ""15/10/26 07:51:41 INFO spark.SecurityManager: Changing modify acls to: romain"",
        ""15/10/26 07:51:41 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(romain); users with modify permissions: Set(romain)"",
        ""15/10/26 07:51:41 INFO yarn.Client: Submitting application 37 to ResourceManager"",
        ""15/10/26 07:51:41 INFO impl.YarnClientImpl: Submitted application application_1444917524249_0037"",
        ""15/10/26 07:51:42 INFO yarn.Client: Application report for application_1444917524249_0037 (state: ACCEPTED)"",
        ""15/10/26 07:51:42 INFO yarn.Client: "",
        ""\t client token: N/A"",
        ""\t diagnostics: N/A"",
        ""\t ApplicationMaster host: N/A"",
        ""\t ApplicationMaster RPC port: -1"",
        ""\t queue: root.romain"",
        ""\t start time: 1445871101568"",
        ""\t final status: UNDEFINED"",
        ""\t tracking URL: http://unreal:8088/proxy/application_1444917524249_0037/"",
        ""\t user: romain"",
        ""15/10/26 07:51:43 INFO yarn.Client: Application report for application_1444917524249_0037 (state: ACCEPTED)"",
        ""15/10/26 07:51:44 INFO yarn.Client: Application report for application_1444917524249_0037 (state: ACCEPTED)"",
        ""15/10/26 07:51:45 INFO yarn.Client: Application report for application_1444917524249_0037 (state: ACCEPTED)"",
        ""15/10/26 07:51:46 INFO yarn.Client: Application report for application_1444917524249_0037 (state: RUNNING)"",
        ""15/10/26 07:51:46 INFO yarn.Client: "",
        ""\t client token: N/A"",
        ""\t diagnostics: N/A"",
        ""\t ApplicationMaster host: 192.168.0.38"",
        ""\t ApplicationMaster RPC port: 0"",
        ""\t queue: root.romain"",
        ""\t start time: 1445871101568"",
        ""\t final status: UNDEFINED"",
        ""\t tracking URL: http://unreal:8088/proxy/application_1444917524249_0037/"",
        ""\t user: romain"",
        ""15/10/26 07:51:47 INFO yarn.Client: Application report for application_1444917524249_0037 (state: RUNNING)"",
        ""15/10/26 07:51:48 INFO yarn.Client: Application report for application_1444917524249_0037 (state: RUNNING)"",
        ""15/10/26 07:51:49 INFO yarn.Client: Application report for application_1444917524249_0037 (state: RUNNING)"",
        ""15/10/26 07:51:50 INFO yarn.Client: Application report for application_1444917524249_0037 (state: RUNNING)"",
        ""15/10/26 07:51:51 INFO yarn.Client: Application report for application_1444917524249_0037 (state: RUNNING)"",
        ""15/10/26 07:51:52 INFO yarn.Client: Application report for application_1444917524249_0037 (state: FINISHED)"",
        ""15/10/26 07:51:52 INFO yarn.Client: "",
        ""\t client token: N/A"",
        ""\t diagnostics: N/A"",
        ""\t ApplicationMaster host: 192.168.0.38"",
        ""\t ApplicationMaster RPC port: 0"",
        ""\t queue: root.romain"",
        ""\t start time: 1445871101568"",
        ""\t final status: SUCCEEDED"",
        ""\t tracking URL: http://unreal:8088/proxy/application_1444917524249_0037/A"",
        ""\t user: romain"",
        ""15/10/26 07:51:52 INFO util.ShutdownHookManager: Shutdown hook called"",
        ""15/10/26 07:51:52 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f7874135-80e6-4324-aa8a-aeecee8b72ce""
    ],
    ""total"": 68
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 16 18:45:51 UTC 2016,,,,,,,,,,"0|i3iylr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Oct/15 15:23;romainr;Is blocking HUE-2948;;;","16/May/16 18:45;vanzin;The spark-submit invocation is running in yarn-client mode. The Livy invocation is running in yarn-cluster mode. You don't see stdout / stderr of applications in yarn-cluster mode - that's expected, since the application is running somewhere remotely.

Batch applications, especially those running through Livy, should not communicate results through stdout / stderr.

LIVY-55 covers exposing YARN logs through Livy, which would allow you to see this output, though.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Spark configured with less verbose logging,LIVY-24,13095662,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,romainr,gmcdonald,26/Oct/15 14:47,09/May/16 18:28,19/Dec/25 04:16,09/May/16 18:28,0.1,,,,,Core,,,,,,,,,,0,,,,,,"{code}
Difficult to see pi when running a batch jar:

/usr/lib/spark/lib/spark-examples.jar
org.apache.spark.examples.SparkPi

UI: Probably make snippet logs resizable?

spark-submit --class org.apache.spark.examples.SparkPi /usr/lib/spark/lib/spark-examples.jar

UI/Livy: maybe we could display stdout/stderr, and not the logging:
Pi is roughly 3.1408416

and get rid of:
15/10/21 01:38:41 INFO
{code}

{code}
You could tweak the spark configuration to be less noisy, but I'm not sure if there's an easy way to get spark-submit let noisy from just the commandline. Here's some docs about it:

http://stackoverflow.com/questions/25193488/how-to-turn-off-info-logging-in-pyspark
{code}

{code}
So I tried but it seems to indeed print less but the Livy fails for batch, the status is always 'failed', will create a jira
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 18:28:16 UTC 2016,,,,,,,,,,"0|i3iylj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Oct/15 14:53;romainr;{code}
curl -X POST --data '{""file"": ""/user/romain/demo_import.py"", ""pyFiles"": ""/user/romain/pyspark_csv.py""}' -H ""Content-Type: application/json"" localhost:8998/batches




batch

logs?
MR logs?curl localhost:8998/batches/2 |  python -m json.tool
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   769    0   769    0     0   126k      0 --:--:-- --:--:-- --:--:--  150k
{
    ""id"": 2,
    ""log"": [
        ""SLF4J: Class path contains multiple SLF4J bindings."",
        ""SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]"",
        ""SLF4J: Found binding in [jar:file:/usr/lib/flume-ng/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]"",
        ""SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation."",
        ""SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]"",
        ""15/10/26 04:26:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"",
        ""15/10/26 04:26:23 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.""
    ],
    ""state"": ""error""
{code};;;","09/May/16 18:28;vanzin;Livy doesn't read Spark's output anymore, so this is just a matter of configuring Spark's log4j.properties.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Status report shouldn't rely on spark-submit console log level,LIVY-23,13095661,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,linchan,gmcdonald,23/Oct/15 23:59,23/Feb/16 23:50,19/Dec/25 04:16,23/Feb/16 23:50,0.1,,,,,Core,,,,,,,,,,0,,,,,,"Basically if we run in yarn mode, after spark-submit, Livy needs the application ID from yarn to track job progress. Currently the way for it to get the application ID is to parse the console output from spark-submit. It looks for a string like this ""Application report for application_1445560046042_0021 (state: ACCEPTED)"" to retrieve the app ID. Then it will transition into running state and start to track progress from talking to YARN.

If console logging is set to WARN level, that particular line will not be emitted. As a result, livy stuck in starting state until spark-submit finishes. It will then go into error state. Also, if the job is stuck in ""starting"" state, the job cannot be killed. The DELETE call will simply timeout. It is because without App ID, livy cannot tell YARN to kill and it simply waits for the state transition.

Relying on the console output might not be a very reliable design so let's figure out a better way to get the App ID.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Feb 23 23:50:54 UTC 2016,,,,,,,,,,"0|i3iylb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Dec/15 09:38;kostas;[~linchan] I think we talked about this and this can be solved by using the new launcher library in Spark 1.6. ;;;","23/Feb/16 23:50;vanzin;I'm gonna close this since that code does not exist anymore in master. As Kostas mentions, we could get this info using the Spark 1.6 API, or by talking directly to YARN.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[livy] Support pyFiles properties,LIVY-20,13095658,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,erickt,romainr,gmcdonald,19/Oct/15 06:42,20/Dec/15 08:54,19/Dec/25 04:16,06/Nov/15 01:38,0.1,,,,,Core,,,,,,,,,,0,,,,,,"https://github.com/cloudera/hue/issues/244

{code}
With a string or garbabe it boots then stay in running:

curl -X POST --data '{""kind"": ""pyspark"", ""pyFiles"": ""hdfs://localhost:8020/user/romain/dwdwdwpyspark_csv.py""}' -H ""Content-Type: application/json"" localhost:8998/sessions

        {
            ""id"": 3,
            ""kind"": ""pyspark"",
            ""log"": [
                ""15/10/08 17:56:56 INFO yarn.Client: Application report for application_1444345227311_0020 (state: ACCEPTED)"",
                ""15/10/08 17:56:57 INFO yarn.Client: Application report for application_1444345227311_0020 (state: ACCEPTED)"",
                ""15/10/08 17:56:58 INFO yarn.Client: Application report for application_1444345227311_0020 (state: ACCEPTED)"",
                ""15/10/08 17:56:59 INFO yarn.Client: Application report for application_1444345227311_0020 (state: ACCEPTED)"",
                ""15/10/08 17:57:00 INFO yarn.Client: Application report for application_1444345227311_0020 (state: ACCEPTED)"",
                ""15/10/08 17:57:01 INFO yarn.Client: Application report for application_1444345227311_0020 (state: ACCEPTED)"",
                ""15/10/08 17:57:02 INFO yarn.Client: Application report for application_1444345227311_0020 (state: ACCEPTED)"",
                ""15/10/08 17:57:03 INFO yarn.Client: Application report for application_1444345227311_0020 (state: ACCEPTED)"",
                ""15/10/08 17:57:04 INFO yarn.Client: Application report for application_1444345227311_0020 (state: ACCEPTED)"",
                ""15/10/08 17:57:05 INFO yarn.Client: Application report for application_1444345227311_0020 (state: ACCEPTED)""
            ],
           
curl localhost:8998/sessions/2/statements -X POST -H 'Content-Type: application/json' -d '{""code"":""1 + 1""}'
java.lang.IllegalStateException: Session is in state starting



I give a list as expected I get:

curl -X POST --data '{""kind"": ""pyspark"", ""pyFiles"": [""hdfs://localhost:8020/user/romaindwdwd/pyspark_csv.py""]}' -H ""Content-Type: application/json"" localhost:8998/sessions


        {
            ""id"": 1,
            ""kind"": ""pyspark"",
            ""log"": [
                ""Error: --py-files given but primary resource is not a Python script"",
                ""Run with --help for usage help or --verbose for debug output""
            ],
            ""state"": ""error""
        },
{code}

{code}
`spark-submit ... --py-files ...` is explicitly checking that the program it's executing ends with .py. Livy with pyspark actually is executing a jar file, hence this issue. We'll need to find another way to pass along the py-files.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 06 01:38:02 UTC 2015,,,,,,,,,,"0|i3iykn:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Nov/15 00:58;erickt;Implemented in https://review.cloudera.org/r/6304/.;;;","06/Nov/15 01:38;erickt;landed in https://github.com/cloudera/hue/commit/e0f40d6e398a583eca6d12ef44cfb8df83159701.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow pluggable Filesystems,LIVY-36,13095674,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,erickt,gmcdonald,15/Oct/15 17:34,09/May/16 18:26,19/Dec/25 04:16,09/May/16 18:26,0.1,,,,,Core,,,,,,,,,,1,,,,,,"This is a placeholder ticket to track a possible filesystem API for Livy. This plugin would expose a simple API that abstracts a variety of filesystem backends to simplify starting a job/session.

Open Questions:

* Is this necessary? Would it be better to assume the user separately uploads whatever they need to the local filesystem/HDFS/Azure/S3/etc?
* Do we need to support multiple filesystems, or would it be fine to just have livy configured to use one?
* For simplicity, this just has a flat namespace, but would it be better to expose a hierarchal namespace?

[~linchan] said in HUE-3007:

--
This is a great proposal. For the versioning, Azure REST APIs use a special field in header (x-ms-version). I believe that is more like your option 3 above. Personally I might like that better but is willing to adapt to whatever options you decides on.

On fs choice, I think majority will be fine with one FS configured. However, the flexibility of mix and match different types could be very useful in some limited cases. Don't think we need to implement it in first version but would be good if the design will allow that extensibility later.
--",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 18:26:55 UTC 2016,,,,,,,,,,"0|i3iyo7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"26/Oct/15 04:52;linchan;Thanks for opening this. We recently thought of a case where supporting multiple file system might be useful. For example, user job could be implemented in a jar which could be uploaded to hdfs/azure storage. However, there could be some reference jars which already exists on the Livy server node under local file system. It would be inconvenient if those reference jars have to be in the same file system as the job jar. In this case, supporting multiple FS could be very useful.
Thanks;;;","09/May/16 18:26;vanzin;I'm reasonably sure that whatever is being proposed here is already handled by the Hadoop APIs. As long as you have the appropriate {{FileSystem}} implementation in your Livy and Spark classpaths, you can tell them to read and write to that fs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Spark SQL support,LIVY-19,13095657,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,jerryshao,erickt,gmcdonald,15/Oct/15 15:56,16/Jan/18 22:04,19/Dec/25 04:16,05/Dec/17 01:30,0.1,,,0.5.0,,Core,,,,,,,,,,0,,,,,,"We should have a Spark SQL mode to better support workflows like [Mojito|https://www.appsflyer.com/blog/meet-mojito-our-new-not-just-support-tool/#.VdsgUjlidaA.twitter].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-194,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Dec 05 01:30:35 UTC 2017,,,,,,,,,,"0|i3iykf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"12/May/16 19:53;prabhu1984;Any updates on this issue? Please increase priority, as it sitting idle for more than 7 months.;;;","12/May/16 20:02;vanzin;It's just not in our current plans to work on this. If you feel like it's really important, you're welcome to write a spec and provide an implementation. But currently it's not in our to do list. We're leaving the bug open in case anyone feels interested in contributing code to implement it.;;;","10/Jun/16 16:31;meisamf;I added a SparkSqlInterpreter that can executed SQL queries on top of Hive. I can contribute it to Livy if there is interest in the community for this feature. I'll send an email to the dev mailing list.;;;","12/Jun/16 03:32;prabhu1984;@Marcelo Vanzin: Can you please review Meisam pull requests and do the needful? We would be happy, if we get this feature implemented. Thanks!;;;","10/Nov/16 18:32;meisam;I'll send a new pull request. The old pull request has been sitting there for a few months now and is staled.;;;","05/Dec/17 01:30;jerryshao;Issue resolved by pull request 68
[https://github.com/apache/incubator-livy/pull/68];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[livy] Support --conf property,LIVY-18,13095656,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,erickt,romainr,gmcdonald,14/Oct/15 23:57,20/Dec/15 08:54,19/Dec/25 04:16,02/Nov/15 22:19,0.1,,,,,Core,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Nov 02 22:19:35 UTC 2015,,,,,,,,,,"0|i3iyk7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"27/Oct/15 16:13;erickt;Copying my comment from my review:

I left out {{--conf}} on purpose because I think it could be used to upload files livy can read into the YARN cluster node. For example, if you do {{spark-submit --conf spark.files=key.pem,cert.pem foo.jar}} could upload the SSL certificate into the YARN working directory. We could blacklist spark.files, spark.jars, and the like, we'd have to be quite vigilant to catch other settings. Instead, we should have a whitelist for safe options.;;;","27/Oct/15 16:38;romainr;https://review.cloudera.org/r/6212/;;;","27/Oct/15 16:40;romainr;How about we just disable the 'spark.files' property?;;;","27/Oct/15 16:44;erickt;I'd rather we add a config option that specifies the ""safe"" spark config options so then we don't need to maintain a safe list. I'm working on a patch to do this right now, just give me a moment :);;;","02/Nov/15 22:19;erickt;reviewed in https://review.cloudera.org/r/6260/, landed in https://github.com/cloudera/hue/commit/a4659f3a528922467e8266a75674bf40a70fcc9c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[livy] Factor Livy out into it's own repository,LIVY-17,13095655,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,erickt,erickt,gmcdonald,08/Oct/15 20:04,20/Dec/15 09:31,19/Dec/25 04:16,20/Dec/15 09:31,0.1,,,0.2,,Core,,,,,,,,,,1,,,,,,It may be easier for external people to contribute to Livy if it has it's own repository / top-level bug ticket system.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Dec 20 09:31:12 UTC 2015,,,,,,,,,,"0|i3iyjz:",9223372036854775807,,,,,,,,,,,,,,,,,,,"08/Oct/15 20:25;erickt;When we factor out Livy, we should make sure to it up with http://gerrit.cloudera.org. Furthermore, it would be nice if we could run the unit tests after a patch is approved, and automatically merge it in if all the tests pass. It'd also be nice if external contributors could see the result of the unit tests to assist in test failures. I'm not sure if our gerrit setup supports this.;;;","20/Dec/15 09:31;kostas;The new repo is located here: https://github.com/cloudera/livy;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[livy] Version number Livy separately from the Hue project,LIVY-16,13095654,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,erickt,romainr,gmcdonald,08/Oct/15 18:44,20/Dec/15 08:54,19/Dec/25 04:16,12/Oct/15 22:04,0.1,,,,,Core,,,,,,,,,,1,,,,,,Could get ride of un-used https://github.com/cloudera/hue/blob/master/apps/spark/java/-version at the same time,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon Oct 12 22:04:34 UTC 2015,,,,,,,,,,"0|i3iyjr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"08/Oct/15 19:59;erickt;Since we don't yet consider Livy to be GA yet, I'd like to consider dropping the Livy version number to below 1.0. I'd also like to transition over into [semver|http://semver.org/], where we're strict about incrementing the major version whenever we decide to make a breaking change.;;;","09/Oct/15 16:54;linchan;When will this decision be made? I am just wondering what version number I should tag Livy with in our experimental builds.

Thanks
Lin;;;","12/Oct/15 19:08;erickt;I submitted https://review.cloudera.org/r/6110/ for review, which downgrades livy to a relatively arbitrary 0.2.0.;;;","12/Oct/15 22:04;erickt;Livy's version has been switched to 0.2.0 in https://github.com/cloudera/hue/commit/35d6eb7d11f9ffee8294f4fe423dc1da274246c0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add versioning to the API,LIVY-15,13095653,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Blocker,Fixed,tc0312,erickt,gmcdonald,08/Oct/15 16:21,04/Feb/16 21:10,19/Dec/25 04:16,04/Feb/16 21:10,0.1,,,0.2,,API,,,,,,,,,,1,,,,,,The Livy API should have versioning in it's api in order to let it evolve over time.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Feb 04 21:10:35 UTC 2016,,,,,,,,,,"0|i3iyjj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"08/Oct/15 19:55;erickt;There really are two aspects to this JIRA ticket. First is we need to decide on a mechanism for how we will version our APIs. Second is that we need to decide how we should change our APIs now to make it more flexible to evolve without needing to break our API.

For the versioning scheme, I can see three common approaches. First is embedding the version number in the API, which is done in many of the Hadoop APIs. For example, [Oozie|https://oozie.apache.org/docs/4.2.0/WebServicesAPI.html#Versions_End-Point] has urls like {{/oozie/v1/admin/status}} and {{/oozie/v2/admin/metrics}}. Second is [Github API|https://developer.github.com/v3/] uses an optional [Accept header|https://developer.github.com/v3/media/#request-specific-version] that looks like {{Accept: application/vnd.github.v3+json}}. Third is using a custom header, like [Stripe|https://stripe.com/docs/api#versioning]. There are [many|http://www.vinaysahni.com/best-practices-for-a-pragmatic-restful-api], [many|https://stackoverflow.com/questions/389169/best-practices-for-api-versioning] [arguments|http://www.troyhunt.com/2014/02/your-api-versioning-is-wrong-which-is.html] on what is the right approach. Personally I lean towards Github's approach, which I'll get into in a second.

On stabilizing a particular API, [~romain] and I talked about the following URL scheme:

{code}
/spark
/spark/sessions
/spark/sessions/uuid
/spark/sessions/uuid/log
/spark/sessions/uuid/statements
/spark/jobs/uuid
/spark/jobs/uuid/log

/spark/contexts
/spark/contexts/uuid

/fs
/fs/uuid

/yarn
/yarn/jobs
/yarn/jobs/uuid
/yarn/jobs/uuid/log

/julia
/julia/sessions
/julia/sessions/uuid
/julia/sessions/uuid/log
/julia/sessions/uuid/statements

/R
/R/sessions
/R/sessions/uuid
/R/sessions/uuid/log
/R/sessions/uuid/statements
{code}

A couple comments:

* This sketches out how the Livy API could evolve over time and grow support for other things livy could grow to support. It splits the Livy URLs into a series of conceptual plugins. All things related to Spark would live under the {{/spark}} namespace, whereas all the URLs necessary for launching a standard yarn job could live under {{/yarn}}.

* On {{/spark}}:
** {{/spark/contexts}} is hypothetical URL that allows for an [Oyala jobserver-like|https://github.com/ooyala/spark-jobserver] endpoint, where one can create a spark context that could then be shared amongst the various spark jobs/sessions. This named context would just be passed in as an argument, just like the filesystem paths are passed in now.
** Question: Is there a better name for {{../sessions}}?

* On {{/fs}}:
** This plugin would expose a simple API that abstracts a variety of filesystem backends to simplify starting a job/session.
** Question: Is this necessary? Would it be better to assume the user separately uploads whatever they need to the local filesystem/HDFS/Azure/S3/etc?
** Question: Do we need to support multiple filesystems, or would it be fine to just have livy configured to use one?
** Question: For simplicity, this just has a flat namespace, but would it be better to expose a hierarchal namespace?

* Question: Is there a better name for interactive notebook sessions than {{sessions}}?
* Question: Should we try to express some mechanism for loading and saving sessions to and from AzureML Studio/Github? I'm leaning more to this being the resposibility of the user of Livy.

* On security:
** Our intention is that we don't directly expose user accounts, but instead follow WebHDFS and on an insecure cluster, just pass the username in as an argument, and for a secure cluster use Kerberos. If multiple users want to share sessions, each session/job can have a {{group}}/{{acl}} fields that can then be queried by the underlying authentication system to approve or deny access.

On the structure of the actual json responses, I'd like switch over to using [jsonapi|http://jsonapi.org/], which would make it easier for users to consume the Livy api.;;;","09/Oct/15 16:51;linchan;This is a great proposal. For the versioning, Azure REST APIs use a special field in header (x-ms-version). I believe that is more like your option 3 above. Personally I might like that better but is willing to adapt to whatever options you decides on.

On fs choice, I think majority will be fine with one FS configured. However, the flexibility of mix and match different types could be very useful in some limited cases. Don't think we need to implement it in first version but would be good if the design will allow that extensibility later.
Is this JIRA for just the API discussion or you want to use it for FS support design discussion as well? 

Just my 2 cents. 

Thanks

;;;","15/Oct/15 17:35;erickt;[~linchan]: That's a good point. I filed HUE-3020 to track the FS design discussion.;;;","06/Jan/16 00:10;tc0312;Here's the link to the [document|https://docs.google.com/document/d/1RAtFGhRTPmQ8m2yL8fxUJTctRCeEndnz7-rpvtKhhA4/edit?usp=sharing];;;","06/Jan/16 20:04;erickt;@Alex: Overall that doc looks great. Exactly the direction I thought we should be going. My only real suggestion would be to use [semantic versions|http://semver.org/], as in major.minor.patch for the versions instead of a single integer. That'd let us add new non-breaking endpoints to Livy without needing to bump the major version, and still allow clients to say they need at least version 3.2 or above. I've found some parsers, [semverfi|https://github.com/softprops/semverfi] and [gfc-semver|https://github.com/gilt/gfc-semver], that would save you the effort of having to write support for yourself. Not sure about their quality though, but the semverfi does have a bit more commits, so that might be the best choice.;;;","07/Jan/16 01:00;vanzin;Personally, I'm favorable to approach 1, mainly because it doesn't require messing with headers, and because I'm more accustomed to using JAX-RS APIs (instead of Scalatra). Approach 1 maps pretty nicely to JAX-RS and allows you to easily create evolving APIs while maintaining backwards compatibility (while being aided by the compiler at that).

We used that approach extensively in CM: https://github.com/cloudera/cm_api/tree/master/java/src/main/java/com/cloudera/api

If you follow the hierarchy, you'll see that new versions of each ""resource"" extend the old version, so that you can keep all existing functionality and override just what you need.

I also would prefer keeping version numbers simple. Just have a strict policy about what, if anything, can change without bumping the API version. For example, CM doesn't allow anything (not even fields) to change without bumping the API version. Spark's REST API, which uses the same versioning approach, allows new fields and operations to be added, but existing fields and operation signatures must be maintained.

Numbers are cheap and adding complicated multi-level API version numbers doesn't really add much benefit, and I think the added confusion (livy version vs. livy API version) is not worth it.

That being said, I'm not against using the proposed approach in Livy; maybe it matches better to how Scalatra works.;;;","07/Jan/16 17:41;vanzin;As an addendum: if I understand the document correctly, another advantage (in my view) of approach 1 is that if someone writes a client against API ""v2"" but tries to run it against a server that only has ""v1"", all calls will fail. With approach 2, some calls will succeed and some calls will fail.;;;","08/Jan/16 22:10;erickt;@Marcelo: I don't have a strong opinion on URL vs Header. The {{Accept}} header is the most theoretically RESTful approach, but there's really no consensus across APIs ([Facebook|https://developers.facebook.com/docs/apps/versions] and [Google|https://developers.google.com/maps/documentation/javascript/versions] also embeds there version number in the url). I expect most users would interact with Livy through libraries, so it won't really even matter. Either way, we should still allow users to specify at least {{Accept: application/json}}, and reject unknown {{Accept}} headers in order to give us the flexibility to return different mime types later on.

On your scenario in your addendum, the versioning scheme is independent of how we handle a v2 client talking to a v1 server. If we assume that real production users will always use the versions, we can just have a filter that checks for the version mismatch and return a hard error, probably a {{406 Not Acceptable}}. Version-less routes are really just for testing, and could also be convenient for the URL-versioning scheme.

The real question, in my opinion, is on how to deal with version mismatches between the client and the server. There are advantages and disadvantages to CMï¿½ï¿½ï¿½s {{increment-version-on-all-changes}}, Sparkï¿½ï¿½ï¿½s {{increment-version-on-breaking-change}}, and Semver semantic versioning schemes.

For Sparkï¿½ï¿½ï¿½s scheme, it sounds like adding a new route might not cause the version to be incremented. This means that it can be hard for a client to know if it gets a 404 from accessing a route whether or not the server does not support this route, or if the resource just doesnï¿½ï¿½ï¿½t exist. Having route tied to a version like CM and Semver makes this case clear, and allows the server to return a sensible error in this case.

On the flipside, CMï¿½ï¿½ï¿½s scheme does create a proliferation of versions. This might cause the ecosystem to have a large number of versions in active use, which may make it difficult to refactor the server to drop support for older versions. Spark and Semverï¿½ï¿½ï¿½s scheme of not changing the (major) version unless thereï¿½ï¿½ï¿½s a breaking change would have the tendency of having the ecosystem more on the same version, so itï¿½ï¿½ï¿½d be easier to drop older versions.

Finally, and this is more of a personal feeling, I feel semantic versioning can help to keep a more long term stable interface, since it allows the API to grow backwards compatibly, and it makes it clear when the API went through an incompatible change. Itï¿½ï¿½ï¿½s the best of both schemes ï¿½ï¿½ï¿½;;;","11/Jan/16 19:31;tc0312;@Marcelo: With approach 2, calling v2 API on a v1 server will still fail because the server validates that the specified version must be in range.

{panel:title=Version validation}
To guard against invalid API version (newer than the latest version or older than the minimum  supported version), a global before() filter will be added for validation.
{panel};;;","11/Jan/16 22:35;tc0312;Let me summary our comments and adding some thought:
1. When to bump the version
We have 2 choices here: {{increment-version-on-all-changes}} & {{increment-version-on-breaking-change}}.
# {{increment-version-on-all-changes}} leads to a larger set of active versions. Code and documentation are more complicated. On the flip side, every time we add a new API, minor version will be bumped. Client has less version checking to do.
# {{increment-version-on-breaking-change}} leads to a smaller set of active versions. Code and documentation are simpler. It also translates to faster development velocity (for us). However, it also makes using new API more problematic as some API/fields might be missing on a particular API version.

I think {{increment-version-on-all-changes}} is better for backwards-compatibility; {{increment-version-on-breaking-change}} for simpler server code base & higher development velocity.

2. The numbering scheme for the version
For numbering scheme, there are 3 choices: an integer {{major}}, semantic versioning without patch version {{major.minor}} & semantic versioning {{major.minor.patch}}.
Patch version doesn't seem to help as we cannot make backwards-compatible bug fixes on an interface. I think using {{major.minor}} to version an API is enough.

# For {{increment-version-on-breaking-change}}, {{major.minor}} doesn't really make sense as we are bumping the version only when a breaking change is made.
# For {{increment-version-on-all-changes}}, I like {{major.minor}} more than {{major}} as we increase the fractional part of the version to indicate that we are making a small change. It tells our users when they can safely upgrade to the latest API.

TL;DR: I think we should use {{major}} if we choose {{increment-version-on-breaking-change}}, {{major.minor}} if we choose {{increment-version-on-all-changes}}.
;;;","13/Jan/16 22:40;tc0312;I'm going to start implementing this item.
Are there any concerns on the approach of {{increment-version-on-all-changes}} & semantic versioning without patch version {{major.minor}}?;;;","13/Jan/16 22:46;vanzin;I'm ok with it, although my gut feeling still tells me that ""minor versions"" for API changes are kinda odd and should be avoided.;;;","13/Jan/16 22:48;erickt;Me too. Thanks Alex!;;;","15/Jan/16 19:07;hshreedharan;@Alex - Is the design doc accurate and inline with your planned implementation? If not, could you please update that too? Thanks!;;;","16/Jan/16 06:19;tc0312;I've updated the document.;;;","25/Jan/16 19:38;hshreedharan;The design looks great. Have you started working on this, Alex? ;;;","28/Jan/16 01:07;tc0312;Work is completed. I will send the PR soon.;;;","29/Jan/16 01:32;tc0312;https://github.com/cloudera/livy/pull/44;;;","04/Feb/16 21:10;vanzin;Commit [6948b1eb|https://github.com/cloudera/livy/commit/6948b1eb333fad77f55be1bf01b801dc071b2985] by  Alex Man <tc.technetium@...> in cloudera/livy:
{code}
LIVY-15. Added support for API versioning.

- Added a new trait ApiVersioningSupport to provide versioning support for Scalatra Servlet.
- Added an enum to define Livy's API versions.

Closes #44
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Make sure to tear down sessions if they run out of memory,LIVY-14,13095652,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,tc0312,erickt,gmcdonald,08/Oct/15 01:43,07/Dec/16 05:07,19/Dec/25 04:16,07/Dec/16 05:07,0.1,,,0.3,,Core,,,,,,,,,,1,,,,,,"Right now livy in yarn mode doesn't handle the yarn app running out of memory. It should poll every now and then to see if the application is actually still running, and error out the task if not.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jun 03 00:10:35 UTC 2016,,,,,,,,,,"0|i3iyjb:",9223372036854775807,,,,,,,,,,,,,,,,,,,"03/Jun/16 00:10;linchan;This is already done with HDI's implementation of Livy recovery (local HA). Alex is trying to upstream those changes now. Should be able to resolve this once that is done.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Send back plots from the interpreter,LIVY-13,13095651,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,purechoc,romainr,gmcdonald,07/Oct/15 18:10,10/Mar/16 01:21,19/Dec/25 04:16,10/Mar/16 01:21,0.1,,,0.2,,Interpreter,,,,,,,,,,2,,,,,,Similar to R plotting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Mar 10 01:21:21 UTC 2016,,,,,,,,,,"0|i3iyj3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"08/Oct/15 02:09;erickt;For python, we can use matplotlib, which I believe is what iPython uses. Not sure what to do for scala-spark.;;;","25/Jan/16 00:55;purechoc;need to make magic for pyspark

like this...
[notebook]
import matplotlib.pyplot as plt
plt.plot(...)
%img plt.show()

[interpreter]
raw_data = plt.show()
img_date = base64encoding(raw_data)
return {""type"" : ""image/png"", data=img_data}
;;;","15/Feb/16 00:29;purechoc;Is there any progress regarding this issue?

;;;","16/Feb/16 09:42;purechoc;fast customized fake_shell.py and result. 
(gray part need to be more impovement...)

!screenshot-1.png!

;;;","22/Feb/16 06:51;purechoc;https://github.com/cloudera/livy/pull/73;;;","10/Mar/16 01:21;vanzin;Commit [e9d85957|https://github.com/cloudera/livy/commit/e9d8595704577742adb8ae2f74a469793362a1f2] by  hyunwoo cho <purechoc.en@...> in cloudera/livy:
{code}
LIVY-13. Send back plots from the interpreter

send back matplotlib plots imgage from the python interpretor
 - use likes %matplot plt
 - working with Hue master branch
 - plan to support another plot library..

Closes #73
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R + Yarn support,LIVY-12,13095650,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Minor,Fixed,purechoc,romainr,gmcdonald,28/Sep/15 17:31,07/Apr/16 18:58,19/Dec/25 04:16,07/Apr/16 18:58,0.1,,,0.2,,Interpreter,,,,,,,,,,2,,,,,,"Currently we only support local mode.
Might need a real R backend",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 07 18:58:19 UTC 2016,,,,,,,,,,"0|i3iyiv:",9223372036854775807,,,,,,,,,,,,,,,,,,,"06/Jan/16 06:41;purechoc;Hi. i have a question! 
means R + Yarn or SparkR + Yarn ?;;;","14/Jan/16 22:01;agu;Does this mean I cannot create a SparkR interactive session on Yarn?
@Lin Chan;;;","04/Mar/16 07:40;purechoc;there is any update?
;;;","29/Mar/16 08:22;purechoc;I tested something like this..  it's working.. with yarn cluster mode

{code}
def apply(): SparkRInterpreter = {    
    val mirror = universe.runtimeMirror(Utils.getContextOrSparkClassLoader)
    val sparkRBackendClass = mirror.classLoader.loadClass(""org.apache.spark.api.r.RBackend"")    
    val ctor = sparkRBackendClass.getDeclaredConstructor().newInstance()
  
    var sparkRBackendPort = 0
    val initialized = new Semaphore(0)
    val sparkRBackendThread = new Thread(""SparkR backend"") {
      override def run() {        
        sparkRBackendPort = sparkRBackendClass.getMethod(""init"").invoke(ctor).asInstanceOf[Int]        
        initialized.release()
        sparkRBackendClass.getMethod(""run"").invoke(ctor)
      }
    }
...
    env.put(""EXISTING_SPARKR_BACKEND_PORT"", sparkRBackendPort.toString)
    env.put(""SPARKR_PACKAGE_DIR"", rPackageDir)
...
    sendRequest(""library(SparkR)"")
{code};;;","07/Apr/16 18:58;vanzin;Commit [8b635202|https://github.com/cloudera/livy/commit/8b6352023b9bef2d158763cd4da49bbef336a0e0] by  purechoc <purechoc.en@...> in cloudera/livy:
{code}
LIVY-12. R + Yarn support

using SparkR in yarn cluster mode

**Changes**
- remove fake_R and SPARKR_DRIVER_R

**How to work**
- ContextLauncher launch SparkRInterpreter  with sparkRArchive file ($SPARK_HOME/R/lib/sparkr.zip)
- SparkRInterpreter Launch a SparkR backend server for the R process
- exec R (all cluster node must installed R and R in $PATH)
- load SparkR library

Closes #96
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add permissions to session visibility,LIVY-10,13095648,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Duplicate,,romainr,gmcdonald,28/Sep/15 17:29,18/Aug/17 18:10,19/Dec/25 04:16,23/Mar/16 21:16,0.1,,,,,Core,,,,,,,,,,2,,,,,,"authorization: if I poll your /sessions/x or /statement/x, I should get a 403 (we don't need to support a login for now (oyalla does), just have the owners saved somewhere. Same as webhdfs)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed Mar 23 21:16:40 UTC 2016,,,,,,,,,,"0|i3iyif:",9223372036854775807,,,,,,,,,,,,,,,,,,,"23/Mar/16 21:16;vanzin;A lot (if not all) of the work for this was done as part of LIVY-68.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[spark] Make Livy GA,LIVY-1,13095639,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Duplicate,,romainr,gmcdonald,28/Sep/15 17:22,20/Dec/15 09:55,19/Dec/25 04:16,20/Dec/15 09:55,,,,,,,,,,,,,,,,1,,,,,,"Python and R YARN shells

# Plotting
# Jupyther backends
# DB 

Follow-up of HUE-2588",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Sun Dec 20 09:55:52 UTC 2015,,,,,,,,,,"0|i3iygf:",9223372036854775807,,,,,,,,,,,,,,,,,,,"20/Dec/15 09:55;kostas;This umbrella JIRA is being closed because all remaining tasks have been converted to top level tasks under the new LIVY project. This JIRA was left over from when livy was under HUE.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[spark] Validate that modules can be imported,LIVY-33,13095671,,Task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Fixed,tc0312,romainr,gmcdonald,27/Aug/15 22:11,25/Oct/16 21:21,19/Dec/25 04:16,25/Oct/16 21:21,0.1,,,0.2,,Core,,,,,,,,,,0,,,,,,"From
https://github.com/seahboonsiew/pyspark-csv
https://www.dropbox.com/s/n81ny8druszbqmn/201408_weather_data%20%282%29.csv?dl=0

this works in the Shell (need to pip install https://pypi.python.org/pypi/python-dateutil):
{code}
pyspark --py-files /home/romain/downloads/pyspark_csv.py

import pyspark_csv as pycsv
from pyspark.sql import SQLContext

sqlContext = SQLContext(sc)

text_rdd = sc.textFile(""/user/romain/bikes/201408_weather_data.csv"")
df = pycsv.csvToDataFrame(sqlContext, text_rdd)

print df
df.show()
{code}

We should test this and see with the UI guys for a better UX in the notebook for adding files!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Fri Nov 06 18:00:42 UTC 2015,,,,,,,,,,"0|i3iynj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"08/Sep/15 21:15;jennykim;Should the files be uploaded to the user's home directory on HDFS by default?;;;","09/Oct/15 00:37;romainr;HDFS mode: file present on HDFS and currently need to be entered in the session with the py-files property hdfs://localhost:8020/user/romain/pyspark_csv.py;;;","26/Oct/15 15:22;romainr;{code}
curl -X POST --data '{""file"": ""/user/romain/demo_import.py"", ""pyFiles"": ""/user/romain/pyspark_csv.py""}' -H ""Content-Type: application/json"" localhost:8998/batches
{code}

Seems blocked by HUE-3038;;;","06/Nov/15 18:00;erickt;If this file is bundled into a .zip file, it should have been fixed in HUE-3025. Uploading .py files still needs to be implemented though.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Spark dynamic allocations to use less resources when idle,LIVY-9,13095647,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,erickt,gmcdonald,20/Aug/15 18:41,14/Apr/16 14:03,19/Dec/25 04:16,14/Apr/16 14:03,0.1,,,,,Core,,,,,,,,,,2,,,,,,Spark now supports [dynamically modifying the number of workers](https://spark.apache.org/docs/1.2.0/job-scheduling.html#dynamic-resource-allocation). Livy interpreters could use this to use less spark resources when they've gone temporarily idle. This would fill in nicely between the time a session has gone idle and before the session timeout kills the session.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Thu Apr 14 14:03:56 UTC 2016,,,,,,,,,,"0|i3iyi7:",9223372036854775807,,,,,,,,,,,,,,,,,,,"14/Apr/16 14:03;vanzin;Livy uses whatever Spark configuration the admin and user provide. Admin can set up dynamic allocation and blacklist the config entries to disable it, for example, if he wants everybody to use dynamic allocation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add autocompletion API,LIVY-7,13095645,,New Feature,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,ppellmont,erickt,gmcdonald,20/Aug/15 18:21,10/Oct/17 07:05,19/Dec/25 04:16,10/Oct/17 00:40,0.1,,,0.5.0,,Interpreter,,,,,,,,,,0,,,,,,Add an ipython-esque autocomplete api to livy.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LIVY-403,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Tue Oct 10 06:58:15 UTC 2017,,,,,,,,,,"0|i3iyhr:",9223372036854775807,,,,,,,,,,,,,,,,,,,"29/Feb/16 21:45;romainr;https://groups.google.com/a/cloudera.org/forum/#!topic/hue-user/sUSS7V9_g2M;;;","24/Sep/17 14:57;githubbot;GitHub user pellmont opened a pull request:

    https://github.com/apache/incubator-livy/pull/51

    LIVY-7 added autocompletion api and implementation for scala 2.11

    I started an implementation of the very old feature request (LIVY-7) for code autocompletion.
    This implementation works with scala 2.11. The scala 2.10 version is not yet implemented (would not fail, but just doesn't return any suggestions).
    
    I'd be happy if somebody could review and comment it.
    
    As for the API: I chose a synchronous call because resolving the code options shouldn't be a very long process (and if it were it wouldn't make sense anyway).

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/pellmont/incubator-livy master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/51.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #51
    
----
commit 2c91f983c3017700edd0e5c4ee5f859dbd91d0c4
Author: Pascal Pellmont <github@ppo2.ch>
Date:   2017-09-24T10:26:59Z

    LIVY-7 added autocompletion api and implementation for scala 2.11

----
;;;","24/Sep/17 15:34;githubbot;Github user codecov-io commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    # [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=h1) Report
    > Merging [#51](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=desc) into [master](https://codecov.io/gh/apache/incubator-livy/commit/1bd92b9f4684fee2bacc0545b56316d5f0553f9a?src=pr&el=desc) will **increase** coverage by `0.01%`.
    > The diff coverage is `47.22%`.
    
    [![Impacted file tree graph](https://codecov.io/gh/apache/incubator-livy/pull/51/graphs/tree.svg?width=650&height=150&src=pr&token=0MkVbiUFwE)](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree)
    
    ```diff
    @@             Coverage Diff              @@
    ##             master      #51      +/-   ##
    ============================================
    + Coverage     70.68%   70.69%   +0.01%     
    - Complexity      790      796       +6     
    ============================================
      Files            97       97              
      Lines          5389     5422      +33     
      Branches        800      804       +4     
    ============================================
    + Hits           3809     3833      +24     
    - Misses         1044     1050       +6     
    - Partials        536      539       +3
    ```
    
    
    | [Impacted Files](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree) | Coverage Î” | Complexity Î” | |
    |---|---|---|---|
    | [...rg/apache/livy/repl/AbstractSparkInterpreter.scala](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9BYnN0cmFjdFNwYXJrSW50ZXJwcmV0ZXIuc2NhbGE=) | `57.93% <0%> (-0.82%)` | `1 <0> (Ã¸)` | |
    | [.../src/main/scala/org/apache/livy/repl/Session.scala](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9TZXNzaW9uLnNjYWxh) | `69.89% <0%> (-3.15%)` | `1 <0> (Ã¸)` | |
    | [...c/main/scala/org/apache/livy/repl/ReplDriver.scala](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9SZXBsRHJpdmVyLnNjYWxh) | `28.2% <0%> (-0.75%)` | `0 <0> (Ã¸)` | |
    | [.../main/scala/org/apache/livy/repl/Interpreter.scala](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree#diff-cmVwbC9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvcmVwbC9JbnRlcnByZXRlci5zY2FsYQ==) | `33.33% <0%> (+8.33%)` | `0 <0> (Ã¸)` | :arrow_down: |
    | [core/src/main/scala/org/apache/livy/msgs.scala](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL2xpdnkvbXNncy5zY2FsYQ==) | `8.33% <0%> (-0.76%)` | `0 <0> (Ã¸)` | |
    | [...c/src/main/java/org/apache/livy/rsc/RSCClient.java](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9SU0NDbGllbnQuamF2YQ==) | `82.71% <100%> (+0.1%)` | `26 <1> (+1)` | :arrow_up: |
    | [...e/livy/server/interactive/InteractiveSession.scala](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uLnNjYWxh) | `61.7% <100%> (+0.61%)` | `43 <1> (+1)` | :arrow_up: |
    | [.../scala/org/apache/livy/repl/SparkInterpreter.scala](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree#diff-cmVwbC9zY2FsYS0yLjExL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9yZXBsL1NwYXJrSW50ZXJwcmV0ZXIuc2NhbGE=) | `82.35% <60%> (-1.81%)` | `15 <0> (+3)` | |
    | [...rc/main/java/org/apache/livy/rsc/BaseProtocol.java](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree#diff-cnNjL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9saXZ5L3JzYy9CYXNlUHJvdG9jb2wuamF2YQ==) | `74.73% <71.42%> (-0.27%)` | `1 <0> (Ã¸)` | |
    | [...server/interactive/InteractiveSessionServlet.scala](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree#diff-c2VydmVyL3NyYy9tYWluL3NjYWxhL29yZy9hcGFjaGUvbGl2eS9zZXJ2ZXIvaW50ZXJhY3RpdmUvSW50ZXJhY3RpdmVTZXNzaW9uU2VydmxldC5zY2FsYQ==) | `63.7% <75%> (+0.37%)` | `5 <0> (Ã¸)` | :arrow_down: |
    | ... and [8 more](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=tree-more) | |
    
    ------
    
    [Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=continue).
    > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
    > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`
    > Powered by [Codecov](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=footer). Last update [1bd92b9...2c91f98](https://codecov.io/gh/apache/incubator-livy/pull/51?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).

;;;","25/Sep/17 01:00;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    Hi @pellmont , thanks a lot for your contribution, is there any blocking issue for you to support auto-completion in scala 2.10 interpreter?
;;;","25/Sep/17 04:34;githubbot;Github user pellmont commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    Hi @jerryshao, no as far as I know there is no blocking issue for scala 2.10. More kind of time constraint, so I wanted to know whether I am on the right track before spending more time ;-)
    For PySpark and R I don't know how to get the autocompletion-information
;;;","25/Sep/17 05:29;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    Yes, we're also discussing to add this feature recently, so it is glad to see the community effort on this feature. I haven't yet looked into details, will spend time on it. 
    
    For the feature completeness, I would suggest to add 2.10 support in this PR also.
;;;","25/Sep/17 05:34;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    CC @zjffdu , please also take a look at this PR from Zeppelin's point.
;;;","25/Sep/17 06:12;githubbot;Github user pellmont commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    I'll work on scala 2.10 of course :-)
    
    as for Zeppelin: Usage of code completion in livy-interpreter is on my TODO-list (... this was the original intention... )
;;;","28/Sep/17 03:54;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r141521606
  
    --- Diff: core/src/main/scala/org/apache/livy/msgs.scala ---
    @@ -60,4 +60,8 @@ case class ExecuteReplyError(execution_count: Int,
     
     case class ExecuteResponse(id: Int, input: Seq[String], output: Seq[String])
     
    +case class CompletionRequest(code: String, kind: Option[String], cursor: Int) extends Content
    --- End diff --
    
    I would suggest to make `kind` as a non-option parameter. Previously because we want to be compatible with old protocol, so we use `Option[kind]` in `ExecuteRequest`. But since this is newly added API, so I think it would be better to make this a non-option parameter.
;;;","28/Sep/17 06:22;githubbot;Github user pellmont commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    If backward compatibility is the reason for the option, I absolutely agree. Will change it. API change for this will never be easier than now.... 
;;;","28/Sep/17 06:27;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    Thank @pellmont , please also add 2.10 support in this PR.
;;;","29/Sep/17 01:52;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r141774465
  
    --- Diff: integration-test/src/main/scala/org/apache/livy/test/framework/LivyRestClient.scala ---
    @@ -188,8 +190,39 @@ class LivyRestClient(val httpClient: AsyncHttpClient, val livyEndpoint: String)
           }
         }
     
    +    class Completion(code: String, cursor: Int) {
    +      val completions = {
    +        val requestBody = Map(""code"" -> code, ""cursor"" -> cursor)
    +        val r = httpClient.preparePost(s""$url/completion"")
    +          .setBody(mapper.writeValueAsString(requestBody))
    +          .execute()
    +          .get()
    +        assertStatusCode(r, HttpServletResponse.SC_OK)
    +
    +        val res = mapper.readValue(r.getResponseBodyAsStream, classOf[CompletionResult])
    +        res.candidates
    +      }
    +
    +      final def result(): Seq[String] = completions
    +
    +      def verifyContaining(expected: List[String]): Unit = {
    +        result().toSet.exists(x => x == expected)
    --- End diff --
    
    Is the comparison here correct? The type of `x` is String, whereas `expected` is `List[String]`, are they comparable?
;;;","29/Sep/17 01:52;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r141774749
  
    --- Diff: integration-test/src/main/scala/org/apache/livy/test/framework/LivyRestClient.scala ---
    @@ -188,8 +190,39 @@ class LivyRestClient(val httpClient: AsyncHttpClient, val livyEndpoint: String)
           }
         }
     
    +    class Completion(code: String, cursor: Int) {
    +      val completions = {
    +        val requestBody = Map(""code"" -> code, ""cursor"" -> cursor)
    +        val r = httpClient.preparePost(s""$url/completion"")
    +          .setBody(mapper.writeValueAsString(requestBody))
    +          .execute()
    +          .get()
    +        assertStatusCode(r, HttpServletResponse.SC_OK)
    +
    +        val res = mapper.readValue(r.getResponseBodyAsStream, classOf[CompletionResult])
    +        res.candidates
    +      }
    +
    +      final def result(): Seq[String] = completions
    +
    +      def verifyContaining(expected: List[String]): Unit = {
    +        result().toSet.exists(x => x == expected)
    +      }
    +
    +      def verifyNone(): Unit = {
    +        assert(result() == List(), s""Expected no completion proposals but found $completions"")
    +      }
    +
    +      private def matchStrings(actual: String, expected: String): Unit = {
    --- End diff --
    
    I don't see the usage of this method `matchStrings`.
;;;","29/Sep/17 01:52;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r141774874
  
    --- Diff: repl/scala-2.10/src/main/scala/org/apache/livy/repl/SparkInterpreter.scala ---
    @@ -133,6 +133,11 @@ class SparkInterpreter(protected override val conf: SparkConf) extends AbstractS
         sparkIMain.interpret(code)
       }
     
    +  override protected def completeCandidates(code: String, cursor: Int) : Array[String] = {
    +    val completer = new org.apache.spark.repl.SparkJLineCompletion(sparkIMain)
    --- End diff --
    
    Please using import to add package names in the header of the file, avoid directly using package name here.
;;;","29/Sep/17 01:52;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r141772926
  
    --- Diff: docs/rest-api.md ---
    @@ -310,6 +310,37 @@ Cancel the specified statement in this session.
       </tr>
     </table>
     
    +### POST /sessions/{sessionId}/completion
    +
    +Runs a statement in a session.
    +
    +#### Request Body
    +
    +<table class=""table"">
    +  <tr><th>Name</th><th>Description</th><th>Type</th></tr>
    +  <tr>
    +    <td>code</td>
    +    <td>The code for which completion proposals are requested</td>
    +    <td>string</td>
    +  </tr>
    +  <tr>
    --- End diff --
    
    @pellmont I think you should also change the docs to reflect your modification of `kind`.
;;;","29/Sep/17 01:52;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r141775343
  
    --- Diff: repl/src/main/scala/org/apache/livy/repl/Session.scala ---
    @@ -169,6 +169,18 @@ class Session(
         statementId
       }
     
    +  def complete(code: String, codeType: String = null, cursor: Int): Array[String] = {
    +    val tpe = if (codeType != null) {
    +      Kind(codeType)
    +    } else if (defaultInterpKind != Shared()) {
    +      defaultInterpKind
    +    } else {
    +      throw new IllegalArgumentException(s""Code type should be specified if session kind is shared"")
    +    }
    --- End diff --
    
    Since code type is always specified and not null, we don't need to add the above code to check kind.
;;;","29/Sep/17 01:52;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r141774159
  
    --- Diff: integration-test/src/main/scala/org/apache/livy/test/framework/LivyRestClient.scala ---
    @@ -188,8 +190,39 @@ class LivyRestClient(val httpClient: AsyncHttpClient, val livyEndpoint: String)
           }
         }
     
    +    class Completion(code: String, cursor: Int) {
    +      val completions = {
    +        val requestBody = Map(""code"" -> code, ""cursor"" -> cursor)
    --- End diff --
    
    Should you specify ""kind"" here in request body?
;;;","29/Sep/17 01:52;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r141774919
  
    --- Diff: repl/scala-2.11/src/main/scala/org/apache/livy/repl/SparkInterpreter.scala ---
    @@ -117,6 +117,11 @@ class SparkInterpreter(protected override val conf: SparkConf) extends AbstractS
         sparkILoop.interpret(code)
       }
     
    +  override protected def completeCandidates(code: String, cursor: Int) : Array[String] = {
    +    val completer = new scala.tools.nsc.interpreter.JLineCompletion(sparkILoop.intp)
    --- End diff --
    
    Same here.
;;;","29/Sep/17 08:10;githubbot;Github user pellmont commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r141811488
  
    --- Diff: integration-test/src/main/scala/org/apache/livy/test/framework/LivyRestClient.scala ---
    @@ -188,8 +190,39 @@ class LivyRestClient(val httpClient: AsyncHttpClient, val livyEndpoint: String)
           }
         }
     
    +    class Completion(code: String, cursor: Int) {
    +      val completions = {
    +        val requestBody = Map(""code"" -> code, ""cursor"" -> cursor)
    --- End diff --
    
    yes I see, the ""kind"" change had not yet arrived everywhere
;;;","29/Sep/17 08:11;githubbot;Github user pellmont commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r141811669
  
    --- Diff: integration-test/src/main/scala/org/apache/livy/test/framework/LivyRestClient.scala ---
    @@ -188,8 +190,39 @@ class LivyRestClient(val httpClient: AsyncHttpClient, val livyEndpoint: String)
           }
         }
     
    +    class Completion(code: String, cursor: Int) {
    +      val completions = {
    +        val requestBody = Map(""code"" -> code, ""cursor"" -> cursor)
    +        val r = httpClient.preparePost(s""$url/completion"")
    +          .setBody(mapper.writeValueAsString(requestBody))
    +          .execute()
    +          .get()
    +        assertStatusCode(r, HttpServletResponse.SC_OK)
    +
    +        val res = mapper.readValue(r.getResponseBodyAsStream, classOf[CompletionResult])
    +        res.candidates
    +      }
    +
    +      final def result(): Seq[String] = completions
    +
    +      def verifyContaining(expected: List[String]): Unit = {
    +        result().toSet.exists(x => x == expected)
    --- End diff --
    
    did I write this? ;-)
    good you review it that carefully... will be fixed and working properly with the next push...
;;;","30/Sep/17 01:59;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r141994407
  
    --- Diff: repl/src/main/scala/org/apache/livy/repl/Interpreter.scala ---
    @@ -28,6 +29,7 @@ object Interpreter {
                               traceback: Seq[String] = Seq()) extends ExecuteResponse
       case class ExecuteIncomplete() extends ExecuteResponse
       case class ExecuteAborted(message: String) extends ExecuteResponse
    +
    --- End diff --
    
    This empty line seems not necessary.
;;;","30/Sep/17 01:59;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r141994675
  
    --- Diff: repl/src/main/scala/org/apache/livy/repl/Session.scala ---
    @@ -169,6 +169,12 @@ class Session(
         statementId
       }
     
    +  def complete(code: String, codeType: String, cursor: Int): Array[String] = {
    +    val tpe = Kind(codeType)
    +    val interp = interpreter(tpe)
    +    interp.complete(code, cursor)
    --- End diff --
    
    Would you please confirm if this complete call in Scala's IMain thread safe?
;;;","02/Oct/17 23:39;githubbot;Github user ajbozarth commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    I haven't found any issues on my first detailed pass through but I'd like to read through the code a couple more times to make sure.
;;;","09/Oct/17 02:35;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    Hi @pellmont can you please trigger the test again to make it pass, thanks!
;;;","09/Oct/17 02:41;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r143378500
  
    --- Diff: integration-test/src/main/scala/org/apache/livy/test/framework/LivyRestClient.scala ---
    @@ -188,8 +190,39 @@ class LivyRestClient(val httpClient: AsyncHttpClient, val livyEndpoint: String)
           }
         }
     
    +    class Completion(code: String, cursor: Int) {
    +      val completions = {
    +        val requestBody = Map(""code"" -> code, ""cursor"" -> cursor)
    +        val r = httpClient.preparePost(s""$url/completion"")
    +          .setBody(mapper.writeValueAsString(requestBody))
    +          .execute()
    +          .get()
    +        assertStatusCode(r, HttpServletResponse.SC_OK)
    +
    +        val res = mapper.readValue(r.getResponseBodyAsStream, classOf[CompletionResult])
    +        res.candidates
    +      }
    +
    +      final def result(): Seq[String] = completions
    +
    +      def verifyContaining(expected: List[String]): Unit = {
    +        result().toSet.exists(x => x == expected)
    +      }
    +
    +      def verifyNone(): Unit = {
    +        assert(result() == List(), s""Expected no completion proposals but found $completions"")
    +      }
    +
    +      private def matchStrings(actual: String, expected: String): Unit = {
    --- End diff --
    
    @pellmont did you address this comment?
;;;","09/Oct/17 02:42;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r143378539
  
    --- Diff: integration-test/src/main/scala/org/apache/livy/test/framework/LivyRestClient.scala ---
    @@ -188,8 +190,40 @@ class LivyRestClient(val httpClient: AsyncHttpClient, val livyEndpoint: String)
           }
         }
     
    +    class Completion(code: String, kind: String, cursor: Int) {
    +      val completions = {
    +        val requestBody = Map(""code"" -> code, ""cursor"" -> cursor, ""kind"" -> kind)
    +        val r = httpClient.preparePost(s""$url/completion"")
    +          .setBody(mapper.writeValueAsString(requestBody))
    +          .execute()
    +          .get()
    +        assertStatusCode(r, HttpServletResponse.SC_OK)
    +
    +        val res = mapper.readValue(r.getResponseBodyAsStream, classOf[CompletionResult])
    +        res.candidates
    +      }
    +
    +      final def result(): Seq[String] = completions
    +
    +      def verifyContaining(expected: List[String]): Unit = {
    +        assert(result().toSet.forall(x => expected.contains(x)))
    +      }
    +
    +      def verifyNone(): Unit = {
    +        assert(result() == List(), s""Expected no completion proposals but found $completions"")
    +      }
    +
    +      private def matchStrings(actual: String, expected: String): Unit = {
    +        val regex = Pattern.compile(expected, Pattern.DOTALL)
    +        assert(regex.matcher(actual).matches(), s""$actual did not match regex $expected"")
    +      }
    +    }
    +
         def run(code: String): Statement = { new Statement(code) }
     
    +    def complete(code: String, kind: String, cursor: Int): Completion =
    +      { new Completion(code, kind, cursor) }
    --- End diff --
    
    Please change the style to:
    
    ```scalal
    def complete(xxx): Completion = {
      xxx
    }
    ```
;;;","09/Oct/17 02:44;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r143378717
  
    --- Diff: repl/scala-2.11/src/main/scala/org/apache/livy/repl/SparkInterpreter.scala ---
    @@ -117,6 +120,20 @@ class SparkInterpreter(protected override val conf: SparkConf) extends AbstractS
         sparkILoop.interpret(code)
       }
     
    +  override protected def completeCandidates(code: String, cursor: Int) : Array[String] = {
    +    val completer : ScalaCompleter = {
    +      try {
    +        val cls = Class.forName(""scala.tools.nsc.interpreter.PresentationCompilerCompleter"")
    +        cls.getDeclaredConstructor(classOf[IMain]).newInstance(sparkILoop.intp)
    +          .asInstanceOf[ScalaCompleter]
    +      }
    +      catch {
    --- End diff --
    
    nit: move this line after `}`.
;;;","09/Oct/17 02:45;githubbot;Github user jerryshao commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r143378758
  
    --- Diff: repl/src/main/scala/org/apache/livy/repl/Session.scala ---
    @@ -169,6 +169,12 @@ class Session(
         statementId
       }
     
    +  def complete(code: String, codeType: String, cursor: Int): Array[String] = {
    +    val tpe = Kind(codeType)
    +    val interp = interpreter(tpe)
    +    interp.complete(code, cursor)
    --- End diff --
    
    Can you please confirm this?
;;;","09/Oct/17 06:37;githubbot;Github user pellmont commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r143391333
  
    --- Diff: integration-test/src/main/scala/org/apache/livy/test/framework/LivyRestClient.scala ---
    @@ -188,8 +190,39 @@ class LivyRestClient(val httpClient: AsyncHttpClient, val livyEndpoint: String)
           }
         }
     
    +    class Completion(code: String, cursor: Int) {
    +      val completions = {
    +        val requestBody = Map(""code"" -> code, ""cursor"" -> cursor)
    +        val r = httpClient.preparePost(s""$url/completion"")
    +          .setBody(mapper.writeValueAsString(requestBody))
    +          .execute()
    +          .get()
    +        assertStatusCode(r, HttpServletResponse.SC_OK)
    +
    +        val res = mapper.readValue(r.getResponseBodyAsStream, classOf[CompletionResult])
    +        res.candidates
    +      }
    +
    +      final def result(): Seq[String] = completions
    +
    +      def verifyContaining(expected: List[String]): Unit = {
    +        result().toSet.exists(x => x == expected)
    +      }
    +
    +      def verifyNone(): Unit = {
    +        assert(result() == List(), s""Expected no completion proposals but found $completions"")
    +      }
    +
    +      private def matchStrings(actual: String, expected: String): Unit = {
    --- End diff --
    
    sorry, this one I didn't see.
;;;","09/Oct/17 06:40;githubbot;Github user pellmont commented on a diff in the pull request:

    https://github.com/apache/incubator-livy/pull/51#discussion_r143391672
  
    --- Diff: repl/src/main/scala/org/apache/livy/repl/Session.scala ---
    @@ -169,6 +169,12 @@ class Session(
         statementId
       }
     
    +  def complete(code: String, codeType: String, cursor: Int): Array[String] = {
    +    val tpe = Kind(codeType)
    +    val interp = interpreter(tpe)
    +    interp.complete(code, cursor)
    --- End diff --
    
    as far as I could see this should be thread safe. Followed up to some methods which should get inlined.
;;;","09/Oct/17 08:46;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    LTGM. @pellmont , can you please rerun the test again to pass the travis, thanks!
;;;","09/Oct/17 08:50;githubbot;Github user pellmont commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    How can I restart the Build? At the PR build I don't see any button to restart a build. On my personal fork builds I can do that. Am I lacking permissions for that?
;;;","09/Oct/17 08:51;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    You can close and reopen the PR to trigger the test.
;;;","09/Oct/17 08:51;githubbot;Github user pellmont closed the pull request at:

    https://github.com/apache/incubator-livy/pull/51
;;;","09/Oct/17 08:52;githubbot;GitHub user pellmont reopened a pull request:

    https://github.com/apache/incubator-livy/pull/51

    LIVY-7 added autocompletion api and implementation for scala 2.11

    I started an implementation of the very old feature request (LIVY-7) for code autocompletion.
    This implementation works with scala 2.11. The scala 2.10 version is not yet implemented (would not fail, but just doesn't return any suggestions).
    
    I'd be happy if somebody could review and comment it.
    
    As for the API: I chose a synchronous call because resolving the code options shouldn't be a very long process (and if it were it wouldn't make sense anyway).

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/pellmont/incubator-livy master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-livy/pull/51.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #51
    
----
commit 2c91f983c3017700edd0e5c4ee5f859dbd91d0c4
Author: Pascal Pellmont <github@ppo2.ch>
Date:   2017-09-24T10:26:59Z

    LIVY-7 added autocompletion api and implementation for scala 2.11

commit ee8050274ee1c454016596ac14e82c604ac823c5
Author: Pascal Pellmont <github@ppo2.ch>
Date:   2017-09-28T17:15:35Z

    LIVY-7 code completion for scala 2.10

commit e44374f8539687e3a230e0f49c33c8912d1e7fca
Author: Pascal Pellmont <github@ppo2.ch>
Date:   2017-09-28T18:02:21Z

    LIVY-7 code completion kind no more optional

commit ec1516791aed17e0690dbc40e32b7bb00355a03f
Author: Pascal Pellmont <github@ppo2.ch>
Date:   2017-09-28T18:02:21Z

    LIVY-7 code completion kind no more optional

commit 1a9ad752a433c81b76e6791ed962c04d34451fed
Author: Pascal Pellmont <github@ppo2.ch>
Date:   2017-09-28T20:31:46Z

    Merge branch 'master' of git@github.com:pellmont/incubator-livy.git

commit 344c95d40e6f22af995659b50436d02a98b68c8a
Author: Pascal Pellmont <github@ppo2.ch>
Date:   2017-09-29T07:39:48Z

    LIVY-7 unit tests, kind non-optional

commit 657452e3bbb3c74a0581a805551247867b8810b7
Author: Pascal Pellmont <github@ppo2.ch>
Date:   2017-10-03T16:48:53Z

    LIVY-7 better results with PresentationCompilerCompleter (available
    since scala 2.11.8)

----
;;;","09/Oct/17 08:52;githubbot;Github user pellmont commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    Thanks for the hint!
;;;","09/Oct/17 09:34;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    Thanks for your work @pellmont , let me merge to master branch!
;;;","09/Oct/17 09:39;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    BTW @pellmont can you please change the title to `[LIVY-7] xxx`. Also PR description should be updated (Scala 2.10 version is also implemented), can you please update it also?
;;;","10/Oct/17 00:39;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-livy/pull/51
;;;","10/Oct/17 00:40;jerryshao;Issue resolved by pull request 51
[https://github.com/apache/incubator-livy/pull/51];;;","10/Oct/17 00:43;githubbot;Github user jerryshao commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    Hi @pellmont , do you have JIRA id, and what is your JIRA id? Can you please provide your Apache JIRA id so that I can put your name on Assignee, thanks!
;;;","10/Oct/17 06:58;githubbot;Github user pellmont commented on the issue:

    https://github.com/apache/incubator-livy/pull/51
  
    My JIRA user name is ""ppellmont""
;;;",,
Enable logging to a file,LIVY-6,13095644,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,,erickt,gmcdonald,17/Aug/15 15:54,09/May/16 18:31,19/Dec/25 04:16,09/May/16 18:31,0.1,,,,,Core,,,,,,,,,,1,,,,,,"Right now livy is logging to STDOUT to ease development, but we should switch to logging to a file in Hue's log directory. See the [logback documentation|http://logback.qos.ch/manual/configuration.html] on how to do this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 18:31:18 UTC 2016,,,,,,,,,,"0|i3iyhj:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/Oct/15 16:57;linchan;Would love to see that capability. However, please make the log directory path configurable as we might want to use Livy independent of Hue in some cases.

Thanks;;;","09/Oct/15 20:12;romainr;Yes, it will be only specific to livy (Hue wil just piggyback on it). 
We should keep the stdout option easy too.;;;","09/May/16 18:31;vanzin;Easily done by having your own log4j.properties in Livy's conf directory.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[spark] Server is getting slower and slower with the time,LIVY-5,13095643,,Sub-task,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Critical,Cannot Reproduce,erickt,romainr,gmcdonald,23/Jul/15 15:13,20/Dec/15 08:54,19/Dec/25 04:16,30/Oct/15 22:58,0.1,,,,,Core,,,,,,,,,,0,,,,,,"https://www.dropbox.com/s/t3jy7r7geqesd0p/HUE.Spark-load-issues.mp4?dl=0

 (like 3x in 30min)

It was livy, basically 30s instead of 10s, I was refreshing the page and tried a few times.
I rebooted livy and it went back to 10s and quick.

It was after 30min of intense running of snippets to do the v1 of the video. But not really sure where is the real problem. We can debug it more and more with bigger demo/examples, first time I keep it for so long.

There might be some calls coming from the UI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,2015-07-23 15:13:52.0,,,,,,,,,,"0|i3iyhb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[spark] Document how to use livy-defaults.conf,LIVY-4,13095642,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Fixed,erickt,erickt,gmcdonald,29/Jun/15 23:47,11/May/16 23:49,19/Dec/25 04:16,11/May/16 23:49,0.1,,,,,Docs,,,,,,,,,,0,,,,,,Need to add how to write a conf/livy-defaults.conf file.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Wed May 11 23:49:11 UTC 2016,,,,,,,,,,"0|i3iyh3:",9223372036854775807,,,,,,,,,,,,,,,,,,,"11/May/16 23:49;vanzin;I updated the docs recently and there's a placeholder conf file with instructions now. (Well, it was there before, but now you don't even need to rename it.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Livy should allow admins to customize the spark classpath in the config file,LIVY-38,13095676,,Improvement,Resolved,LIVY,Livy,software,vanzin,,http://livy.incubator.apache.org/,Major,Not A Bug,erickt,erickt,gmcdonald,21/May/15 03:14,09/May/16 18:24,19/Dec/25 04:16,09/May/16 18:24,0.1,,,,,Core,,,,,,,,,,0,,,,,,"Livy supports the ability to pass in spark configuration options on the commandline through the {{LIVY_SERVER_JAVA_OPTS=-Dspark.driver.extraClassPath=...}} environment variable, but it should be possible to also specify these options in the {{conf/livy-defaults.conf}} file for ease of configuration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,,Mon May 09 18:24:52 UTC 2016,,,,,,,,,,"0|i3iyon:",9223372036854775807,,,,,,,,,,,,,,,,,,,"09/May/16 18:24;vanzin;Livy uses the Spark configuration, so this functionality is already there. Just change the Spark config used by Livy.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
